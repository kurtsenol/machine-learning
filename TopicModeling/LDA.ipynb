{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LDA.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"127XkgZN_lIDecaLzVB5Zdaey_b5X8fxB","authorship_tag":"ABX9TyNSRMWDQ34koxzDNz1bFuZa"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eDYD5nWf-XLL","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLTlRNqH9sQX","colab_type":"code","colab":{}},"source":["DATA_PATH = 'drive/My Drive/nipstxt/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gx5XS7NW-A_P","colab_type":"code","outputId":"7161a836-7047-40dc-df8e-4526780fdc18","executionInfo":{"status":"ok","timestamp":1590836292948,"user_tz":-180,"elapsed":2833,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n","folders"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['nips00',\n"," 'nips01',\n"," 'nips02',\n"," 'nips03',\n"," 'nips04',\n"," 'nips05',\n"," 'nips06',\n"," 'nips07',\n"," 'nips08',\n"," 'nips09',\n"," 'nips10',\n"," 'nips11',\n"," 'nips12']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"I1rYfv9J-Lxo","colab_type":"code","colab":{}},"source":["# # Read all texts into a list.\n","# papers = []\n","\n","# for folder in folders:\n","#   file_names = os.listdir(DATA_PATH + folder)\n","#   for file_name in file_names:\n","#     with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:\n","#       data = f.read()\n","#     papers.append(data)\n","# len(papers)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KsL4XPcAzer2","colab_type":"code","colab":{}},"source":["import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovO9LUExnLVa","colab_type":"code","colab":{}},"source":["# with open(\"drive/My Drive/nipstxt/papers_pickle.txt\", \"wb\") as fp:   #Pickling\n","#   pickle.dump(papers, fp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mN-GOcVtzu3V","colab_type":"code","colab":{}},"source":["with open(\"drive/My Drive/nipstxt/papers_pickle.txt\", \"rb\") as fp:   # Unpickling\n","  papers = pickle.load(fp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KdUIysxUK_2Z","colab_type":"code","outputId":"6a33de5b-1467-4879-d988-88499c7964f4","executionInfo":{"status":"ok","timestamp":1590836294095,"user_tz":-180,"elapsed":3961,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(papers)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1740"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"9vuItb4m-qX-","colab_type":"code","outputId":"c0c0a0e1-0dfc-446a-be44-5b8d2bc2cd32","executionInfo":{"status":"ok","timestamp":1590836294097,"user_tz":-180,"elapsed":3954,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":336}},"source":["print(papers[0][:1000])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1 \n","CONNECTIVITY VERSUS ENTROPY \n","Yaser S. Abu-Mostafa \n","California Institute of Technology \n","Pasadena, CA 91125 \n","ABSTRACT \n","How does the connectivity of a neural network (number of synapses per \n","neuron) relate to the complexity of the problems it can handle (measured by \n","the entropy)? Switching theory would suggest no relation at all, since all Boolean \n","functions can be implemented using a circuit with very low connectivity (e.g., \n","using two-input NAND gates). However, for a network that learns a problem \n","from examples using a local learning rule, we prove that the entropy of the \n","problem becomes a lower bound for the connectivity of the network. \n","INTRODUCTION \n","The most distinguishing feature of neural networks is their ability to spon- \n","taneously learn the desired function from 'training' samples, i.e., their ability \n","to program themselves. Clearly, a given neural network cannot just learn any \n","function, there must be some restrictions on which networks can learn which \n","functions. One obv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HsKmLuZ8NJvd","colab_type":"code","outputId":"046e9a23-8bbb-4cc6-c995-c96fdac469e0","executionInfo":{"status":"ok","timestamp":1590836295996,"user_tz":-180,"elapsed":5845,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"WE4QUlAkNm-W","colab_type":"code","colab":{}},"source":["stop_words = nltk.corpus.stopwords.words('english')\n","wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n","wnl = nltk.stem.wordnet.WordNetLemmatizer()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3Um26paNn2M","colab_type":"code","colab":{}},"source":["def normalize_corpus(papers):\n","  norm_papers = []\n","  for paper in papers:\n","    paper = paper.lower()\n","    paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n","    paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n","    paper_tokens = [token for token in paper_tokens if len(token) > 1]\n","    paper_tokens = [token for token in paper_tokens if token not in stop_words]\n","    \n","    paper_tokens = list(filter(None, paper_tokens))\n","    if paper_tokens:\n","      norm_papers.append(paper_tokens)\n","  return norm_papers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FIVKpYRzOxBa","colab_type":"code","outputId":"f2538dca-e07b-4e1e-a47a-f18b964e9702","executionInfo":{"status":"ok","timestamp":1590836325596,"user_tz":-180,"elapsed":35432,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["norm_papers = normalize_corpus(papers)\n","print(len(norm_papers))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1740\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F5nqymNlOxEX","colab_type":"code","outputId":"7e19c092-80da-4984-ed50-64f6b54ea347","executionInfo":{"status":"ok","timestamp":1590836325597,"user_tz":-180,"elapsed":35424,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# viewing a processed paper\n","print(norm_papers[0][:50])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['connectivity', 'versus', 'entropy', 'yaser', 'abu', 'mostafa', 'california', 'institute', 'technology', 'pasadena', 'ca', 'abstract', 'doe', 'connectivity', 'neural', 'network', 'number', 'synapsis', 'per', 'neuron', 'relate', 'complexity', 'problem', 'handle', 'measured', 'entropy', 'switching', 'theory', 'would', 'suggest', 'relation', 'since', 'boolean', 'function', 'implemented', 'using', 'circuit', 'low', 'connectivity', 'using', 'two', 'input', 'nand', 'gate', 'however', 'network', 'learns', 'problem', 'example', 'using']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iOOpRS4A64X0","colab_type":"code","colab":{}},"source":["import gensim\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-SR-CMJ9ssN","colab_type":"code","colab":{}},"source":["bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter=b'_') # higher threshold fewer phrases.\n","bigram_model = gensim.models.phrases.Phraser(bigram)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HSBEatXN9t-K","colab_type":"code","outputId":"f4497427-b573-406d-be1b-0c1e7ac16e31","executionInfo":{"status":"ok","timestamp":1590836344150,"user_tz":-180,"elapsed":53964,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# sample demonstration\n","print(bigram_model[norm_papers[0]][:50])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['connectivity', 'versus', 'entropy', 'yaser', 'abu_mostafa', 'california_institute', 'technology_pasadena', 'ca_abstract', 'doe', 'connectivity', 'neural_network', 'number', 'synapsis', 'per', 'neuron', 'relate', 'complexity', 'problem', 'handle', 'measured', 'entropy', 'switching', 'theory', 'would', 'suggest', 'relation', 'since', 'boolean_function', 'implemented', 'using', 'circuit', 'low', 'connectivity', 'using', 'two', 'input', 'nand', 'gate', 'however', 'network', 'learns', 'problem', 'example', 'using', 'local', 'learning', 'rule', 'prove', 'entropy', 'problem']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QZyY8BLQ-iRT","colab_type":"text"},"source":["Let’s generate phrases for all our tokenized research papers and build a vocabulary that will help us obtain a unique term/phrase to number mapping (since machine or deep learning only works on numeric tensors)."]},{"cell_type":"code","metadata":{"id":"bFh46xDM90kE","colab_type":"code","colab":{}},"source":["norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BCx36Fhh-s7X","colab_type":"code","outputId":"79e5c99c-8d25-4fc4-a989-5e241513bb58","executionInfo":{"status":"ok","timestamp":1590836353481,"user_tz":-180,"elapsed":63284,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["# Create a dictionary representation of the documents.\n","dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n","\n","print('Sample word to number mappings:', list(dictionary.items())[:15])\n","print('Total Vocabulary Size:', len(dictionary))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sample word to number mappings: [(0, '0a'), (1, '2h'), (2, '2h2'), (3, '2he'), (4, '2n'), (5, '__c'), (6, '_c'), (7, '_k'), (8, 'a2'), (9, 'ability'), (10, 'abu_mostafa'), (11, 'access'), (12, 'accommodate'), (13, 'according'), (14, 'accumulated')]\n","Total Vocabulary Size: 78892\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SF7Rz39-_v-X","colab_type":"text"},"source":["Several of these terms are not very useful since they are specific to a paper or even a paragraph in a research paper. Hence, it is time to prune\n","our vocabulary and start removing terms. Leveraging document frequency is a great way to achieve this. By now, you probably realize that the document frequency of a term is basically the total number of times that term occurs across all the documents in a corpus."]},{"cell_type":"code","metadata":{"id":"tc6PNXqH_0Kk","colab_type":"code","outputId":"7d34087a-c865-4a06-9423-fe4137577937","executionInfo":{"status":"ok","timestamp":1590836353484,"user_tz":-180,"elapsed":63279,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n","\n","dictionary.filter_extremes(no_below=20, no_above=0.6)\n","\n","print('Total Vocabulary Size:', len(dictionary))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Total Vocabulary Size: 7756\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lWiJ9hBqAkra","colab_type":"code","outputId":"17d870b0-0660-4d2e-baaf-a893f3acb61d","executionInfo":{"status":"ok","timestamp":1590836353486,"user_tz":-180,"elapsed":63273,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print('Sample word to number mappings:', list(dictionary.items())[:15])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sample word to number mappings: [(0, '2n'), (1, '_c'), (2, 'a2'), (3, 'ability'), (4, 'abu_mostafa'), (5, 'access'), (6, 'accommodate'), (7, 'according'), (8, 'accumulated'), (9, 'acknowledgement_work'), (10, 'addison_wesley'), (11, 'afosr'), (12, 'aip'), (13, 'air_force'), (14, 'although')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-OcVpvc0APYn","colab_type":"text"},"source":["We removed all terms that occur fewer than 20 times across all documents and all\n","terms that occur in more than 60% of all the documents. We are interested in finding different themes and topics and not recurring themes. Hence, this suits our scenario perfectly. We can now perform feature engineering by leveraging a simple Bag of Words model."]},{"cell_type":"code","metadata":{"id":"U39C8CYI_0HJ","colab_type":"code","outputId":"f2a4ab4c-b759-4791-a3e4-ea4ca408da7a","executionInfo":{"status":"ok","timestamp":1590836354950,"user_tz":-180,"elapsed":64729,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Transforming corpus into bag of words vectors\n","\n","bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n","print(bow_corpus[1][:50])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[(3, 2), (4, 1), (11, 1), (13, 1), (15, 1), (21, 1), (27, 1), (30, 1), (33, 1), (35, 2), (36, 1), (45, 1), (46, 1), (47, 1), (48, 2), (57, 2), (64, 7), (67, 4), (71, 3), (78, 1), (92, 3), (94, 2), (97, 9), (104, 1), (113, 1), (115, 4), (118, 4), (120, 2), (124, 4), (125, 3), (128, 1), (133, 5), (136, 3), (138, 1), (142, 3), (144, 6), (152, 2), (155, 1), (156, 1), (158, 1), (162, 1), (168, 1), (172, 1), (177, 12), (199, 1), (203, 14), (204, 1), (213, 1), (215, 1), (218, 4)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dm5ZmUJo_0EA","colab_type":"code","outputId":"5bebee7c-b4b6-46ce-9dfa-4648861399b1","executionInfo":{"status":"ok","timestamp":1590836354951,"user_tz":-180,"elapsed":64719,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# viewing actual terms and their counts\n","print([(dictionary[idx] , freq) for idx, freq in bow_corpus[1][:50]])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('ability', 2), ('abu_mostafa', 1), ('afosr', 1), ('air_force', 1), ('american_institute', 1), ('appendix', 1), ('assume', 1), ('asymptotic', 1), ('axe', 1), ('become', 2), ('becomes', 1), ('bt', 1), ('ca_abstract', 1), ('california_institute', 1), ('cannot', 2), ('complete', 2), ('connected', 7), ('consider', 4), ('corresponding', 3), ('denote', 1), ('ea', 3), ('ed', 2), ('element', 9), ('environment', 1), ('expected', 1), ('expression', 4), ('fact', 4), ('final', 2), ('fixed', 4), ('follows', 3), ('furthermore', 1), ('get', 5), ('go', 3), ('going', 1), ('hand', 3), ('hence', 6), ('implemented', 2), ('independent', 1), ('independently', 1), ('interested', 1), ('ity', 1), ('know', 1), ('le', 1), ('let', 12), ('need', 1), ('neuron', 14), ('next_section', 1), ('occur', 1), ('office_scientific', 1), ('otherwise', 4)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yxKPqEe4_0A-","colab_type":"code","outputId":"b048e714-9136-4dab-9e9e-0d894dd7e67b","executionInfo":{"status":"ok","timestamp":1590836354951,"user_tz":-180,"elapsed":64711,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# total papers in the corpus\n","\n","print('Total number of papers:', len(bow_corpus))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Total number of papers: 1740\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KZBkx5Z4LbKi","colab_type":"text"},"source":["### Latent Dirichlet Allocation"]},{"cell_type":"code","metadata":{"id":"Nt1U3AzdMSF8","colab_type":"code","colab":{}},"source":["TOTAL_TOPICS = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1o4Cs_rsBqam","colab_type":"code","colab":{}},"source":["lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, alpha='auto',\n","                                   eta='auto', random_state=42, iterations=500, num_topics=TOTAL_TOPICS,\n","                                   passes=20, eval_every=None)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"szxcugE7MzUi","colab_type":"text"},"source":["Viewing the topics in our trained topic model is quite easy and we can generate them with the following code."]},{"cell_type":"code","metadata":{"id":"TlOqX5mcBrDS","colab_type":"code","outputId":"0ecef8c9-11b4-43d3-c5a5-2a438d36c2f1","executionInfo":{"status":"ok","timestamp":1590836470345,"user_tz":-180,"elapsed":180093,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":541}},"source":["for topic_id, topic in lda_model.print_topics(num_topics=10, num_words=20):\n","  print('Topic #'+str(topic_id+1)+':')\n","  print(topic)\n","  print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Topic #1:\n","0.014*\"control\" + 0.008*\"unit\" + 0.007*\"state\" + 0.007*\"trajectory\" + 0.006*\"target\" + 0.006*\"dynamic\" + 0.006*\"movement\" + 0.006*\"motor\" + 0.005*\"position\" + 0.005*\"module\" + 0.005*\"task\" + 0.005*\"controller\" + 0.005*\"representation\" + 0.004*\"pattern\" + 0.004*\"change\" + 0.004*\"subject\" + 0.004*\"hand\" + 0.004*\"behavior\" + 0.004*\"arm\" + 0.003*\"activation\"\n","\n","Topic #2:\n","0.012*\"chip\" + 0.011*\"circuit\" + 0.008*\"image\" + 0.007*\"current\" + 0.007*\"neuron\" + 0.006*\"voltage\" + 0.006*\"analog\" + 0.004*\"processor\" + 0.004*\"signal\" + 0.004*\"implementation\" + 0.004*\"bit\" + 0.004*\"design\" + 0.004*\"pixel\" + 0.004*\"line\" + 0.003*\"vector\" + 0.003*\"neural\" + 0.003*\"device\" + 0.003*\"digital\" + 0.003*\"application\" + 0.003*\"architecture\"\n","\n","Topic #3:\n","0.027*\"state\" + 0.014*\"action\" + 0.009*\"policy\" + 0.008*\"control\" + 0.007*\"step\" + 0.007*\"reinforcement_learning\" + 0.006*\"optimal\" + 0.005*\"task\" + 0.005*\"environment\" + 0.004*\"reward\" + 0.004*\"goal\" + 0.004*\"agent\" + 0.004*\"td\" + 0.004*\"cost\" + 0.004*\"robot\" + 0.003*\"path\" + 0.003*\"controller\" + 0.003*\"current\" + 0.003*\"probability\" + 0.003*\"solution\"\n","\n","Topic #4:\n","0.023*\"image\" + 0.012*\"object\" + 0.012*\"unit\" + 0.011*\"feature\" + 0.008*\"map\" + 0.008*\"visual\" + 0.007*\"layer\" + 0.006*\"region\" + 0.005*\"local\" + 0.005*\"representation\" + 0.004*\"view\" + 0.004*\"pattern\" + 0.004*\"position\" + 0.004*\"motion\" + 0.004*\"structure\" + 0.004*\"orientation\" + 0.004*\"location\" + 0.004*\"receptive_field\" + 0.004*\"pixel\" + 0.004*\"direction\"\n","\n","Topic #5:\n","0.009*\"distribution\" + 0.007*\"probability\" + 0.006*\"vector\" + 0.005*\"matrix\" + 0.005*\"approximation\" + 0.005*\"class\" + 0.004*\"variable\" + 0.004*\"estimate\" + 0.004*\"density\" + 0.004*\"sample\" + 0.004*\"linear\" + 0.004*\"gaussian\" + 0.003*\"equation\" + 0.003*\"component\" + 0.003*\"let\" + 0.003*\"solution\" + 0.003*\"bound\" + 0.003*\"theory\" + 0.003*\"optimal\" + 0.003*\"xi\"\n","\n","Topic #6:\n","0.012*\"state\" + 0.011*\"training\" + 0.010*\"speech\" + 0.009*\"word\" + 0.008*\"hmm\" + 0.007*\"signal\" + 0.007*\"recognition\" + 0.007*\"sequence\" + 0.006*\"mlp\" + 0.005*\"frame\" + 0.005*\"speaker\" + 0.005*\"context\" + 0.005*\"probability\" + 0.005*\"feature\" + 0.004*\"vector\" + 0.004*\"trained\" + 0.004*\"speech_recognition\" + 0.004*\"nonlinear\" + 0.004*\"prediction\" + 0.004*\"acoustic\"\n","\n","Topic #7:\n","0.009*\"state\" + 0.008*\"vector\" + 0.008*\"unit\" + 0.007*\"node\" + 0.007*\"neuron\" + 0.005*\"memory\" + 0.005*\"dynamic\" + 0.005*\"pattern\" + 0.005*\"matrix\" + 0.005*\"equation\" + 0.004*\"net\" + 0.004*\"sequence\" + 0.004*\"layer\" + 0.004*\"threshold\" + 0.004*\"activation\" + 0.004*\"linear\" + 0.004*\"size\" + 0.004*\"let\" + 0.003*\"recurrent\" + 0.003*\"rule\"\n","\n","Topic #8:\n","0.020*\"neuron\" + 0.017*\"cell\" + 0.010*\"response\" + 0.008*\"stimulus\" + 0.007*\"activity\" + 0.007*\"signal\" + 0.006*\"spike\" + 0.006*\"pattern\" + 0.005*\"synaptic\" + 0.005*\"frequency\" + 0.005*\"neural\" + 0.004*\"effect\" + 0.004*\"firing\" + 0.004*\"cortical\" + 0.004*\"noise\" + 0.004*\"connection\" + 0.003*\"et_al\" + 0.003*\"temporal\" + 0.003*\"change\" + 0.003*\"current\"\n","\n","Topic #9:\n","0.015*\"training\" + 0.011*\"unit\" + 0.009*\"pattern\" + 0.008*\"classifier\" + 0.008*\"task\" + 0.006*\"feature\" + 0.006*\"classification\" + 0.006*\"hidden_unit\" + 0.006*\"trained\" + 0.006*\"class\" + 0.006*\"rule\" + 0.005*\"layer\" + 0.005*\"training_set\" + 0.004*\"architecture\" + 0.004*\"net\" + 0.004*\"recognition\" + 0.004*\"node\" + 0.004*\"representation\" + 0.004*\"word\" + 0.004*\"character\"\n","\n","Topic #10:\n","0.009*\"training\" + 0.005*\"prediction\" + 0.005*\"distribution\" + 0.005*\"variable\" + 0.004*\"noise\" + 0.004*\"prior\" + 0.004*\"estimate\" + 0.004*\"test\" + 0.004*\"regression\" + 0.004*\"kernel\" + 0.004*\"linear\" + 0.003*\"class\" + 0.003*\"sample\" + 0.003*\"training_set\" + 0.003*\"vector\" + 0.003*\"bayesian\" + 0.003*\"tree\" + 0.003*\"step\" + 0.003*\"approximation\" + 0.003*\"gaussian\"\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qLGppCR0NnUj","colab_type":"text"},"source":["all the weights are the same sign and tell us the importance of each term in the topic. We can also view the overall mean coherence score of the model."]},{"cell_type":"code","metadata":{"id":"-6Wy4GvYBqX8","colab_type":"code","outputId":"613ec042-6564-4394-fc1c-aaff2a355e61","executionInfo":{"status":"ok","timestamp":1590836470345,"user_tz":-180,"elapsed":180085,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["topics_coherences = lda_model.top_topics(bow_corpus, topn=20)\n","\n","avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n","\n","print('Avg. Coherence Score:', avg_coherence_score)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Avg. Coherence Score: -1.0190235012946323\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Eb3YbZz4Oq1M","colab_type":"text"},"source":["Let’s now look at the output of our LDA topic model in an easier to understand format. One way is to visualize the topics as tuples of terms and weights."]},{"cell_type":"code","metadata":{"id":"M0KhMSKLBqRy","colab_type":"code","outputId":"5ddccc5c-cf29-4e6a-bb47-9cb62a8435b0","executionInfo":{"status":"ok","timestamp":1590836470346,"user_tz":-180,"elapsed":180079,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":574}},"source":["topics_with_wts = [item[0] for item in topics_coherences]\n","\n","print('LDA Topics with Weights')\n","print('='*50)\n","\n","for idx, topic in enumerate(topics_with_wts):\n","  print('Topic #'+str(idx+1)+':')\n","  print([(term, round(wt, 3)) for wt, term in topic])\n","  print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["LDA Topics with Weights\n","==================================================\n","Topic #1:\n","[('neuron', 0.02), ('cell', 0.017), ('response', 0.01), ('stimulus', 0.008), ('activity', 0.007), ('signal', 0.007), ('spike', 0.006), ('pattern', 0.006), ('synaptic', 0.005), ('frequency', 0.005), ('neural', 0.005), ('effect', 0.004), ('firing', 0.004), ('cortical', 0.004), ('noise', 0.004), ('connection', 0.004), ('et_al', 0.003), ('temporal', 0.003), ('change', 0.003), ('current', 0.003)]\n","\n","Topic #2:\n","[('distribution', 0.009), ('probability', 0.007), ('vector', 0.006), ('matrix', 0.005), ('approximation', 0.005), ('class', 0.005), ('variable', 0.004), ('estimate', 0.004), ('density', 0.004), ('sample', 0.004), ('linear', 0.004), ('gaussian', 0.004), ('equation', 0.003), ('component', 0.003), ('let', 0.003), ('solution', 0.003), ('bound', 0.003), ('theory', 0.003), ('optimal', 0.003), ('xi', 0.003)]\n","\n","Topic #3:\n","[('state', 0.009), ('vector', 0.008), ('unit', 0.008), ('node', 0.007), ('neuron', 0.007), ('memory', 0.005), ('dynamic', 0.005), ('pattern', 0.005), ('matrix', 0.005), ('equation', 0.005), ('net', 0.004), ('sequence', 0.004), ('layer', 0.004), ('threshold', 0.004), ('activation', 0.004), ('linear', 0.004), ('size', 0.004), ('let', 0.004), ('recurrent', 0.003), ('rule', 0.003)]\n","\n","Topic #4:\n","[('chip', 0.012), ('circuit', 0.011), ('image', 0.008), ('current', 0.007), ('neuron', 0.007), ('voltage', 0.006), ('analog', 0.006), ('processor', 0.004), ('signal', 0.004), ('implementation', 0.004), ('bit', 0.004), ('design', 0.004), ('pixel', 0.004), ('line', 0.004), ('vector', 0.003), ('neural', 0.003), ('device', 0.003), ('digital', 0.003), ('application', 0.003), ('architecture', 0.003)]\n","\n","Topic #5:\n","[('training', 0.009), ('prediction', 0.005), ('distribution', 0.005), ('variable', 0.005), ('noise', 0.004), ('prior', 0.004), ('estimate', 0.004), ('test', 0.004), ('regression', 0.004), ('kernel', 0.004), ('linear', 0.004), ('class', 0.003), ('sample', 0.003), ('training_set', 0.003), ('vector', 0.003), ('bayesian', 0.003), ('tree', 0.003), ('step', 0.003), ('approximation', 0.003), ('gaussian', 0.003)]\n","\n","Topic #6:\n","[('training', 0.015), ('unit', 0.011), ('pattern', 0.009), ('classifier', 0.008), ('task', 0.008), ('feature', 0.006), ('classification', 0.006), ('hidden_unit', 0.006), ('trained', 0.006), ('class', 0.006), ('rule', 0.006), ('layer', 0.005), ('training_set', 0.005), ('architecture', 0.004), ('net', 0.004), ('recognition', 0.004), ('node', 0.004), ('representation', 0.004), ('word', 0.004), ('character', 0.004)]\n","\n","Topic #7:\n","[('image', 0.023), ('object', 0.012), ('unit', 0.012), ('feature', 0.011), ('map', 0.008), ('visual', 0.008), ('layer', 0.007), ('region', 0.006), ('local', 0.005), ('representation', 0.005), ('view', 0.004), ('pattern', 0.004), ('position', 0.004), ('motion', 0.004), ('structure', 0.004), ('orientation', 0.004), ('location', 0.004), ('receptive_field', 0.004), ('pixel', 0.004), ('direction', 0.004)]\n","\n","Topic #8:\n","[('state', 0.027), ('action', 0.014), ('policy', 0.009), ('control', 0.008), ('step', 0.007), ('reinforcement_learning', 0.007), ('optimal', 0.006), ('task', 0.005), ('environment', 0.005), ('reward', 0.004), ('goal', 0.004), ('agent', 0.004), ('td', 0.004), ('cost', 0.004), ('robot', 0.004), ('path', 0.003), ('controller', 0.003), ('current', 0.003), ('probability', 0.003), ('solution', 0.003)]\n","\n","Topic #9:\n","[('state', 0.012), ('training', 0.011), ('speech', 0.01), ('word', 0.009), ('hmm', 0.008), ('signal', 0.007), ('recognition', 0.007), ('sequence', 0.007), ('mlp', 0.006), ('frame', 0.005), ('speaker', 0.005), ('context', 0.005), ('probability', 0.005), ('feature', 0.005), ('vector', 0.004), ('trained', 0.004), ('speech_recognition', 0.004), ('nonlinear', 0.004), ('prediction', 0.004), ('acoustic', 0.004)]\n","\n","Topic #10:\n","[('control', 0.014), ('unit', 0.008), ('state', 0.007), ('trajectory', 0.007), ('target', 0.006), ('dynamic', 0.006), ('movement', 0.006), ('motor', 0.006), ('position', 0.005), ('module', 0.005), ('task', 0.005), ('controller', 0.005), ('representation', 0.005), ('pattern', 0.004), ('change', 0.004), ('subject', 0.004), ('hand', 0.004), ('behavior', 0.004), ('arm', 0.004), ('activation', 0.003)]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QosWZ1K7QdYo","colab_type":"text"},"source":["We can also view the topics as a list of terms without the weights when we want to understand the context or theme conveyed by each topic."]},{"cell_type":"code","metadata":{"id":"I1jGXjmVBqPA","colab_type":"code","outputId":"4f97db0f-f47f-465c-b273-9705348cb3b4","executionInfo":{"status":"ok","timestamp":1590836470621,"user_tz":-180,"elapsed":180347,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":574}},"source":["print('LDA Topics without Weights')\n","print('='*50)\n","\n","for idx, topic in enumerate(topics_with_wts):\n","  print('Topic #'+str(idx+1)+':')\n","  print([term for wt, term in topic])\n","  print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["LDA Topics without Weights\n","==================================================\n","Topic #1:\n","['neuron', 'cell', 'response', 'stimulus', 'activity', 'signal', 'spike', 'pattern', 'synaptic', 'frequency', 'neural', 'effect', 'firing', 'cortical', 'noise', 'connection', 'et_al', 'temporal', 'change', 'current']\n","\n","Topic #2:\n","['distribution', 'probability', 'vector', 'matrix', 'approximation', 'class', 'variable', 'estimate', 'density', 'sample', 'linear', 'gaussian', 'equation', 'component', 'let', 'solution', 'bound', 'theory', 'optimal', 'xi']\n","\n","Topic #3:\n","['state', 'vector', 'unit', 'node', 'neuron', 'memory', 'dynamic', 'pattern', 'matrix', 'equation', 'net', 'sequence', 'layer', 'threshold', 'activation', 'linear', 'size', 'let', 'recurrent', 'rule']\n","\n","Topic #4:\n","['chip', 'circuit', 'image', 'current', 'neuron', 'voltage', 'analog', 'processor', 'signal', 'implementation', 'bit', 'design', 'pixel', 'line', 'vector', 'neural', 'device', 'digital', 'application', 'architecture']\n","\n","Topic #5:\n","['training', 'prediction', 'distribution', 'variable', 'noise', 'prior', 'estimate', 'test', 'regression', 'kernel', 'linear', 'class', 'sample', 'training_set', 'vector', 'bayesian', 'tree', 'step', 'approximation', 'gaussian']\n","\n","Topic #6:\n","['training', 'unit', 'pattern', 'classifier', 'task', 'feature', 'classification', 'hidden_unit', 'trained', 'class', 'rule', 'layer', 'training_set', 'architecture', 'net', 'recognition', 'node', 'representation', 'word', 'character']\n","\n","Topic #7:\n","['image', 'object', 'unit', 'feature', 'map', 'visual', 'layer', 'region', 'local', 'representation', 'view', 'pattern', 'position', 'motion', 'structure', 'orientation', 'location', 'receptive_field', 'pixel', 'direction']\n","\n","Topic #8:\n","['state', 'action', 'policy', 'control', 'step', 'reinforcement_learning', 'optimal', 'task', 'environment', 'reward', 'goal', 'agent', 'td', 'cost', 'robot', 'path', 'controller', 'current', 'probability', 'solution']\n","\n","Topic #9:\n","['state', 'training', 'speech', 'word', 'hmm', 'signal', 'recognition', 'sequence', 'mlp', 'frame', 'speaker', 'context', 'probability', 'feature', 'vector', 'trained', 'speech_recognition', 'nonlinear', 'prediction', 'acoustic']\n","\n","Topic #10:\n","['control', 'unit', 'state', 'trajectory', 'target', 'dynamic', 'movement', 'motor', 'position', 'module', 'task', 'controller', 'representation', 'pattern', 'change', 'subject', 'hand', 'behavior', 'arm', 'activation']\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Iqn7MvpyRWWK","colab_type":"text"},"source":["We can use perplexity and coherence scores as measures to evaluate the topic\n","model. Typically, lower the perplexity, the better the model. Similarly, the lower the UMass score and the higher the Cv score in coherence, the better the model."]},{"cell_type":"code","metadata":{"id":"GF7RZZ_k_z4n","colab_type":"code","colab":{}},"source":["cv_coherence_model_lda = gensim.models.CoherenceModel( model=lda_model, corpus=bow_corpus, texts=norm_corpus_bigrams,\n","                                                      dictionary=dictionary, coherence='c_v')\n","\n","avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n","\n","umass_coherence_model_lda = gensim.models.CoherenceModel( model=lda_model, corpus=bow_corpus, texts=norm_corpus_bigrams,\n","                                                          dictionary=dictionary, coherence='u_mass')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0X9vARjRVbo","colab_type":"code","outputId":"b0caaef5-a630-4ef7-b603-62851690f367","executionInfo":{"status":"ok","timestamp":1590836532753,"user_tz":-180,"elapsed":242470,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["\n","avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n","perplexity = lda_model.log_perplexity(bow_corpus)\n","\n","print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n","print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n","print('Model Perplexity:', perplexity)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Avg. Coherence Score (Cv): 0.4952768212161648\n","Avg. Coherence Score (UMass): -1.019023501294632\n","Model Perplexity: -7.786467030429068\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6ThWeo4SUH6k","colab_type":"text"},"source":["### LDA Models with MALLET"]},{"cell_type":"code","metadata":{"id":"5orXYqxNUDp-","colab_type":"code","outputId":"41c22ad5-bbef-4633-8067-ff8d7dd95816","executionInfo":{"status":"ok","timestamp":1590836534857,"user_tz":-180,"elapsed":244565,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-05-30 11:02:13--  http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n","Resolving mallet.cs.umass.edu (mallet.cs.umass.edu)... 128.119.246.70\n","Connecting to mallet.cs.umass.edu (mallet.cs.umass.edu)|128.119.246.70|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16184794 (15M) [application/zip]\n","Saving to: ‘mallet-2.0.8.zip’\n","\n","mallet-2.0.8.zip    100%[===================>]  15.43M  18.1MB/s    in 0.9s    \n","\n","2020-05-30 11:02:14 (18.1 MB/s) - ‘mallet-2.0.8.zip’ saved [16184794/16184794]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H0q8wamCVmpp","colab_type":"code","outputId":"d5aa7d5d-a9e5-4f3f-fc8a-6f51026e75e3","executionInfo":{"status":"ok","timestamp":1590836537094,"user_tz":-180,"elapsed":246794,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!unzip mallet-2.0.8.zip"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Archive:  mallet-2.0.8.zip\n","   creating: mallet-2.0.8/\n","   creating: mallet-2.0.8/bin/\n","  inflating: mallet-2.0.8/bin/classifier2info  \n","  inflating: mallet-2.0.8/bin/csv2classify  \n","  inflating: mallet-2.0.8/bin/csv2vectors  \n","  inflating: mallet-2.0.8/bin/mallet  \n","  inflating: mallet-2.0.8/bin/mallet.bat  \n","  inflating: mallet-2.0.8/bin/mallethon  \n","  inflating: mallet-2.0.8/bin/prepend-license.sh  \n","  inflating: mallet-2.0.8/bin/svmlight2vectors  \n","  inflating: mallet-2.0.8/bin/text2classify  \n","  inflating: mallet-2.0.8/bin/text2vectors  \n","  inflating: mallet-2.0.8/bin/vectors2classify  \n","  inflating: mallet-2.0.8/bin/vectors2info  \n","  inflating: mallet-2.0.8/bin/vectors2topics  \n","  inflating: mallet-2.0.8/bin/vectors2vectors  \n","  inflating: mallet-2.0.8/build.xml  \n","   creating: mallet-2.0.8/class/\n","   creating: mallet-2.0.8/class/cc/\n","   creating: mallet-2.0.8/class/cc/mallet/\n","   creating: mallet-2.0.8/class/cc/mallet/classify/\n","  inflating: mallet-2.0.8/class/cc/mallet/classify/AdaBoost.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/AdaBoostM2.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/AdaBoostM2Trainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/AdaBoostTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/BaggingClassifier.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/BaggingTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/BalancedWinnow.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/BalancedWinnowTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/Boostable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/C45$Node.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/C45.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/C45Trainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/Classification.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/Classifier.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/ClassifierAccuracyEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/ClassifierEnsemble.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/ClassifierEnsembleTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/ClassifierEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/ClassifierTrainer$ByActiveLearning.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/ClassifierTrainer$ByIncrements.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/ClassifierTrainer$ByInstanceIncrements.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/ClassifierTrainer$ByOptimization.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/ClassifierTrainer$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/ClassifierTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/ConfidencePredictingClassifier.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/ConfidencePredictingClassifierTrainer.class  \n","   creating: mallet-2.0.8/class/cc/mallet/classify/constraints/\n","   creating: mallet-2.0.8/class/cc/mallet/classify/constraints/ge/\n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/ge/MaxEntFLGEConstraints$MaxEntFLGEConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/ge/MaxEntFLGEConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/ge/MaxEntGEConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/ge/MaxEntKLFLGEConstraints$MaxEntKLFLGEConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/ge/MaxEntKLFLGEConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/ge/MaxEntL2FLGEConstraints$MaxEntL2FLGEConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/ge/MaxEntL2FLGEConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/ge/MaxEntRangeL2FLGEConstraints$MaxEntL2IndGEConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/ge/MaxEntRangeL2FLGEConstraints.class  \n","   creating: mallet-2.0.8/class/cc/mallet/classify/constraints/pr/\n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/pr/MaxEntFLPRConstraints$MaxEntFLPRConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/pr/MaxEntFLPRConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/pr/MaxEntL2FLPRConstraints$MaxEntL2FLPRConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/pr/MaxEntL2FLPRConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/constraints/pr/MaxEntPRConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/DecisionTree$Node.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/DecisionTree.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/DecisionTreeTrainer$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/DecisionTreeTrainer.class  \n","   creating: mallet-2.0.8/class/cc/mallet/classify/evaluate/\n","  inflating: mallet-2.0.8/class/cc/mallet/classify/evaluate/AccuracyCoverage$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/evaluate/AccuracyCoverage$ClassificationComparator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/evaluate/AccuracyCoverage.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/evaluate/ConfusionMatrix.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/evaluate/Graph$Legend.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/evaluate/Graph.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/evaluate/Graph2.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/evaluate/GraphItem.class  \n","   creating: mallet-2.0.8/class/cc/mallet/classify/examples/\n","  inflating: mallet-2.0.8/class/cc/mallet/classify/examples/DocumentClassifier.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/FeatureConstraintUtil$Element.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/FeatureConstraintUtil.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/FeatureSelectingClassifierTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MaxEnt.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MaxEntGERangeTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MaxEntGETrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MaxEntL1Trainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MaxEntOptimizableByGE.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MaxEntOptimizableByLabelDistribution.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MaxEntOptimizableByLabelLikelihood.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MaxEntPRTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MaxEntTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MCMaxEnt.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MCMaxEntTrainer$MaximizableTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MCMaxEntTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MostFrequentClassAssignmentTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/MostFrequentClassifier.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/NaiveBayes.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/NaiveBayesEMTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/NaiveBayesTrainer$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/NaiveBayesTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/PRAuxClassifier.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/PRAuxClassifierOptimizable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/RandomAssignmentTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/RandomClassifier.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/RankMaxEnt.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/RankMaxEntTrainer$MaximizableTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/RankMaxEntTrainer.class  \n","   creating: mallet-2.0.8/class/cc/mallet/classify/tests/\n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tests/TestClassifiers.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tests/TestMaxEntTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tests/TestNaiveBayes.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tests/TestStaticParameters$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tests/TestStaticParameters$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tests/TestStaticParameters.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/Trial.class  \n","   creating: mallet-2.0.8/class/cc/mallet/classify/tui/\n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Calo2Classify$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Calo2Classify$2.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Calo2Classify$ReportOption.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Calo2Classify.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Classifier2Info.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Csv2Classify.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Csv2Vectors.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/SvmLight2Classify.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/SvmLight2Vectors.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Text2Classify.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Text2Vectors.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Vectors2Classify$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Vectors2Classify$2.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Vectors2Classify$ReportOption.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Vectors2Classify.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Vectors2FeatureConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Vectors2Info$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Vectors2Info.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/tui/Vectors2Vectors.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/Winnow.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/classify/WinnowTrainer.class  \n","   creating: mallet-2.0.8/class/cc/mallet/cluster/\n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/Clusterer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/Clustering.class  \n","   creating: mallet-2.0.8/class/cc/mallet/cluster/clustering_scorer/\n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/clustering_scorer/ClusteringScorer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/clustering_scorer/PairwiseScorer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/Clusterings.class  \n","   creating: mallet-2.0.8/class/cc/mallet/cluster/evaluate/\n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/evaluate/AccuracyEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/evaluate/BCubedEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/evaluate/ClusteringEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/evaluate/ClusteringEvaluators.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/evaluate/MUCEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/evaluate/PairF1Evaluator.class  \n","   creating: mallet-2.0.8/class/cc/mallet/cluster/evaluate/tests/\n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/evaluate/tests/TestClusteringEvaluators.class  \n","   creating: mallet-2.0.8/class/cc/mallet/cluster/examples/\n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/examples/FirstOrderClusterExample$OverlappingFeaturePipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/examples/FirstOrderClusterExample.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/GreedyAgglomerative.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/GreedyAgglomerativeByDensity.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/HillClimbingClusterer.class  \n","   creating: mallet-2.0.8/class/cc/mallet/cluster/iterator/\n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/iterator/AllPairsIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/iterator/ClusterSampleIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/iterator/NeighborIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/iterator/NodeClusterSampleIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/iterator/PairSampleIterator.class  \n","   creating: mallet-2.0.8/class/cc/mallet/cluster/iterator/tests/\n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/iterator/tests/TestIterators.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/KBestClusterer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/KMeans.class  \n","   creating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/\n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/AgglomerativeNeighbor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/ClassifyingNeighborEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/MedoidEvaluator$Average.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/MedoidEvaluator$CombiningStrategy.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/MedoidEvaluator$Maximum.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/MedoidEvaluator$Minimum.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/MedoidEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/Neighbor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/NeighborEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/PairwiseEvaluator$Average.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/PairwiseEvaluator$CombiningStrategy.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/PairwiseEvaluator$Maximum.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/PairwiseEvaluator$Minimum.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/PairwiseEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/RandomEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/neighbor_evaluator/RankingNeighborEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/Record.class  \n","   creating: mallet-2.0.8/class/cc/mallet/cluster/tui/\n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/tui/Clusterings2Clusterer$ClusteringPipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/tui/Clusterings2Clusterer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/tui/Clusterings2Clusterings.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/tui/Clusterings2Info.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/tui/Text2Clusterings.class  \n","   creating: mallet-2.0.8/class/cc/mallet/cluster/util/\n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/util/ClusterUtils.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/cluster/util/PairwiseMatrix.class  \n","   creating: mallet-2.0.8/class/cc/mallet/examples/\n","  inflating: mallet-2.0.8/class/cc/mallet/examples/TestCRFPipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/examples/TopicModel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/examples/TrainCRF.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/examples/TrainHMM.class  \n","   creating: mallet-2.0.8/class/cc/mallet/extract/\n","  inflating: mallet-2.0.8/class/cc/mallet/extract/AccuracyCoverageEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/BIOTokenizationFilter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/BIOTokenizationFilterWithTokenIndices.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/ConfidenceTokenizationFilter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/CRFExtractor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/DefaultTokenizationFilter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/DocumentExtraction$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/DocumentExtraction.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/DocumentViewer$DualLabeledSpans.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/DocumentViewer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/ExactMatchComparator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/Extraction.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/ExtractionConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/ExtractionEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/Extractor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/Field.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/FieldCleaner.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/FieldComparator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/HierarchicalTokenizationFilter$TagStart.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/HierarchicalTokenizationFilter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/LabeledSpan.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/LabeledSpans.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/LatticeViewer$ExtorInfo.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/LatticeViewer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/PerDocumentF1Evaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/PerFieldF1Evaluator.class  \n","   creating: mallet-2.0.8/class/cc/mallet/extract/pipe/\n","  inflating: mallet-2.0.8/class/cc/mallet/extract/pipe/TokenSequence2Tokenization.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/PunctuationIgnoringComparator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/Record.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/RegexFieldCleaner.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/Span.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/StringSpan.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/StringTokenization.class  \n","   creating: mallet-2.0.8/class/cc/mallet/extract/test/\n","  inflating: mallet-2.0.8/class/cc/mallet/extract/test/TestDocumentExtraction.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/test/TestDocumentViewer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/test/TestLatticeViewer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/test/TestPerDocumentF1Evaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/Tokenization.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/TokenizationFilter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/extract/TransducerExtractionConfidenceEstimator.class  \n","   creating: mallet-2.0.8/class/cc/mallet/fst/\n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CacheStaleIndicator.class  \n","   creating: mallet-2.0.8/class/cc/mallet/fst/confidence/\n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/ConfidenceCorrectorEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/ConfidenceEvaluator$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/ConfidenceEvaluator$ConfidenceComparator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/ConfidenceEvaluator$EntityConfidence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/ConfidenceEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/ConstrainedForwardBackwardConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/ConstrainedViterbiTransducerCorrector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/GammaAverageConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/GammaProductConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/InstanceWithConfidence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/IsolatedSegmentTransducerCorrector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/MaxEntConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/MaxEntSequenceConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/MinSegmentConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/NBestViterbiConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/PipedInstanceWithConfidence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/QBCSequenceConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/RandomConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/RandomSequenceConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/SegmentProductConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/SequenceConfidenceInstance.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/TransducerConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/TransducerCorrector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/TransducerSequenceConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/ViterbiConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/confidence/ViterbiRatioConfidenceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRF$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRF$2.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRF$Factors$Incrementor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRF$Factors$WeightedIncrementor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRF$Factors.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRF$State.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRF$TransitionIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRF.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFCacheStaleIndicator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFOptimizableByBatchLabelLikelihood$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFOptimizableByBatchLabelLikelihood.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFOptimizableByGradientValues.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFOptimizableByLabelLikelihood$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFOptimizableByLabelLikelihood.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFTrainerByL1LabelLikelihood.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFTrainerByLabelLikelihood.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFTrainerByStochasticGradient.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFTrainerByThreadedLabelLikelihood.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFTrainerByValueGradients$OptimizableCRF.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFTrainerByValueGradients.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/CRFWriter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/FeatureTransducer$State.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/FeatureTransducer$Transition.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/FeatureTransducer$TransitionIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/FeatureTransducer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/HMM$Incrementor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/HMM$State.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/HMM$TransitionIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/HMM$WeightedIncrementor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/HMM.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/HMMTrainerByLikelihood.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/InstanceAccuracyEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/LabelDistributionEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MaxLattice.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MaxLatticeDefault$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MaxLatticeDefault$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MaxLatticeDefault$ViterbiNode$PreviousStateIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MaxLatticeDefault$ViterbiNode.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MaxLatticeDefault$WeightCache.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MaxLatticeDefault.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MaxLatticeFactory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MEMM$State.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MEMM$TransitionIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MEMM.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MEMMTrainer$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MEMMTrainer$MEMMOptimizableByLabelLikelihood.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MEMMTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/MultiSegmentationEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/NoopTransducerTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/PerClassAccuracyEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/Segment.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SegmentationEvaluator$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SegmentationEvaluator$2.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SegmentationEvaluator.class  \n","   creating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/\n","   creating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/\n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/GEConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/OneLabelGEConstraints$OneLabelGEConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/OneLabelGEConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/OneLabelKLGEConstraints$OneLabelGEKLConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/OneLabelKLGEConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/OneLabelL2GEConstraints$OneLabelGEL2Constraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/OneLabelL2GEConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/OneLabelL2RangeGEConstraints$OneLabelL2IndGEConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/OneLabelL2RangeGEConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/SelfTransitionGEConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/TwoLabelGEConstraints$TwoLabelGEConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/TwoLabelGEConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/TwoLabelKLGEConstraints$TwoLabelKLGEConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/TwoLabelKLGEConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/TwoLabelL2GEConstraints$TwoLabelL2GEConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/constraints/TwoLabelL2GEConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/CRFOptimizableByEntropyRegularization.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/CRFOptimizableByGE.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/CRFTrainerByEntropyRegularization.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/CRFTrainerByGE.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/CRFTrainerByLikelihoodAndGE.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/EntropyLattice$LatticeNode.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/EntropyLattice.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/FSTConstraintUtil.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/GELattice$LatticeNode.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/GELattice.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/GELatticeTask.class  \n","   creating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/\n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/CachedDotTransitionIterator.class  \n","   creating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/constraints/\n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/constraints/OneLabelL2IndPRConstraints$OneLabelL2IndPRConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/constraints/OneLabelL2IndPRConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/constraints/OneLabelL2PRConstraints$OneLabelPRConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/constraints/OneLabelL2PRConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/constraints/PRConstraint.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/ConstraintsOptimizableByPR$ExpectationTask.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/ConstraintsOptimizableByPR.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/CRFOptimizableByKL$ExpectationTask.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/CRFOptimizableByKL.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/CRFTrainerByPR.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/PRAuxiliaryModel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/SumLatticeDefaultCachedDot$LatticeNode.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/SumLatticeDefaultCachedDot.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/SumLatticeKL.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/SumLatticePR$LatticeNode.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/pr/SumLatticePR.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/StateLabelMap.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/SumLatticeTask.class  \n","   creating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/tui/\n","  inflating: mallet-2.0.8/class/cc/mallet/fst/semi_supervised/tui/SimpleTaggerWithConstraints.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/ShallowTransducerTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SimpleTagger$SimpleTaggerSentence2FeatureVectorSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SimpleTagger.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLattice.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeBeam$Factory$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeBeam$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeBeam$LatticeNode.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeBeam$NBestSlist.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeBeam$NBForBackNode.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeBeam.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeConstrained.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeDefault$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeDefault$LatticeNode.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeDefault.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeFactory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeScaling$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeScaling$LatticeNode.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/SumLatticeScaling.class  \n","   creating: mallet-2.0.8/class/cc/mallet/fst/tests/\n","  inflating: mallet-2.0.8/class/cc/mallet/fst/tests/TestCRF$TestCRF2String.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/tests/TestCRF$TestCRFTokenSequenceRemoveSpaces.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/tests/TestCRF.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/tests/TestFeatureTransducer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/tests/TestMEMM$TestMEMM2String.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/tests/TestMEMM$TestMEMMTokenSequenceRemoveSpaces.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/tests/TestMEMM.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/tests/TestSumNegLogProb2.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/ThreadedOptimizable$GradientHandler.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/ThreadedOptimizable$ValueHandler.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/ThreadedOptimizable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/TokenAccuracyEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/Transducer$Incrementor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/Transducer$State.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/Transducer$TransitionIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/Transducer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/TransducerEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/TransducerTrainer$ByIncrements.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/TransducerTrainer$ByInstanceIncrements.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/TransducerTrainer$ByOptimization.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/TransducerTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/fst/ViterbiWriter.class  \n","   creating: mallet-2.0.8/class/cc/mallet/grmm/\n","   creating: mallet-2.0.8/class/cc/mallet/grmm/examples/\n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/examples/CrossTemplate1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/examples/ModelReaderExample.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/examples/SimpleCrfExample.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/examples/SimpleFactorExample.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/examples/SimpleGraphExample.class  \n","   creating: mallet-2.0.8/class/cc/mallet/grmm/inference/\n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/AbstractBeliefPropagation$AbstractMessageStrategy.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/AbstractBeliefPropagation$MaxProductMessageStrategy.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/AbstractBeliefPropagation$MessageStrategy.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/AbstractBeliefPropagation$SumProductMessageStrategy.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/AbstractBeliefPropagation.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/AbstractInferencer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/BruteForceInferencer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/ExactSampler.class  \n","   creating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/\n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/AbstractMessageStrategy.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/BPRegionGenerator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/ClusterVariationalRegionGenerator$BaseRegionComputer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/ClusterVariationalRegionGenerator$ByFactorRegionComputer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/ClusterVariationalRegionGenerator$Grid2x2RegionComputer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/ClusterVariationalRegionGenerator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/FactorizedRegion.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/FullMessageStrategy.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/Kikuchi4SquareRegionGenerator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/MessageArray.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/MessageStrategy.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/ParentChildGBP$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/ParentChildGBP.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/Region.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/RegionEdge.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/RegionGraph.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/RegionGraphGenerator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/gbp/SparseMessageSender.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/GibbsSampler.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/Inferencer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/JunctionTree$Sepset.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/JunctionTree.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/JunctionTreeInferencer$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/JunctionTreeInferencer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/JunctionTreePropagation$MaxProductMessageStrategy.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/JunctionTreePropagation$MessageStrategy.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/JunctionTreePropagation$SumProductMessageStrategy.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/JunctionTreePropagation.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/LoopyBP.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/MessageArray$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/MessageArray$Iterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/MessageArray$ToMsgsIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/MessageArray.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/RandomGraphs$FactorGenerator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/RandomGraphs$UniformFactorGenerator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/RandomGraphs.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/ResidualBP.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/Sampler.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/SamplingInferencer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/TreeBP.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/TRP$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/TRP$AlmostRandomTreeFactory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/TRP$ConvergenceTerminator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/TRP$DefaultConvergenceTerminator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/TRP$IterationTerminator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/TRP$SimpleUnionFind.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/TRP$TerminationCondition.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/TRP$TreeFactory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/TRP$TreeListFactory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/TRP.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/Utils.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/inference/VariableElimination.class  \n","   creating: mallet-2.0.8/class/cc/mallet/grmm/learning/\n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRF$BigramTemplate.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRF$FixedFactorTemplate.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRF$GraphPostProcessor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRF$MaximizableACRF.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRF$PairwiseFactorTemplate.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRF$SequenceTemplate.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRF$Template.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRF$UnigramTemplate.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRF$UnrolledGraph.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRF$UnrolledVarSet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRF.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRFEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/AcrfSerialEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/ACRFTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/DefaultAcrfTrainer$FileEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/DefaultAcrfTrainer$LogEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/DefaultAcrfTrainer$TestResults.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/DefaultAcrfTrainer.class  \n","   creating: mallet-2.0.8/class/cc/mallet/grmm/learning/extract/\n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/extract/ACRFExtractor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/extract/ACRFExtractorTrainer$CheckpointingEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/extract/ACRFExtractorTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/extract/AcrfExtractorTui.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/GenericAcrfData2TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/GenericAcrfTui.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/MultiSegmentationEvaluatorACRF$TestResults.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/MultiSegmentationEvaluatorACRF.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/PiecewiseACRFTrainer$Maxable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/PiecewiseACRFTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/PseudolikelihoodACRFTrainer$CliquesIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/PseudolikelihoodACRFTrainer$EdgesIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/PseudolikelihoodACRFTrainer$Maxable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/PseudolikelihoodACRFTrainer$VariablesIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/PseudolikelihoodACRFTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/PwplACRFTrainer$Maxable$WrongWrong.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/PwplACRFTrainer$Maxable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/PwplACRFTrainer.class  \n","   creating: mallet-2.0.8/class/cc/mallet/grmm/learning/templates/\n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/templates/SimilarTokensTemplate$CapWordsBinner.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/templates/SimilarTokensTemplate$FeatureVectorBinner.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/templates/SimilarTokensTemplate$TokenInfo.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/templates/SimilarTokensTemplate$WordFeatureBinner.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/learning/templates/SimilarTokensTemplate.class  \n","   creating: mallet-2.0.8/class/cc/mallet/grmm/test/\n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestAbstractBeliefPropagation.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestAssignment.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestAssignmentIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestBetaFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestBitVarSet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestDirectedModel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestFactorGraph.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestFactors.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestGenericAcrfData2TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestGibbsSampler.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestHashClique.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestInference$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestInference.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestListVarSet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestLogTableFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestMIntInt2ObjectMap.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestNormalFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestPottsFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestRandomGraphs.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestTableFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestTRP.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestUndirectedModel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestUniformFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/test/TestUniNormalFactor.class  \n","   creating: mallet-2.0.8/class/cc/mallet/grmm/types/\n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/AbstractAssignmentIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/AbstractFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/AbstractTableFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/Assignment.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/AssignmentIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/BetaFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/BidirectionalIntObjectMap.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/BinaryUnaryFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/BitVarSet$Iterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/BitVarSet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/BoltzmannPairFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/BoltzmannUnaryFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/ConstantFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/CPT.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/DenseAssignmentIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/DirectedModel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/DiscreteFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/Factor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/FactorGraph$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/FactorGraph$2.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/FactorGraph$3.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/FactorGraph.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/Factors$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/Factors.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/HashVarSet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/ListVarSet$Iterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/ListVarSet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/LogTableFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/NormalFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/ParameterizedFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/PottsTableFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/SkeletonFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/SparseAssignmentIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/TableFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/Tree.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/UndirectedGrid.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/UndirectedModel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/UniformFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/UniNormalFactor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/Universe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/UnmodifiableVarSet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/Variable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/types/VarSet.class  \n","   creating: mallet-2.0.8/class/cc/mallet/grmm/util/\n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/CachingOptimizable$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/CachingOptimizable$Base.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/CachingOptimizable$ByBatchGradient.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/CachingOptimizable$ByGradient.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/CachingOptimizable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/CSIntInt2ObjectMultiMap$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/CSIntInt2ObjectMultiMap.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/Flops$Watch.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/Flops.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/GeneralUtils.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/Graphs.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/LabelsAssignment.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/LabelsSequence2Assignment.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/Matrices.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/MIntInt2ObjectMap$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/MIntInt2ObjectMap.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/ModelReader.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/Models.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/ModelWriter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/PipedIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/RememberTokenizationPipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/SliceLabelsSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/grmm/util/THashMultiMap.class  \n","   creating: mallet-2.0.8/class/cc/mallet/optimize/\n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/AGIS.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/BackTrackLineSearch.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/ConjugateGradient.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/GradientAscent.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/GradientBracketLineOptimizer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/InvalidOptimizableException.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/LimitedMemoryBFGS.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/LineOptimizer$ByGradient.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/LineOptimizer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/Optimizable$ByBatchGradient.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/Optimizable$ByCombiningBatchGradient.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/Optimizable$ByGISUpdate.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/Optimizable$ByGradient.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/Optimizable$ByGradientValue.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/Optimizable$ByHessian.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/Optimizable$ByValue.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/Optimizable$ByVotedPerceptron.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/Optimizable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/OptimizableCollection$ByGradientValue.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/OptimizableCollection.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/OptimizationException.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/Optimizer$ByBatches.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/Optimizer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/OptimizerEvaluator$ByBatchGradient.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/OptimizerEvaluator$ByGradient.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/OptimizerEvaluator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/OrthantWiseLimitedMemoryBFGS.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/StochasticMetaAscent.class  \n","   creating: mallet-2.0.8/class/cc/mallet/optimize/tests/\n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/tests/TestOptimizable$SimplePoly.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/tests/TestOptimizable$WrongSimplePoly.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/tests/TestOptimizable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/tests/TestOptimizer$SimplePoly.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/optimize/tests/TestOptimizer.class  \n","   creating: mallet-2.0.8/class/cc/mallet/pipe/\n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/AddClassifierTokenPredictions$TokenClassifiers.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/AddClassifierTokenPredictions.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Array2FeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/AugmentableFeatureVectorAddConjunctions.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/AugmentableFeatureVectorLogScale.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/BranchingPipe$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/BranchingPipe$BranchingInstanceIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/BranchingPipe$GateKeepingInstanceIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/BranchingPipe$PeekingInstanceIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/BranchingPipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/CharSequence2CharNGrams.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/CharSequence2TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/CharSequenceArray2TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/CharSequenceLowercase.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/CharSequenceRemoveHTML$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/CharSequenceRemoveHTML$ParserGetter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/CharSequenceRemoveHTML$TagStripper.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/CharSequenceRemoveHTML.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/CharSequenceRemoveUUEncodedBlocks.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/CharSequenceReplace.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/CharSubsequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Classification2ConfidencePredictingFeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Csv2Array.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Csv2FeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Directory2FileIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/FeatureCountPipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/FeatureDocFreqPipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/FeatureSequence2AugmentableFeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/FeatureSequence2FeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/FeatureSequenceConvolution.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/FeatureValueString2FeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/FeatureVectorConjunctions.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/FeatureVectorSequence2FeatureVectors$FeatureVectorIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/FeatureVectorSequence2FeatureVectors.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Filename2CharSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/FilterEmptyFeatureVectors$FilteringPipeInstanceIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/FilterEmptyFeatureVectors.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/FixedVocabTokenizer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Input2CharSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/InstanceListTrimFeaturesByCount.class  \n","   creating: mallet-2.0.8/class/cc/mallet/pipe/iterator/\n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/ArrayDataAndTargetIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/ArrayIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/ConcatenatedInstanceIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/CsvIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/DBInstanceIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/EmptyInstanceIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/FileIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/FileListIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/FileUriIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/LineGroupIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/LineIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/ParenGroupIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/PatternMatchIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/PipeExtendedIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/PipeInputIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/RandomFeatureVectorIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/RandomTokenSequenceIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/SegmentIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/SelectiveFileLineIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/SimpleFileLineIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/StringArrayIterator.class  \n","   creating: mallet-2.0.8/class/cc/mallet/pipe/iterator/tests/\n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/tests/TestPatternMatchIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/iterator/UnlabeledFileIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/LineGroupString2TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/MakeAmpersandXMLFriendly.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/NGramPreprocessor$ReplacementSet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/NGramPreprocessor.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Noop.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Pipe$SimplePipeInstanceIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Pipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/PipeException.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/PipeUtils.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/PrintInput.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/PrintInputAndTarget.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/PrintTokenSequenceFeatures.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/SaveDataInSource.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/SelectiveSGML2TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/SerialPipes$Predicate.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/SerialPipes.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/SGML2TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/SimpleTaggerSentence2StringTokenization.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/SimpleTaggerSentence2TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/SimpleTokenizer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/SourceLocation2TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/StringAddNewLineDelimiter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/StringList2FeatureSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/SvmLight2FeatureVectorAndLabel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Target2Double.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Target2FeatureSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Target2Integer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Target2Label.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Target2LabelSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TargetRememberLastLabel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TargetStringToFeatures.class  \n","   creating: mallet-2.0.8/class/cc/mallet/pipe/tests/\n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tests/TestInstancePipe$Array2ArrayIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tests/TestInstancePipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tests/TestIterators.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tests/TestPipeUtils$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tests/TestPipeUtils$StupidPipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tests/TestPipeUtils.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tests/TestRainbowStyle.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tests/TestSGML2TokenSequence$Array2ArrayIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tests/TestSGML2TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tests/TestSpacePipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/Token2FeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TokenSequence2FeatureSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TokenSequence2FeatureSequenceWithBigrams.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TokenSequence2FeatureVectorSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TokenSequence2TokenInstances$TokenInstanceIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TokenSequence2TokenInstances.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TokenSequenceLowercase.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TokenSequenceMatchDataAndTarget.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TokenSequenceNGrams.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TokenSequenceParseFeatureString.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TokenSequenceRemoveNonAlpha.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TokenSequenceRemoveStopPatterns.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/TokenSequenceRemoveStopwords.class  \n","   creating: mallet-2.0.8/class/cc/mallet/pipe/tsf/\n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/CountMatches.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/CountMatchesAlignedWithOffsets.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/CountMatchesMatching.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/FeaturesInWindow.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/FeaturesOfFirstMention.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/LexiconMembership.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/OffsetConjunctions.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/OffsetFeatureConjunction.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/OffsetPropertyConjunctions.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/RegexMatches.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/SequencePrintingPipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/Target2BIOFormat.class  \n","   creating: mallet-2.0.8/class/cc/mallet/pipe/tsf/tests/\n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/tests/TestOffsetConjunctions.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/tests/TestOffsetFeatureConjunctions.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/tests/TestSequencePrintingPipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/TokenFirstPosition.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/TokenText.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/TokenTextCharNGrams.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/TokenTextCharPrefix.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/TokenTextCharSuffix.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/TokenTextNGrams.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/TrieLexiconMembership$TrieLexicon.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/TrieLexiconMembership.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/tsf/WordVectors.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/pipe/ValueString2FeatureVector.class  \n","   creating: mallet-2.0.8/class/cc/mallet/regression/\n","  inflating: mallet-2.0.8/class/cc/mallet/regression/CoordinateDescent.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/regression/LeastSquares.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/regression/LinearRegression.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/regression/LinearRegressionTrainer.class  \n","   creating: mallet-2.0.8/class/cc/mallet/regression/tui/\n","  inflating: mallet-2.0.8/class/cc/mallet/regression/tui/Regression.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/regression/tui/RegressionImporter.class  \n","   creating: mallet-2.0.8/class/cc/mallet/share/\n","   creating: mallet-2.0.8/class/cc/mallet/share/casutton/\n","   creating: mallet-2.0.8/class/cc/mallet/share/casutton/ner/\n","  inflating: mallet-2.0.8/class/cc/mallet/share/casutton/ner/ConllNer2003Sentence2TokenSequence.class  \n","   creating: mallet-2.0.8/class/cc/mallet/share/mccallum/\n","   creating: mallet-2.0.8/class/cc/mallet/share/mccallum/ner/\n","  inflating: mallet-2.0.8/class/cc/mallet/share/mccallum/ner/ConllNer2003Sentence2TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/share/mccallum/ner/TokenSequenceDocHeader.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/share/mccallum/ner/TUI.class  \n","   creating: mallet-2.0.8/class/cc/mallet/share/upenn/\n","  inflating: mallet-2.0.8/class/cc/mallet/share/upenn/MaxEntShell.class  \n","   creating: mallet-2.0.8/class/cc/mallet/share/upenn/ner/\n","  inflating: mallet-2.0.8/class/cc/mallet/share/upenn/ner/FeatureWindow.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/share/upenn/ner/LengthBins.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/share/upenn/ner/ListMember.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/share/upenn/ner/LongRegexMatches.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/share/upenn/ner/NEPipes.class  \n","   creating: mallet-2.0.8/class/cc/mallet/share/weili/\n","   creating: mallet-2.0.8/class/cc/mallet/share/weili/ner/\n","   creating: mallet-2.0.8/class/cc/mallet/share/weili/ner/enron/\n","  inflating: mallet-2.0.8/class/cc/mallet/share/weili/ner/enron/EnronMessage2TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/share/weili/ner/enron/TUI.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/share/weili/ner/WordTransformation.class  \n","   creating: mallet-2.0.8/class/cc/mallet/topics/\n","  inflating: mallet-2.0.8/class/cc/mallet/topics/AbstractTopicReports.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/DMROptimizable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/DMRTopicModel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/HierarchicalLDA$NCRPNode.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/HierarchicalLDA.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/HierarchicalPAM.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/JSONTopicReports.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/LabeledLDA.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/LDA$1WordProb.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/LDA.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/LDAHyper$Topication.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/LDAHyper.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/LDAStream.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/MarginalProbEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/MultinomialHMM.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/NPTopicModel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/PAM4L$IDSorter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/PAM4L.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/ParallelTopicModel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/PolylingualTopicModel$TopicAssignment.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/PolylingualTopicModel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/RTopicModel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/SimpleLDA.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/TopicalNGrams$1WordProb.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/TopicalNGrams.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/TopicAssignment.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/TopicInferencer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/TopicModelDiagnostics$TopicScores.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/TopicModelDiagnostics.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/TopicReports.class  \n","   creating: mallet-2.0.8/class/cc/mallet/topics/tui/\n","  inflating: mallet-2.0.8/class/cc/mallet/topics/tui/DMRLoader.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/tui/EvaluateTopics.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/tui/HierarchicalLDATUI.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/tui/InferTopics.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/tui/TopicTrainer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/tui/Vectors2Topics.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/WeightedTopicModel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/WordEmbeddingRunnable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/WordEmbeddings.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/topics/WorkerRunnable.class  \n","   creating: mallet-2.0.8/class/cc/mallet/types/\n","  inflating: mallet-2.0.8/class/cc/mallet/types/Alphabet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/AlphabetCarrying.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/AlphabetFactory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/ArrayListSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/ArraySequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/AugmentableFeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/BiNormalSeparation$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/BiNormalSeparation.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/CachedMetric.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/ChainedInstanceIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/ConstantMatrix.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/CrossValidationIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/DenseMatrix.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/DenseVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Dirichlet$Estimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Dirichlet$MethodOfMomentsEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Dirichlet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/EuclideanDistance.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/ExpGain$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/ExpGain.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureConjunction$List.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureConjunction.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureCounter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureCounts$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureCounts.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureInducer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureSelection.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureSelector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureSequenceWithBigrams.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureVectorSequence$Iterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/FeatureVectorSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/GainRatio$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/GainRatio.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/GradientGain$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/GradientGain.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/HashedSparseVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/IDSorter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/IndexedSparseVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/InfiniteDistance.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/InfoGain$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/InfoGain.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Instance.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/InstanceList$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/InstanceList$CrossValidationIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/InstanceList$NotYetSetPipe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/InstanceList.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/InstanceListTUI.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/InvertedIndex.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/KLGain.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Label.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/LabelAlphabet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Labeler.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Labeling.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Labelings.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Labels.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/LabelSequence$Iterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/LabelSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/LabelsSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/LabelVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/LogNumber.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/ManhattenDistance.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Matrix.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Matrix2.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Matrixn.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/MatrixOps.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Metric.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Minkowski.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/MultiInstanceList$MultiIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/MultiInstanceList.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Multinomial$Estimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Multinomial$LaplaceEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Multinomial$Logged.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Multinomial$MAPEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Multinomial$MEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Multinomial$MLEstimator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Multinomial.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/NormalizedDotProductMetric.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/NullLabel.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/PagedInstanceList.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/PartiallyRankedFeatureVector$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/PartiallyRankedFeatureVector$PerLabelFactory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/PartiallyRankedFeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/PerLabelFeatureCounts$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/PerLabelFeatureCounts.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/PerLabelInfoGain$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/PerLabelInfoGain.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/PropertyHolder.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/RankedFeatureVector$EntryWithOriginalIndex.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/RankedFeatureVector$Factory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/RankedFeatureVector$PerLabelFactory.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/RankedFeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/ROCData.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Sequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/SequencePair.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/SequencePairAlignment.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/SingleInstanceIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/SparseMatrixn.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/SparseVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/StringEditFeatureVectorSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/StringEditVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/StringKernel.class  \n","   creating: mallet-2.0.8/class/cc/mallet/types/tests/\n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestAlphabet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestAugmentableFeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestBiNormalSeparation$BinaryTestData.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestBiNormalSeparation.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestFeatureSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestFeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestHashedSparseVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestIndexedSparseVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestInstanceListWeights.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestLabelAlphabet$Labelee.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestLabelAlphabet.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestLabelsSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestLabelVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestMatrix.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestMatrixn.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestMultinomial.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestPagedInstanceList.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestRankedFeatureVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestSerializable$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestSerializable$WriteMe.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestSerializable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestSparseMatrixn.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestSparseVector.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/tests/TestToken.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Token.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/TokenSequence.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/types/Vector.class  \n","   creating: mallet-2.0.8/class/cc/mallet/util/\n","  inflating: mallet-2.0.8/class/cc/mallet/util/Addable.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/ArrayListUtils.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/ArrayUtils.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/BshInterpreter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/BulkLoader.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CharSequenceLexer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CollectionUtils$1Accumulator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CollectionUtils$Fn.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CollectionUtils.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/ColorUtils.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$Boolean.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$Double.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$DoubleArray.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$File.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$Integer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$IntegerArray.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$List$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$List$2.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$List.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$ListProviding.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$Object.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$ObjectFromBean.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$Set.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$SpacedStrings.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption$String.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/CommandOption.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/DBBulkLoader.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/DBInstanceStore.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/DirectoryFilter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/DocumentLengths.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/DoubleList.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/FeatureCooccurrenceCounter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/FeatureCountTool.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/FileUtils.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/InstanceListPrinter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/IoUtils.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/Lexer.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/MalletLogger.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/MalletProgressMessageLogger.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/Maths.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/MVNormal.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/PlainLogFormatter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/PrintUtilities.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/ProgressMessageLogFormatter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/ProgressMessageLogRecord.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/PropertyList$Iterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/PropertyList$NumericIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/PropertyList$NumericProperty.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/PropertyList$ObjectIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/PropertyList$ObjectProperty.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/PropertyList.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/Randoms$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/Randoms.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/RegexFileFilter.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/Replacement.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/Replacer.class  \n","   creating: mallet-2.0.8/class/cc/mallet/util/resources/\n","  inflating: mallet-2.0.8/class/cc/mallet/util/resources/logging.properties  \n","   creating: mallet-2.0.8/class/cc/mallet/util/resources/wn/\n","  inflating: mallet-2.0.8/class/cc/mallet/util/resources/wn/Examples.class  \n","   creating: mallet-2.0.8/class/cc/mallet/util/search/\n","  inflating: mallet-2.0.8/class/cc/mallet/util/search/AStar.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/search/AStarNode$NextNodeIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/search/AStarNode.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/search/AStarState.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/search/MinHeap.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/search/PriorityQueue.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/search/QueueElement.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/search/SearchNode$NextNodeIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/search/SearchNode.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/search/SearchState$NextStateIterator.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/search/SearchState.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/Sequences.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/StatFunctions.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/Strings.class  \n","   creating: mallet-2.0.8/class/cc/mallet/util/tests/\n","  inflating: mallet-2.0.8/class/cc/mallet/util/tests/TestAStar$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/tests/TestAStar$State$NextStates.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/tests/TestAStar$State.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/tests/TestAStar.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/tests/TestMaths.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/tests/TestPriorityQueue$1.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/tests/TestPriorityQueue$Item.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/tests/TestPriorityQueue.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/tests/TestPropertyList.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/tests/TestRandom.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/tests/TestStrings.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/Timing.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/Univariate.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/UriUtils.class  \n","  inflating: mallet-2.0.8/class/cc/mallet/util/VectorStats.class  \n","   creating: mallet-2.0.8/dist/\n","  inflating: mallet-2.0.8/dist/mallet-deps.jar  \n","  inflating: mallet-2.0.8/dist/mallet.jar  \n","   creating: mallet-2.0.8/lib/\n","  inflating: mallet-2.0.8/lib/bsh.jar  \n","  inflating: mallet-2.0.8/lib/derby.jar  \n","  inflating: mallet-2.0.8/lib/grmm-deps.jar  \n","  inflating: mallet-2.0.8/lib/jdom-1.0.jar  \n","  inflating: mallet-2.0.8/lib/jgrapht-0.6.0.jar  \n","  inflating: mallet-2.0.8/lib/junit-4.5.jar  \n","  inflating: mallet-2.0.8/lib/jwnl-1.3.jar  \n","  inflating: mallet-2.0.8/lib/LICENSES  \n","  inflating: mallet-2.0.8/lib/Makefile  \n","  inflating: mallet-2.0.8/lib/mallet-deps.jar  \n","  inflating: mallet-2.0.8/lib/mtj-0.9.9.jar  \n","  inflating: mallet-2.0.8/lib/openjgraph.jar  \n","  inflating: mallet-2.0.8/lib/trove-2.0.2.jar  \n","  inflating: mallet-2.0.8/LICENSE    \n","  inflating: mallet-2.0.8/Makefile   \n","  inflating: mallet-2.0.8/pom.xml    \n","  inflating: mallet-2.0.8/README.md  \n","   creating: mallet-2.0.8/sample-data/\n","   creating: mallet-2.0.8/sample-data/numeric/\n","  inflating: mallet-2.0.8/sample-data/numeric/boxes.txt  \n","  inflating: mallet-2.0.8/sample-data/numeric/puffins.txt  \n","  inflating: mallet-2.0.8/sample-data/README  \n","   creating: mallet-2.0.8/sample-data/web/\n","   creating: mallet-2.0.8/sample-data/web/de/\n","  inflating: mallet-2.0.8/sample-data/web/de/apollo8.txt  \n","  inflating: mallet-2.0.8/sample-data/web/de/fiv.txt  \n","  inflating: mallet-2.0.8/sample-data/web/de/habichtsadler.txt  \n","  inflating: mallet-2.0.8/sample-data/web/de/hoechst.txt  \n","  inflating: mallet-2.0.8/sample-data/web/de/indogermanische.txt  \n","  inflating: mallet-2.0.8/sample-data/web/de/konrad.txt  \n","  inflating: mallet-2.0.8/sample-data/web/de/marcellinus.txt  \n","  inflating: mallet-2.0.8/sample-data/web/de/rostock.txt  \n","  inflating: mallet-2.0.8/sample-data/web/de/sadat.txt  \n","  inflating: mallet-2.0.8/sample-data/web/de/t40.txt  \n","  inflating: mallet-2.0.8/sample-data/web/de/ulrich.txt  \n","  inflating: mallet-2.0.8/sample-data/web/de/wildenstein.txt  \n","   creating: mallet-2.0.8/sample-data/web/en/\n","  inflating: mallet-2.0.8/sample-data/web/en/elizabeth_needham.txt  \n","  inflating: mallet-2.0.8/sample-data/web/en/equipartition_theorem.txt  \n","  inflating: mallet-2.0.8/sample-data/web/en/gunnhild.txt  \n","  inflating: mallet-2.0.8/sample-data/web/en/hawes.txt  \n","  inflating: mallet-2.0.8/sample-data/web/en/hill.txt  \n","  inflating: mallet-2.0.8/sample-data/web/en/shiloh.txt  \n","  inflating: mallet-2.0.8/sample-data/web/en/sunderland_echo.txt  \n","  inflating: mallet-2.0.8/sample-data/web/en/thespis.txt  \n","  inflating: mallet-2.0.8/sample-data/web/en/thylacine.txt  \n","  inflating: mallet-2.0.8/sample-data/web/en/uranus.txt  \n","  inflating: mallet-2.0.8/sample-data/web/en/yard.txt  \n","  inflating: mallet-2.0.8/sample-data/web/en/zinta.txt  \n","   creating: mallet-2.0.8/src/\n","   creating: mallet-2.0.8/src/cc/\n","   creating: mallet-2.0.8/src/cc/mallet/\n","   creating: mallet-2.0.8/src/cc/mallet/classify/\n","  inflating: mallet-2.0.8/src/cc/mallet/classify/AdaBoost.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/AdaBoostM2.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/AdaBoostM2Trainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/AdaBoostTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/BaggingClassifier.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/BaggingTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/BalancedWinnow.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/BalancedWinnowTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/Boostable.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/C45.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/C45Trainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/Classification.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/Classifier.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/ClassifierAccuracyEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/ClassifierEnsemble.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/ClassifierEnsembleTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/ClassifierEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/ClassifierTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/ConfidencePredictingClassifier.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/ConfidencePredictingClassifierTrainer.java  \n","   creating: mallet-2.0.8/src/cc/mallet/classify/constraints/\n","   creating: mallet-2.0.8/src/cc/mallet/classify/constraints/ge/\n","  inflating: mallet-2.0.8/src/cc/mallet/classify/constraints/ge/MaxEntFLGEConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/constraints/ge/MaxEntGEConstraint.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/constraints/ge/MaxEntKLFLGEConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/constraints/ge/MaxEntL2FLGEConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/constraints/ge/MaxEntRangeL2FLGEConstraints.java  \n","   creating: mallet-2.0.8/src/cc/mallet/classify/constraints/pr/\n","  inflating: mallet-2.0.8/src/cc/mallet/classify/constraints/pr/MaxEntFLPRConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/constraints/pr/MaxEntL2FLPRConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/constraints/pr/MaxEntPRConstraint.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/DecisionTree.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/DecisionTreeTrainer.java  \n","   creating: mallet-2.0.8/src/cc/mallet/classify/evaluate/\n","  inflating: mallet-2.0.8/src/cc/mallet/classify/evaluate/AccuracyCoverage.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/evaluate/ConfusionMatrix.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/evaluate/Graph.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/evaluate/Graph2.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/evaluate/GraphItem.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/evaluate/package.html  \n","   creating: mallet-2.0.8/src/cc/mallet/classify/examples/\n","  inflating: mallet-2.0.8/src/cc/mallet/classify/examples/DocumentClassifier.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/examples/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/FeatureConstraintUtil.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/FeatureSelectingClassifierTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MaxEnt.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MaxEntGERangeTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MaxEntGETrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MaxEntL1Trainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MaxEntOptimizableByGE.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MaxEntOptimizableByLabelDistribution.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MaxEntOptimizableByLabelLikelihood.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MaxEntPRTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MaxEntTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MCMaxEnt.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MCMaxEntTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MostFrequentClassAssignmentTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/MostFrequentClassifier.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/NaiveBayes.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/NaiveBayesEMTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/NaiveBayesTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/PRAuxClassifier.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/PRAuxClassifierOptimizable.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/RandomAssignmentTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/RandomClassifier.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/RankMaxEnt.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/RankMaxEntTrainer.java  \n","   creating: mallet-2.0.8/src/cc/mallet/classify/tests/\n","   creating: mallet-2.0.8/src/cc/mallet/classify/tests/NaiveBayesData/\n","   creating: mallet-2.0.8/src/cc/mallet/classify/tests/NaiveBayesData/learn/\n","   creating: mallet-2.0.8/src/cc/mallet/classify/tests/NaiveBayesData/learn/a/\n"," extracting: mallet-2.0.8/src/cc/mallet/classify/tests/NaiveBayesData/learn/a/m1  \n"," extracting: mallet-2.0.8/src/cc/mallet/classify/tests/NaiveBayesData/learn/a/ma  \n","   creating: mallet-2.0.8/src/cc/mallet/classify/tests/NaiveBayesData/learn/b/\n"," extracting: mallet-2.0.8/src/cc/mallet/classify/tests/NaiveBayesData/learn/b/m2  \n","   creating: mallet-2.0.8/src/cc/mallet/classify/tests/NaiveBayesData/update/\n","   creating: mallet-2.0.8/src/cc/mallet/classify/tests/NaiveBayesData/update/b/\n"," extracting: mallet-2.0.8/src/cc/mallet/classify/tests/NaiveBayesData/update/b/m3  \n"," extracting: mallet-2.0.8/src/cc/mallet/classify/tests/NaiveBayesData/update/b/m4  \n"," extracting: mallet-2.0.8/src/cc/mallet/classify/tests/NaiveBayesData/update/b/m5  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tests/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tests/TestClassifiers.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tests/TestMaxEntTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tests/TestNaiveBayes.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tests/TestStaticParameters.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/Trial.java  \n","   creating: mallet-2.0.8/src/cc/mallet/classify/tui/\n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/Calo2Classify.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/Classifier2Info.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/Csv2Classify.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/Csv2Vectors.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/SvmLight2Classify.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/SvmLight2Vectors.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/Text2Classify.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/Text2Vectors.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/TODO  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/Vectors2Classify.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/Vectors2FeatureConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/Vectors2Info.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/tui/Vectors2Vectors.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/Winnow.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/classify/WinnowTrainer.java  \n","   creating: mallet-2.0.8/src/cc/mallet/cluster/\n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/Clusterer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/Clustering.java  \n","   creating: mallet-2.0.8/src/cc/mallet/cluster/clustering_scorer/\n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/clustering_scorer/ClusteringScorer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/clustering_scorer/PairwiseScorer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/Clusterings.java  \n","   creating: mallet-2.0.8/src/cc/mallet/cluster/evaluate/\n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/evaluate/AccuracyEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/evaluate/BCubedEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/evaluate/ClusteringEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/evaluate/ClusteringEvaluators.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/evaluate/MUCEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/evaluate/PairF1Evaluator.java  \n","   creating: mallet-2.0.8/src/cc/mallet/cluster/evaluate/tests/\n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/evaluate/tests/TestClusteringEvaluators.java  \n","   creating: mallet-2.0.8/src/cc/mallet/cluster/examples/\n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/examples/FirstOrderClusterExample.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/GreedyAgglomerative.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/GreedyAgglomerativeByDensity.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/HillClimbingClusterer.java  \n","   creating: mallet-2.0.8/src/cc/mallet/cluster/iterator/\n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/iterator/AllPairsIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/iterator/ClusterSampleIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/iterator/NeighborIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/iterator/NodeClusterSampleIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/iterator/PairSampleIterator.java  \n","   creating: mallet-2.0.8/src/cc/mallet/cluster/iterator/tests/\n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/iterator/tests/TestIterators.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/KBestClusterer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/KMeans.java  \n","   creating: mallet-2.0.8/src/cc/mallet/cluster/neighbor_evaluator/\n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/neighbor_evaluator/AgglomerativeNeighbor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/neighbor_evaluator/ClassifyingNeighborEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/neighbor_evaluator/MedoidEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/neighbor_evaluator/Neighbor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/neighbor_evaluator/NeighborEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/neighbor_evaluator/PairwiseEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/neighbor_evaluator/RandomEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/neighbor_evaluator/RankingNeighborEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/README  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/Record.java  \n","   creating: mallet-2.0.8/src/cc/mallet/cluster/tui/\n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/tui/Clusterings2Clusterer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/tui/Clusterings2Clusterings.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/tui/Clusterings2Info.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/tui/Text2Clusterings.java  \n","   creating: mallet-2.0.8/src/cc/mallet/cluster/util/\n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/util/ClusterUtils.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/cluster/util/PairwiseMatrix.java  \n","   creating: mallet-2.0.8/src/cc/mallet/examples/\n","  inflating: mallet-2.0.8/src/cc/mallet/examples/TestCRFPipe.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/examples/TopicModel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/examples/TrainCRF.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/examples/TrainHMM.java  \n","   creating: mallet-2.0.8/src/cc/mallet/extract/\n","  inflating: mallet-2.0.8/src/cc/mallet/extract/AccuracyCoverageEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/BIOTokenizationFilter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/BIOTokenizationFilterWithTokenIndices.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/ConfidenceTokenizationFilter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/CRFExtractor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/DefaultTokenizationFilter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/DocumentExtraction.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/DocumentViewer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/ExactMatchComparator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/Extraction.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/ExtractionConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/ExtractionEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/Extractor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/Field.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/FieldCleaner.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/FieldComparator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/HierarchicalTokenizationFilter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/LabeledSpan.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/LabeledSpans.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/LatticeViewer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/PerDocumentF1Evaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/PerFieldF1Evaluator.java  \n","   creating: mallet-2.0.8/src/cc/mallet/extract/pipe/\n","  inflating: mallet-2.0.8/src/cc/mallet/extract/pipe/TokenSequence2Tokenization.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/PunctuationIgnoringComparator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/Record.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/RegexFieldCleaner.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/Span.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/StringSpan.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/StringTokenization.java  \n","   creating: mallet-2.0.8/src/cc/mallet/extract/test/\n","  inflating: mallet-2.0.8/src/cc/mallet/extract/test/TestDocumentExtraction.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/test/TestDocumentViewer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/test/TestLatticeViewer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/test/TestPerDocumentF1Evaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/Tokenization.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/TokenizationFilter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/extract/TransducerExtractionConfidenceEstimator.java  \n","   creating: mallet-2.0.8/src/cc/mallet/fst/\n","  inflating: mallet-2.0.8/src/cc/mallet/fst/CacheStaleIndicator.java  \n","   creating: mallet-2.0.8/src/cc/mallet/fst/confidence/\n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/ConfidenceCorrectorEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/ConfidenceEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/ConstrainedForwardBackwardConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/ConstrainedViterbiTransducerCorrector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/GammaAverageConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/GammaProductConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/InstanceWithConfidence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/IsolatedSegmentTransducerCorrector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/MaxEntConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/MaxEntSequenceConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/MinSegmentConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/NBestViterbiConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/PipedInstanceWithConfidence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/QBCSequenceConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/RandomConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/RandomSequenceConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/SegmentProductConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/SequenceConfidenceInstance.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/TransducerConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/TransducerCorrector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/TransducerSequenceConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/ViterbiConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/confidence/ViterbiRatioConfidenceEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/CRF.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/CRFCacheStaleIndicator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/CRFOptimizableByBatchLabelLikelihood.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/CRFOptimizableByGradientValues.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/CRFOptimizableByLabelLikelihood.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/CRFTrainerByL1LabelLikelihood.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/CRFTrainerByLabelLikelihood.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/CRFTrainerByStochasticGradient.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/CRFTrainerByThreadedLabelLikelihood.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/CRFTrainerByValueGradients.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/CRFWriter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/FeatureTransducer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/HMM.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/HMMTrainerByLikelihood.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/InstanceAccuracyEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/LabelDistributionEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/MaxLattice.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/MaxLatticeDefault.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/MaxLatticeFactory.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/MEMM.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/MEMMTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/MultiSegmentationEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/NoopTransducerTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/PerClassAccuracyEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/Segment.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/SegmentationEvaluator.java  \n","   creating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/\n","   creating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/constraints/\n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/constraints/GEConstraint.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/constraints/OneLabelGEConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/constraints/OneLabelKLGEConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/constraints/OneLabelL2GEConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/constraints/OneLabelL2RangeGEConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/constraints/SelfTransitionGEConstraint.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/constraints/TwoLabelGEConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/constraints/TwoLabelKLGEConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/constraints/TwoLabelL2GEConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/CRFOptimizableByEntropyRegularization.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/CRFOptimizableByGE.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/CRFTrainerByEntropyRegularization.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/CRFTrainerByGE.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/CRFTrainerByLikelihoodAndGE.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/EntropyLattice.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/FSTConstraintUtil.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/GELattice.java  \n","   creating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/\n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/CachedDotTransitionIterator.java  \n","   creating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/constraints/\n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/constraints/OneLabelL2IndPRConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/constraints/OneLabelL2PRConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/constraints/PRConstraint.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/ConstraintsOptimizableByPR.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/CRFOptimizableByKL.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/CRFTrainerByPR.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/PRAuxiliaryModel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/SumLatticeDefaultCachedDot.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/SumLatticeKL.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/pr/SumLatticePR.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/StateLabelMap.java  \n","   creating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/tui/\n","  inflating: mallet-2.0.8/src/cc/mallet/fst/semi_supervised/tui/SimpleTaggerWithConstraints.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/ShallowTransducerTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/SimpleTagger.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/SumLattice.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/SumLatticeBeam.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/SumLatticeConstrained.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/SumLatticeDefault.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/SumLatticeFactory.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/SumLatticeScaling.java  \n","   creating: mallet-2.0.8/src/cc/mallet/fst/tests/\n","  inflating: mallet-2.0.8/src/cc/mallet/fst/tests/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/tests/TestCRF.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/tests/TestFeatureTransducer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/tests/TestMEMM.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/tests/TestSumNegLogProb2.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/ThreadedOptimizable.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/TokenAccuracyEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/Transducer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/TransducerEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/TransducerTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/fst/ViterbiWriter.java  \n","   creating: mallet-2.0.8/src/cc/mallet/grmm/\n","   creating: mallet-2.0.8/src/cc/mallet/grmm/examples/\n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/examples/CrossTemplate1.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/examples/ModelReaderExample.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/examples/SimpleCrfExample.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/examples/SimpleFactorExample.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/examples/SimpleGraphExample.java  \n","   creating: mallet-2.0.8/src/cc/mallet/grmm/inference/\n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/AbstractBeliefPropagation.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/AbstractInferencer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/BruteForceInferencer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/ExactSampler.java  \n","   creating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/\n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/AbstractMessageStrategy.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/BPRegionGenerator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/ClusterVariationalRegionGenerator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/FactorizedRegion.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/FullMessageStrategy.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/Kikuchi4SquareRegionGenerator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/MessageArray.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/MessageStrategy.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/ParentChildGBP.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/Region.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/RegionEdge.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/RegionGraph.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/RegionGraphGenerator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/gbp/SparseMessageSender.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/GibbsSampler.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/Inferencer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/JunctionTree.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/JunctionTreeInferencer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/JunctionTreePropagation.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/LoopyBP.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/MessageArray.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/RandomGraphs.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/ResidualBP.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/Sampler.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/SamplingInferencer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/TreeBP.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/TRP.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/Utils.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/inference/VariableElimination.java  \n","   creating: mallet-2.0.8/src/cc/mallet/grmm/learning/\n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/ACRF.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/ACRFEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/AcrfSerialEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/ACRFTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/DefaultAcrfTrainer.java  \n","   creating: mallet-2.0.8/src/cc/mallet/grmm/learning/extract/\n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/extract/ACRFExtractor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/extract/ACRFExtractorTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/extract/AcrfExtractorTui.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/GenericAcrfData2TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/GenericAcrfTui.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/MultiSegmentationEvaluatorACRF.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/PiecewiseACRFTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/PseudolikelihoodACRFTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/PwplACRFTrainer.java  \n","   creating: mallet-2.0.8/src/cc/mallet/grmm/learning/templates/\n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/learning/templates/SimilarTokensTemplate.java  \n","   creating: mallet-2.0.8/src/cc/mallet/grmm/test/\n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestAbstractBeliefPropagation.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestAssignment.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestAssignmentIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestBetaFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestBitVarSet.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestDirectedModel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestFactorGraph.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestFactors.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestGenericAcrfData2TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestGibbsSampler.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestHashClique.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestInference.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestListVarSet.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestLogTableFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestMIntInt2ObjectMap.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestNormalFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestPottsFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestRandomGraphs.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestTableFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestTRP.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestUndirectedModel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestUniformFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/test/TestUniNormalFactor.java  \n","   creating: mallet-2.0.8/src/cc/mallet/grmm/types/\n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/AbstractAssignmentIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/AbstractFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/AbstractTableFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/Assignment.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/AssignmentIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/BetaFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/BidirectionalIntObjectMap.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/BinaryUnaryFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/BitVarSet.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/BoltzmannPairFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/BoltzmannUnaryFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/ConstantFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/CPT.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/DenseAssignmentIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/DirectedModel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/DiscreteFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/Factor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/FactorGraph.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/Factors.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/HashVarSet.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/ListVarSet.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/LogTableFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/NormalFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/ParameterizedFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/PottsTableFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/SkeletonFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/SparseAssignmentIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/TableFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/Tree.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/UndirectedGrid.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/UndirectedModel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/UniformFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/UniNormalFactor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/Universe.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/UnmodifiableVarSet.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/Variable.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/types/VarSet.java  \n","   creating: mallet-2.0.8/src/cc/mallet/grmm/util/\n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/CachingOptimizable.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/CSIntInt2ObjectMultiMap.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/Flops.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/GeneralUtils.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/Graphs.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/LabelsAssignment.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/LabelsSequence2Assignment.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/Matrices.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/MIntInt2ObjectMap.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/ModelReader.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/Models.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/ModelWriter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/PipedIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/RememberTokenizationPipe.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/SliceLabelsSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/grmm/util/THashMultiMap.java  \n","   creating: mallet-2.0.8/src/cc/mallet/optimize/\n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/AGIS.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/BackTrackLineSearch.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/ConjugateGradient.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/GradientAscent.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/GradientBracketLineOptimizer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/InvalidOptimizableException.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/LimitedMemoryBFGS.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/LineOptimizer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/Optimizable.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/OptimizableCollection.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/OptimizationException.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/Optimizer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/OptimizerEvaluator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/OrthantWiseLimitedMemoryBFGS.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/StochasticMetaAscent.java  \n","   creating: mallet-2.0.8/src/cc/mallet/optimize/tests/\n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/tests/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/tests/TestOptimizable.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/optimize/tests/TestOptimizer.java  \n","   creating: mallet-2.0.8/src/cc/mallet/pipe/\n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/AddClassifierTokenPredictions.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Array2FeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/AugmentableFeatureVectorAddConjunctions.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/AugmentableFeatureVectorLogScale.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/BranchingPipe.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/CharSequence2CharNGrams.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/CharSequence2TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/CharSequenceArray2TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/CharSequenceLowercase.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/CharSequenceRemoveHTML.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/CharSequenceRemoveUUEncodedBlocks.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/CharSequenceReplace.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/CharSubsequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Classification2ConfidencePredictingFeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Csv2Array.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Csv2FeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Directory2FileIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/FeatureCountPipe.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/FeatureDocFreqPipe.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/FeatureSequence2AugmentableFeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/FeatureSequence2FeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/FeatureSequenceConvolution.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/FeatureValueString2FeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/FeatureVectorConjunctions.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/FeatureVectorSequence2FeatureVectors.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Filename2CharSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/FilterEmptyFeatureVectors.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/FixedVocabTokenizer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/HACKING  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Input2CharSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/InstanceListTrimFeaturesByCount.java  \n","   creating: mallet-2.0.8/src/cc/mallet/pipe/iterator/\n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/ArrayDataAndTargetIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/ArrayIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/ConcatenatedInstanceIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/CsvIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/DBInstanceIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/EmptyInstanceIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/FileIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/FileListIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/FileUriIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/LineGroupIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/LineIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/ParenGroupIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/PatternMatchIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/PipeExtendedIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/PipeInputIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/RandomFeatureVectorIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/RandomTokenSequenceIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/SegmentIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/SelectiveFileLineIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/SimpleFileLineIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/StringArrayIterator.java  \n","   creating: mallet-2.0.8/src/cc/mallet/pipe/iterator/tests/\n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/tests/TestPatternMatchIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/iterator/UnlabeledFileIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/LineGroupString2TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/MakeAmpersandXMLFriendly.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/NGramPreprocessor.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Noop.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Pipe.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/PipeException.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/PipeUtils.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/PrintInput.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/PrintInputAndTarget.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/PrintTokenSequenceFeatures.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/SaveDataInSource.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/SelectiveSGML2TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/SerialPipes.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/SGML2TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/SimpleTaggerSentence2StringTokenization.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/SimpleTaggerSentence2TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/SimpleTokenizer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/SourceLocation2TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/StringAddNewLineDelimiter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/StringList2FeatureSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/SvmLight2FeatureVectorAndLabel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Target2Double.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Target2FeatureSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Target2Integer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Target2Label.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Target2LabelSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TargetRememberLastLabel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TargetStringToFeatures.java  \n","   creating: mallet-2.0.8/src/cc/mallet/pipe/tests/\n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tests/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tests/TestInstancePipe.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tests/TestIterators.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tests/TestPipeUtils.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tests/TestRainbowStyle.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tests/TestSGML2TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tests/TestSpacePipe.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/Token2FeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TokenSequence2FeatureSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TokenSequence2FeatureSequenceWithBigrams.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TokenSequence2FeatureVectorSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TokenSequence2TokenInstances.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TokenSequenceLowercase.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TokenSequenceMatchDataAndTarget.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TokenSequenceNGrams.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TokenSequenceParseFeatureString.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TokenSequenceRemoveNonAlpha.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TokenSequenceRemoveStopPatterns.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/TokenSequenceRemoveStopwords.java  \n","   creating: mallet-2.0.8/src/cc/mallet/pipe/tsf/\n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/CountMatches.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/CountMatchesAlignedWithOffsets.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/CountMatchesMatching.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/FeaturesInWindow.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/FeaturesOfFirstMention.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/LexiconMembership.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/OffsetConjunctions.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/OffsetFeatureConjunction.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/OffsetPropertyConjunctions.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/RegexMatches.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/SequencePrintingPipe.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/Target2BIOFormat.java  \n","   creating: mallet-2.0.8/src/cc/mallet/pipe/tsf/tests/\n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/tests/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/tests/TestOffsetConjunctions.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/tests/TestOffsetFeatureConjunctions.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/tests/TestSequencePrintingPipe.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/TokenFirstPosition.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/TokenText.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/TokenTextCharNGrams.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/TokenTextCharPrefix.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/TokenTextCharSuffix.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/TokenTextNGrams.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/TrieLexiconMembership.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/tsf/WordVectors.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/pipe/ValueString2FeatureVector.java  \n","   creating: mallet-2.0.8/src/cc/mallet/regression/\n","  inflating: mallet-2.0.8/src/cc/mallet/regression/CoordinateDescent.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/regression/LeastSquares.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/regression/LinearRegression.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/regression/LinearRegressionTrainer.java  \n","   creating: mallet-2.0.8/src/cc/mallet/regression/tui/\n","  inflating: mallet-2.0.8/src/cc/mallet/regression/tui/Regression.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/regression/tui/RegressionImporter.java  \n","   creating: mallet-2.0.8/src/cc/mallet/share/\n","   creating: mallet-2.0.8/src/cc/mallet/share/casutton/\n","   creating: mallet-2.0.8/src/cc/mallet/share/casutton/ner/\n","  inflating: mallet-2.0.8/src/cc/mallet/share/casutton/ner/ConllNer2003Sentence2TokenSequence.java  \n","   creating: mallet-2.0.8/src/cc/mallet/share/mccallum/\n","   creating: mallet-2.0.8/src/cc/mallet/share/mccallum/ner/\n","  inflating: mallet-2.0.8/src/cc/mallet/share/mccallum/ner/ConllNer2003Sentence2TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/share/mccallum/ner/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/share/mccallum/ner/TokenSequenceDocHeader.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/share/mccallum/ner/TUI.java  \n","   creating: mallet-2.0.8/src/cc/mallet/share/upenn/\n","  inflating: mallet-2.0.8/src/cc/mallet/share/upenn/MaxEntShell.java  \n","   creating: mallet-2.0.8/src/cc/mallet/share/upenn/ner/\n","  inflating: mallet-2.0.8/src/cc/mallet/share/upenn/ner/FeatureWindow.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/share/upenn/ner/LengthBins.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/share/upenn/ner/ListMember.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/share/upenn/ner/LongRegexMatches.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/share/upenn/ner/NEPipes.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/share/upenn/package.html  \n","   creating: mallet-2.0.8/src/cc/mallet/share/weili/\n","   creating: mallet-2.0.8/src/cc/mallet/share/weili/ner/\n","   creating: mallet-2.0.8/src/cc/mallet/share/weili/ner/enron/\n","  inflating: mallet-2.0.8/src/cc/mallet/share/weili/ner/enron/EnronMessage2TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/share/weili/ner/enron/TUI.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/share/weili/ner/WordTransformation.java  \n","   creating: mallet-2.0.8/src/cc/mallet/topics/\n","  inflating: mallet-2.0.8/src/cc/mallet/topics/AbstractTopicReports.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/DMROptimizable.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/DMRTopicModel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/HierarchicalLDA.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/HierarchicalPAM.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/JSONTopicReports.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/LabeledLDA.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/LDA.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/LDAHyper.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/LDAStream.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/MarginalProbEstimator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/MultinomialHMM.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/NPTopicModel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/PAM4L.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/ParallelTopicModel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/PolylingualTopicModel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/RTopicModel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/SimpleLDA.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/TopicalNGrams.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/TopicAssignment.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/TopicInferencer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/TopicModelDiagnostics.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/TopicReports.java  \n","   creating: mallet-2.0.8/src/cc/mallet/topics/tui/\n","  inflating: mallet-2.0.8/src/cc/mallet/topics/tui/DMRLoader.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/tui/EvaluateTopics.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/tui/HierarchicalLDATUI.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/tui/InferTopics.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/tui/TopicTrainer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/tui/Vectors2Topics.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/WeightedTopicModel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/WordEmbeddingRunnable.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/WordEmbeddings.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/topics/WorkerRunnable.java  \n","   creating: mallet-2.0.8/src/cc/mallet/types/\n","  inflating: mallet-2.0.8/src/cc/mallet/types/Alphabet.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/AlphabetCarrying.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/AlphabetFactory.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/ArrayListSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/ArraySequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/AugmentableFeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/BiNormalSeparation.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/CachedMetric.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/ChainedInstanceIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/ConstantMatrix.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/CrossValidationIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/DenseMatrix.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/DenseVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Dirichlet.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/EuclideanDistance.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/ExpGain.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/FeatureConjunction.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/FeatureCounter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/FeatureCounts.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/FeatureInducer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/FeatureSelection.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/FeatureSelector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/FeatureSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/FeatureSequenceWithBigrams.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/FeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/FeatureVectorSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/GainRatio.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/GradientGain.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/HashedSparseVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/IDSorter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/IndexedSparseVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/InfiniteDistance.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/InfoGain.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Instance.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/InstanceList.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/InstanceListTUI.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/InvertedIndex.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/KLGain.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Label.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/LabelAlphabet.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Labeler.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Labeling.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Labelings.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Labels.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/LabelSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/LabelsSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/LabelVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/LogNumber.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/ManhattenDistance.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Matrix.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Matrix2.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Matrixn.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/MatrixOps.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Metric.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Minkowski.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/MultiInstanceList.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Multinomial.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/NormalizedDotProductMetric.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/NullLabel.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/PagedInstanceList.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/PartiallyRankedFeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/PerLabelFeatureCounts.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/PerLabelInfoGain.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/PropertyHolder.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/RankedFeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/ROCData.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Sequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/SequencePair.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/SequencePairAlignment.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/SingleInstanceIterator.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/SparseMatrixn.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/SparseVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/StringEditFeatureVectorSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/StringEditVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/StringKernel.java  \n","   creating: mallet-2.0.8/src/cc/mallet/types/tests/\n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestAlphabet.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestAugmentableFeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestBiNormalSeparation.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestFeatureSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestFeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestHashedSparseVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestIndexedSparseVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestInstanceListWeights.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestLabelAlphabet.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestLabelsSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestLabelVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestMatrix.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestMatrixn.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestMultinomial.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestPagedInstanceList.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestRankedFeatureVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestSerializable.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestSparseMatrixn.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestSparseVector.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/tests/TestToken.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Token.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/TokenSequence.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/types/Vector.java  \n","   creating: mallet-2.0.8/src/cc/mallet/util/\n","  inflating: mallet-2.0.8/src/cc/mallet/util/Addable.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/ArrayListUtils.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/ArrayUtils.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/BshInterpreter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/BulkLoader.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/CharSequenceLexer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/CollectionUtils.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/ColorUtils.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/CommandOption.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/DBBulkLoader.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/DBInstanceStore.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/DirectoryFilter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/DocumentLengths.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/DoubleList.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/FeatureCooccurrenceCounter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/FeatureCountTool.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/FileUtils.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/InstanceListPrinter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/IoUtils.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/Lexer.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/MalletLogger.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/MalletProgressMessageLogger.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/Maths.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/MVNormal.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/PlainLogFormatter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/PrintUtilities.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/ProgressMessageLogFormatter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/ProgressMessageLogRecord.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/PropertyList.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/Randoms.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/RegexFileFilter.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/Replacement.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/Replacer.java  \n","   creating: mallet-2.0.8/src/cc/mallet/util/resources/\n","  inflating: mallet-2.0.8/src/cc/mallet/util/resources/logging.properties  \n","   creating: mallet-2.0.8/src/cc/mallet/util/resources/wn/\n","  inflating: mallet-2.0.8/src/cc/mallet/util/resources/wn/database_properties.xml  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/resources/wn/Examples.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/resources/wn/file_properties.xml  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/resources/wn/jwnl-properties.xml  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/resources/wn/jwnl_properties.dtd  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/resources/wn/jwnl_properties.xsd  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/resources/wn/map_properties.xml  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/resources/wn/README  \n","   creating: mallet-2.0.8/src/cc/mallet/util/search/\n","  inflating: mallet-2.0.8/src/cc/mallet/util/search/AStar.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/search/AStarNode.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/search/AStarState.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/search/MinHeap.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/search/PriorityQueue.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/search/QueueElement.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/search/SearchNode.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/search/SearchState.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/Sequences.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/StatFunctions.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/Strings.java  \n","   creating: mallet-2.0.8/src/cc/mallet/util/tests/\n","  inflating: mallet-2.0.8/src/cc/mallet/util/tests/package.html  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/tests/TestAStar.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/tests/TestMaths.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/tests/TestPriorityQueue.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/tests/TestPropertyList.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/tests/TestRandom.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/tests/TestStrings.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/Timing.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/Univariate.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/UriUtils.java  \n","  inflating: mallet-2.0.8/src/cc/mallet/util/VectorStats.java  \n","   creating: mallet-2.0.8/stoplists/\n","  inflating: mallet-2.0.8/stoplists/de.txt  \n","  inflating: mallet-2.0.8/stoplists/en.txt  \n","  inflating: mallet-2.0.8/stoplists/fi.txt  \n","  inflating: mallet-2.0.8/stoplists/fr.txt  \n","  inflating: mallet-2.0.8/stoplists/jp.txt  \n","  inflating: mallet-2.0.8/stoplists/README  \n","   creating: mallet-2.0.8/test/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MmGslcdMUCVH","colab_type":"code","colab":{}},"source":["MALLET_PATH = '/content/mallet-2.0.8/bin/mallet'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9cOLbgkuVahb","colab_type":"code","outputId":"b6b82257-097b-41af-dfcf-04886e23e171","executionInfo":{"status":"ok","timestamp":1590836666568,"user_tz":-180,"elapsed":376256,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["lda_mallet = gensim.models.wrappers.LdaMallet( mallet_path=MALLET_PATH, corpus=bow_corpus,\n","                                              num_topics=TOTAL_TOPICS, id2word=dictionary,\n","                                               iterations=500, workers=16)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"QBej7zBjZ6-L","colab_type":"text"},"source":["We can now look at the generated topics by leveraging the following code snippet."]},{"cell_type":"code","metadata":{"id":"1Z5ZkyhKVbSf","colab_type":"code","colab":{}},"source":["topics = [[(term, round(wt, 3)) for term, wt in lda_mallet.show_topic(n, topn=20)] for n in range(0, TOTAL_TOPICS)]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OkmjXutOaMIh","colab_type":"code","outputId":"7f073010-67e2-4250-bb40-af0281a162d0","executionInfo":{"status":"ok","timestamp":1590836666572,"user_tz":-180,"elapsed":376250,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":541}},"source":["for idx, topic in enumerate(topics):\n","  print('Topic #'+str(idx+1)+':')\n","  print([term for term, wt in topic])\n","  print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Topic #1:\n","['equation', 'vector', 'matrix', 'dynamic', 'state', 'solution', 'rate', 'convergence', 'eq', 'neuron', 'noise', 'gradient', 'rule', 'energy', 'linear', 'nonlinear', 'theory', 'attractor', 'optimal', 'fixed_point']\n","\n","Topic #2:\n","['signal', 'circuit', 'current', 'neuron', 'chip', 'channel', 'analog', 'voltage', 'noise', 'neural', 'bit', 'source', 'implementation', 'processor', 'design', 'computation', 'gain', 'filter', 'frequency', 'synapse']\n","\n","Topic #3:\n","['training', 'training_set', 'prediction', 'test', 'cluster', 'solution', 'experiment', 'technique', 'average', 'distance', 'search', 'measure', 'clustering', 'optimization', 'size', 'selection', 'cost', 'table', 'procedure', 'local']\n","\n","Topic #4:\n","['class', 'bound', 'tree', 'probability', 'size', 'theorem', 'node', 'linear', 'threshold', 'theory', 'approximation', 'loss', 'defined', 'complexity', 'distribution', 'proof', 'machine', 'sample', 'hypothesis', 'polynomial']\n","\n","Topic #5:\n","['cell', 'neuron', 'response', 'stimulus', 'activity', 'pattern', 'spike', 'synaptic', 'effect', 'cortical', 'et_al', 'neural', 'firing', 'brain', 'connection', 'cortex', 'frequency', 'mechanism', 'simulation', 'signal']\n","\n","Topic #6:\n","['unit', 'pattern', 'layer', 'node', 'hidden_unit', 'rule', 'net', 'architecture', 'representation', 'activation', 'structure', 'memory', 'task', 'training', 'connection', 'recurrent', 'level', 'sequence', 'module', 'back_propagation']\n","\n","Topic #7:\n","['distribution', 'estimate', 'gaussian', 'variable', 'probability', 'prior', 'density', 'sample', 'mixture', 'estimation', 'component', 'variance', 'approximation', 'bayesian', 'noise', 'likelihood', 'log', 'structure', 'step', 'observation']\n","\n","Topic #8:\n","['state', 'control', 'action', 'step', 'task', 'trajectory', 'policy', 'environment', 'controller', 'reinforcement_learning', 'goal', 'optimal', 'path', 'change', 'dynamic', 'robot', 'transition', 'position', 'move', 'current']\n","\n","Topic #9:\n","['training', 'word', 'classifier', 'classification', 'class', 'feature', 'recognition', 'trained', 'speech', 'context', 'sequence', 'experiment', 'character', 'pattern', 'test', 'hmm', 'vector', 'database', 'mlp', 'task']\n","\n","Topic #10:\n","['image', 'object', 'feature', 'motion', 'visual', 'map', 'location', 'direction', 'pixel', 'representation', 'field', 'position', 'region', 'local', 'view', 'face', 'filter', 'target', 'edge', 'surface']\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4gweQKC7ac1-","colab_type":"text"},"source":["We can also evaluate our model using the perplexity and coherence metrics, as we\n","did before."]},{"cell_type":"code","metadata":{"id":"v7oYyZWwZ-X2","colab_type":"code","colab":{}},"source":["cv_coherence_model_lda_mallet = gensim.models. CoherenceModel (model=lda_mallet, corpus=bow_corpus,\n","                                                               texts=norm_corpus_bigrams, dictionary=dictionary,\n","                                                               coherence='c_v')\n","\n","avg_coherence_cv = cv_coherence_model_lda_mallet.get_coherence()\n","\n","umass_coherence_model_lda_mallet = gensim.models. CoherenceModel (model=lda_mallet, corpus=bow_corpus,\n","                                                                   texts=norm_corpus_bigrams, dictionary=dictionary,\n","                                                                  coherence='u_mass') \n","\n","avg_coherence_umass = umass_coherence_model_lda_mallet.get_coherence()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XO9-exGmZ-WF","colab_type":"code","outputId":"9aee6e5f-ac82-4aac-e5e0-8d751d987ea8","executionInfo":{"status":"ok","timestamp":1590836727060,"user_tz":-180,"elapsed":436729,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["# from STDOUT: <500> LL/token: -8.53533\n","perplexity = -8.53533\n","\n","print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n","print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n","print('Model Perplexity:', perplexity)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Avg. Coherence Score (Cv): 0.5126503544321696\n","Avg. Coherence Score (UMass): -1.0730079498955711\n","Model Perplexity: -8.53533\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"edwjGDDjGWvM","colab_type":"code","outputId":"222d56e6-8156-44c5-b430-80f4fc843ba1","executionInfo":{"status":"ok","timestamp":1590836727061,"user_tz":-180,"elapsed":436722,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["lda_mallet"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<gensim.models.wrappers.ldamallet.LdaMallet at 0x7f013f985ac8>"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"SYjgkRPqRXgO","colab_type":"code","outputId":"4c26a38a-bc57-4754-f59f-b069983eca22","executionInfo":{"status":"ok","timestamp":1590836748258,"user_tz":-180,"elapsed":457912,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["lda_mallet[bow_corpus]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[[(0, 0.05555555555555555),\n","  (1, 0.11454046639231824),\n","  (2, 0.04046639231824417),\n","  (3, 0.41649519890260633),\n","  (4, 0.08864883401920438),\n","  (5, 0.08007544581618656),\n","  (6, 0.061213991769547324),\n","  (7, 0.0913923182441701),\n","  (8, 0.022290809327846366),\n","  (9, 0.029320987654320986)],\n"," [(0, 0.49514939852541706),\n","  (1, 0.059888759539516216),\n","  (2, 0.07023670935195962),\n","  (3, 0.07838571982925882),\n","  (4, 0.03686457120682964),\n","  (5, 0.10412624498771178),\n","  (6, 0.018755659035053676),\n","  (7, 0.0662268787996378),\n","  (8, 0.018108912171775963),\n","  (9, 0.0522571465528392)],\n"," [(0, 0.25919793966151583),\n","  (1, 0.1938925680647535),\n","  (2, 0.02823767476085357),\n","  (3, 0.07036423841059603),\n","  (4, 0.08857615894039735),\n","  (5, 0.1364054451802796),\n","  (6, 0.01894775570272259),\n","  (7, 0.014440765268579838),\n","  (8, 0.17062178072111847),\n","  (9, 0.019315673289183224)],\n"," [(0, 0.20686374447842337),\n","  (1, 0.14977913693510023),\n","  (2, 0.02215426435609922),\n","  (3, 0.3921848453958546),\n","  (4, 0.03771661569826707),\n","  (5, 0.12205232755691471),\n","  (6, 0.010805300713557594),\n","  (7, 0.02833843017329256),\n","  (8, 0.01073734284743459),\n","  (9, 0.019367991845056064)],\n"," [(0, 0.12716763005780346),\n","  (1, 0.031791907514450865),\n","  (2, 0.0905587668593449),\n","  (3, 0.10308285163776493),\n","  (4, 0.03516377649325626),\n","  (5, 0.3052344251766217),\n","  (6, 0.12893384714193962),\n","  (7, 0.0728965960179833),\n","  (8, 0.08076429030186255),\n","  (9, 0.024405908798972382)],\n"," [(0, 0.049985532407407406),\n","  (1, 0.017361111111111112),\n","  (2, 0.05056423611111111),\n","  (3, 0.021122685185185185),\n","  (4, 0.4854600694444444),\n","  (5, 0.025535300925925927),\n","  (6, 0.013454861111111112),\n","  (7, 0.022858796296296297),\n","  (8, 0.008318865740740741),\n","  (9, 0.3053385416666667)],\n"," [(0, 0.33771765000626336),\n","  (1, 0.022172115745960168),\n","  (2, 0.14418138544406867),\n","  (3, 0.02618063384692472),\n","  (4, 0.032569209570336974),\n","  (5, 0.35901290241763756),\n","  (6, 0.018288863835650763),\n","  (7, 0.024802705749718153),\n","  (8, 0.01991732431416761),\n","  (9, 0.015157209069272206)],\n"," [(0, 0.025461443058291386),\n","  (1, 0.19610418034580873),\n","  (2, 0.011381046180783543),\n","  (3, 0.020573429634493327),\n","  (4, 0.6512001167286788),\n","  (5, 0.024367111694754508),\n","  (6, 0.02035456336178595),\n","  (7, 0.02560735390676297),\n","  (8, 0.007952141241701322),\n","  (9, 0.016998613846939525)],\n"," [(0, 0.07357859531772576),\n","  (1, 0.08175399479747306),\n","  (2, 0.018023039762170196),\n","  (3, 0.07775919732441472),\n","  (4, 0.3634336677814939),\n","  (5, 0.01244890375325158),\n","  (6, 0.10126347082868822),\n","  (7, 0.026477146042363432),\n","  (8, 0.22398736529171312),\n","  (9, 0.021274619100706056)],\n"," [(0, 0.1201463560334528),\n","  (1, 0.3793309438470729),\n","  (2, 0.04771505376344086),\n","  (3, 0.015008960573476702),\n","  (4, 0.2193847072879331),\n","  (5, 0.013963560334528077),\n","  (6, 0.018145161290322582),\n","  (7, 0.01978793309438471),\n","  (8, 0.14396654719235363),\n","  (9, 0.022550776583034646)],\n"," [(0, 0.0281881473934454),\n","  (1, 0.05145185939887925),\n","  (2, 0.038376634403124464),\n","  (3, 0.019358125318390217),\n","  (4, 0.7087224769343974),\n","  (5, 0.06294220863757287),\n","  (6, 0.014207279107941357),\n","  (7, 0.029490009622459946),\n","  (8, 0.024056149883964446),\n","  (9, 0.02320710929982453)],\n"," [(0, 0.09802686817800166),\n","  (1, 0.3731458158410299),\n","  (2, 0.022460117548278754),\n","  (3, 0.04617968094038622),\n","  (4, 0.012244612370556952),\n","  (5, 0.3484466834592779),\n","  (6, 0.010775258886090119),\n","  (7, 0.03820319059613769),\n","  (8, 0.01014553596417576),\n","  (9, 0.04037223621606492)],\n"," [(0, 0.18432883750802825),\n","  (1, 0.10982658959537572),\n","  (2, 0.05443159922928709),\n","  (3, 0.4844251766217084),\n","  (4, 0.012524084778420038),\n","  (5, 0.03596660244059088),\n","  (6, 0.02119460500963391),\n","  (7, 0.051862556197816316),\n","  (8, 0.016377649325626204),\n","  (9, 0.029062299293513165)],\n"," [(0, 0.0395738203957382),\n","  (1, 0.07559614408929477),\n","  (2, 0.03716387620497209),\n","  (3, 0.03056823947234906),\n","  (4, 0.047691527143581935),\n","  (5, 0.13685946220192796),\n","  (6, 0.015854895991882292),\n","  (7, 0.03462709284627093),\n","  (8, 0.5299340436326738),\n","  (9, 0.05213089802130898)],\n"," [(0, 0.008809320829781187),\n","  (1, 0.214170692431562),\n","  (2, 0.017997537179122856),\n","  (3, 0.0508667235009946),\n","  (4, 0.5847305105617127),\n","  (5, 0.02889078336648669),\n","  (6, 0.016860850620441414),\n","  (7, 0.05380316377758833),\n","  (8, 0.013545514824287203),\n","  (9, 0.010324902908023112)],\n"," [(0, 0.36326058201058203),\n","  (1, 0.07027116402116403),\n","  (2, 0.03571428571428571),\n","  (3, 0.09292328042328042),\n","  (4, 0.17708333333333334),\n","  (5, 0.023148148148148147),\n","  (6, 0.022982804232804233),\n","  (7, 0.15790343915343916),\n","  (8, 0.030753968253968252),\n","  (9, 0.02595899470899471)],\n"," [(0, 0.04627343392775491),\n","  (1, 0.5606767261088248),\n","  (2, 0.01527206218564243),\n","  (3, 0.02103337905807041),\n","  (4, 0.20265203475080013),\n","  (5, 0.04087791495198902),\n","  (6, 0.013625971650663005),\n","  (7, 0.041883859167809775),\n","  (8, 0.024508459076360303),\n","  (9, 0.03319615912208504)],\n"," [(0, 0.08383635144198524),\n","  (1, 0.10261569416498995),\n","  (2, 0.1889112452492734),\n","  (3, 0.19919517102615694),\n","  (4, 0.029063268499888217),\n","  (5, 0.2074670243684328),\n","  (6, 0.06952828079588642),\n","  (7, 0.02638050525374469),\n","  (8, 0.07131678962664878),\n","  (9, 0.02168566957299352)],\n"," [(0, 0.521081941129674),\n","  (1, 0.06709095730575446),\n","  (2, 0.08697958101299391),\n","  (3, 0.03248475205515779),\n","  (4, 0.0615221426677274),\n","  (5, 0.13431450543622384),\n","  (6, 0.014717581543357202),\n","  (7, 0.02121453195438876),\n","  (8, 0.04892601431980908),\n","  (9, 0.011667992574913818)],\n"," [(0, 0.0732170802593338),\n","  (1, 0.144086742678292),\n","  (2, 0.04761904761904762),\n","  (3, 0.04158283031522469),\n","  (4, 0.5083836351441986),\n","  (5, 0.0261569416498994),\n","  (6, 0.0466130114017438),\n","  (7, 0.028504359490274987),\n","  (8, 0.05555555555555556),\n","  (9, 0.02828079588642969)],\n"," [(0, 0.10650488709133805),\n","  (1, 0.09762947983372655),\n","  (2, 0.082687338501292),\n","  (3, 0.03123244579260757),\n","  (4, 0.1559375351084148),\n","  (5, 0.03640040444893832),\n","  (6, 0.01752612066059993),\n","  (7, 0.2961465003932142),\n","  (8, 0.029996629592180656),\n","  (9, 0.1459386585776879)],\n"," [(0, 0.051290054099042866),\n","  (1, 0.09300873907615481),\n","  (2, 0.01674989596337911),\n","  (3, 0.024968789013732832),\n","  (4, 0.6429463171036205),\n","  (5, 0.07168123179359134),\n","  (6, 0.033395755305867664),\n","  (7, 0.028297960882230546),\n","  (8, 0.015085310029130253),\n","  (9, 0.022575946733250103)],\n"," [(0, 0.13618618618618616),\n","  (1, 0.035435435435435425),\n","  (2, 0.034234234234234225),\n","  (3, 0.02162162162162162),\n","  (4, 0.33798798798798796),\n","  (5, 0.2573573573573573),\n","  (6, 0.01336336336336336),\n","  (7, 0.05210210210210209),\n","  (8, 0.09564564564564562),\n","  (9, 0.016066066066066063)],\n"," [(0, 0.31001622060016226),\n","  (1, 0.04622871046228711),\n","  (2, 0.04339010543390106),\n","  (3, 0.15896188158961885),\n","  (4, 0.016524736415247366),\n","  (5, 0.19049067315490675),\n","  (6, 0.014598540145985403),\n","  (7, 0.013990267639902678),\n","  (8, 0.030920519059205193),\n","  (9, 0.17487834549878348)],\n"," [(0, 0.14982183349530287),\n","  (1, 0.03385163589245222),\n","  (2, 0.031098153547133137),\n","  (3, 0.09912536443148688),\n","  (4, 0.043245869776482024),\n","  (5, 0.343699384515711),\n","  (6, 0.21201814058956917),\n","  (7, 0.041626174279235505),\n","  (8, 0.02105604146420473),\n","  (9, 0.024457402008422415)],\n"," [(0, 0.06844271171756552),\n","  (1, 0.16475344740451953),\n","  (2, 0.1146487618222511),\n","  (3, 0.2742040285899935),\n","  (4, 0.06663778788535124),\n","  (5, 0.19673669771135657),\n","  (6, 0.020215146920799943),\n","  (7, 0.013789618078117103),\n","  (8, 0.04006930907515703),\n","  (9, 0.040502490794888456)],\n"," [(0, 0.15059445178335532),\n","  (1, 0.05915162189931013),\n","  (2, 0.24233083810362538),\n","  (3, 0.049757815940114476),\n","  (4, 0.025832966387788044),\n","  (5, 0.16996917657419636),\n","  (6, 0.10538676060472624),\n","  (7, 0.017906942609716713),\n","  (8, 0.0654630852781447),\n","  (9, 0.11360634081902243)],\n"," [(0, 0.1982461050471126),\n","  (1, 0.020151133501259445),\n","  (2, 0.0700625058307678),\n","  (3, 0.031159623099169698),\n","  (4, 0.019684672077619182),\n","  (5, 0.4880119414124452),\n","  (6, 0.05690829368411232),\n","  (7, 0.03554436048138819),\n","  (8, 0.010355443604813882),\n","  (9, 0.06987592126131169)],\n"," [(0, 0.012762159750111558),\n","  (1, 0.10004462293618921),\n","  (2, 0.009281570727353862),\n","  (3, 0.009727800089245875),\n","  (4, 0.5958054439982152),\n","  (5, 0.026773761713520753),\n","  (6, 0.009638554216867472),\n","  (7, 0.19794734493529678),\n","  (8, 0.011780455153949132),\n","  (9, 0.026238286479250337)],\n"," [(0, 0.11488314883148831),\n","  (1, 0.45510455104551045),\n","  (2, 0.018040180401804017),\n","  (3, 0.04264042640426404),\n","  (4, 0.08126281262812628),\n","  (5, 0.03952439524395244),\n","  (6, 0.00959409594095941),\n","  (7, 0.024928249282492824),\n","  (8, 0.03575235752357524),\n","  (9, 0.17826978269782698)],\n"," [(0, 0.4414239482200647),\n","  (1, 0.30032362459546924),\n","  (2, 0.039697950377562026),\n","  (3, 0.05911542610571737),\n","  (4, 0.031067961165048542),\n","  (5, 0.020496224379719527),\n","  (6, 0.03559870550161812),\n","  (7, 0.027831715210355986),\n","  (8, 0.018338727076591153),\n","  (9, 0.02610571736785329)],\n"," [(0, 0.018674136321195144),\n","  (1, 0.05280630770826849),\n","  (2, 0.029878618113912233),\n","  (3, 0.0196078431372549),\n","  (4, 0.49133727565100116),\n","  (5, 0.03153854134246291),\n","  (6, 0.01514679946052495),\n","  (7, 0.15800394231766782),\n","  (8, 0.031746031746031744),\n","  (9, 0.15126050420168066)],\n"," [(0, 0.41459268325853393),\n","  (1, 0.1002311981504148),\n","  (2, 0.025431796545627636),\n","  (3, 0.07765537875696994),\n","  (4, 0.035835713314293485),\n","  (5, 0.25982592139262883),\n","  (6, 0.029579763361893104),\n","  (7, 0.014143886848905208),\n","  (8, 0.03352373181014552),\n","  (9, 0.009179926560587515)],\n"," [(0, 0.5327211053197651),\n","  (1, 0.07702490278811948),\n","  (2, 0.02912219740216762),\n","  (3, 0.11665425663936461),\n","  (4, 0.06569041118557128),\n","  (5, 0.05179118060726401),\n","  (6, 0.023827252419955328),\n","  (7, 0.07454289732770747),\n","  (8, 0.017704972284272362),\n","  (9, 0.010920824025812858)],\n"," [(0, 0.2095368201925579),\n","  (1, 0.05418943533697632),\n","  (2, 0.25032526671870936),\n","  (3, 0.07780379911527452),\n","  (4, 0.012945615404631797),\n","  (5, 0.24902419984387197),\n","  (6, 0.037991152745251104),\n","  (7, 0.019971376528753578),\n","  (8, 0.05549050221181369),\n","  (9, 0.03272183190215977)],\n"," [(0, 0.06906565656565657),\n","  (1, 0.045075757575757575),\n","  (2, 0.023989898989898988),\n","  (3, 0.02904040404040404),\n","  (4, 0.42853535353535355),\n","  (5, 0.036616161616161616),\n","  (6, 0.02361111111111111),\n","  (7, 0.11477272727272728),\n","  (8, 0.05568181818181818),\n","  (9, 0.1736111111111111)],\n"," [(0, 0.37290469348659006),\n","  (1, 0.06381704980842912),\n","  (2, 0.16403256704980843),\n","  (3, 0.06202107279693487),\n","  (4, 0.048670977011494254),\n","  (5, 0.08465038314176246),\n","  (6, 0.02819683908045977),\n","  (7, 0.061242816091954026),\n","  (8, 0.020114942528735632),\n","  (9, 0.09434865900383142)],\n"," [(0, 0.17457658663688816),\n","  (1, 0.02766921024877474),\n","  (2, 0.10223959302686272),\n","  (3, 0.055462497673552956),\n","  (4, 0.27377628885166577),\n","  (5, 0.045908555121285444),\n","  (6, 0.04330293442521249),\n","  (7, 0.049134561697375775),\n","  (8, 0.013090142068366525),\n","  (9, 0.21483963025001554)],\n"," [(0, 0.03436941764916042),\n","  (1, 0.08538763844230084),\n","  (2, 0.008788853161843517),\n","  (3, 0.0481600571632726),\n","  (4, 0.04187209717756342),\n","  (5, 0.512325830653805),\n","  (6, 0.037799214005001795),\n","  (7, 0.14712397284744555),\n","  (8, 0.07809932118613792),\n","  (9, 0.006073597713469097)],\n"," [(0, 0.0813030638612034),\n","  (1, 0.6184016242155776),\n","  (2, 0.03765227021040975),\n","  (3, 0.01153562200073828),\n","  (4, 0.022517534145441123),\n","  (5, 0.07936507936507936),\n","  (6, 0.010705057216685123),\n","  (7, 0.015042451088962717),\n","  (8, 0.0945921004060539),\n","  (9, 0.028885197489848653)],\n"," [(0, 0.5384615384615384),\n","  (1, 0.23931623931623933),\n","  (2, 0.02744904667981591),\n","  (3, 0.02465483234714004),\n","  (4, 0.05958251150558843),\n","  (5, 0.030982905982905984),\n","  (6, 0.010108481262327416),\n","  (7, 0.011259040105193952),\n","  (8, 0.00977975016436555),\n","  (9, 0.048405654174884946)],\n"," [(0, 0.43700282268411605),\n","  (1, 0.1370284834488068),\n","  (2, 0.06244119408091695),\n","  (3, 0.10580788640834832),\n","  (4, 0.03053630998203747),\n","  (5, 0.1274484646309127),\n","  (6, 0.015054315285262168),\n","  (7, 0.017449319989735698),\n","  (8, 0.039346505859207945),\n","  (9, 0.027884697630656062)],\n"," [(0, 0.019365895458440446),\n","  (1, 0.0094830048557555),\n","  (2, 0.025649814338760353),\n","  (3, 0.01633818908883176),\n","  (4, 0.07860611253927449),\n","  (5, 0.2453584690088546),\n","  (6, 0.009597257926306769),\n","  (7, 0.3592687803484719),\n","  (8, 0.012910596972293631),\n","  (9, 0.22342187946301056)],\n"," [(0, 0.536970062286518),\n","  (1, 0.026321076953988345),\n","  (2, 0.03576451677717501),\n","  (3, 0.24241510950371709),\n","  (4, 0.011854530841872614),\n","  (5, 0.03385573638738196),\n","  (6, 0.03767329716696805),\n","  (7, 0.016877637130801686),\n","  (8, 0.015973477998794453),\n","  (9, 0.0422945549527828)],\n"," [(0, 0.07256605139341296),\n","  (1, 0.7495475931958017),\n","  (2, 0.017281939920376403),\n","  (3, 0.013662685486789722),\n","  (4, 0.0494028230184582),\n","  (5, 0.02252985884907709),\n","  (6, 0.016558089033659067),\n","  (7, 0.022348896127397756),\n","  (8, 0.019363011219688746),\n","  (9, 0.0167390517553384)],\n"," [(0, 0.0976102808461483),\n","  (1, 0.5396000288787813),\n","  (2, 0.014511587611002815),\n","  (3, 0.02238105551945708),\n","  (4, 0.09111255505017689),\n","  (5, 0.10677929391379684),\n","  (6, 0.0100353765071114),\n","  (7, 0.08620316222655404),\n","  (8, 0.016821890116237095),\n","  (9, 0.014944769330734242)],\n"," [(0, 0.41147741147741135),\n","  (1, 0.13091846425179757),\n","  (2, 0.023199023199023196),\n","  (3, 0.0875050875050875),\n","  (4, 0.12291412291412289),\n","  (5, 0.07393840727174059),\n","  (6, 0.04965404965404964),\n","  (7, 0.02889702889702889),\n","  (8, 0.014787681454348117),\n","  (9, 0.05670872337539003)],\n"," [(0, 0.3217101519954483),\n","  (1, 0.1867837112899293),\n","  (2, 0.011623181337885069),\n","  (3, 0.08997805413313825),\n","  (4, 0.024546858489799237),\n","  (5, 0.2103551979192067),\n","  (6, 0.049418840933105744),\n","  (7, 0.019913842152320574),\n","  (8, 0.04153458506055434),\n","  (9, 0.04413557668861253)],\n"," [(0, 0.6270330422872796),\n","  (1, 0.031244649888717686),\n","  (2, 0.0561547680191748),\n","  (3, 0.03852080123266564),\n","  (4, 0.010529019003595274),\n","  (5, 0.1605889402499572),\n","  (6, 0.011813045711350795),\n","  (7, 0.03269988015750728),\n","  (8, 0.017719568567026195),\n","  (9, 0.01369628488272556)],\n"," [(0, 0.4321913811786279),\n","  (1, 0.10327581895473865),\n","  (2, 0.25197966158206214),\n","  (3, 0.0524297741101942),\n","  (4, 0.01642077185963157),\n","  (5, 0.02325581395348837),\n","  (6, 0.014670334250229221),\n","  (7, 0.06376594148537133),\n","  (8, 0.013169959156455777),\n","  (9, 0.028840543469200626)],\n"," [(0, 0.1542684081014575),\n","  (1, 0.04836267272383116),\n","  (2, 0.10950217679348855),\n","  (3, 0.05612341472648116),\n","  (4, 0.04296801060003786),\n","  (5, 0.02858224493658906),\n","  (6, 0.09085746734809767),\n","  (7, 0.03236797274275979),\n","  (8, 0.014101836077985992),\n","  (9, 0.42286579594927126)],\n"," [(0, 0.21554341226472376),\n","  (1, 0.16803278688524592),\n","  (2, 0.022920461445051613),\n","  (3, 0.025349119611414696),\n","  (4, 0.006754705525197329),\n","  (5, 0.37302671523983005),\n","  (6, 0.015558591378263511),\n","  (7, 0.05062234365513055),\n","  (8, 0.11103521554341228),\n","  (9, 0.01115664845173042)],\n"," [(0, 0.08467287084038352),\n","  (1, 0.17364636209813875),\n","  (2, 0.06324027072758037),\n","  (3, 0.021573604060913704),\n","  (4, 0.2918076706147772),\n","  (5, 0.009658770445572475),\n","  (6, 0.022913141567963903),\n","  (7, 0.024323181049069373),\n","  (8, 0.016638465877044557),\n","  (9, 0.2915256627185561)],\n"," [(0, 0.2589956504547252),\n","  (1, 0.06155265585870569),\n","  (2, 0.07539211809674444),\n","  (3, 0.14168973243706343),\n","  (4, 0.11282456834058259),\n","  (5, 0.11967839725846845),\n","  (6, 0.05008567286147358),\n","  (7, 0.05628047976802426),\n","  (8, 0.047185976011598796),\n","  (9, 0.0763147489126137)],\n"," [(0, 0.026570048309178744),\n","  (1, 0.014130434782608696),\n","  (2, 0.026207729468599034),\n","  (3, 0.022584541062801933),\n","  (4, 0.3288647342995169),\n","  (5, 0.11002415458937198),\n","  (6, 0.012439613526570048),\n","  (7, 0.015942028985507246),\n","  (8, 0.013164251207729469),\n","  (9, 0.4300724637681159)],\n"," [(0, 0.040119679042567656),\n","  (1, 0.07194342445260438),\n","  (2, 0.027607779137766898),\n","  (3, 0.02107983136134911),\n","  (4, 0.4079967360261118),\n","  (5, 0.26546987624099005),\n","  (6, 0.01808785529715762),\n","  (7, 0.05467156262749898),\n","  (8, 0.028287773697810416),\n","  (9, 0.06473548211614306)],\n"," [(0, 0.046143250688705235),\n","  (1, 0.017102846648301195),\n","  (2, 0.011880165289256199),\n","  (3, 0.05366161616161616),\n","  (4, 0.017676767676767676),\n","  (5, 0.7304292929292929),\n","  (6, 0.017791551882460975),\n","  (7, 0.005050505050505051),\n","  (8, 0.03254132231404959),\n","  (9, 0.067722681359045)],\n"," [(0, 0.1343417972267243),\n","  (1, 0.06582027732757069),\n","  (2, 0.020349360705924725),\n","  (3, 0.1547811993517018),\n","  (4, 0.25814874842427515),\n","  (5, 0.1257878624167117),\n","  (6, 0.06356924185125158),\n","  (7, 0.10075634792004322),\n","  (8, 0.01413650279128399),\n","  (9, 0.062308661984512875)],\n"," [(0, 0.06881503860355823),\n","  (1, 0.03541456864719704),\n","  (2, 0.10792212151728765),\n","  (3, 0.16347767707284322),\n","  (4, 0.013763007720711646),\n","  (5, 0.36236992279288344),\n","  (6, 0.05169519973145349),\n","  (7, 0.036253776435045314),\n","  (8, 0.14585431352802952),\n","  (9, 0.014434373950990263)],\n"," [(0, 0.05086006508600652),\n","  (1, 0.04026034402603441),\n","  (2, 0.035611343561134365),\n","  (3, 0.17154811715481175),\n","  (4, 0.03207810320781033),\n","  (5, 0.07503486750348676),\n","  (6, 0.04063226406322641),\n","  (7, 0.2067875406787541),\n","  (8, 0.17433751743375178),\n","  (9, 0.17284983728498376)],\n"," [(0, 0.3116748256935173),\n","  (1, 0.17816347722889778),\n","  (2, 0.24699599465954605),\n","  (3, 0.025070464322800773),\n","  (4, 0.12060525144637294),\n","  (5, 0.03115264797507788),\n","  (6, 0.012609405132769619),\n","  (7, 0.04005340453938585),\n","  (8, 0.012461059190031152),\n","  (9, 0.021213469811600653)],\n"," [(0, 0.012646793134598015),\n","  (1, 0.3474856970791931),\n","  (2, 0.037739636655625824),\n","  (3, 0.029408812606644588),\n","  (4, 0.22764227642276424),\n","  (5, 0.07889190003011143),\n","  (6, 0.01806684733514002),\n","  (7, 0.04757603131586872),\n","  (8, 0.15647897219712942),\n","  (9, 0.044063033222924826)],\n"," [(0, 0.01531909483854564),\n","  (1, 0.03025680142384948),\n","  (2, 0.11727688787185354),\n","  (3, 0.04494024917365878),\n","  (4, 0.03229087210780575),\n","  (5, 0.2648741418764302),\n","  (6, 0.013475718281210271),\n","  (7, 0.30778032036613273),\n","  (8, 0.11238240528858377),\n","  (9, 0.06140350877192982)],\n"," [(0, 0.01466373391988269),\n","  (1, 0.32206891954942346),\n","  (2, 0.015130307271878958),\n","  (3, 0.09171499033526628),\n","  (4, 0.027727787775778178),\n","  (5, 0.3363993867893088),\n","  (6, 0.005665533559954676),\n","  (7, 0.12604145837499167),\n","  (8, 0.00993134706392055),\n","  (9, 0.050656535359594745)],\n"," [(0, 0.2861895257104838),\n","  (1, 0.2958844216329246),\n","  (2, 0.01691854386465165),\n","  (3, 0.12185153502518772),\n","  (4, 0.024617431803060547),\n","  (5, 0.15644900674840795),\n","  (6, 0.015492823876057408),\n","  (7, 0.014352247885182016),\n","  (8, 0.036783575705731396),\n","  (9, 0.0314608877483129)],\n"," [(0, 0.17663043478260873),\n","  (1, 0.09661835748792272),\n","  (2, 0.023399758454106283),\n","  (3, 0.05872584541062802),\n","  (4, 0.42557367149758457),\n","  (5, 0.11171497584541064),\n","  (6, 0.015247584541062804),\n","  (7, 0.05193236714975846),\n","  (8, 0.017210144927536235),\n","  (9, 0.022946859903381647)],\n"," [(0, 0.2055243445692884),\n","  (1, 0.02372034956304619),\n","  (2, 0.10143570536828964),\n","  (3, 0.2222222222222222),\n","  (4, 0.01482521847690387),\n","  (5, 0.1931960049937578),\n","  (6, 0.02044319600499376),\n","  (7, 0.04915730337078652),\n","  (8, 0.09737827715355805),\n","  (9, 0.07209737827715355)],\n"," [(0, 0.1884698153811303),\n","  (1, 0.3499830105334692),\n","  (2, 0.026956620228791476),\n","  (3, 0.18880960471174532),\n","  (4, 0.1061275342620908),\n","  (5, 0.018122097632800992),\n","  (6, 0.012685468342960695),\n","  (7, 0.031487144636991725),\n","  (8, 0.01075999546947559),\n","  (9, 0.06659870880054365)],\n"," [(0, 0.05611430395913154),\n","  (1, 0.02498403575989783),\n","  (2, 0.014367816091954023),\n","  (3, 0.0913154533844189),\n","  (4, 0.015565134099616858),\n","  (5, 0.24281609195402298),\n","  (6, 0.03009259259259259),\n","  (7, 0.03089080459770115),\n","  (8, 0.46368135376756064),\n","  (9, 0.03017241379310345)],\n"," [(0, 0.13434193121693122),\n","  (1, 0.013062169312169311),\n","  (2, 0.05704365079365079),\n","  (3, 0.18716931216931218),\n","  (4, 0.03125),\n","  (5, 0.48578042328042326),\n","  (6, 0.032242063492063495),\n","  (7, 0.01603835978835979),\n","  (8, 0.015625),\n","  (9, 0.027447089947089946)],\n"," [(0, 0.023493442754847542),\n","  (1, 0.09558291093410574),\n","  (2, 0.01834419502775767),\n","  (3, 0.04779145546705287),\n","  (4, 0.5759916324724436),\n","  (5, 0.11794995574865237),\n","  (6, 0.011022608415801756),\n","  (7, 0.018424652023493446),\n","  (8, 0.03298736825166949),\n","  (9, 0.05841177890417573)],\n"," [(0, 0.4076797385620915),\n","  (1, 0.14133986928104575),\n","  (2, 0.02920751633986928),\n","  (3, 0.0519812091503268),\n","  (4, 0.02389705882352941),\n","  (5, 0.18218954248366012),\n","  (6, 0.01613562091503268),\n","  (7, 0.05014297385620915),\n","  (8, 0.05974264705882353),\n","  (9, 0.03768382352941176)],\n"," [(0, 0.20232856618995232),\n","  (1, 0.23954895489548955),\n","  (2, 0.04611294462779611),\n","  (3, 0.058672533920058674),\n","  (4, 0.014118078474514118),\n","  (5, 0.021268793546021268),\n","  (6, 0.035386872020535386),\n","  (7, 0.0231023102310231),\n","  (8, 0.009900990099009901),\n","  (9, 0.3495599559955996)],\n"," [(0, 0.018935978358881878),\n","  (1, 0.045586614567678596),\n","  (2, 0.045119059546471635),\n","  (3, 0.020706008082022512),\n","  (4, 0.6262231573322647),\n","  (5, 0.14113482283004378),\n","  (6, 0.029489363123267547),\n","  (7, 0.027652539825668773),\n","  (8, 0.01593026750826571),\n","  (9, 0.029222188825434998)],\n"," [(0, 0.38291001134677494),\n","  (1, 0.031159989526053946),\n","  (2, 0.22667364929737283),\n","  (3, 0.02190800384044689),\n","  (4, 0.01920223444182596),\n","  (5, 0.1848651479444881),\n","  (6, 0.016671030810857995),\n","  (7, 0.0827441738675046),\n","  (8, 0.008641005498821682),\n","  (9, 0.025224753425853192)],\n"," [(0, 0.411511645156505),\n","  (1, 0.11808337041981903),\n","  (2, 0.04094348019581665),\n","  (3, 0.17400979083222076),\n","  (4, 0.019433318498739062),\n","  (5, 0.06957424714434061),\n","  (6, 0.031894377688770216),\n","  (7, 0.044503782821539835),\n","  (8, 0.014834594273846612),\n","  (9, 0.07521139296840233)],\n"," [(0, 0.1582411795681938),\n","  (1, 0.36749605055292267),\n","  (2, 0.09926276987888363),\n","  (3, 0.014349657714586626),\n","  (4, 0.05621379673512376),\n","  (5, 0.16403370194839392),\n","  (6, 0.02790942601369142),\n","  (7, 0.07326224328593998),\n","  (8, 0.01816745655608215),\n","  (9, 0.021063717746182205)],\n"," [(0, 0.028524410312671424),\n","  (1, 0.04168952276467362),\n","  (2, 0.03687450478454319),\n","  (3, 0.09471567014079357),\n","  (4, 0.08758456756262573),\n","  (5, 0.22478210519900046),\n","  (6, 0.010909977448649967),\n","  (7, 0.4220149936002926),\n","  (8, 0.04089717803376608),\n","  (9, 0.012007070152983485)],\n"," [(0, 0.15139986342795825),\n","  (1, 0.12788996195493121),\n","  (2, 0.021363769388352355),\n","  (3, 0.03375280460442884),\n","  (4, 0.053848405033655254),\n","  (5, 0.2929470295580919),\n","  (6, 0.019022534386889084),\n","  (7, 0.029167886059896595),\n","  (8, 0.19968783533313822),\n","  (9, 0.07091991025265827)],\n"," [(0, 0.19222446086112976),\n","  (1, 0.06372658756809194),\n","  (2, 0.03574360122378927),\n","  (3, 0.028356092828893367),\n","  (4, 0.052384150436534584),\n","  (5, 0.08044175807775539),\n","  (6, 0.08223266920379076),\n","  (7, 0.017759868666517423),\n","  (8, 0.05447354675024252),\n","  (9, 0.392657264383255)],\n"," [(0, 0.07363479120336051),\n","  (1, 0.0904373610081542),\n","  (2, 0.08096532410839305),\n","  (3, 0.013919775965735936),\n","  (4, 0.04678362573099415),\n","  (5, 0.17568569310600446),\n","  (6, 0.011448809817972161),\n","  (7, 0.3145539906103286),\n","  (8, 0.07544683304505395),\n","  (9, 0.11712379540400296)],\n"," [(0, 0.23685691770798153),\n","  (1, 0.04030169987616796),\n","  (2, 0.020713722841382416),\n","  (3, 0.04806934594168637),\n","  (4, 0.5034335247101205),\n","  (5, 0.03467297084318361),\n","  (6, 0.06878306878306878),\n","  (7, 0.016548463356973995),\n","  (8, 0.011257458065968704),\n","  (9, 0.01936282787346617)],\n"," [(0, 0.05297765822311888),\n","  (1, 0.01658820892176642),\n","  (2, 0.04049914070088919),\n","  (3, 0.05514458641560188),\n","  (4, 0.04176940895165508),\n","  (5, 0.06754838227602182),\n","  (6, 0.017410147201673765),\n","  (7, 0.6835537622356721),\n","  (8, 0.013674064111185833),\n","  (9, 0.010834640962415005)],\n"," [(0, 0.0331301765900922),\n","  (1, 0.11517424597593375),\n","  (2, 0.022659790592280044),\n","  (3, 0.021722144085013283),\n","  (4, 0.6313486482262853),\n","  (5, 0.04547585560243788),\n","  (6, 0.013439599937490233),\n","  (7, 0.017815283638068447),\n","  (8, 0.01328332551961244),\n","  (9, 0.08595092983278638)],\n"," [(0, 0.04551539491298527),\n","  (1, 0.213743864346274),\n","  (2, 0.013386880856760375),\n","  (3, 0.023501413059645992),\n","  (4, 0.15885765283355643),\n","  (5, 0.20362933214338838),\n","  (6, 0.02840993604045813),\n","  (7, 0.2248995983935743),\n","  (8, 0.04834151420496802),\n","  (9, 0.03971441320838911)],\n"," [(0, 0.47767393561786087),\n","  (1, 0.03975671265390891),\n","  (2, 0.015427978044800474),\n","  (3, 0.32888295505117937),\n","  (4, 0.024922118380062305),\n","  (5, 0.0179498590713544),\n","  (6, 0.022993621124462245),\n","  (7, 0.041240172081293575),\n","  (8, 0.013944518617415814),\n","  (9, 0.01720812935766207)],\n"," [(0, 0.10908476865923676),\n","  (1, 0.04778790949003715),\n","  (2, 0.043566362715298894),\n","  (3, 0.03208375548801081),\n","  (4, 0.21268152651131378),\n","  (5, 0.14437689969604867),\n","  (6, 0.014268828098615334),\n","  (7, 0.3281830462681527),\n","  (8, 0.023049645390070924),\n","  (9, 0.04491725768321514)],\n"," [(0, 0.17247904839145714),\n","  (1, 0.07353338740200054),\n","  (2, 0.022032981886996487),\n","  (3, 0.09259259259259259),\n","  (4, 0.4503919978372533),\n","  (5, 0.07042443903757772),\n","  (6, 0.03271154366044877),\n","  (7, 0.0535279805352798),\n","  (8, 0.010813733441470668),\n","  (9, 0.021492295214922953)],\n"," [(0, 0.23089133089133093),\n","  (1, 0.043589743589743594),\n","  (2, 0.012454212454212457),\n","  (3, 0.36080586080586086),\n","  (4, 0.016971916971916974),\n","  (5, 0.22380952380952385),\n","  (6, 0.012332112332112333),\n","  (7, 0.014652014652014654),\n","  (8, 0.04566544566544567),\n","  (9, 0.038827838827838836)],\n"," [(0, 0.3090392372570591),\n","  (1, 0.05995599559955996),\n","  (2, 0.038137147048038146),\n","  (3, 0.17253392005867257),\n","  (4, 0.009442610927759445),\n","  (5, 0.2007700770077008),\n","  (6, 0.016501650165016504),\n","  (7, 0.014393105977264394),\n","  (8, 0.1536486982031537),\n","  (9, 0.025577557755775582)],\n"," [(0, 0.07106462858675248),\n","  (1, 0.011888799499418968),\n","  (2, 0.25332975775453653),\n","  (3, 0.08795923840171628),\n","  (4, 0.07097523911683204),\n","  (5, 0.22830070617681236),\n","  (6, 0.012872083668543845),\n","  (7, 0.055242692410834),\n","  (8, 0.07285241798516134),\n","  (9, 0.13551443639939215)],\n"," [(0, 0.20092330290630575),\n","  (1, 0.02318749344245095),\n","  (2, 0.061588500681985105),\n","  (3, 0.03766656174588186),\n","  (4, 0.07208057916273214),\n","  (5, 0.1811981953625013),\n","  (6, 0.10743888364284965),\n","  (7, 0.021718602455146365),\n","  (8, 0.04144370999895079),\n","  (9, 0.2527541706011961)],\n"," [(0, 0.06860959319975714),\n","  (1, 0.02924509208662214),\n","  (2, 0.05596033191661607),\n","  (3, 0.042096741550293464),\n","  (4, 0.06334750050597045),\n","  (5, 0.2487350738716859),\n","  (6, 0.012750455373406194),\n","  (7, 0.41277069419145923),\n","  (8, 0.015583889900829791),\n","  (9, 0.05090062740335964)],\n"," [(0, 0.13257925515543248),\n","  (1, 0.059633733456448144),\n","  (2, 0.2813942751615882),\n","  (3, 0.04501385041551247),\n","  (4, 0.021160357032933213),\n","  (5, 0.14766081871345033),\n","  (6, 0.04401354262850109),\n","  (7, 0.15320098491843648),\n","  (8, 0.03162511542012928),\n","  (9, 0.0837180670975685)],\n"," [(0, 0.1518684603886398),\n","  (1, 0.02969606377678127),\n","  (2, 0.3475834578973593),\n","  (3, 0.09815645241654211),\n","  (4, 0.0071748878923766825),\n","  (5, 0.217339312406577),\n","  (6, 0.01395117090184355),\n","  (7, 0.05819631290483309),\n","  (8, 0.049825610363726965),\n","  (9, 0.026208271051320384)],\n"," [(0, 0.14878542510121456),\n","  (1, 0.0398110661268556),\n","  (2, 0.18353576248313092),\n","  (3, 0.08670715249662618),\n","  (4, 0.028565002249212774),\n","  (5, 0.40778227620332885),\n","  (6, 0.016194331983805668),\n","  (7, 0.04678362573099415),\n","  (8, 0.026540710751237068),\n","  (9, 0.015294646873594242)],\n"," [(0, 0.029531888155827836),\n","  (1, 0.020839878521311132),\n","  (2, 0.15917897162006492),\n","  (3, 0.08105560791705937),\n","  (4, 0.025133521834747093),\n","  (5, 0.2018012357314902),\n","  (6, 0.026704366949418787),\n","  (7, 0.26568227039480574),\n","  (8, 0.17635354487380878),\n","  (9, 0.013718714001466122)],\n"," [(0, 0.033381020505484034),\n","  (1, 0.009918931807343826),\n","  (2, 0.3326657129232237),\n","  (3, 0.04206008583690988),\n","  (4, 0.02002861230329042),\n","  (5, 0.4662851692894612),\n","  (6, 0.025751072961373394),\n","  (7, 0.029661421077730094),\n","  (8, 0.02260371959942776),\n","  (9, 0.017644253695755844)],\n"," [(0, 0.2988923502942194),\n","  (1, 0.041190723433714084),\n","  (2, 0.04517133956386292),\n","  (3, 0.18535825545171336),\n","  (4, 0.013326410522672201),\n","  (5, 0.16848390446521286),\n","  (6, 0.1113707165109034),\n","  (7, 0.04473866389754239),\n","  (8, 0.08186223606784353),\n","  (9, 0.009605399792315679)],\n"," [(0, 0.06555555555555555),\n","  (1, 0.3291111111111111),\n","  (2, 0.0451111111111111),\n","  (3, 0.07033333333333332),\n","  (4, 0.013222222222222219),\n","  (5, 0.1551111111111111),\n","  (6, 0.07377777777777776),\n","  (7, 0.025333333333333326),\n","  (8, 0.1995555555555555),\n","  (9, 0.022888888888888886)],\n"," [(0, 0.20083102493074786),\n","  (1, 0.03785780240073868),\n","  (2, 0.11711295783317942),\n","  (3, 0.08402585410895659),\n","  (4, 0.011849799938442594),\n","  (5, 0.2274546014158202),\n","  (6, 0.15697137580794088),\n","  (7, 0.045244690674053546),\n","  (8, 0.09464450600184669),\n","  (9, 0.024007386888273308)],\n"," [(0, 0.19831743100441165),\n","  (1, 0.05468349235662256),\n","  (2, 0.16682056017236074),\n","  (3, 0.018364625012824463),\n","  (4, 0.012208884785062072),\n","  (5, 0.3069662460244178),\n","  (6, 0.06720016415307276),\n","  (7, 0.1026982661331692),\n","  (8, 0.0363188673437981),\n","  (9, 0.0364214630142608)],\n"," [(0, 0.3209669920966992),\n","  (1, 0.04277080427708043),\n","  (2, 0.07438400743840075),\n","  (3, 0.029660622966062297),\n","  (4, 0.01682938168293817),\n","  (5, 0.40567178056717806),\n","  (6, 0.020362622036262202),\n","  (7, 0.0601580660158066),\n","  (8, 0.018503021850302184),\n","  (9, 0.010692701069270108)],\n"," [(0, 0.04606767261088248),\n","  (1, 0.00983081847279378),\n","  (2, 0.28612254229538175),\n","  (3, 0.1727251943301326),\n","  (4, 0.050983081847279364),\n","  (5, 0.3286465477823502),\n","  (6, 0.015317786922725191),\n","  (7, 0.044924554183813435),\n","  (8, 0.02537722908093278),\n","  (9, 0.020004572473708272)],\n"," [(0, 0.12102484871893912),\n","  (1, 0.025878717651602937),\n","  (2, 0.33822582721771605),\n","  (3, 0.07879490150637312),\n","  (4, 0.012617484228144717),\n","  (5, 0.25196343504570623),\n","  (6, 0.06064117419853226),\n","  (7, 0.021501223123471098),\n","  (8, 0.05845242693446634),\n","  (9, 0.030899961375048284)],\n"," [(0, 0.2564822615370214),\n","  (1, 0.08817747823645043),\n","  (2, 0.0805017317232987),\n","  (3, 0.07554057848918844),\n","  (4, 0.22371992885893477),\n","  (5, 0.014883459702330806),\n","  (6, 0.17944397641112048),\n","  (7, 0.02723953945520921),\n","  (8, 0.013760179724796406),\n","  (9, 0.04025086586164935)],\n"," [(0, 0.03419726421886249),\n","  (1, 0.03767698584113271),\n","  (2, 0.12347012239020878),\n","  (3, 0.020278377729781617),\n","  (4, 0.06863450923926086),\n","  (5, 0.13054955603551716),\n","  (6, 0.07763378929685626),\n","  (7, 0.0267578593712503),\n","  (8, 0.45632349412047035),\n","  (9, 0.024478041756659467)],\n"," [(0, 0.015099938901981321),\n","  (1, 0.015798201972593174),\n","  (2, 0.05289342759884787),\n","  (3, 0.015885484856419656),\n","  (4, 0.014401675831369469),\n","  (5, 0.3379593261761369),\n","  (6, 0.00907741991795409),\n","  (7, 0.021384306537487997),\n","  (8, 0.5044950685170638),\n","  (9, 0.013005149690145762)],\n"," [(0, 0.023558726922641295),\n","  (1, 0.11564163551931136),\n","  (2, 0.020160833616491108),\n","  (3, 0.02525767357571639),\n","  (4, 0.14112583531543776),\n","  (5, 0.12628836787858194),\n","  (6, 0.01744251897157096),\n","  (7, 0.0381696681390871),\n","  (8, 0.4597349643221203),\n","  (9, 0.0326197757390418)],\n"," [(0, 0.14281596771869146),\n","  (1, 0.08401787001008791),\n","  (2, 0.019455252918287938),\n","  (3, 0.023346303501945526),\n","  (4, 0.024787433347744633),\n","  (5, 0.3056636402939905),\n","  (6, 0.04352212134313302),\n","  (7, 0.009511456982274102),\n","  (8, 0.3314598645337945),\n","  (9, 0.01542008935005044)],\n"," [(0, 0.04899064500246183),\n","  (1, 0.1107828655834564),\n","  (2, 0.012145084523223368),\n","  (3, 0.008616445100935498),\n","  (4, 0.16494337764647954),\n","  (5, 0.3143771541112752),\n","  (6, 0.009601181683899555),\n","  (7, 0.1136550139504349),\n","  (8, 0.1682258329230264),\n","  (9, 0.04866239947480714)],\n"," [(0, 0.0197869101978691),\n","  (1, 0.05568239472349061),\n","  (2, 0.07851344495180111),\n","  (3, 0.041730086250634194),\n","  (4, 0.06012176560121765),\n","  (5, 0.44837645865043124),\n","  (6, 0.03995433789954338),\n","  (7, 0.05276509386098427),\n","  (8, 0.1665398274987316),\n","  (9, 0.0365296803652968)],\n"," [(0, 0.04566817415979428),\n","  (1, 0.39877627028465024),\n","  (2, 0.02500665070497473),\n","  (3, 0.07049747273210961),\n","  (4, 0.016671100469983154),\n","  (5, 0.1007360113505365),\n","  (6, 0.08371020661523457),\n","  (7, 0.02846501729183294),\n","  (8, 0.220537376961958),\n","  (9, 0.009931719428926135)],\n"," [(0, 0.03801598955783979),\n","  (1, 0.19285364659814003),\n","  (2, 0.06134769130363845),\n","  (3, 0.04519497471039322),\n","  (4, 0.04486865720345897),\n","  (5, 0.050252896067874045),\n","  (6, 0.17767988252569752),\n","  (7, 0.04095284712024801),\n","  (8, 0.2938489149942895),\n","  (9, 0.05498449991842063)],\n"," [(0, 0.05736434108527132),\n","  (1, 0.08124031007751938),\n","  (2, 0.020775193798449613),\n","  (3, 0.011576227390180879),\n","  (4, 0.21457364341085272),\n","  (5, 0.06046511627906977),\n","  (6, 0.013746770025839794),\n","  (7, 0.03679586563307494),\n","  (8, 0.029767441860465118),\n","  (9, 0.4736950904392765)],\n"," [(0, 0.00714103545014027),\n","  (1, 0.08509733911417156),\n","  (2, 0.029244240414860154),\n","  (3, 0.010881577828785174),\n","  (4, 0.03136954858454476),\n","  (5, 0.2577573748193488),\n","  (6, 0.00569582589475474),\n","  (7, 0.09759415115191702),\n","  (8, 0.08118677208195188),\n","  (9, 0.39403213465952563)],\n"," [(0, 0.021054825932874714),\n","  (1, 0.2790285595163644),\n","  (2, 0.06389410047946634),\n","  (3, 0.01698978528246821),\n","  (4, 0.03595997498436523),\n","  (5, 0.09724828017510945),\n","  (6, 0.021263289555972485),\n","  (7, 0.05701480091723994),\n","  (8, 0.075046904315197),\n","  (9, 0.3324994788409423)],\n"," [(0, 0.02600276625172891),\n","  (1, 0.1376671277086215),\n","  (2, 0.09414476717381282),\n","  (3, 0.024250806823420932),\n","  (4, 0.02581834946980175),\n","  (5, 0.04942369755647764),\n","  (6, 0.014107883817427386),\n","  (7, 0.03550023052097741),\n","  (8, 0.3691101890272015),\n","  (9, 0.2239741816505302)],\n"," [(0, 0.012725546058879392),\n","  (1, 0.03580246913580247),\n","  (2, 0.03181386514719848),\n","  (3, 0.0150997150997151),\n","  (4, 0.11196581196581197),\n","  (5, 0.11386514719848054),\n","  (6, 0.01063627730294397),\n","  (7, 0.4062678062678063),\n","  (8, 0.021367521367521368),\n","  (9, 0.24045584045584045)],\n"," [(0, 0.04976564653984009),\n","  (1, 0.055969120485249525),\n","  (2, 0.07637165701681832),\n","  (3, 0.05913978494623657),\n","  (4, 0.03791011855527985),\n","  (5, 0.1843121036669424),\n","  (6, 0.026606010476978224),\n","  (7, 0.31155224703611806),\n","  (8, 0.12089881444720156),\n","  (9, 0.07747449682933555)],\n"," [(0, 0.023781330175646338),\n","  (1, 0.10657193605683836),\n","  (2, 0.023781330175646338),\n","  (3, 0.024768107361357806),\n","  (4, 0.48292875468719165),\n","  (5, 0.03374777975133215),\n","  (6, 0.034537201499901325),\n","  (7, 0.03049141503848431),\n","  (8, 0.019242155121373596),\n","  (9, 0.22014999013222813)],\n"," [(0, 0.16057405903059846),\n","  (1, 0.01687877967325571),\n","  (2, 0.02879321238378916),\n","  (3, 0.021572344074374947),\n","  (4, 0.6650419712970486),\n","  (5, 0.015344345157505193),\n","  (6, 0.011824171856665767),\n","  (7, 0.013448867226283964),\n","  (8, 0.007762433432620275),\n","  (9, 0.058759815867858126)],\n"," [(0, 0.015503875968992248),\n","  (1, 0.30798572659037776),\n","  (2, 0.010582010582010581),\n","  (3, 0.015380829334317706),\n","  (4, 0.5863172142241909),\n","  (5, 0.016611295681063124),\n","  (6, 0.010582010582010581),\n","  (7, 0.014273409622246831),\n","  (8, 0.01107419712070875),\n","  (9, 0.011689430294081456)],\n"," [(0, 0.016737891737891742),\n","  (1, 0.05208333333333334),\n","  (2, 0.020655270655270657),\n","  (3, 0.012642450142450145),\n","  (4, 0.8186431623931625),\n","  (5, 0.018340455840455842),\n","  (6, 0.022881054131054134),\n","  (7, 0.019052706552706557),\n","  (8, 0.009615384615384618),\n","  (9, 0.0093482905982906)],\n"," [(0, 0.030184751496226906),\n","  (1, 0.11562147627721399),\n","  (2, 0.025153959580189088),\n","  (3, 0.011362650706913002),\n","  (4, 0.41894353369763204),\n","  (5, 0.011275912915257178),\n","  (6, 0.0741608118657299),\n","  (7, 0.19559372018388413),\n","  (8, 0.01994969208083962),\n","  (9, 0.09775349119611415)],\n"," [(0, 0.08351834231948409),\n","  (1, 0.19991542446347393),\n","  (2, 0.050216724812348026),\n","  (3, 0.051908235542869226),\n","  (4, 0.278464954012052),\n","  (5, 0.019875251083624063),\n","  (6, 0.03414737287239666),\n","  (7, 0.09292737075800825),\n","  (8, 0.012263452796278676),\n","  (9, 0.17676287133946506)],\n"," [(0, 0.0627666057282145),\n","  (1, 0.06845419459679057),\n","  (2, 0.03026609790777981),\n","  (3, 0.16940889701401585),\n","  (4, 0.37416209628275443),\n","  (5, 0.02782855982124721),\n","  (6, 0.06784481007515743),\n","  (7, 0.018078407475116798),\n","  (8, 0.015234613040828763),\n","  (9, 0.16595571805809467)],\n"," [(0, 0.16290726817042606),\n","  (1, 0.015037593984962405),\n","  (2, 0.015733778891673628),\n","  (3, 0.027151211361737676),\n","  (4, 0.6615148983570036),\n","  (5, 0.013505987190197716),\n","  (6, 0.02756892230576441),\n","  (7, 0.027429685324422165),\n","  (8, 0.016012252854358117),\n","  (9, 0.03313840155945419)],\n"," [(0, 0.5362228458580189),\n","  (1, 0.0397048123895645),\n","  (2, 0.021203617087620828),\n","  (3, 0.08460658975158507),\n","  (4, 0.14800956241554933),\n","  (5, 0.07119842012264838),\n","  (6, 0.018397256002494543),\n","  (7, 0.015383016318470014),\n","  (8, 0.010809687142708658),\n","  (9, 0.054464192911339776)],\n"," [(0, 0.030222222222222223),\n","  (1, 0.0184),\n","  (2, 0.052),\n","  (3, 0.011022222222222221),\n","  (4, 0.332),\n","  (5, 0.12275555555555556),\n","  (6, 0.015022222222222222),\n","  (7, 0.02568888888888889),\n","  (8, 0.015644444444444443),\n","  (9, 0.37724444444444444)],\n"," [(0, 0.07060049498502019),\n","  (1, 0.10251400286570275),\n","  (2, 0.022534844340237073),\n","  (3, 0.029047805132213105),\n","  (4, 0.5582909990881855),\n","  (5, 0.010811514914680213),\n","  (6, 0.038165950240979546),\n","  (7, 0.019148104728409537),\n","  (8, 0.016542920411619123),\n","  (9, 0.132343363292953)],\n"," [(0, 0.039552147985882934),\n","  (1, 0.26019228428866986),\n","  (2, 0.01825483753194597),\n","  (3, 0.02300109529025192),\n","  (4, 0.30083972252646957),\n","  (5, 0.22587318972861145),\n","  (6, 0.01922842886698309),\n","  (7, 0.044541803577948166),\n","  (8, 0.0547645125958379),\n","  (9, 0.013751977607399296)],\n"," [(0, 0.029328821206993795),\n","  (1, 0.014946418499717992),\n","  (2, 0.0874224478285392),\n","  (3, 0.5129723632261703),\n","  (4, 0.013959390862944163),\n","  (5, 0.22179921037789058),\n","  (6, 0.021150592216582064),\n","  (7, 0.02523970671178793),\n","  (8, 0.04512126339537507),\n","  (9, 0.02805978567399887)],\n"," [(0, 0.15390946502057612),\n","  (1, 0.018198445358939185),\n","  (2, 0.038042981252857794),\n","  (3, 0.07361682670324646),\n","  (4, 0.00722450845907636),\n","  (5, 0.13964334705075446),\n","  (6, 0.0986739826245999),\n","  (7, 0.07059899405578418),\n","  (8, 0.3898491083676269),\n","  (9, 0.010242341106538638)],\n"," [(0, 0.3988711194731891),\n","  (1, 0.039197240514267796),\n","  (2, 0.047872896414759065),\n","  (3, 0.19494094282429184),\n","  (4, 0.02006898714330511),\n","  (5, 0.14706804640953278),\n","  (6, 0.04013797428661022),\n","  (7, 0.07536322776209887),\n","  (8, 0.02278666248562768),\n","  (9, 0.01369290268631755)],\n"," [(0, 0.26044330775788577),\n","  (1, 0.03324808184143223),\n","  (2, 0.17490764421710714),\n","  (3, 0.032821824381926684),\n","  (4, 0.014350667803353225),\n","  (5, 0.3894572321682296),\n","  (6, 0.021739130434782608),\n","  (7, 0.017760727479397557),\n","  (8, 0.040068201193520885),\n","  (9, 0.015203182722364308)],\n"," [(0, 0.12925305688463584),\n","  (1, 0.02983785220627326),\n","  (2, 0.035287081339712915),\n","  (3, 0.08811802232854865),\n","  (4, 0.012958532695374801),\n","  (5, 0.48710792131844766),\n","  (6, 0.017610313662945243),\n","  (7, 0.07004253056884636),\n","  (8, 0.0588782562466773),\n","  (9, 0.07090643274853801)],\n"," [(0, 0.0205761316872428),\n","  (1, 0.023221634332745444),\n","  (2, 0.009798157946306094),\n","  (3, 0.04614932392710171),\n","  (4, 0.06496178718400941),\n","  (5, 0.5107779737409367),\n","  (6, 0.019008426415833824),\n","  (7, 0.09318048206937096),\n","  (8, 0.11267881638252009),\n","  (9, 0.09964726631393298)],\n"," [(0, 0.04919423240033927),\n","  (1, 0.028668363019508057),\n","  (2, 0.039016115351993216),\n","  (3, 0.06344359626802375),\n","  (4, 0.03986429177268872),\n","  (5, 0.5589482612383376),\n","  (6, 0.032400339270568276),\n","  (7, 0.05648854961832061),\n","  (8, 0.08956743002544529),\n","  (9, 0.04240882103477523)],\n"," [(0, 0.6604840134251898),\n","  (1, 0.11800035329447091),\n","  (2, 0.010157216039568978),\n","  (3, 0.015809927574633453),\n","  (4, 0.13734322557851966),\n","  (5, 0.0160748984278396),\n","  (6, 0.00839074368486133),\n","  (7, 0.007860801978449036),\n","  (8, 0.010598834128245891),\n","  (9, 0.01527998586822116)],\n"," [(0, 0.026490616176714825),\n","  (1, 0.08428832419863809),\n","  (2, 0.017854177047002154),\n","  (3, 0.011044676963959473),\n","  (4, 0.4167912306925759),\n","  (5, 0.045341305430991526),\n","  (6, 0.010878591596080383),\n","  (7, 0.3397276199966782),\n","  (8, 0.01029729280850357),\n","  (9, 0.03728616508885566)],\n"," [(0, 0.06233333333333332),\n","  (1, 0.03966666666666666),\n","  (2, 0.13711111111111107),\n","  (3, 0.028999999999999995),\n","  (4, 0.05899999999999998),\n","  (5, 0.437111111111111),\n","  (6, 0.08377777777777777),\n","  (7, 0.03755555555555555),\n","  (8, 0.01944444444444444),\n","  (9, 0.09499999999999997)],\n"," [(0, 0.23999488556450585),\n","  (1, 0.04091548395345864),\n","  (2, 0.2110983250223757),\n","  (3, 0.032348804500703245),\n","  (4, 0.031070195627157658),\n","  (5, 0.26748497634573587),\n","  (6, 0.014448280271065083),\n","  (7, 0.11162255466052935),\n","  (8, 0.02646720368239356),\n","  (9, 0.024549290372075185)],\n"," [(0, 0.1),\n","  (1, 0.04814814814814815),\n","  (2, 0.17777777777777778),\n","  (3, 0.07488425925925926),\n","  (4, 0.03449074074074074),\n","  (5, 0.10219907407407407),\n","  (6, 0.01423611111111111),\n","  (7, 0.040625),\n","  (8, 0.046064814814814815),\n","  (9, 0.36157407407407405)],\n"," [(0, 0.1890547263681592),\n","  (1, 0.054088531700472),\n","  (2, 0.08036739380022963),\n","  (3, 0.05370582982523281),\n","  (4, 0.12055109070034443),\n","  (5, 0.33167495854063017),\n","  (6, 0.020410766679423396),\n","  (7, 0.05702257941063911),\n","  (8, 0.013139431049878811),\n","  (9, 0.07998469192499043)],\n"," [(0, 0.473269649740238),\n","  (1, 0.10340204457851516),\n","  (2, 0.01642366348248701),\n","  (3, 0.2022792022792023),\n","  (4, 0.01893748952572482),\n","  (5, 0.06502430031841797),\n","  (6, 0.014245014245014245),\n","  (7, 0.06988436400201106),\n","  (8, 0.015250544662309368),\n","  (9, 0.021283727166080107)],\n"," [(0, 0.02237926972909305),\n","  (1, 0.013447192775814684),\n","  (2, 0.09511189634864546),\n","  (3, 0.025520219866509618),\n","  (4, 0.023655280722418532),\n","  (5, 0.5552610914801728),\n","  (6, 0.0088339222614841),\n","  (7, 0.07086768747546132),\n","  (8, 0.17137809187279152),\n","  (9, 0.013545347467608953)],\n"," [(0, 0.09057971014492754),\n","  (1, 0.07088442958008176),\n","  (2, 0.029728725380899292),\n","  (3, 0.09894091415830546),\n","  (4, 0.047658862876254184),\n","  (5, 0.4336677814938685),\n","  (6, 0.013285024154589372),\n","  (7, 0.16852471200297287),\n","  (8, 0.033537718320327016),\n","  (9, 0.013192121887774062)],\n"," [(0, 0.033602150537634407),\n","  (1, 0.5124551971326164),\n","  (2, 0.016845878136200716),\n","  (3, 0.029659498207885306),\n","  (4, 0.18476702508960574),\n","  (5, 0.030555555555555555),\n","  (6, 0.01675627240143369),\n","  (7, 0.03306451612903226),\n","  (8, 0.0260752688172043),\n","  (9, 0.11621863799283154)],\n"," [(0, 0.023493642896627972),\n","  (1, 0.6677722498618021),\n","  (2, 0.015754560530679935),\n","  (3, 0.024599226091763404),\n","  (4, 0.15574903261470427),\n","  (5, 0.03178551686014373),\n","  (6, 0.013957987838584854),\n","  (7, 0.036898839137645106),\n","  (8, 0.015616362631288004),\n","  (9, 0.014372581536760642)],\n"," [(0, 0.07241278217531516),\n","  (1, 0.5010260920551158),\n","  (2, 0.015537965406039285),\n","  (3, 0.02931691586045148),\n","  (4, 0.1861624157138669),\n","  (5, 0.09625720707514902),\n","  (6, 0.030880484706342227),\n","  (7, 0.029414638913319652),\n","  (8, 0.025310270692856445),\n","  (9, 0.013681227401544024)],\n"," [(0, 0.11531886916502301),\n","  (1, 0.41025641025641024),\n","  (2, 0.055095332018408945),\n","  (3, 0.04760026298487837),\n","  (4, 0.01814595660749507),\n","  (5, 0.0252465483234714),\n","  (6, 0.03076923076923077),\n","  (7, 0.01761998685075608),\n","  (8, 0.009598948060486522),\n","  (9, 0.2703484549638396)],\n"," [(0, 0.10948905109489052),\n","  (1, 0.38861854555285213),\n","  (2, 0.015139226818058935),\n","  (3, 0.04230873208975399),\n","  (4, 0.25831305758313056),\n","  (5, 0.06974858069748581),\n","  (6, 0.016761286834279535),\n","  (7, 0.019464720194647202),\n","  (8, 0.016761286834279535),\n","  (9, 0.06339551230062179)],\n"," [(0, 0.07324007733424316),\n","  (1, 0.6568861594450132),\n","  (2, 0.019447287615148415),\n","  (3, 0.03514158990105767),\n","  (4, 0.11804844762879564),\n","  (5, 0.015580575457750484),\n","  (6, 0.01410212669168657),\n","  (7, 0.017627658364608216),\n","  (8, 0.008415785283748438),\n","  (9, 0.04151029227794838)],\n"," [(0, 0.0192684896734741),\n","  (1, 0.6644744432906428),\n","  (2, 0.01546094380985347),\n","  (3, 0.026883581400715364),\n","  (4, 0.06818968501211493),\n","  (5, 0.08341986846659745),\n","  (6, 0.011884158301603788),\n","  (7, 0.05307488173531789),\n","  (8, 0.019845390561901467),\n","  (9, 0.03749855774777894)],\n"," [(0, 0.02925885624471044),\n","  (1, 0.5935195260548907),\n","  (2, 0.020191028896143156),\n","  (3, 0.03905210978116311),\n","  (4, 0.19937129730383268),\n","  (5, 0.012453149558699072),\n","  (6, 0.01438761939306009),\n","  (7, 0.03518317011244106),\n","  (8, 0.03022609116189095),\n","  (9, 0.026357151493168907)],\n"," [(0, 0.01917775380558552),\n","  (1, 0.15857605177993528),\n","  (2, 0.1109912501498262),\n","  (3, 0.04338966798513724),\n","  (4, 0.03547884454033321),\n","  (5, 0.16193215869591274),\n","  (6, 0.04398897279156179),\n","  (7, 0.019057892844300612),\n","  (8, 0.1627711854249071),\n","  (9, 0.2446362219825003)],\n"," [(0, 0.05444355484387511),\n","  (1, 0.6361533671381551),\n","  (2, 0.0690330041811227),\n","  (3, 0.032470420781069306),\n","  (4, 0.013699848767903214),\n","  (5, 0.06894404412418825),\n","  (6, 0.010319366604394628),\n","  (7, 0.02126145360733031),\n","  (8, 0.03344898140734811),\n","  (9, 0.060225958544613475)],\n"," [(0, 0.06151126000372231),\n","  (1, 0.6090638377070537),\n","  (2, 0.07165456914200632),\n","  (3, 0.03936348408710218),\n","  (4, 0.05332216638749302),\n","  (5, 0.05667225013958682),\n","  (6, 0.01554066629443514),\n","  (7, 0.061697375767727526),\n","  (8, 0.012841987716359575),\n","  (9, 0.018332402754513307)],\n"," [(0, 0.24649743376335137),\n","  (1, 0.14773200166458594),\n","  (2, 0.048550423082258286),\n","  (3, 0.035649882091829656),\n","  (4, 0.09834928561520322),\n","  (5, 0.2388680815647108),\n","  (6, 0.027881814398668332),\n","  (7, 0.11430156748508809),\n","  (8, 0.02524622000277431),\n","  (9, 0.016923290331530033)],\n"," [(0, 0.14669078848183326),\n","  (1, 0.17307402382029247),\n","  (2, 0.11005578169757274),\n","  (3, 0.09392431780491482),\n","  (4, 0.024875621890547265),\n","  (5, 0.29232624755012815),\n","  (6, 0.03844414292175486),\n","  (7, 0.01959897482285542),\n","  (8, 0.06331976481230213),\n","  (9, 0.037690336197798886)],\n"," [(0, 0.24126268320180386),\n","  (1, 0.1421771263935864),\n","  (2, 0.14881623449830894),\n","  (3, 0.015282475259927347),\n","  (4, 0.1302768382813479),\n","  (5, 0.23124138794939247),\n","  (6, 0.03432293623950897),\n","  (7, 0.022547914317925594),\n","  (8, 0.018539396216961046),\n","  (9, 0.015533007641237632)],\n"," [(0, 0.10870200029201343),\n","  (1, 0.4501387063804935),\n","  (2, 0.02847130968024529),\n","  (3, 0.009198423127463863),\n","  (4, 0.040151846984961305),\n","  (5, 0.22733245729303547),\n","  (6, 0.009417433201927288),\n","  (7, 0.059278726821433786),\n","  (8, 0.056212585778945834),\n","  (9, 0.011096510439480217)],\n"," [(0, 0.022813337027800872),\n","  (1, 0.027800868199870696),\n","  (2, 0.016994550660386076),\n","  (3, 0.055324651334626405),\n","  (4, 0.015516763646439459),\n","  (5, 0.52636926203011),\n","  (6, 0.011545211046457931),\n","  (7, 0.05052184353929991),\n","  (8, 0.23238200794310523),\n","  (9, 0.04073150457190358)],\n"," [(0, 0.4275599128540305),\n","  (1, 0.0196078431372549),\n","  (2, 0.08278867102396514),\n","  (3, 0.17120551924473493),\n","  (4, 0.01924473493100944),\n","  (5, 0.1461510530137981),\n","  (6, 0.04357298474945534),\n","  (7, 0.024328249818445898),\n","  (8, 0.018336964415395788),\n","  (9, 0.04720406681190995)],\n"," [(0, 0.061638280616382816),\n","  (1, 0.19518788861854558),\n","  (2, 0.042624132648463556),\n","  (3, 0.04649905379832388),\n","  (4, 0.3330629899972966),\n","  (5, 0.15869153825358207),\n","  (6, 0.03919978372533118),\n","  (7, 0.10408218437415519),\n","  (8, 0.010182932324051546),\n","  (9, 0.008831215643867714)],\n"," [(0, 0.03679028795617737),\n","  (1, 0.186098156784366),\n","  (2, 0.021763268931823232),\n","  (3, 0.013990672884743507),\n","  (4, 0.6602265156562293),\n","  (5, 0.017765933821896517),\n","  (6, 0.008290769116885041),\n","  (7, 0.031164408912576803),\n","  (8, 0.012584203123843365),\n","  (9, 0.01132578281145903)],\n"," [(0, 0.21551505947861302),\n","  (1, 0.049481144014173625),\n","  (2, 0.16552771450265755),\n","  (3, 0.06314856998228296),\n","  (4, 0.009997468995191092),\n","  (5, 0.02366489496330043),\n","  (6, 0.1524930397367755),\n","  (7, 0.036952670210073396),\n","  (8, 0.028220703619336877),\n","  (9, 0.2549987344975955)],\n"," [(0, 0.1261384335154827),\n","  (1, 0.09578020643594413),\n","  (2, 0.019429265330904676),\n","  (3, 0.075591985428051),\n","  (4, 0.4203096539162113),\n","  (5, 0.061627200971463264),\n","  (6, 0.02899210686095932),\n","  (7, 0.06633272616879174),\n","  (8, 0.023679417122040074),\n","  (9, 0.0821190042501518)],\n"," [(0, 0.12159947558177647),\n","  (1, 0.15077023926581448),\n","  (2, 0.01720747295968535),\n","  (3, 0.02917076368403802),\n","  (4, 0.578498852835136),\n","  (5, 0.01933792199278925),\n","  (6, 0.031137332022287776),\n","  (7, 0.023107177974434612),\n","  (8, 0.013110455588331694),\n","  (9, 0.016060308095706327)],\n"," [(0, 0.08844938882021701),\n","  (1, 0.01950281554731493),\n","  (2, 0.03378656777915121),\n","  (3, 0.04642219475346793),\n","  (4, 0.02554594149155336),\n","  (5, 0.6349402554594149),\n","  (6, 0.013322345831616535),\n","  (7, 0.020738909490454607),\n","  (8, 0.09627798379343497),\n","  (9, 0.021013597033374538)],\n"," [(0, 0.19165900030665442),\n","  (1, 0.033936420320964944),\n","  (2, 0.012777266687110295),\n","  (3, 0.03782070939384648),\n","  (4, 0.6261882858019013),\n","  (5, 0.020136972298885825),\n","  (6, 0.021568026167842178),\n","  (7, 0.028007768578145766),\n","  (8, 0.014821629357047943),\n","  (9, 0.013083921087600942)],\n"," [(0, 0.03535353535353535),\n","  (1, 0.0907888407888408),\n","  (2, 0.06156806156806157),\n","  (3, 0.03198653198653199),\n","  (4, 0.03006253006253006),\n","  (5, 0.12518037518037517),\n","  (6, 0.03751803751803752),\n","  (7, 0.050024050024050026),\n","  (8, 0.44203944203944207),\n","  (9, 0.09547859547859548)],\n"," [(0, 0.05887781469176818),\n","  (1, 0.022886674049464747),\n","  (2, 0.049464747139165745),\n","  (3, 0.022886674049464747),\n","  (4, 0.035437430786267994),\n","  (5, 0.23126614987080105),\n","  (6, 0.06459948320413436),\n","  (7, 0.038390550018456995),\n","  (8, 0.42414174972314506),\n","  (9, 0.05204872646733112)],\n"," [(0, 0.053211439287388654),\n","  (1, 0.09411626816690108),\n","  (2, 0.05625879043600562),\n","  (3, 0.03012189404594468),\n","  (4, 0.026840131270511016),\n","  (5, 0.02273792780121894),\n","  (6, 0.039732770745428976),\n","  (7, 0.06000937646507267),\n","  (8, 0.022855133614627286),\n","  (9, 0.5941162681669011)],\n"," [(0, 0.054759898904802005),\n","  (1, 0.047458579050828405),\n","  (2, 0.06758401198165308),\n","  (3, 0.12122063090892068),\n","  (4, 0.025835439483291204),\n","  (5, 0.23701207525975843),\n","  (6, 0.22755780211551058),\n","  (7, 0.05139005897219881),\n","  (8, 0.1422821304876907),\n","  (9, 0.02489937283534587)],\n"," [(0, 0.46649703138252757),\n","  (1, 0.04022779595298679),\n","  (2, 0.22985581000848176),\n","  (3, 0.058524173027989825),\n","  (4, 0.01660002423361202),\n","  (5, 0.05064824912153156),\n","  (6, 0.02702047740215679),\n","  (7, 0.05925118138858597),\n","  (8, 0.01599418393311523),\n","  (9, 0.03538107354901248)],\n"," [(0, 0.018260120585701978),\n","  (1, 0.02093023255813953),\n","  (2, 0.03290267011197243),\n","  (3, 0.628423772609819),\n","  (4, 0.012230835486649437),\n","  (5, 0.1565891472868217),\n","  (6, 0.0338501291989664),\n","  (7, 0.03746770025839793),\n","  (8, 0.051593453919035304),\n","  (9, 0.007751937984496122)],\n"," [(0, 0.088315748339195),\n","  (1, 0.047153836133906474),\n","  (2, 0.20203204376709652),\n","  (3, 0.13091051191871825),\n","  (4, 0.0239676957144718),\n","  (5, 0.1249185879901003),\n","  (6, 0.23694151361208807),\n","  (7, 0.0914419695193435),\n","  (8, 0.034909469844991534),\n","  (9, 0.019408623160088578)],\n"," [(0, 0.3759412304866851),\n","  (1, 0.05381083562901746),\n","  (2, 0.15151515151515155),\n","  (3, 0.026629935720844815),\n","  (4, 0.04113865932047751),\n","  (5, 0.11717171717171719),\n","  (6, 0.01763085399449036),\n","  (7, 0.019834710743801658),\n","  (8, 0.054178145087236),\n","  (9, 0.14214876033057855)],\n"," [(0, 0.013329068031563232),\n","  (1, 0.5670718703348262),\n","  (2, 0.011729579867775645),\n","  (3, 0.025911708253358926),\n","  (4, 0.1528044359138409),\n","  (5, 0.017381104713158457),\n","  (6, 0.018554062699936022),\n","  (7, 0.05331627212625293),\n","  (8, 0.015035188739603326),\n","  (9, 0.12486670931968437)],\n"," [(0, 0.05854956753160346),\n","  (1, 0.08582834331337326),\n","  (2, 0.06852960745176315),\n","  (3, 0.05588822355289421),\n","  (4, 0.32801064537591484),\n","  (5, 0.14038589487691283),\n","  (6, 0.06453759148369927),\n","  (7, 0.05854956753160346),\n","  (8, 0.05588822355289421),\n","  (9, 0.08383233532934131)],\n"," [(0, 0.03083466241360978),\n","  (1, 0.0786815523657629),\n","  (2, 0.03189792663476874),\n","  (3, 0.031366294524189264),\n","  (4, 0.5449229133439659),\n","  (5, 0.07442849548112707),\n","  (6, 0.028176501860712386),\n","  (7, 0.06645401382243488),\n","  (8, 0.06166932482721956),\n","  (9, 0.051568314726209465)],\n"," [(0, 0.04771900363839911),\n","  (1, 0.08690176322418136),\n","  (2, 0.041561712846347604),\n","  (3, 0.061992723201791215),\n","  (4, 0.014973411698852504),\n","  (5, 0.1432969493422894),\n","  (6, 0.0899804086202071),\n","  (7, 0.12062692415337252),\n","  (8, 0.35698292751189475),\n","  (9, 0.035964175762664426)],\n"," [(0, 0.01668441604543841),\n","  (1, 0.08271210507632233),\n","  (2, 0.018991835285764997),\n","  (3, 0.025736599219027333),\n","  (4, 0.6153709620163295),\n","  (5, 0.05875044373446929),\n","  (6, 0.01455449059282925),\n","  (7, 0.021654242101526448),\n","  (8, 0.023606673766418177),\n","  (9, 0.12193823216187434)],\n"," [(0, 0.01752051452650255),\n","  (1, 0.22288755821689954),\n","  (2, 0.015635395874916833),\n","  (3, 0.01330671989354624),\n","  (4, 0.2826569084054114),\n","  (5, 0.036926147704590816),\n","  (6, 0.028942115768463075),\n","  (7, 0.026280771789753826),\n","  (8, 0.06198713683743624),\n","  (9, 0.2938567309824795)],\n"," [(0, 0.0383747178329571),\n","  (1, 0.15086531226486077),\n","  (2, 0.02119388011035866),\n","  (3, 0.016052169551040878),\n","  (4, 0.6177577125658389),\n","  (5, 0.04614998745924253),\n","  (6, 0.018058690744920988),\n","  (7, 0.014672686230248304),\n","  (8, 0.015926761976423374),\n","  (9, 0.06094808126410834)],\n"," [(0, 0.009057092026524341),\n","  (1, 0.08879184861717612),\n","  (2, 0.008895358240336406),\n","  (3, 0.020621057738961668),\n","  (4, 0.7799611838913149),\n","  (5, 0.0163351124049814),\n","  (6, 0.01722464822901504),\n","  (7, 0.03606663431990943),\n","  (8, 0.015607310367135694),\n","  (9, 0.0074397541646449945)],\n"," [(0, 0.022261798753339265),\n","  (1, 0.06253091916493518),\n","  (2, 0.01820520431384189),\n","  (3, 0.013851785890966653),\n","  (4, 0.7134659147125754),\n","  (5, 0.11388146828930441),\n","  (6, 0.010289898090432372),\n","  (7, 0.014247551202137129),\n","  (8, 0.01682002572474522),\n","  (9, 0.014445433857722368)],\n"," [(0, 0.05100927441352973),\n","  (1, 0.21058374249863612),\n","  (2, 0.03346062920531006),\n","  (3, 0.044280778323331514),\n","  (4, 0.38534278959810875),\n","  (5, 0.032915075468266955),\n","  (6, 0.11429350791052918),\n","  (7, 0.02354973631569376),\n","  (8, 0.01745771958537916),\n","  (9, 0.08710674668121476)],\n"," [(0, 0.04787484685703515),\n","  (1, 0.06172839506172839),\n","  (2, 0.009047215154085382),\n","  (3, 0.01620959381773631),\n","  (4, 0.5459428894543399),\n","  (5, 0.03411554047686363),\n","  (6, 0.008764489680520215),\n","  (7, 0.19941570068796532),\n","  (8, 0.01130901894260673),\n","  (9, 0.06559230986711903)],\n"," [(0, 0.03305203938115331),\n","  (1, 0.23508137432188064),\n","  (2, 0.10046212577858148),\n","  (3, 0.02471368294153104),\n","  (4, 0.3379545911191481),\n","  (5, 0.043198714084790035),\n","  (6, 0.018083182640144666),\n","  (7, 0.046614426361261806),\n","  (8, 0.013060076351215592),\n","  (9, 0.14777978702029335)],\n"," [(0, 0.30683624801271864),\n","  (1, 0.05131602190425721),\n","  (2, 0.023935700406288646),\n","  (3, 0.036830948595654486),\n","  (4, 0.3408408408408409),\n","  (5, 0.12806924571630457),\n","  (6, 0.014661720544073487),\n","  (7, 0.021021021021021026),\n","  (8, 0.017311429076134963),\n","  (9, 0.059176823882706245)],\n"," [(0, 0.1849838969404187),\n","  (1, 0.04780595813204509),\n","  (2, 0.017914653784219005),\n","  (3, 0.015499194847020935),\n","  (4, 0.6342592592592594),\n","  (5, 0.026570048309178747),\n","  (6, 0.014291465378421901),\n","  (7, 0.013486312399355879),\n","  (8, 0.016706924315619973),\n","  (9, 0.028482286634460553)],\n"," [(0, 0.020591233435270136),\n","  (1, 0.034454638124362905),\n","  (2, 0.01467889908256881),\n","  (3, 0.013557594291539247),\n","  (4, 0.8002038735983691),\n","  (5, 0.032517838939857296),\n","  (6, 0.018450560652395517),\n","  (7, 0.012538226299694191),\n","  (8, 0.008970438328236494),\n","  (9, 0.04403669724770643)],\n"," [(0, 0.07919081020636373),\n","  (1, 0.018196604655891024),\n","  (2, 0.02409271119243672),\n","  (3, 0.02734573548846193),\n","  (4, 0.6880146386093321),\n","  (5, 0.025719223340449324),\n","  (6, 0.013723696248856358),\n","  (7, 0.012300498119345329),\n","  (8, 0.00986072989732642),\n","  (9, 0.10155535224153706)],\n"," [(0, 0.020367339516275687),\n","  (1, 0.09783597017639571),\n","  (2, 0.030732860520094562),\n","  (3, 0.10683760683760683),\n","  (4, 0.41998545190034553),\n","  (5, 0.18294235315511911),\n","  (6, 0.04609929078014184),\n","  (7, 0.01845790143662484),\n","  (8, 0.04937261320240044),\n","  (9, 0.027368612474995453)],\n"," [(0, 0.41160315825609345),\n","  (1, 0.0511500171644353),\n","  (2, 0.02574665293511844),\n","  (3, 0.022313765877102647),\n","  (4, 0.34237326925277495),\n","  (5, 0.018079871838883172),\n","  (6, 0.05275203112484267),\n","  (7, 0.04153793340199108),\n","  (8, 0.01624899874127475),\n","  (9, 0.018194301407483696)],\n"," [(0, 0.014157888775625776),\n","  (1, 0.058896817306603226),\n","  (2, 0.02865556688186657),\n","  (3, 0.017782308302185974),\n","  (4, 0.6943028655566881),\n","  (5, 0.015290519877675837),\n","  (6, 0.016876203420545924),\n","  (7, 0.01495073054706082),\n","  (8, 0.021859780269566198),\n","  (9, 0.11722731906218142)],\n"," [(0, 0.06945622467250966),\n","  (1, 0.057581754782772596),\n","  (2, 0.024879841673734804),\n","  (3, 0.029403449250777494),\n","  (4, 0.5641315615870324),\n","  (5, 0.031099802092168505),\n","  (6, 0.018471397606257656),\n","  (7, 0.04259730468381868),\n","  (8, 0.013099613608519462),\n","  (9, 0.14927905004240882)],\n"," [(0, 0.14272809394760613),\n","  (1, 0.02489210077285958),\n","  (2, 0.04988457291980327),\n","  (3, 0.022583559168925023),\n","  (4, 0.5348790524942286),\n","  (5, 0.11442336645588679),\n","  (6, 0.04315969085616782),\n","  (7, 0.013449764127270902),\n","  (8, 0.012144936264177456),\n","  (9, 0.04185486299307437)],\n"," [(0, 0.05666350110794555),\n","  (1, 0.1623931623931624),\n","  (2, 0.013295346628679962),\n","  (3, 0.015933312229608525),\n","  (4, 0.6442967183707925),\n","  (5, 0.04674475044845415),\n","  (6, 0.01899335232668566),\n","  (7, 0.021209243431465654),\n","  (8, 0.010024269283528543),\n","  (9, 0.010446343779677113)],\n"," [(0, 0.017032918542052775),\n","  (1, 0.051628276409849086),\n","  (2, 0.16988791810078546),\n","  (3, 0.05480540111199365),\n","  (4, 0.016238637366516637),\n","  (5, 0.08198746800811932),\n","  (6, 0.04042008648839467),\n","  (7, 0.029211896566940253),\n","  (8, 0.5202541699761716),\n","  (9, 0.018533227429176596)],\n"," [(0, 0.02224320637918372),\n","  (1, 0.040499422935683566),\n","  (2, 0.12600986255377192),\n","  (3, 0.026649879341097474),\n","  (4, 0.0471094323785542),\n","  (5, 0.18570978910922256),\n","  (6, 0.026754800125904945),\n","  (7, 0.021298919315916487),\n","  (8, 0.44885111740635825),\n","  (9, 0.054873570454307005)],\n"," [(0, 0.013625175808720115),\n","  (1, 0.00879043600562588),\n","  (2, 0.0964310829817159),\n","  (3, 0.0249648382559775),\n","  (4, 0.010812236286919833),\n","  (5, 0.025492264416315052),\n","  (6, 0.13071378340365686),\n","  (7, 0.025492264416315052),\n","  (8, 0.6505801687763714),\n","  (9, 0.013097749648382562)],\n"," [(0, 0.013595532896334061),\n","  (1, 0.022335518329691672),\n","  (2, 0.02071700250869952),\n","  (3, 0.010115723881200939),\n","  (4, 0.008982762806506433),\n","  (5, 0.02201181516549324),\n","  (6, 0.11936554179817108),\n","  (7, 0.010034798090151332),\n","  (8, 0.7585983652990208),\n","  (9, 0.014242939224730921)],\n"," [(0, 0.0599210620739146),\n","  (1, 0.030020332496112904),\n","  (2, 0.06530319339791892),\n","  (3, 0.1038153330941275),\n","  (4, 0.008731013036718097),\n","  (5, 0.07881832316708527),\n","  (6, 0.28919985647649804),\n","  (7, 0.04078459514412151),\n","  (8, 0.31072838177251527),\n","  (9, 0.01267790934098792)],\n"," [(0, 0.05089848527613134),\n","  (1, 0.0750776178379904),\n","  (2, 0.020227678991438517),\n","  (3, 0.05456769216295042),\n","  (4, 0.031799793019098695),\n","  (5, 0.20039514535704206),\n","  (6, 0.012889265217800358),\n","  (7, 0.05278012983347446),\n","  (8, 0.4778436353372848),\n","  (9, 0.023520556966788973)],\n"," [(0, 0.03219802717290155),\n","  (1, 0.4442583286804393),\n","  (2, 0.15633724176437747),\n","  (3, 0.029778522240833803),\n","  (4, 0.03666480550902662),\n","  (5, 0.08189093616229295),\n","  (6, 0.023264470500651408),\n","  (7, 0.018239344872510704),\n","  (8, 0.13921459147589804),\n","  (9, 0.03815373162106831)],\n"," [(0, 0.02316745436011491),\n","  (1, 0.20164952275044018),\n","  (2, 0.03428783245297007),\n","  (3, 0.015197850060235382),\n","  (4, 0.02335279399499583),\n","  (5, 0.08414419423593736),\n","  (6, 0.05597256973403762),\n","  (7, 0.0163098878695209),\n","  (8, 0.3561301084236864),\n","  (9, 0.18978778611806135)],\n"," [(0, 0.1357832988267771),\n","  (1, 0.08712905452035886),\n","  (2, 0.03674948240165631),\n","  (3, 0.01932367149758454),\n","  (4, 0.06832298136645963),\n","  (5, 0.06418219461697723),\n","  (6, 0.028295376121463076),\n","  (7, 0.04089026915113872),\n","  (8, 0.43668046928916493),\n","  (9, 0.0826432022084196)],\n"," [(0, 0.0225951519140068),\n","  (1, 0.23099703849950642),\n","  (2, 0.05067456400131622),\n","  (3, 0.018536799385762862),\n","  (4, 0.028189097290775473),\n","  (5, 0.18273554897444336),\n","  (6, 0.0587912690578041),\n","  (7, 0.050345508390918066),\n","  (8, 0.28671712186026105),\n","  (9, 0.07041790062520566)],\n"," [(0, 0.09610961096109608),\n","  (1, 0.05300530053005299),\n","  (2, 0.0108010801080108),\n","  (3, 0.079007900790079),\n","  (4, 0.1612161216121612),\n","  (5, 0.21882188218821877),\n","  (6, 0.012101210121012097),\n","  (7, 0.04950495049504949),\n","  (8, 0.026202620262026195),\n","  (9, 0.2932293229322932)],\n"," [(0, 0.01825168107588857),\n","  (1, 0.008965738072366315),\n","  (2, 0.032740954210694846),\n","  (3, 0.021213576689081012),\n","  (4, 0.0068844060198527054),\n","  (5, 0.20437079731027857),\n","  (6, 0.04050592379122638),\n","  (7, 0.015449887928274095),\n","  (8, 0.09390009606147935),\n","  (9, 0.5577169388408582)],\n"," [(0, 0.023899007420256337),\n","  (1, 0.041052327262214514),\n","  (2, 0.058109280138768434),\n","  (3, 0.022068035077575408),\n","  (4, 0.12855353184928206),\n","  (5, 0.24072467957984003),\n","  (6, 0.0163823841187241),\n","  (7, 0.017827888599787994),\n","  (8, 0.029391924448299123),\n","  (9, 0.421990941505252)],\n"," [(0, 0.010466222645099905),\n","  (1, 0.06977481763399937),\n","  (2, 0.02970715720477852),\n","  (3, 0.024315466751242205),\n","  (4, 0.12844909609895339),\n","  (5, 0.07210064488846601),\n","  (6, 0.032455862141875465),\n","  (7, 0.1874405328258801),\n","  (8, 0.023681150227296756),\n","  (9, 0.4216090495824083)],\n"," [(0, 0.12745670363884024),\n","  (1, 0.07569566063436466),\n","  (2, 0.05137186223000584),\n","  (3, 0.040474800544853085),\n","  (4, 0.08445222805993384),\n","  (5, 0.02918855808523059),\n","  (6, 0.05565285074917299),\n","  (7, 0.07336057598754621),\n","  (8, 0.027048063825647013),\n","  (9, 0.43529869624440554)],\n"," [(0, 0.024095117845117846),\n","  (1, 0.06828703703703703),\n","  (2, 0.0493476430976431),\n","  (3, 0.02251683501683502),\n","  (4, 0.24442340067340068),\n","  (5, 0.0484006734006734),\n","  (6, 0.010627104377104377),\n","  (7, 0.3063973063973064),\n","  (8, 0.015361952861952861),\n","  (9, 0.21054292929292928)],\n"," [(0, 0.058444444444444445),\n","  (1, 0.08588888888888889),\n","  (2, 0.2927777777777778),\n","  (3, 0.07933333333333334),\n","  (4, 0.016555555555555556),\n","  (5, 0.13233333333333333),\n","  (6, 0.06444444444444444),\n","  (7, 0.19333333333333333),\n","  (8, 0.025222222222222222),\n","  (9, 0.051666666666666666)],\n"," [(0, 0.0763147489126137),\n","  (1, 0.022406748385396077),\n","  (2, 0.0512719124818769),\n","  (3, 0.02688809806247529),\n","  (4, 0.030051403716884147),\n","  (5, 0.05285356530908133),\n","  (6, 0.016343745881112433),\n","  (7, 0.6835376301568473),\n","  (8, 0.01700276789244761),\n","  (9, 0.023329379201265325)],\n"," [(0, 0.11354547035298208),\n","  (1, 0.11441488436793601),\n","  (2, 0.11006781429316641),\n","  (3, 0.028690662493479395),\n","  (4, 0.2491740566857938),\n","  (5, 0.15753781950965048),\n","  (6, 0.031125021735350374),\n","  (7, 0.10972004868718484),\n","  (8, 0.02729960006955312),\n","  (9, 0.058424621804903494)],\n"," [(0, 0.034478923367812254),\n","  (1, 0.13613613613613615),\n","  (2, 0.10799688577466356),\n","  (3, 0.0659548437326215),\n","  (4, 0.07329551773996218),\n","  (5, 0.2988544099655211),\n","  (6, 0.017906795684573464),\n","  (7, 0.16016016016016016),\n","  (8, 0.06528750973195417),\n","  (9, 0.03992881770659548)],\n"," [(0, 0.43707277844792913),\n","  (1, 0.017960058973327968),\n","  (2, 0.16861010588392975),\n","  (3, 0.06942769065808871),\n","  (4, 0.01822811955501943),\n","  (5, 0.06996381182147164),\n","  (6, 0.035920117946655936),\n","  (7, 0.08457311352365633),\n","  (8, 0.063798418442568),\n","  (9, 0.034445784747352895)],\n"," [(0, 0.4231099656357388),\n","  (1, 0.11311569301260023),\n","  (2, 0.27491408934707906),\n","  (3, 0.04367124856815578),\n","  (4, 0.0143184421534937),\n","  (5, 0.02620274914089347),\n","  (6, 0.02663230240549828),\n","  (7, 0.036941580756013746),\n","  (8, 0.019043528064146623),\n","  (9, 0.0220504009163803)],\n"," [(0, 0.02176220806794055),\n","  (1, 0.012561924982307147),\n","  (2, 0.02724699221514508),\n","  (3, 0.019285208775654636),\n","  (4, 0.030520169851380043),\n","  (5, 0.5462668082094834),\n","  (6, 0.01556970983722576),\n","  (7, 0.045824486907289456),\n","  (8, 0.26769285208775656),\n","  (9, 0.01326963906581741)],\n"," [(0, 0.0603207810320781),\n","  (1, 0.031845653184565316),\n","  (2, 0.07635983263598327),\n","  (3, 0.04625755462575546),\n","  (4, 0.021385402138540215),\n","  (5, 0.4664109716410972),\n","  (6, 0.01813110181311018),\n","  (7, 0.03161320316132032),\n","  (8, 0.16352859135285913),\n","  (9, 0.08414690841469084)],\n"," [(0, 0.13042707779549884),\n","  (1, 0.03437887648413964),\n","  (2, 0.025695552011341487),\n","  (3, 0.04873294346978557),\n","  (4, 0.01364522417153996),\n","  (5, 0.5397838029416977),\n","  (6, 0.016037568669147618),\n","  (7, 0.1596668438773702),\n","  (8, 0.02383483962431331),\n","  (9, 0.007797270955165692)],\n"," [(0, 0.04831433998100665),\n","  (1, 0.04226020892687559),\n","  (2, 0.058167141500474834),\n","  (3, 0.17248338081671416),\n","  (4, 0.011396011396011397),\n","  (5, 0.1177587844254511),\n","  (6, 0.37428774928774927),\n","  (7, 0.0852326685660019),\n","  (8, 0.05555555555555555),\n","  (9, 0.03454415954415954)],\n"," [(0, 0.018468059339993945),\n","  (1, 0.05722070844686648),\n","  (2, 0.10818447875668584),\n","  (3, 0.023312140478353012),\n","  (4, 0.012312039559995963),\n","  (5, 0.1873044706832173),\n","  (6, 0.012614794631143405),\n","  (7, 0.013422141487536583),\n","  (8, 0.3960036330608538),\n","  (9, 0.1711575335553537)],\n"," [(0, 0.028822882288228824),\n","  (1, 0.020095342867620096),\n","  (2, 0.18115144847818115),\n","  (3, 0.04686468646864687),\n","  (4, 0.04598459845984598),\n","  (5, 0.2459112577924459),\n","  (6, 0.014374770810414375),\n","  (7, 0.014081408140814081),\n","  (8, 0.30986431976530987),\n","  (9, 0.09284928492849286)],\n"," [(0, 0.04143747708104144),\n","  (1, 0.05922258892555922),\n","  (2, 0.08269160249358269),\n","  (3, 0.04143747708104144),\n","  (4, 0.021268793546021268),\n","  (5, 0.1301796846351302),\n","  (6, 0.028786211954528785),\n","  (7, 0.02841950861752842),\n","  (8, 0.46442977631096444),\n","  (9, 0.10212687935460213)],\n"," [(0, 0.0359882005899705),\n","  (1, 0.016027531956735497),\n","  (2, 0.026352015732546707),\n","  (3, 0.02527040314650934),\n","  (4, 0.07089478859390363),\n","  (5, 0.6706981317600786),\n","  (6, 0.010914454277286136),\n","  (7, 0.09449360865290068),\n","  (8, 0.026253687315634218),\n","  (9, 0.023107177974434612)],\n"," [(0, 0.03871553574376754),\n","  (1, 0.01015354135710748),\n","  (2, 0.04490671949810137),\n","  (3, 0.07388145946838369),\n","  (4, 0.018821198613174838),\n","  (5, 0.3366352980023114),\n","  (6, 0.011391778107974244),\n","  (7, 0.4374277695228661),\n","  (8, 0.009328050189862968),\n","  (9, 0.018738649496450387)],\n"," [(0, 0.01887099398080365),\n","  (1, 0.030421343744916222),\n","  (2, 0.1870831299821051),\n","  (3, 0.012526435659671386),\n","  (4, 0.010899625833740037),\n","  (5, 0.4925980152920124),\n","  (6, 0.025866276232308447),\n","  (7, 0.03652188059215879),\n","  (8, 0.10110623068163332),\n","  (9, 0.08410606800065074)],\n"," [(0, 0.13181685622630504),\n","  (1, 0.06124234470691164),\n","  (2, 0.14333624963546224),\n","  (3, 0.03762029746281715),\n","  (4, 0.029454651501895596),\n","  (5, 0.28258967629046366),\n","  (6, 0.015602216389617965),\n","  (7, 0.12875473899095946),\n","  (8, 0.0568678915135608),\n","  (9, 0.11271507728200642)],\n"," [(0, 0.08655520724486243),\n","  (1, 0.04083942877046326),\n","  (2, 0.09613375130616511),\n","  (3, 0.05529432253570185),\n","  (4, 0.4357366771159875),\n","  (5, 0.15882967607105541),\n","  (6, 0.02351097178683386),\n","  (7, 0.04371299198885407),\n","  (8, 0.009056078021595264),\n","  (9, 0.050330895158481374)],\n"," [(0, 0.15891472868217055),\n","  (1, 0.03133074935400517),\n","  (2, 0.2559754521963824),\n","  (3, 0.10739664082687339),\n","  (4, 0.11094961240310078),\n","  (5, 0.18362403100775193),\n","  (6, 0.041343669250646),\n","  (7, 0.022771317829457363),\n","  (8, 0.042958656330749356),\n","  (9, 0.04473514211886305)],\n"," [(0, 0.06025163919900762),\n","  (1, 0.07177033492822966),\n","  (2, 0.06716285663654084),\n","  (3, 0.1798688640793904),\n","  (4, 0.1619705830232146),\n","  (5, 0.22603225234804183),\n","  (6, 0.11155413786992735),\n","  (7, 0.023214602161970584),\n","  (8, 0.05325181640971115),\n","  (9, 0.044922913343965976)],\n"," [(0, 0.5143951833607007),\n","  (1, 0.09337712096332787),\n","  (2, 0.07914614121510674),\n","  (3, 0.09447181171319104),\n","  (4, 0.04389709906951287),\n","  (5, 0.04356869184455392),\n","  (6, 0.03459222769567598),\n","  (7, 0.06382047071702246),\n","  (8, 0.01510673234811166),\n","  (9, 0.01762452107279694)],\n"," [(0, 0.1545138888888889),\n","  (1, 0.02618634259259259),\n","  (2, 0.04673032407407408),\n","  (3, 0.021122685185185185),\n","  (4, 0.01461226851851852),\n","  (5, 0.38700810185185186),\n","  (6, 0.20442708333333334),\n","  (7, 0.03443287037037037),\n","  (8, 0.05555555555555555),\n","  (9, 0.05541087962962963)],\n"," [(0, 0.16001330671989356),\n","  (1, 0.03005100909292526),\n","  (2, 0.15047682412951874),\n","  (3, 0.07440674207141273),\n","  (4, 0.023619427811044577),\n","  (5, 0.4465513417609226),\n","  (6, 0.020625415834996674),\n","  (7, 0.05622089155023287),\n","  (8, 0.023064981148813484),\n","  (9, 0.014970059880239521)],\n"," [(0, 0.024607870761716916),\n","  (1, 0.03493941955480417),\n","  (2, 0.17328825021132713),\n","  (3, 0.026298487836949377),\n","  (4, 0.024138254907485678),\n","  (5, 0.6175448483140791),\n","  (6, 0.01934817319432704),\n","  (7, 0.019629942706865782),\n","  (8, 0.04292288907673523),\n","  (9, 0.017281863435709588)],\n"," [(0, 0.19129019129019129),\n","  (1, 0.052231718898385564),\n","  (2, 0.115995115995116),\n","  (3, 0.09293175959842627),\n","  (4, 0.04409171075837742),\n","  (5, 0.32967032967032966),\n","  (6, 0.026862026862026864),\n","  (7, 0.06701940035273368),\n","  (8, 0.060507393840727175),\n","  (9, 0.019400352733686066)],\n"," [(0, 0.02042801556420234),\n","  (1, 0.03480328577604843),\n","  (2, 0.25194552529182884),\n","  (3, 0.3492217898832685),\n","  (4, 0.013942931258106358),\n","  (5, 0.0756593169044531),\n","  (6, 0.047341115434500654),\n","  (7, 0.04950281020319931),\n","  (8, 0.09803285776048423),\n","  (9, 0.05912235192390835)],\n"," [(0, 0.024568863690054336),\n","  (1, 0.016379242460036223),\n","  (2, 0.1689896842271045),\n","  (3, 0.027403732577368296),\n","  (4, 0.02425387825813056),\n","  (5, 0.22978187258839278),\n","  (6, 0.24734231041814317),\n","  (7, 0.01842664776754075),\n","  (8, 0.20592172612016693),\n","  (9, 0.03693204189306245)],\n"," [(0, 0.4208084824387011),\n","  (1, 0.06273470289374862),\n","  (2, 0.12303954053457035),\n","  (3, 0.04506295559973492),\n","  (4, 0.052794345040865914),\n","  (5, 0.0421912966644577),\n","  (6, 0.022752374641042634),\n","  (7, 0.03799425668212945),\n","  (8, 0.032250938811574995),\n","  (9, 0.16037110669317428)],\n"," [(0, 0.07851558198436294),\n","  (1, 0.013544763792533858),\n","  (2, 0.5616121572514039),\n","  (3, 0.06089637705098556),\n","  (4, 0.013324523730866641),\n","  (5, 0.1101200308336086),\n","  (6, 0.03744081048342693),\n","  (7, 0.027199647615901327),\n","  (8, 0.08721506442021802),\n","  (9, 0.010131042836691993)],\n"," [(0, 0.02068442711717566),\n","  (1, 0.048624648039852725),\n","  (2, 0.07699805068226122),\n","  (3, 0.05815464587394412),\n","  (4, 0.010179770413688545),\n","  (5, 0.14100064977257962),\n","  (6, 0.04256010396361274),\n","  (7, 0.022200563136235654),\n","  (8, 0.3703703703703704),\n","  (9, 0.20922677063027942)],\n"," [(0, 0.022205712398877333),\n","  (1, 0.04465907214792802),\n","  (2, 0.22486379395740466),\n","  (3, 0.05679379230642232),\n","  (4, 0.013868251609707775),\n","  (5, 0.07231302625061913),\n","  (6, 0.029552583787353474),\n","  (7, 0.06059105167574707),\n","  (8, 0.45220406141654285),\n","  (9, 0.02294865444939739)],\n"," [(0, 0.022337962962962962),\n","  (1, 0.06655092592592593),\n","  (2, 0.24178240740740742),\n","  (3, 0.05983796296296296),\n","  (4, 0.016319444444444445),\n","  (5, 0.2023148148148148),\n","  (6, 0.08101851851851852),\n","  (7, 0.017939814814814815),\n","  (8, 0.27847222222222223),\n","  (9, 0.013425925925925926)],\n"," [(0, 0.1358024691358025),\n","  (1, 0.02399907695857852),\n","  (2, 0.2855659397715473),\n","  (3, 0.03069112726433599),\n","  (4, 0.03715241721472252),\n","  (5, 0.3140648436598593),\n","  (6, 0.04707511249567325),\n","  (7, 0.019730010384215995),\n","  (8, 0.0897657782392985),\n","  (9, 0.016153224875966314)],\n"," [(0, 0.29306690887072356),\n","  (1, 0.032394792612776266),\n","  (2, 0.04738116863457463),\n","  (3, 0.07023917650620648),\n","  (4, 0.027399333938843477),\n","  (5, 0.018922191946715107),\n","  (6, 0.16651528913109295),\n","  (7, 0.01952770208900999),\n","  (8, 0.014380865879503482),\n","  (9, 0.310172570390554)],\n"," [(0, 0.11085420680796401),\n","  (1, 0.05574823378291585),\n","  (2, 0.03134232498394347),\n","  (3, 0.46473988439306346),\n","  (4, 0.010404624277456646),\n","  (5, 0.1667308927424534),\n","  (6, 0.02376364804110468),\n","  (7, 0.019524727039177903),\n","  (8, 0.10404624277456645),\n","  (9, 0.012845215157353882)],\n"," [(0, 0.03540217585161405),\n","  (1, 0.02577135723203139),\n","  (2, 0.02577135723203139),\n","  (3, 0.036026395576957373),\n","  (4, 0.016408061351881578),\n","  (5, 0.037542357767076866),\n","  (6, 0.024522917781344748),\n","  (7, 0.7610130194399858),\n","  (8, 0.028179061886927054),\n","  (9, 0.009363295880149813)],\n"," [(0, 0.5333785924417289),\n","  (1, 0.02251640642679339),\n","  (2, 0.025684544014482915),\n","  (3, 0.02975786377008373),\n","  (4, 0.14777098891151844),\n","  (5, 0.04944557592215433),\n","  (6, 0.07479067662367052),\n","  (7, 0.011088481556913329),\n","  (8, 0.01040959493097986),\n","  (9, 0.09515727540167458)],\n"," [(0, 0.027460482289142725),\n","  (1, 0.14584054459443868),\n","  (2, 0.01684550594207915),\n","  (3, 0.6793584862120687),\n","  (4, 0.015114803276797047),\n","  (5, 0.04603669089650398),\n","  (6, 0.010268835814007154),\n","  (7, 0.033575631706472826),\n","  (8, 0.010268835814007154),\n","  (9, 0.01523018345448252)],\n"," [(0, 0.1529498993750662),\n","  (1, 0.01874801398156975),\n","  (2, 0.11047558521343077),\n","  (3, 0.18631500900328354),\n","  (4, 0.21512551636479185),\n","  (5, 0.034424319457684566),\n","  (6, 0.12202097235462345),\n","  (7, 0.017265120220315646),\n","  (8, 0.037813790911979665),\n","  (9, 0.10486177311725453)],\n"," [(0, 0.24504249291784702),\n","  (1, 0.023764557758892038),\n","  (2, 0.21340887629839472),\n","  (3, 0.02045955303745672),\n","  (4, 0.01872836008813346),\n","  (5, 0.39628580421781556),\n","  (6, 0.022033364809568776),\n","  (7, 0.028171230720805792),\n","  (8, 0.01872836008813346),\n","  (9, 0.01337740006295247)],\n"," [(0, 0.016263335935467083),\n","  (1, 0.4549830861306271),\n","  (2, 0.021988030184751498),\n","  (3, 0.011969815248503773),\n","  (4, 0.23132969034608378),\n","  (5, 0.019255789747593028),\n","  (6, 0.014181628935727296),\n","  (7, 0.10057246942492844),\n","  (8, 0.02498048399687744),\n","  (9, 0.10447567004944054)],\n"," [(0, 0.04410678484752559),\n","  (1, 0.4384298828743273),\n","  (2, 0.03207766170729134),\n","  (3, 0.03461010868418276),\n","  (4, 0.023319615912208505),\n","  (5, 0.021314762055502798),\n","  (6, 0.042629524111005596),\n","  (7, 0.0778727445394112),\n","  (8, 0.0072807850585628366),\n","  (9, 0.27835813020998207)],\n"," [(0, 0.03023164507263448),\n","  (1, 0.7085787200628191),\n","  (2, 0.025618374558303895),\n","  (3, 0.04220651747153514),\n","  (4, 0.03052610914801728),\n","  (5, 0.0877502944640754),\n","  (6, 0.017078916372202595),\n","  (7, 0.014526894385551633),\n","  (8, 0.02856301531213193),\n","  (9, 0.014919513152728704)],\n"," [(0, 0.3069139966273187),\n","  (1, 0.3395165823496346),\n","  (2, 0.15158328649053776),\n","  (3, 0.022297170695147087),\n","  (4, 0.03372681281618887),\n","  (5, 0.031290987446130784),\n","  (6, 0.023608768971332208),\n","  (7, 0.030541502716882142),\n","  (8, 0.016863406408094434),\n","  (9, 0.04365748547873337)],\n"," [(0, 0.035822021116138754),\n","  (1, 0.6782302664655605),\n","  (2, 0.02488687782805429),\n","  (3, 0.045877325289089985),\n","  (4, 0.08471593765711412),\n","  (5, 0.018979386626445445),\n","  (6, 0.022750125691302157),\n","  (7, 0.05857214680744091),\n","  (8, 0.017596782302664652),\n","  (9, 0.012569130216189037)],\n"," [(0, 0.173031588873173),\n","  (1, 0.47414741474147404),\n","  (2, 0.03127455602703127),\n","  (3, 0.046204620462046195),\n","  (4, 0.05783435486405782),\n","  (5, 0.1213264183561213),\n","  (6, 0.014301430143014299),\n","  (7, 0.027974225994027966),\n","  (8, 0.026402640264026396),\n","  (9, 0.027502750275027497)],\n"," [(0, 0.03837078075305222),\n","  (1, 0.27228890940802297),\n","  (2, 0.0221606648199446),\n","  (3, 0.06237816764132553),\n","  (4, 0.0074894839437775725),\n","  (5, 0.4487534626038781),\n","  (6, 0.017646455319585513),\n","  (7, 0.04103826818508259),\n","  (8, 0.03570329332102185),\n","  (9, 0.05417051400430902)],\n"," [(0, 0.13827420676735747),\n","  (1, 0.37454630605315536),\n","  (2, 0.0624048706240487),\n","  (3, 0.030675564922140266),\n","  (4, 0.013815712445849433),\n","  (5, 0.2777192366233462),\n","  (6, 0.012644889357218124),\n","  (7, 0.030441400304414),\n","  (8, 0.018616087109237796),\n","  (9, 0.04086172579323264)],\n"," [(0, 0.017824616185271924),\n","  (1, 0.38862867551392144),\n","  (2, 0.0521727816809784),\n","  (3, 0.0195160031225605),\n","  (4, 0.02810304449648712),\n","  (5, 0.23224043715846995),\n","  (6, 0.01678376268540203),\n","  (7, 0.1405152224824356),\n","  (8, 0.07988550611501431),\n","  (9, 0.024329950559458757)],\n"," [(0, 0.15774093132583702),\n","  (1, 0.13340956737183154),\n","  (2, 0.018105584143319996),\n","  (3, 0.09078203417826061),\n","  (4, 0.2759036909980307),\n","  (5, 0.20805539673464205),\n","  (6, 0.02223492789530526),\n","  (7, 0.04161107934692841),\n","  (8, 0.015945619719204628),\n","  (9, 0.03621116828663999)],\n"," [(0, 0.04203718674211803),\n","  (1, 0.018144255816042395),\n","  (2, 0.06628940986257073),\n","  (3, 0.7045719931734483),\n","  (4, 0.018234078864636665),\n","  (5, 0.0479655079493398),\n","  (6, 0.015629210455402855),\n","  (7, 0.026946914578280787),\n","  (8, 0.026946914578280787),\n","  (9, 0.033234527979879636)],\n"," [(0, 0.0867208672086721),\n","  (1, 0.06233062330623307),\n","  (2, 0.09936766034327012),\n","  (3, 0.06684733514001807),\n","  (4, 0.047425474254742556),\n","  (5, 0.4981933152664861),\n","  (6, 0.038843721770551044),\n","  (7, 0.03748870822041554),\n","  (8, 0.03252032520325204),\n","  (9, 0.030261969286359533)],\n"," [(0, 0.10797385620915034),\n","  (1, 0.051372549019607854),\n","  (2, 0.18522875816993467),\n","  (3, 0.08248366013071896),\n","  (4, 0.09934640522875818),\n","  (5, 0.22875816993464054),\n","  (6, 0.1100653594771242),\n","  (7, 0.06993464052287583),\n","  (8, 0.04339869281045752),\n","  (9, 0.02143790849673203)],\n"," [(0, 0.016918152720621856),\n","  (1, 0.04275262917238226),\n","  (2, 0.05227861606462429),\n","  (3, 0.03101661332114007),\n","  (4, 0.22908093278463648),\n","  (5, 0.037951531778692274),\n","  (6, 0.014860539551897577),\n","  (7, 0.018747142203932327),\n","  (8, 0.49946654473403446),\n","  (9, 0.05692729766803841)],\n"," [(0, 0.10192592592592592),\n","  (1, 0.6101728395061728),\n","  (2, 0.13362962962962963),\n","  (3, 0.0134320987654321),\n","  (4, 0.03664197530864197),\n","  (5, 0.043160493827160494),\n","  (6, 0.014617283950617285),\n","  (7, 0.018074074074074076),\n","  (8, 0.014222222222222223),\n","  (9, 0.014123456790123457)],\n"," [(0, 0.018232990699951053),\n","  (1, 0.024473813020068527),\n","  (2, 0.036588350465002445),\n","  (3, 0.016519823788546256),\n","  (4, 0.031938325991189426),\n","  (5, 0.22369065100342633),\n","  (6, 0.023127753303964757),\n","  (7, 0.023617229564366127),\n","  (8, 0.5766030347528145),\n","  (9, 0.025208027410670582)],\n"," [(0, 0.5172345679012346),\n","  (1, 0.08325925925925926),\n","  (2, 0.05787654320987654),\n","  (3, 0.020148148148148148),\n","  (4, 0.02745679012345679),\n","  (5, 0.09906172839506173),\n","  (6, 0.0214320987654321),\n","  (7, 0.12523456790123458),\n","  (8, 0.02360493827160494),\n","  (9, 0.024691358024691357)],\n"," [(0, 0.06518182962124072),\n","  (1, 0.016610041525103814),\n","  (2, 0.2845098779413615),\n","  (3, 0.056373474267019004),\n","  (4, 0.02378255945639864),\n","  (5, 0.4278344029193406),\n","  (6, 0.014093368566754751),\n","  (7, 0.07449351956713225),\n","  (8, 0.018371712595948157),\n","  (9, 0.018749213539700518)],\n"," [(0, 0.34986945169712796),\n","  (1, 0.02901073397156948),\n","  (2, 0.09747606614447346),\n","  (3, 0.2344067304902814),\n","  (4, 0.03220191470844212),\n","  (5, 0.044386422976501305),\n","  (6, 0.10733971569480708),\n","  (7, 0.030461270670147953),\n","  (8, 0.026689875253843923),\n","  (9, 0.048157818392805335)],\n"," [(0, 0.3937950937950938),\n","  (1, 0.024386724386724387),\n","  (2, 0.10966810966810966),\n","  (3, 0.1012987012987013),\n","  (4, 0.018614718614718615),\n","  (5, 0.18513708513708513),\n","  (6, 0.020923520923520924),\n","  (7, 0.11341991341991342),\n","  (8, 0.018903318903318905),\n","  (9, 0.013852813852813853)],\n"," [(0, 0.48105081826012064),\n","  (1, 0.10658914728682171),\n","  (2, 0.031653746770025845),\n","  (3, 0.08548664944013783),\n","  (4, 0.02497846683893196),\n","  (5, 0.17312661498708012),\n","  (6, 0.022179155900086137),\n","  (7, 0.021963824289405687),\n","  (8, 0.03273040482342809),\n","  (9, 0.020241171403962106)],\n"," [(0, 0.08863346104725416),\n","  (1, 0.02567049808429119),\n","  (2, 0.153639846743295),\n","  (3, 0.06768837803320563),\n","  (4, 0.019412515964240103),\n","  (5, 0.32873563218390806),\n","  (6, 0.01673052362707535),\n","  (7, 0.26002554278416345),\n","  (8, 0.018773946360153258),\n","  (9, 0.020689655172413793)],\n"," [(0, 0.047160731472569765),\n","  (1, 0.14757779916586458),\n","  (2, 0.20971019142337713),\n","  (3, 0.05518126403593197),\n","  (4, 0.01914233771789113),\n","  (5, 0.21559191530317606),\n","  (6, 0.10170035290343277),\n","  (7, 0.05186611057640893),\n","  (8, 0.12961180622393323),\n","  (9, 0.022457491177414177)],\n"," [(0, 0.024008350730688938),\n","  (1, 0.09429366736256091),\n","  (2, 0.28682440269079107),\n","  (3, 0.04128972396195779),\n","  (4, 0.06123869171885874),\n","  (5, 0.2739503595453492),\n","  (6, 0.034794711203897016),\n","  (7, 0.04198561818603573),\n","  (8, 0.10705172813732314),\n","  (9, 0.0345627464625377)],\n"," [(0, 0.05034901018423161),\n","  (1, 0.03432887058015791),\n","  (2, 0.03158256093374528),\n","  (3, 0.03478658885456002),\n","  (4, 0.04073692642178739),\n","  (5, 0.09326009840942899),\n","  (6, 0.020254033642293168),\n","  (7, 0.014532555212266849),\n","  (8, 0.10905137887630163),\n","  (9, 0.5711179768852271)],\n"," [(0, 0.3836901121304791),\n","  (1, 0.05137614678899083),\n","  (2, 0.036289500509684),\n","  (3, 0.029765545361875638),\n","  (4, 0.1979612640163099),\n","  (5, 0.08970438328236494),\n","  (6, 0.07686034658511723),\n","  (7, 0.01712538226299694),\n","  (8, 0.020387359836901122),\n","  (9, 0.09683995922528033)],\n"," [(0, 0.042270531400966184),\n","  (1, 0.15990338164251208),\n","  (2, 0.045491143317230275),\n","  (3, 0.02181964573268921),\n","  (4, 0.6510466988727859),\n","  (5, 0.029790660225442835),\n","  (6, 0.01634460547504026),\n","  (7, 0.015136876006441223),\n","  (8, 0.009259259259259259),\n","  (9, 0.008937198067632851)],\n"," [(0, 0.044733044733044736),\n","  (1, 0.020202020202020204),\n","  (2, 0.03405483405483405),\n","  (3, 0.03424723424723425),\n","  (4, 0.6587782587782588),\n","  (5, 0.07417027417027416),\n","  (6, 0.014141414141414142),\n","  (7, 0.020394420394420396),\n","  (8, 0.03838383838383838),\n","  (9, 0.0608946608946609)],\n"," [(0, 0.03699837624681049),\n","  (1, 0.01960102064486198),\n","  (2, 0.02238459754117374),\n","  (3, 0.04697286012526096),\n","  (4, 0.6696822083043378),\n","  (5, 0.031315240083507306),\n","  (6, 0.022268615170494086),\n","  (7, 0.01612154952447228),\n","  (8, 0.011366272326606355),\n","  (9, 0.12328926003247506)],\n"," [(0, 0.14780552985918388),\n","  (1, 0.010484119642306507),\n","  (2, 0.05416795148525028),\n","  (3, 0.030938431493473122),\n","  (4, 0.3245965669647446),\n","  (5, 0.04974817555761127),\n","  (6, 0.03319971220063727),\n","  (7, 0.017062390790420393),\n","  (8, 0.010175763182238668),\n","  (9, 0.32182135882413404)],\n"," [(0, 0.04807892004153687),\n","  (1, 0.030218068535825548),\n","  (2, 0.01505711318795431),\n","  (3, 0.025025960539979235),\n","  (4, 0.7520249221183801),\n","  (5, 0.01287642782969886),\n","  (6, 0.027829698857736243),\n","  (7, 0.020664589823468332),\n","  (8, 0.015160955347871237),\n","  (9, 0.05306334371754933)],\n"," [(0, 0.3742162026586406),\n","  (1, 0.021570102834211187),\n","  (2, 0.03160270880361174),\n","  (3, 0.04439428141459744),\n","  (4, 0.2947078003511412),\n","  (5, 0.031853523952846755),\n","  (6, 0.02708803611738149),\n","  (7, 0.020065211938801102),\n","  (8, 0.020316027088036117),\n","  (9, 0.13418610484073237)],\n"," [(0, 0.0167989417989418),\n","  (1, 0.06481481481481481),\n","  (2, 0.01798941798941799),\n","  (3, 0.01335978835978836),\n","  (4, 0.6156084656084656),\n","  (5, 0.1914021164021164),\n","  (6, 0.009656084656084656),\n","  (7, 0.019312169312169312),\n","  (8, 0.013227513227513227),\n","  (9, 0.03783068783068783)],\n"," [(0, 0.0484528135667878),\n","  (1, 0.07157802004184562),\n","  (2, 0.05550049554013875),\n","  (3, 0.025878207245898028),\n","  (4, 0.6644642660499945),\n","  (5, 0.05517013544763792),\n","  (6, 0.02830084792423742),\n","  (7, 0.014095363946701906),\n","  (8, 0.013104283669199428),\n","  (9, 0.02345556656755864)],\n"," [(0, 0.12334352701325178),\n","  (1, 0.09086937527304499),\n","  (2, 0.046890927624872576),\n","  (3, 0.023591087811271297),\n","  (4, 0.4967234600262123),\n","  (5, 0.06494830348041357),\n","  (6, 0.02985291976117664),\n","  (7, 0.03888160768894714),\n","  (8, 0.061598951507208385),\n","  (9, 0.023299839813601283)],\n"," [(0, 0.25220458553791886),\n","  (1, 0.08978675645342311),\n","  (2, 0.10293410293410293),\n","  (3, 0.06685906685906685),\n","  (4, 0.2797819464486131),\n","  (5, 0.025493025493025494),\n","  (6, 0.06301106301106302),\n","  (7, 0.06156806156806157),\n","  (8, 0.03639570306236973),\n","  (9, 0.0219656886323553)],\n"," [(0, 0.4428767391730354),\n","  (1, 0.054477758181461874),\n","  (2, 0.02861062120321379),\n","  (3, 0.04134822653341171),\n","  (4, 0.17381932196747008),\n","  (5, 0.055065647658240235),\n","  (6, 0.051734273956496164),\n","  (7, 0.10934744268077598),\n","  (8, 0.017636684303350966),\n","  (9, 0.025083284342543595)],\n"," [(0, 0.47166015856111715),\n","  (1, 0.02863566441841203),\n","  (2, 0.03987693764051592),\n","  (3, 0.04579339723109692),\n","  (4, 0.1102828067684298),\n","  (5, 0.1263755768548101),\n","  (6, 0.059756241864868075),\n","  (7, 0.015264465743698974),\n","  (8, 0.018459353922612713),\n","  (9, 0.08389539699443854)],\n"," [(0, 0.38821954484605076),\n","  (1, 0.09052878179384202),\n","  (2, 0.1693440428380187),\n","  (3, 0.10157295850066932),\n","  (4, 0.029618473895582323),\n","  (5, 0.03447121820615796),\n","  (6, 0.021084337349397585),\n","  (7, 0.08099062918340025),\n","  (8, 0.03346720214190093),\n","  (9, 0.050702811244979905)],\n"," [(0, 0.2825552825552826),\n","  (1, 0.08872508872508873),\n","  (2, 0.021021021021021026),\n","  (3, 0.03944853944853945),\n","  (4, 0.4197379197379198),\n","  (5, 0.03712803712803713),\n","  (6, 0.033579033579033586),\n","  (7, 0.03808353808353809),\n","  (8, 0.02115752115752116),\n","  (9, 0.018564018564018566)],\n"," [(0, 0.41801697530864196),\n","  (1, 0.08429783950617284),\n","  (2, 0.03742283950617284),\n","  (3, 0.02141203703703704),\n","  (4, 0.06066743827160494),\n","  (5, 0.2351466049382716),\n","  (6, 0.012249228395061729),\n","  (7, 0.03819444444444445),\n","  (8, 0.032310956790123455),\n","  (9, 0.06028163580246913)],\n"," [(0, 0.5736040609137056),\n","  (1, 0.1260011280315849),\n","  (2, 0.027862380146644106),\n","  (3, 0.023914269599548788),\n","  (4, 0.12679075014100394),\n","  (5, 0.02763677382966723),\n","  (6, 0.008798646362098138),\n","  (7, 0.016130851663846587),\n","  (8, 0.013197969543147208),\n","  (9, 0.05606316976875352)],\n"," [(0, 0.5296057347670252),\n","  (1, 0.022365591397849466),\n","  (2, 0.08587813620071685),\n","  (3, 0.03541218637992832),\n","  (4, 0.014623655913978496),\n","  (5, 0.1899641577060932),\n","  (6, 0.024229390681003587),\n","  (7, 0.05060931899641578),\n","  (8, 0.014480286738351257),\n","  (9, 0.032831541218638)],\n"," [(0, 0.5186736474694591),\n","  (1, 0.09703315881326353),\n","  (2, 0.04188481675392671),\n","  (3, 0.05794066317626528),\n","  (4, 0.04712041884816755),\n","  (5, 0.06352530541012218),\n","  (6, 0.030133798720186157),\n","  (7, 0.09691681210005819),\n","  (8, 0.026759744037230953),\n","  (9, 0.020011634671320538)],\n"," [(0, 0.16147779161477793),\n","  (1, 0.04469351044693511),\n","  (2, 0.026982150269821505),\n","  (3, 0.029334440293344408),\n","  (4, 0.4948111249481113),\n","  (5, 0.043448180434481815),\n","  (6, 0.09893455098934552),\n","  (7, 0.016189290161892904),\n","  (8, 0.012038190120381903),\n","  (9, 0.07209077072090772)],\n"," [(0, 0.36149082836779994),\n","  (1, 0.14440939362075006),\n","  (2, 0.02570393737586166),\n","  (3, 0.025937609533824037),\n","  (4, 0.2938427386376913),\n","  (5, 0.01460450987264867),\n","  (6, 0.05199205514662927),\n","  (7, 0.04171048019628461),\n","  (8, 0.019394789110877435),\n","  (9, 0.020913658137632896)],\n"," [(0, 0.24634858812074004),\n","  (1, 0.11954992967651197),\n","  (2, 0.044682462403981396),\n","  (3, 0.05128205128205129),\n","  (4, 0.3407984420642649),\n","  (5, 0.021097046413502112),\n","  (6, 0.015146597425078439),\n","  (7, 0.01655306718597858),\n","  (8, 0.013523747700962893),\n","  (9, 0.1310180677269285)],\n"," [(0, 0.08458125346644481),\n","  (1, 0.09816971713810316),\n","  (2, 0.0294878905527824),\n","  (3, 0.030227398779811426),\n","  (4, 0.014697726012201887),\n","  (5, 0.07607690885561103),\n","  (6, 0.17803660565723795),\n","  (7, 0.23377703826955074),\n","  (8, 0.24357552227768534),\n","  (9, 0.01136993899057127)],\n"," [(0, 0.05466237942122185),\n","  (1, 0.13754912468738834),\n","  (2, 0.04870787185899725),\n","  (3, 0.011432654519471237),\n","  (4, 0.11789924973204713),\n","  (5, 0.2961772061450518),\n","  (6, 0.03739430749077051),\n","  (7, 0.022150768131475523),\n","  (8, 0.2573538168393473),\n","  (9, 0.016672621174228887)],\n"," [(0, 0.32237396729050755),\n","  (1, 0.2065418984994099),\n","  (2, 0.017197774405665153),\n","  (3, 0.08126791434833924),\n","  (4, 0.04451188669701569),\n","  (5, 0.11650649131681),\n","  (6, 0.10419828022255945),\n","  (7, 0.02512223908278537),\n","  (8, 0.057494520316978596),\n","  (9, 0.024785027819929188)],\n"," [(0, 0.05974981604120677),\n","  (1, 0.05121412803532009),\n","  (2, 0.15673289183222958),\n","  (3, 0.08138337012509197),\n","  (4, 0.027520235467255336),\n","  (5, 0.3582045621780721),\n","  (6, 0.10286975717439294),\n","  (7, 0.06298749080206034),\n","  (8, 0.06946284032376748),\n","  (9, 0.029874908020603386)],\n"," [(0, 0.5282977558839628),\n","  (1, 0.07006020799124248),\n","  (2, 0.020142309797482213),\n","  (3, 0.05987958401751505),\n","  (4, 0.16562671045429667),\n","  (5, 0.06830870279146141),\n","  (6, 0.02200328407224959),\n","  (7, 0.018500273672687466),\n","  (8, 0.03065134099616858),\n","  (9, 0.016529830322933772)],\n"," [(0, 0.029834553837808517),\n","  (1, 0.02061296446975861),\n","  (2, 0.02124581864207576),\n","  (3, 0.5936172136334871),\n","  (4, 0.01970888708073411),\n","  (5, 0.027212729409637464),\n","  (6, 0.01600216978573366),\n","  (7, 0.18470301057770544),\n","  (8, 0.04863936352951813),\n","  (9, 0.03842328903354127)],\n"," [(0, 0.027684563758389263),\n","  (1, 0.01407531692766592),\n","  (2, 0.12360178970917227),\n","  (3, 0.02115958240119314),\n","  (4, 0.012583892617449664),\n","  (5, 0.607475764354959),\n","  (6, 0.009228187919463088),\n","  (7, 0.10188292319164802),\n","  (8, 0.0714951528709918),\n","  (9, 0.01081282624906786)],\n"," [(0, 0.026239799121155052),\n","  (1, 0.026616446955430008),\n","  (2, 0.044695543000627745),\n","  (3, 0.027871939736346517),\n","  (4, 0.015568110483364721),\n","  (5, 0.037790332705586946),\n","  (6, 0.03766478342749529),\n","  (7, 0.0842435655994978),\n","  (8, 0.6824858757062147),\n","  (9, 0.016823603264281232)],\n"," [(0, 0.00927997348579004),\n","  (1, 0.007788549175573784),\n","  (2, 0.017317093379733202),\n","  (3, 0.009445687298036291),\n","  (4, 0.007954262987820034),\n","  (5, 0.21327367636092467),\n","  (6, 0.009445687298036291),\n","  (7, 0.03720275084928329),\n","  (8, 0.6742895020299942),\n","  (9, 0.014002817134808186)],\n"," [(0, 0.041628684290489215),\n","  (1, 0.013977514433302947),\n","  (2, 0.05428947634964043),\n","  (3, 0.03382963638205206),\n","  (4, 0.009318342955535299),\n","  (5, 0.049123873189506735),\n","  (6, 0.13936999898713665),\n","  (7, 0.06472196900638104),\n","  (8, 0.5811809986832777),\n","  (9, 0.012559505722678012)],\n"," [(0, 0.012114328979746356),\n","  (1, 0.057637705848949464),\n","  (2, 0.04751088396744274),\n","  (3, 0.016373272761688436),\n","  (4, 0.038992996403558584),\n","  (5, 0.031137611205754305),\n","  (6, 0.00889646034450123),\n","  (7, 0.010978610637895136),\n","  (8, 0.7438955139125497),\n","  (9, 0.032462615937914066)],\n"," [(0, 0.04321713017365191),\n","  (1, 0.013187100143621882),\n","  (2, 0.12051181616399008),\n","  (3, 0.028985507246376812),\n","  (4, 0.017756887322104713),\n","  (5, 0.10301605953779867),\n","  (6, 0.02559080819950385),\n","  (7, 0.07494450972711843),\n","  (8, 0.5603864734299517),\n","  (9, 0.012403708055881969)],\n"," [(0, 0.05720702866957326),\n","  (1, 0.06553045316422249),\n","  (2, 0.0688333993922579),\n","  (3, 0.03871052979257498),\n","  (4, 0.021270973708548024),\n","  (5, 0.043070418813581715),\n","  (6, 0.21429515127493726),\n","  (7, 0.026291451975161844),\n","  (8, 0.43797066983749505),\n","  (9, 0.02681992337164751)],\n"," [(0, 0.04988179669030733),\n","  (1, 0.01276595744680851),\n","  (2, 0.10047281323877069),\n","  (3, 0.11241134751773049),\n","  (4, 0.1524822695035461),\n","  (5, 0.06382978723404255),\n","  (6, 0.1520094562647754),\n","  (7, 0.018912529550827423),\n","  (8, 0.22789598108747045),\n","  (9, 0.10933806146572105)],\n"," [(0, 0.01818181818181818),\n","  (1, 0.018062982768865123),\n","  (2, 0.0344622697563874),\n","  (3, 0.08758169934640522),\n","  (4, 0.01057635175282234),\n","  (5, 0.04194890077243019),\n","  (6, 0.11004159239453357),\n","  (7, 0.026500297088532382),\n","  (8, 0.6358882947118242),\n","  (9, 0.01675579322638146)],\n"," [(0, 0.010316108271815257),\n","  (1, 0.04694302479651713),\n","  (2, 0.03927692598902139),\n","  (3, 0.024039371569184176),\n","  (4, 0.03161082718152565),\n","  (5, 0.13013439333711907),\n","  (6, 0.015616127200454287),\n","  (7, 0.12625402233579405),\n","  (8, 0.48230172250615183),\n","  (9, 0.09350747681241718)],\n"," [(0, 0.027562446167097333),\n","  (1, 0.048234280792420335),\n","  (2, 0.07708871662360035),\n","  (3, 0.04597329888027563),\n","  (4, 0.022609819121447033),\n","  (5, 0.09894487510766582),\n","  (6, 0.023255813953488375),\n","  (7, 0.039298018949181744),\n","  (8, 0.5993755383290268),\n","  (9, 0.017657192075796732)],\n"," [(0, 0.019308943089430895),\n","  (1, 0.17231255645889793),\n","  (2, 0.03692411924119241),\n","  (3, 0.04392502258355917),\n","  (4, 0.12646793134598014),\n","  (5, 0.2302393857271906),\n","  (6, 0.026196928635953028),\n","  (7, 0.05871725383920506),\n","  (8, 0.21318879855465223),\n","  (9, 0.07271906052393858)],\n"," [(0, 0.03384762588207152),\n","  (1, 0.19782322688673604),\n","  (2, 0.01495036478890085),\n","  (3, 0.01530917354383447),\n","  (4, 0.5908384164573616),\n","  (5, 0.06829326635569907),\n","  (6, 0.017581628991747398),\n","  (7, 0.011721085994498266),\n","  (8, 0.025475421600287047),\n","  (9, 0.02415978949886377)],\n"," [(0, 0.015045135406218652),\n","  (1, 0.383149448345035),\n","  (2, 0.10174969352501947),\n","  (3, 0.0261896801515658),\n","  (4, 0.029198707232809533),\n","  (5, 0.10509305694862361),\n","  (6, 0.039340242951075445),\n","  (7, 0.014376462721497823),\n","  (8, 0.1525688175638025),\n","  (9, 0.13328875515435193)],\n"," [(0, 0.01043800604819042),\n","  (1, 0.015413130426299873),\n","  (2, 0.13120671154033753),\n","  (3, 0.010828211881767632),\n","  (4, 0.007999219588332846),\n","  (5, 0.21432055409228368),\n","  (6, 0.08974734172275875),\n","  (7, 0.020778460637986537),\n","  (8, 0.0716027704614184),\n","  (9, 0.42766559360062434)],\n"," [(0, 0.07278582930756844),\n","  (1, 0.2714975845410628),\n","  (2, 0.02383252818035427),\n","  (3, 0.08953301127214171),\n","  (4, 0.0214170692431562),\n","  (5, 0.12190016103059581),\n","  (6, 0.012238325281803542),\n","  (7, 0.01288244766505636),\n","  (8, 0.015780998389694042),\n","  (9, 0.35813204508856683)],\n"," [(0, 0.044710806697108064),\n","  (1, 0.014364535768645357),\n","  (2, 0.033295281582952814),\n","  (3, 0.014364535768645357),\n","  (4, 0.3308599695585997),\n","  (5, 0.1634322678843227),\n","  (6, 0.01493531202435312),\n","  (7, 0.012842465753424657),\n","  (8, 0.017408675799086757),\n","  (9, 0.3537861491628615)],\n"," [(0, 0.04454277286135693),\n","  (1, 0.019862340216322518),\n","  (2, 0.06529006882989184),\n","  (3, 0.05614552605703048),\n","  (4, 0.026253687315634218),\n","  (5, 0.27266470009832844),\n","  (6, 0.023893805309734513),\n","  (7, 0.03618485742379548),\n","  (8, 0.06981317600786627),\n","  (9, 0.3853490658800393)],\n"," [(0, 0.09103078982597054),\n","  (1, 0.017689806846433353),\n","  (2, 0.09523809523809523),\n","  (3, 0.27787339835532604),\n","  (4, 0.013864983744501817),\n","  (5, 0.04685408299866131),\n","  (6, 0.015681774717919296),\n","  (7, 0.02906865557467967),\n","  (8, 0.008988334289539109),\n","  (9, 0.4037100784088736)],\n"," [(0, 0.020380168528316676),\n","  (1, 0.07505388986870469),\n","  (2, 0.014893200078385264),\n","  (3, 0.022241818538114834),\n","  (4, 0.25004899078973153),\n","  (5, 0.02792475014697237),\n","  (6, 0.018224573780129337),\n","  (7, 0.025377229080932786),\n","  (8, 0.011953752694493435),\n","  (9, 0.5339016264942191)],\n"," [(0, 0.04896214896214896),\n","  (1, 0.10170940170940171),\n","  (2, 0.022466422466422466),\n","  (3, 0.016727716727716727),\n","  (4, 0.20354090354090354),\n","  (5, 0.021123321123321125),\n","  (6, 0.050793650793650794),\n","  (7, 0.026617826617826617),\n","  (8, 0.022344322344322345),\n","  (9, 0.4857142857142857)],\n"," [(0, 0.02904793461453501),\n","  (1, 0.027943450408659157),\n","  (2, 0.08891097857300641),\n","  (3, 0.052794345040865914),\n","  (4, 0.028716589352772257),\n","  (5, 0.055886900817318313),\n","  (6, 0.04086591561740667),\n","  (7, 0.030483764082173626),\n","  (8, 0.081290037552463),\n","  (9, 0.5640600839407997)],\n"," [(0, 0.17250233426704015),\n","  (1, 0.09722222222222222),\n","  (2, 0.10434173669467788),\n","  (3, 0.08823529411764706),\n","  (4, 0.22175536881419233),\n","  (5, 0.018557422969187675),\n","  (6, 0.04843604108309991),\n","  (7, 0.03816526610644258),\n","  (8, 0.014355742296918767),\n","  (9, 0.19642857142857142)],\n"," [(0, 0.02642390289449113),\n","  (1, 0.23370681605975724),\n","  (2, 0.047432306255835666),\n","  (3, 0.03015873015873016),\n","  (4, 0.08011204481792718),\n","  (5, 0.018300653594771243),\n","  (6, 0.01437908496732026),\n","  (7, 0.02642390289449113),\n","  (8, 0.013818860877684407),\n","  (9, 0.5092436974789916)],\n"," [(0, 0.07725453985841797),\n","  (1, 0.31640504770698674),\n","  (2, 0.0396019287986047),\n","  (3, 0.06360931568687801),\n","  (4, 0.3072740330358059),\n","  (5, 0.015081563558017852),\n","  (6, 0.06689237714168463),\n","  (7, 0.05447830101569714),\n","  (8, 0.010875141069046886),\n","  (9, 0.04852775212886016)],\n"," [(0, 0.014528998451828036),\n","  (1, 0.17113254733833513),\n","  (2, 0.016672621174228894),\n","  (3, 0.007740859830891986),\n","  (4, 0.6816720257234727),\n","  (5, 0.010718113612004289),\n","  (6, 0.0090508514945814),\n","  (7, 0.015958080266761942),\n","  (8, 0.010360843158270813),\n","  (9, 0.062165058949624874)],\n"," [(0, 0.013048982374135749),\n","  (1, 0.38056285909046644),\n","  (2, 0.011880416788392249),\n","  (3, 0.03184341221151037),\n","  (4, 0.4643100594020839),\n","  (5, 0.013048982374135749),\n","  (6, 0.014022787028921999),\n","  (7, 0.018210147044502873),\n","  (8, 0.011198753530041874),\n","  (9, 0.041873600155808745)],\n"," [(0, 0.018518518518518514),\n","  (1, 0.5835016835016834),\n","  (2, 0.01784511784511784),\n","  (3, 0.026430976430976423),\n","  (4, 0.03198653198653198),\n","  (5, 0.01784511784511784),\n","  (6, 0.030639730639730633),\n","  (7, 0.022895622895622893),\n","  (8, 0.014646464646464644),\n","  (9, 0.23569023569023564)],\n"," [(0, 0.023638108951283897),\n","  (1, 0.5153587712982961),\n","  (2, 0.02159827213822894),\n","  (3, 0.0199184065274778),\n","  (4, 0.10943124550035997),\n","  (5, 0.031197504199664026),\n","  (6, 0.014398848092152628),\n","  (7, 0.026997840172786176),\n","  (8, 0.019318454523638107),\n","  (9, 0.21814254859611232)],\n"," [(0, 0.1328432939171194),\n","  (1, 0.10621071694897198),\n","  (2, 0.07137530627463513),\n","  (3, 0.022690955576861616),\n","  (4, 0.03419622882710131),\n","  (5, 0.13678491530840525),\n","  (6, 0.03717907744753382),\n","  (7, 0.41546820070310003),\n","  (8, 0.019601576648556513),\n","  (9, 0.023649728347714925)],\n"," [(0, 0.048234280792420314),\n","  (1, 0.33714777900824405),\n","  (2, 0.02350190722283745),\n","  (3, 0.02657807308970099),\n","  (4, 0.023748000492186533),\n","  (5, 0.0617694106066199),\n","  (6, 0.026208933185677366),\n","  (7, 0.28915959148517284),\n","  (8, 0.043066322136089574),\n","  (9, 0.12058570198105079)],\n"," [(0, 0.006916733173716414),\n","  (1, 0.043983328899530015),\n","  (2, 0.06366941562472289),\n","  (3, 0.01002039549525583),\n","  (4, 0.015429635541367386),\n","  (5, 0.2423516892790636),\n","  (6, 0.007182761372705506),\n","  (7, 0.14241376252549437),\n","  (8, 0.09426265850846856),\n","  (9, 0.37376961957967547)],\n"," [(0, 0.1301169590643275),\n","  (1, 0.037071846282372596),\n","  (2, 0.035192147034252295),\n","  (3, 0.015664160401002505),\n","  (4, 0.06317878028404345),\n","  (5, 0.010442773600668337),\n","  (6, 0.022451963241436924),\n","  (7, 0.6517335004177109),\n","  (8, 0.013680033416875521),\n","  (9, 0.02046783625730994)],\n"," [(0, 0.08058109181704687),\n","  (1, 0.39654976733628416),\n","  (2, 0.1420951083872432),\n","  (3, 0.017818635796163888),\n","  (4, 0.019634547724435365),\n","  (5, 0.069685620247418),\n","  (6, 0.03620474406991261),\n","  (7, 0.05311542390194075),\n","  (8, 0.030189535807513334),\n","  (9, 0.15412552491204176)],\n"," [(0, 0.06002034587995931),\n","  (1, 0.010738103311857128),\n","  (2, 0.13077879507177576),\n","  (3, 0.025771447948457104),\n","  (4, 0.010512037978975925),\n","  (5, 0.10591160845484344),\n","  (6, 0.025997513281338307),\n","  (7, 0.6011077201311179),\n","  (8, 0.015485475302362382),\n","  (9, 0.013676952639312762)],\n"," [(0, 0.014822848879248013),\n","  (1, 0.029284164859002173),\n","  (2, 0.0631477464449265),\n","  (3, 0.010604965051819718),\n","  (4, 0.01626898047722343),\n","  (5, 0.1669076885996626),\n","  (6, 0.018558688840684505),\n","  (7, 0.5201253314051579),\n","  (8, 0.010122921185827912),\n","  (9, 0.15015666425644736)],\n"," [(0, 0.01929260450160772),\n","  (1, 0.021674407526497557),\n","  (2, 0.04287245444801715),\n","  (3, 0.028819816601167083),\n","  (4, 0.11015838990115517),\n","  (5, 0.1501726807193045),\n","  (6, 0.02679528403001072),\n","  (7, 0.4849350958675718),\n","  (8, 0.013457187090627605),\n","  (9, 0.10182207931404073)],\n"," [(0, 0.018485915492957746),\n","  (1, 0.009291862284820032),\n","  (2, 0.01584507042253521),\n","  (3, 0.02679968701095462),\n","  (4, 0.013302034428794992),\n","  (5, 0.08470266040688576),\n","  (6, 0.01633411580594679),\n","  (7, 0.7912754303599374),\n","  (8, 0.012030516431924883),\n","  (9, 0.011932707355242567)],\n"," [(0, 0.1387945136529508),\n","  (1, 0.04165093746067698),\n","  (2, 0.03787592802315339),\n","  (3, 0.052975965773247764),\n","  (4, 0.01723920976469108),\n","  (5, 0.05096262740656852),\n","  (6, 0.017868378004278342),\n","  (7, 0.5870139675349189),\n","  (8, 0.03825342896690575),\n","  (9, 0.01736504341260853)],\n"," [(0, 0.023921938936103244),\n","  (1, 0.06767390620081838),\n","  (2, 0.1395971041863393),\n","  (3, 0.024394082467736857),\n","  (4, 0.119452313503305),\n","  (5, 0.05366698142902109),\n","  (6, 0.030531948378973876),\n","  (7, 0.43232609379918163),\n","  (8, 0.034781240163676425),\n","  (9, 0.07365439093484419)],\n"," [(0, 0.07227615965480044),\n","  (1, 0.06340644851971715),\n","  (2, 0.05477645930720365),\n","  (3, 0.08629989212513486),\n","  (4, 0.02948579647608774),\n","  (5, 0.055255903152343286),\n","  (6, 0.06460505813256624),\n","  (7, 0.4849574493587439),\n","  (8, 0.07647129329977227),\n","  (9, 0.01246553997363059)],\n"," [(0, 0.11653865847414235),\n","  (1, 0.02027649769585254),\n","  (2, 0.03430619559651819),\n","  (3, 0.039426523297491044),\n","  (4, 0.015053763440860218),\n","  (5, 0.26830517153097805),\n","  (6, 0.04444444444444445),\n","  (7, 0.4381976446492576),\n","  (8, 0.016077828981054792),\n","  (9, 0.0073732718894009225)],\n"," [(0, 0.013984485960887144),\n","  (1, 0.062274664044575566),\n","  (2, 0.02414508904184421),\n","  (3, 0.012345679012345682),\n","  (4, 0.5436468917294877),\n","  (5, 0.09155468152518301),\n","  (6, 0.012454932808915114),\n","  (7, 0.15339233038348085),\n","  (8, 0.02283404348301104),\n","  (9, 0.06336720201026987)],\n"," [(0, 0.03588888888888889),\n","  (1, 0.02711111111111111),\n","  (2, 0.035),\n","  (3, 0.02411111111111111),\n","  (4, 0.6198888888888889),\n","  (5, 0.16),\n","  (6, 0.02288888888888889),\n","  (7, 0.04055555555555555),\n","  (8, 0.020555555555555556),\n","  (9, 0.014)],\n"," [(0, 0.10366826156299844),\n","  (1, 0.02206273258904838),\n","  (2, 0.2154439128123339),\n","  (3, 0.03056884635832005),\n","  (4, 0.05648591174906966),\n","  (5, 0.1780967570441255),\n","  (6, 0.01834130781499203),\n","  (7, 0.014752791068580545),\n","  (8, 0.22222222222222227),\n","  (9, 0.13835725677830943)],\n"," [(0, 0.01701701701701702),\n","  (1, 0.01639139139139139),\n","  (2, 0.18005505505505506),\n","  (3, 0.03403403403403404),\n","  (4, 0.06794294294294294),\n","  (5, 0.25075075075075076),\n","  (6, 0.015765765765765764),\n","  (7, 0.024399399399399398),\n","  (8, 0.35185185185185186),\n","  (9, 0.041791791791791794)],\n"," [(0, 0.0566159520516367),\n","  (1, 0.10115260488704472),\n","  (2, 0.06011987090825265),\n","  (3, 0.0413093591516828),\n","  (4, 0.019271553711387737),\n","  (5, 0.11295527893038267),\n","  (6, 0.015767634854771784),\n","  (7, 0.2690640848317197),\n","  (8, 0.3076071922544952),\n","  (9, 0.016136468418626097)],\n"," [(0, 0.04845731591898541),\n","  (1, 0.10164679159568424),\n","  (2, 0.022241150861253074),\n","  (3, 0.08593602120007571),\n","  (4, 0.11167897028203669),\n","  (5, 0.13420405072875258),\n","  (6, 0.027919742570509172),\n","  (7, 0.007287526026878666),\n","  (8, 0.42277115275411686),\n","  (9, 0.037857278061707356)],\n"," [(0, 0.03722362160649315),\n","  (1, 0.029387069689336694),\n","  (2, 0.06297229219143578),\n","  (3, 0.03498460677301988),\n","  (4, 0.03162608452280997),\n","  (5, 0.18108032465715088),\n","  (6, 0.04673943464875455),\n","  (7, 0.29415057374755116),\n","  (8, 0.25860621326616295),\n","  (9, 0.023229778897285197)],\n"," [(0, 0.03176241256736613),\n","  (1, 0.022703818369453045),\n","  (2, 0.03497305354890494),\n","  (3, 0.05687421167297328),\n","  (4, 0.04368765049879601),\n","  (5, 0.2860910446049765),\n","  (6, 0.04013301226923518),\n","  (7, 0.014333218667583993),\n","  (8, 0.3208347666552001),\n","  (9, 0.14860681114551083)],\n"," [(0, 0.027058146229130685),\n","  (1, 0.017731721358664364),\n","  (2, 0.08232584916522741),\n","  (3, 0.02694300518134715),\n","  (4, 0.05100748416810593),\n","  (5, 0.2473229706390328),\n","  (6, 0.022337363270005756),\n","  (7, 0.038802533103051234),\n","  (8, 0.17305699481865286),\n","  (9, 0.3134139320667818)],\n"," [(0, 0.021963824289405683),\n","  (1, 0.013824289405684755),\n","  (2, 0.03294573643410853),\n","  (3, 0.014211886304909561),\n","  (4, 0.10064599483204134),\n","  (5, 0.20155038759689922),\n","  (6, 0.019767441860465116),\n","  (7, 0.02248062015503876),\n","  (8, 0.21395348837209302),\n","  (9, 0.358656330749354)],\n"," [(0, 0.030882514562039786),\n","  (1, 0.01714474118034949),\n","  (2, 0.21200131882624465),\n","  (3, 0.07890977030442906),\n","  (4, 0.20353885042312342),\n","  (5, 0.14957687657984395),\n","  (6, 0.05539070227497527),\n","  (7, 0.028684470820969338),\n","  (8, 0.20705572040883613),\n","  (9, 0.016815034619188922)],\n"," [(0, 0.110272536687631),\n","  (1, 0.25911949685534585),\n","  (2, 0.1123689727463312),\n","  (3, 0.17693920335429766),\n","  (4, 0.01970649895178197),\n","  (5, 0.2025157232704402),\n","  (6, 0.026205450733752612),\n","  (7, 0.02222222222222222),\n","  (8, 0.028511530398322844),\n","  (9, 0.042138364779874204)],\n"," [(0, 0.0988109881098811),\n","  (1, 0.015067650676506767),\n","  (2, 0.01670766707667077),\n","  (3, 0.19567445674456746),\n","  (4, 0.011582615826158263),\n","  (5, 0.5628331283312834),\n","  (6, 0.01742517425174252),\n","  (7, 0.02665026650266503),\n","  (8, 0.023062730627306276),\n","  (9, 0.03218532185321854)],\n"," [(0, 0.018637074975103144),\n","  (1, 0.03314838526106132),\n","  (2, 0.02802674633660549),\n","  (3, 0.052496798975672214),\n","  (4, 0.042538056622563665),\n","  (5, 0.46720728410869256),\n","  (6, 0.020059752454118653),\n","  (7, 0.03528240147958458),\n","  (8, 0.23317683881064163),\n","  (9, 0.06942666097595675)],\n"," [(0, 0.015590689503732982),\n","  (1, 0.009881422924901186),\n","  (2, 0.01932367149758454),\n","  (3, 0.021519543258673692),\n","  (4, 0.03140096618357488),\n","  (5, 0.5225076855511638),\n","  (6, 0.012516469038208168),\n","  (7, 0.05138339920948617),\n","  (8, 0.282389108476065),\n","  (9, 0.033487044356609576)],\n"," [(0, 0.029154518950437316),\n","  (1, 0.022675736961451247),\n","  (2, 0.052262174711154304),\n","  (3, 0.03876471223409999),\n","  (4, 0.07958103876471223),\n","  (5, 0.546485260770975),\n","  (6, 0.01749271137026239),\n","  (7, 0.026239067055393587),\n","  (8, 0.15041572184429328),\n","  (9, 0.0369290573372206)],\n"," [(0, 0.15964740450538686),\n","  (1, 0.019479812819675698),\n","  (2, 0.03787136793992817),\n","  (3, 0.06442485580585483),\n","  (4, 0.03700076178038959),\n","  (5, 0.47545978887800633),\n","  (6, 0.019806290129502666),\n","  (7, 0.10186092066601371),\n","  (8, 0.06311894656654696),\n","  (9, 0.02132985090869518)],\n"," [(0, 0.041641275644931953),\n","  (1, 0.027219175299614057),\n","  (2, 0.12797074954296161),\n","  (3, 0.024172252691448303),\n","  (4, 0.03158643103798497),\n","  (5, 0.46424944139752183),\n","  (6, 0.05108673573024578),\n","  (7, 0.019804996953077393),\n","  (8, 0.02447694495226488),\n","  (9, 0.18779199674994923)],\n"," [(0, 0.12955082742316784),\n","  (1, 0.01995271867612293),\n","  (2, 0.023451536643026006),\n","  (3, 0.08822695035460992),\n","  (4, 0.04),\n","  (5, 0.3633096926713948),\n","  (6, 0.017872340425531916),\n","  (7, 0.02458628841607565),\n","  (8, 0.02458628841607565),\n","  (9, 0.26846335697399526)],\n"," [(0, 0.32772061116308077),\n","  (1, 0.023282403076603262),\n","  (2, 0.026192703461178673),\n","  (3, 0.01808543810414718),\n","  (4, 0.1259744309323355),\n","  (5, 0.20600769150815923),\n","  (6, 0.0205799812909261),\n","  (7, 0.06932751273256418),\n","  (8, 0.16266500363787548),\n","  (9, 0.020164224093129613)],\n"," [(0, 0.04735504735504736),\n","  (1, 0.013398013398013399),\n","  (2, 0.09367059367059367),\n","  (3, 0.03291753291753292),\n","  (4, 0.1021021021021021),\n","  (5, 0.4471009471009471),\n","  (6, 0.04134904134904135),\n","  (7, 0.03557403557403557),\n","  (8, 0.09205359205359205),\n","  (9, 0.09447909447909449)],\n"," [(0, 0.04721719911593329),\n","  (1, 0.015873015873015872),\n","  (2, 0.09885473176612417),\n","  (3, 0.14567008237894313),\n","  (4, 0.1611412497488447),\n","  (5, 0.21297970665059274),\n","  (6, 0.11804299778983324),\n","  (7, 0.04792043399638336),\n","  (8, 0.030942334739803096),\n","  (9, 0.12135824794052642)],\n"," [(0, 0.022825150732127476),\n","  (1, 0.043281653746770024),\n","  (2, 0.05006459948320414),\n","  (3, 0.07934969853574504),\n","  (4, 0.21877691645133507),\n","  (5, 0.3714470284237726),\n","  (6, 0.04220499569336779),\n","  (7, 0.04952627045650301),\n","  (8, 0.042312661498708014),\n","  (9, 0.08021102497846684)],\n"," [(0, 0.022812913738669928),\n","  (1, 0.02709033506467054),\n","  (2, 0.2611263876158469),\n","  (3, 0.16987473266116715),\n","  (4, 0.02973826255219473),\n","  (5, 0.1415622772176393),\n","  (6, 0.2550157857215603),\n","  (7, 0.05642122415724616),\n","  (8, 0.01619309501985946),\n","  (9, 0.020164986251145742)],\n"," [(0, 0.09561561561561562),\n","  (1, 0.025465465465465464),\n","  (2, 0.1039039039039039),\n","  (3, 0.12552552552552554),\n","  (4, 0.1033033033033033),\n","  (5, 0.3127927927927928),\n","  (6, 0.07363363363363364),\n","  (7, 0.0448048048048048),\n","  (8, 0.06630630630630631),\n","  (9, 0.04864864864864865)],\n"," [(0, 0.030735212824765063),\n","  (1, 0.019237147595356552),\n","  (2, 0.32338308457711445),\n","  (3, 0.14405749032614704),\n","  (4, 0.013930348258706468),\n","  (5, 0.08877833056937534),\n","  (6, 0.16981757877280265),\n","  (7, 0.07429519071310116),\n","  (8, 0.022664455500276397),\n","  (9, 0.11310116086235489)],\n"," [(0, 0.023759608665269043),\n","  (1, 0.02212904728627999),\n","  (2, 0.21337060330771024),\n","  (3, 0.3073608199394363),\n","  (4, 0.010831586303284416),\n","  (5, 0.16643372932681108),\n","  (6, 0.1522245515956208),\n","  (7, 0.07465641742371303),\n","  (8, 0.01607267645003494),\n","  (9, 0.013160959701840206)],\n"," [(0, 0.10164365548980933),\n","  (1, 0.01669953977646285),\n","  (2, 0.40157790927021697),\n","  (3, 0.0547008547008547),\n","  (4, 0.014201183431952662),\n","  (5, 0.020512820512820513),\n","  (6, 0.2395792241946088),\n","  (7, 0.06022353714661407),\n","  (8, 0.021827744904667983),\n","  (9, 0.06903353057199212)],\n"," [(0, 0.09037940379403796),\n","  (1, 0.049322493224932255),\n","  (2, 0.3563685636856369),\n","  (3, 0.059756097560975614),\n","  (4, 0.03373983739837399),\n","  (5, 0.27357723577235776),\n","  (6, 0.06151761517615177),\n","  (7, 0.03428184281842819),\n","  (8, 0.018563685636856373),\n","  (9, 0.022493224932249326)],\n"," [(0, 0.12012761825495907),\n","  (1, 0.056179775280898875),\n","  (2, 0.2502427521154113),\n","  (3, 0.07185462616174226),\n","  (4, 0.018033014287695937),\n","  (5, 0.08433902066860868),\n","  (6, 0.30780968234151757),\n","  (7, 0.04702455264253017),\n","  (8, 0.02663337494798169),\n","  (9, 0.01775558329865446)],\n"," [(0, 0.09498773221170698),\n","  (1, 0.030026872298165672),\n","  (2, 0.17443626591891576),\n","  (3, 0.10854071737352494),\n","  (4, 0.02476924874401215),\n","  (5, 0.1607664446781166),\n","  (6, 0.07909802547026522),\n","  (7, 0.018460100479027925),\n","  (8, 0.049187989251080734),\n","  (9, 0.259726603575184)],\n"," [(0, 0.2718855218855219),\n","  (1, 0.04304954304954305),\n","  (2, 0.049663299663299666),\n","  (3, 0.3398268398268398),\n","  (4, 0.05146705146705147),\n","  (5, 0.09319384319384319),\n","  (6, 0.062169312169312166),\n","  (7, 0.026936026936026935),\n","  (8, 0.021645021645021644),\n","  (9, 0.040163540163540165)],\n"," [(0, 0.04911323328785812),\n","  (1, 0.03880551765954222),\n","  (2, 0.07200242534485372),\n","  (3, 0.3015006821282401),\n","  (4, 0.03243898741852357),\n","  (5, 0.09201152038805517),\n","  (6, 0.13718356828861603),\n","  (7, 0.017280582082764895),\n","  (8, 0.06957708049113233),\n","  (9, 0.19008640291041382)],\n"," [(0, 0.1428341384863124),\n","  (1, 0.013365539452495974),\n","  (2, 0.07793880837359098),\n","  (3, 0.2529790660225443),\n","  (4, 0.02818035426731079),\n","  (5, 0.044444444444444446),\n","  (6, 0.2768115942028985),\n","  (7, 0.02576489533011272),\n","  (8, 0.019162640901771336),\n","  (9, 0.11851851851851852)],\n"," [(0, 0.02866535174227482),\n","  (1, 0.04720578566732413),\n","  (2, 0.22287968441814596),\n","  (3, 0.02656147271531887),\n","  (4, 0.1888231426692965),\n","  (5, 0.10979618671926364),\n","  (6, 0.1985535831689678),\n","  (7, 0.01591058514135437),\n","  (8, 0.021433267587113742),\n","  (9, 0.14017094017094017)],\n"," [(0, 0.03634589584095622),\n","  (1, 0.013904134650567142),\n","  (2, 0.04549335284790828),\n","  (3, 0.021344066349554824),\n","  (4, 0.0154896938651055),\n","  (5, 0.3423588242468594),\n","  (6, 0.22234418831564826),\n","  (7, 0.18819368215636054),\n","  (8, 0.05122575923893158),\n","  (9, 0.06330040248810831)],\n"," [(0, 0.021426062521952935),\n","  (1, 0.01721109940288023),\n","  (2, 0.10806697108066972),\n","  (3, 0.01170823088631308),\n","  (4, 0.021426062521952935),\n","  (5, 0.24317995550872265),\n","  (6, 0.2026694766420794),\n","  (7, 0.02751434258283574),\n","  (8, 0.32150802013815716),\n","  (9, 0.025289778714436252)],\n"," [(0, 0.174223341729639),\n","  (1, 0.026448362720403025),\n","  (2, 0.1837391547719004),\n","  (3, 0.11083123425692697),\n","  (4, 0.011055135740274282),\n","  (5, 0.2108872096277638),\n","  (6, 0.09585782255807446),\n","  (7, 0.04366078925272881),\n","  (8, 0.11992723201791214),\n","  (9, 0.023369717324377277)],\n"," [(0, 0.02188295165394402),\n","  (1, 0.010602205258693808),\n","  (2, 0.07421543681085666),\n","  (3, 0.04368108566581849),\n","  (4, 0.017133163698049195),\n","  (5, 0.49745547073791346),\n","  (6, 0.03825275657336726),\n","  (7, 0.061238337574215436),\n","  (8, 0.17879558948261237),\n","  (9, 0.05674300254452926)],\n"," [(0, 0.012262742589038174),\n","  (1, 0.020580081040733632),\n","  (2, 0.29771806355299635),\n","  (3, 0.015781616549370867),\n","  (4, 0.012582640221795692),\n","  (5, 0.20313499680102368),\n","  (6, 0.011409682235018127),\n","  (7, 0.011836212412028152),\n","  (8, 0.39496694391128173),\n","  (9, 0.019727020686713587)],\n"," [(0, 0.07273290003095018),\n","  (1, 0.02940266171463943),\n","  (2, 0.22500773754255649),\n","  (3, 0.01748684617765398),\n","  (4, 0.09857629216960694),\n","  (5, 0.37542556484060663),\n","  (6, 0.012380068090374497),\n","  (7, 0.030795419374806562),\n","  (8, 0.06406685236768803),\n","  (9, 0.0741256576911173)],\n"," [(0, 0.021091936623271773),\n","  (1, 0.039358159249167426),\n","  (2, 0.1495610051468362),\n","  (3, 0.3600766979513573),\n","  (4, 0.05479866787768695),\n","  (5, 0.15813906549601373),\n","  (6, 0.05863356544555454),\n","  (7, 0.019174487839337975),\n","  (8, 0.11393682510848724),\n","  (9, 0.02522958926228681)],\n"," [(0, 0.017508417508417508),\n","  (1, 0.018742985409652076),\n","  (2, 0.145679012345679),\n","  (3, 0.28540965207631874),\n","  (4, 0.017957351290684626),\n","  (5, 0.07441077441077441),\n","  (6, 0.26835016835016834),\n","  (7, 0.02345679012345679),\n","  (8, 0.1314253647586981),\n","  (9, 0.017059483726150394)],\n"," [(0, 0.14881749219098617),\n","  (1, 0.0534359660865685),\n","  (2, 0.009928603302097277),\n","  (3, 0.5066934404283802),\n","  (4, 0.012382864792503346),\n","  (5, 0.16198125836680052),\n","  (6, 0.013052208835341365),\n","  (7, 0.07072735385988398),\n","  (8, 0.010040160642570281),\n","  (9, 0.012940651494868362)],\n"," [(0, 0.2639773531493277),\n","  (1, 0.019957537154989383),\n","  (2, 0.5023354564755839),\n","  (3, 0.06751592356687898),\n","  (4, 0.012172682236376504),\n","  (5, 0.01670205237084218),\n","  (6, 0.02335456475583864),\n","  (7, 0.04543524416135881),\n","  (8, 0.02392073602264685),\n","  (9, 0.024628450106157114)],\n"," [(0, 0.3087755452515885),\n","  (1, 0.015627683324746695),\n","  (2, 0.09204877211059591),\n","  (3, 0.24094109565516056),\n","  (4, 0.026446848703417482),\n","  (5, 0.050661171217585434),\n","  (6, 0.09668555727288339),\n","  (7, 0.05220676627168126),\n","  (8, 0.09015971148892324),\n","  (9, 0.026446848703417482)],\n"," [(0, 0.18089248277927522),\n","  (1, 0.01961665169212339),\n","  (2, 0.2619047619047619),\n","  (3, 0.09808325846061695),\n","  (4, 0.021263851452530697),\n","  (5, 0.05720275531596286),\n","  (6, 0.2746331236897275),\n","  (7, 0.02620545073375262),\n","  (8, 0.04043126684636118),\n","  (9, 0.019766397124887692)],\n"," [(0, 0.06997587038952086),\n","  (1, 0.02746179478340802),\n","  (2, 0.22509479489831094),\n","  (3, 0.11053659657589338),\n","  (4, 0.011030679076180628),\n","  (5, 0.0996208204067563),\n","  (6, 0.2167068826841319),\n","  (7, 0.010456164540962886),\n","  (8, 0.18039756405837068),\n","  (9, 0.048718832586464436)],\n"," [(0, 0.2779202279202279),\n","  (1, 0.024216524216524215),\n","  (2, 0.11210826210826211),\n","  (3, 0.04358974358974359),\n","  (4, 0.02621082621082621),\n","  (5, 0.2723646723646724),\n","  (6, 0.021794871794871794),\n","  (7, 0.1066951566951567),\n","  (8, 0.05698005698005698),\n","  (9, 0.05811965811965812)],\n"," [(0, 0.398989898989899),\n","  (1, 0.04401154401154401),\n","  (2, 0.1001082251082251),\n","  (3, 0.04365079365079365),\n","  (4, 0.024170274170274172),\n","  (5, 0.17929292929292928),\n","  (6, 0.027417027417027416),\n","  (7, 0.057178932178932176),\n","  (8, 0.016774891774891776),\n","  (9, 0.10840548340548341)],\n"," [(0, 0.06758016405667412),\n","  (1, 0.02703206562266965),\n","  (2, 0.35570469798657717),\n","  (3, 0.07550335570469799),\n","  (4, 0.011278896346010439),\n","  (5, 0.19323266219239374),\n","  (6, 0.1524049217002237),\n","  (7, 0.09004474272930649),\n","  (8, 0.015380313199105145),\n","  (9, 0.011838180462341537)],\n"," [(0, 0.19079032466692541),\n","  (1, 0.04682447290130644),\n","  (2, 0.01707411719053163),\n","  (3, 0.473160005173975),\n","  (4, 0.040874401759151475),\n","  (5, 0.1644030526451947),\n","  (6, 0.01862630966239814),\n","  (7, 0.0263872720217307),\n","  (8, 0.010347949812443412),\n","  (9, 0.011512094166343296)],\n"," [(0, 0.309332358674464),\n","  (1, 0.028508771929824563),\n","  (2, 0.3139619883040936),\n","  (3, 0.0723684210526316),\n","  (4, 0.03959551656920079),\n","  (5, 0.0838206627680312),\n","  (6, 0.05835769980506823),\n","  (7, 0.013036062378167642),\n","  (8, 0.06615497076023394),\n","  (9, 0.014863547758284601)],\n"," [(0, 0.5385416666666667),\n","  (1, 0.016319444444444445),\n","  (2, 0.14548611111111112),\n","  (3, 0.09010416666666667),\n","  (4, 0.01579861111111111),\n","  (5, 0.06631944444444444),\n","  (6, 0.04618055555555556),\n","  (7, 0.026909722222222224),\n","  (8, 0.036631944444444446),\n","  (9, 0.017708333333333333)],\n"," [(0, 0.0159853249475891),\n","  (1, 0.014019916142557652),\n","  (2, 0.22746331236897274),\n","  (3, 0.19680293501048218),\n","  (4, 0.011137316561844864),\n","  (5, 0.3810272536687631),\n","  (6, 0.019261006289308175),\n","  (7, 0.04612159329140461),\n","  (8, 0.030791404612159328),\n","  (9, 0.05738993710691824)],\n"," [(0, 0.03393277670734318),\n","  (1, 0.016698779704560053),\n","  (2, 0.31845429244273177),\n","  (3, 0.3993791479340612),\n","  (4, 0.018090344679940057),\n","  (5, 0.11956754442303576),\n","  (6, 0.02172982230785699),\n","  (7, 0.022800256904303148),\n","  (8, 0.0354313851423678),\n","  (9, 0.013915649753800044)],\n"," [(0, 0.5075984055804684),\n","  (1, 0.021300448430493276),\n","  (2, 0.1609367214748381),\n","  (3, 0.04035874439461884),\n","  (4, 0.023916292974588943),\n","  (5, 0.08208769307424017),\n","  (6, 0.07000498256103639),\n","  (7, 0.026656701544593923),\n","  (8, 0.025161933233682116),\n","  (9, 0.04197807673143997)],\n"," [(0, 0.05286195286195287),\n","  (1, 0.039730639730639734),\n","  (2, 0.3336700336700337),\n","  (3, 0.15471380471380475),\n","  (4, 0.01548821548821549),\n","  (5, 0.22643097643097645),\n","  (6, 0.03636363636363637),\n","  (7, 0.01750841750841751),\n","  (8, 0.1085858585858586),\n","  (9, 0.014646464646464649)],\n"," [(0, 0.07921374764595104),\n","  (1, 0.012358757062146895),\n","  (2, 0.07144538606403014),\n","  (3, 0.36370056497175146),\n","  (4, 0.019538606403013185),\n","  (5, 0.024599811676082866),\n","  (6, 0.10145951035781546),\n","  (7, 0.016478342749529192),\n","  (8, 0.28001412429378536),\n","  (9, 0.03119114877589454)],\n"," [(0, 0.05918003565062389),\n","  (1, 0.0368389780154486),\n","  (2, 0.030540701128936424),\n","  (3, 0.584194890077243),\n","  (4, 0.019013666072489603),\n","  (5, 0.15163398692810456),\n","  (6, 0.012834224598930482),\n","  (7, 0.024004753416518122),\n","  (8, 0.038027332144979206),\n","  (9, 0.04373143196672608)],\n"," [(0, 0.0779303330790745),\n","  (1, 0.1069158403254513),\n","  (2, 0.021866259852529876),\n","  (3, 0.6446732773963896),\n","  (4, 0.01932367149758454),\n","  (5, 0.025680142384947875),\n","  (6, 0.04513094330027968),\n","  (7, 0.017289600813628275),\n","  (8, 0.01817950673785914),\n","  (9, 0.023010424612255276)],\n"," [(0, 0.0380952380952381),\n","  (1, 0.01855921855921856),\n","  (2, 0.2032967032967033),\n","  (3, 0.16446886446886447),\n","  (4, 0.026617826617826617),\n","  (5, 0.09499389499389499),\n","  (6, 0.07313797313797314),\n","  (7, 0.038583638583638585),\n","  (8, 0.32735042735042735),\n","  (9, 0.014896214896214897)],\n"," [(0, 0.017853392144507457),\n","  (1, 0.023209409787859694),\n","  (2, 0.16971224532661205),\n","  (3, 0.052404956941818945),\n","  (4, 0.011657214870825458),\n","  (5, 0.07760974585171182),\n","  (6, 0.027515227893299726),\n","  (7, 0.015122873345935728),\n","  (8, 0.590107120352867),\n","  (9, 0.014807813484562067)],\n"," [(0, 0.18712394705174487),\n","  (1, 0.04131568391496188),\n","  (2, 0.2837946249498596),\n","  (3, 0.058162855996791),\n","  (4, 0.023265142398716403),\n","  (5, 0.2057761732851985),\n","  (6, 0.04412354592860007),\n","  (7, 0.06999598876855193),\n","  (8, 0.05154432410750099),\n","  (9, 0.0348977135980746)],\n"," [(0, 0.04288083392561005),\n","  (1, 0.04169628050225065),\n","  (2, 0.43674484719260837),\n","  (3, 0.0412224591329069),\n","  (4, 0.020848140251125327),\n","  (5, 0.062425965411040035),\n","  (6, 0.03257521914238332),\n","  (7, 0.028666192845297322),\n","  (8, 0.04323619995261786),\n","  (9, 0.24970386164416014)],\n"," [(0, 0.02953852622726795),\n","  (1, 0.565121412803532),\n","  (2, 0.08031115315883527),\n","  (3, 0.0355303269210554),\n","  (4, 0.03679175864606328),\n","  (5, 0.08956165247555976),\n","  (6, 0.010091453800063072),\n","  (7, 0.047198570377378325),\n","  (8, 0.0804162724692526),\n","  (9, 0.025438873120992328)],\n"," [(0, 0.06636329161926433),\n","  (1, 0.7071166729869802),\n","  (2, 0.0298318796612312),\n","  (3, 0.04816078877512325),\n","  (4, 0.01921375300214891),\n","  (5, 0.03678422449753509),\n","  (6, 0.02603969156870181),\n","  (7, 0.024649222601441034),\n","  (8, 0.024522816331690057),\n","  (9, 0.017317658955884215)],\n"," [(0, 0.017604023776863285),\n","  (1, 0.42912665752171936),\n","  (2, 0.14128943758573392),\n","  (3, 0.034064929126657525),\n","  (4, 0.06024234110653865),\n","  (5, 0.14940557841792412),\n","  (6, 0.06229995427526293),\n","  (7, 0.04012345679012346),\n","  (8, 0.0230909922267947),\n","  (9, 0.04275262917238227)],\n"," [(0, 0.2060074428495481),\n","  (1, 0.37493354598617756),\n","  (2, 0.06259968102073366),\n","  (3, 0.023790536948431685),\n","  (4, 0.027910685805422646),\n","  (5, 0.21384901648059543),\n","  (6, 0.02033492822966507),\n","  (7, 0.028442317916002127),\n","  (8, 0.029771398192450824),\n","  (9, 0.012360446570972886)],\n"," [(0, 0.025171283884420612),\n","  (1, 0.38993148644623177),\n","  (2, 0.024426571343461424),\n","  (3, 0.02770330652368186),\n","  (4, 0.07685433422698838),\n","  (5, 0.0761096216860292),\n","  (6, 0.012660113196306225),\n","  (7, 0.2590110217456062),\n","  (8, 0.028745904081024724),\n","  (9, 0.07938635686624963)],\n"," [(0, 0.05944055944055944),\n","  (1, 0.4642579642579643),\n","  (2, 0.039627039627039624),\n","  (3, 0.02292152292152292),\n","  (4, 0.014763014763014764),\n","  (5, 0.297008547008547),\n","  (6, 0.014568764568764568),\n","  (7, 0.035547785547785545),\n","  (8, 0.03515928515928516),\n","  (9, 0.016705516705516704)],\n"," [(0, 0.014749262536873156),\n","  (1, 0.6394156482652058),\n","  (2, 0.02682961090040736),\n","  (3, 0.01783958421126563),\n","  (4, 0.015732546705998034),\n","  (5, 0.09565950273914876),\n","  (6, 0.013063632532659082),\n","  (7, 0.018541930046354824),\n","  (8, 0.03876949009692372),\n","  (9, 0.11939879196516365)],\n"," [(0, 0.018843404808317088),\n","  (1, 0.30851202079272255),\n","  (2, 0.04626380766731644),\n","  (3, 0.047953216374269005),\n","  (4, 0.061858349577647825),\n","  (5, 0.13931124106562703),\n","  (6, 0.01364522417153996),\n","  (7, 0.01806367771280052),\n","  (8, 0.200909681611436),\n","  (9, 0.1446393762183236)],\n"," [(0, 0.08968899212801652),\n","  (1, 0.48019099238611435),\n","  (2, 0.12349980642663569),\n","  (3, 0.02193831462124145),\n","  (4, 0.02426119499290231),\n","  (5, 0.1020776874435411),\n","  (6, 0.014453477868112014),\n","  (7, 0.06168537875854949),\n","  (8, 0.04271518905665247),\n","  (9, 0.03948896631823461)],\n"," [(0, 0.04202279202279202),\n","  (1, 0.03828347578347578),\n","  (2, 0.057692307692307696),\n","  (3, 0.5224358974358975),\n","  (4, 0.020655270655270654),\n","  (5, 0.15616096866096865),\n","  (6, 0.0771011396011396),\n","  (7, 0.02012108262108262),\n","  (8, 0.026175213675213676),\n","  (9, 0.03935185185185185)],\n"," [(0, 0.19230210602759623),\n","  (1, 0.046332607116920846),\n","  (2, 0.06681190994916485),\n","  (3, 0.030210602759622368),\n","  (4, 0.02178649237472767),\n","  (5, 0.05432098765432099),\n","  (6, 0.07334785766158315),\n","  (7, 0.037037037037037035),\n","  (8, 0.02207697893972404),\n","  (9, 0.45577342047930286)],\n"," [(0, 0.03329506314580942),\n","  (1, 0.03635667814772293),\n","  (2, 0.021941574180380154),\n","  (3, 0.031253986477867084),\n","  (4, 0.5147340221967088),\n","  (5, 0.08355657609388954),\n","  (6, 0.025258323765786454),\n","  (7, 0.1492537313432836),\n","  (8, 0.04005612960836842),\n","  (9, 0.06429391504018371)],\n"," [(0, 0.06479876757173118),\n","  (1, 0.0252262661274793),\n","  (2, 0.016175621028307337),\n","  (3, 0.03552859618717504),\n","  (4, 0.6469285576737916),\n","  (5, 0.02050837666088966),\n","  (6, 0.12122087425380319),\n","  (7, 0.012227999229732332),\n","  (8, 0.043231272867321396),\n","  (9, 0.01415366839976892)],\n"," [(0, 0.46817451489309114),\n","  (1, 0.010876282288963047),\n","  (2, 0.02088740575948585),\n","  (3, 0.059448770238536654),\n","  (4, 0.2291434927697442),\n","  (5, 0.04881967618341368),\n","  (6, 0.08725744654554445),\n","  (7, 0.046100605611172914),\n","  (8, 0.012235817575083428),\n","  (9, 0.01705598813496478)],\n"," [(0, 0.03805407237901517),\n","  (1, 0.16399694889397406),\n","  (2, 0.032884142723959654),\n","  (3, 0.08051529790660225),\n","  (4, 0.4224086787015849),\n","  (5, 0.036104754640223745),\n","  (6, 0.07975252140011865),\n","  (7, 0.04246122552758708),\n","  (8, 0.02703618950758539),\n","  (9, 0.07678616831934909)],\n"," [(0, 0.031309660175639555),\n","  (1, 0.06567392134402443),\n","  (2, 0.1190339824360443),\n","  (3, 0.03264604810996564),\n","  (4, 0.4836769759450172),\n","  (5, 0.13946162657502864),\n","  (6, 0.015654830087819777),\n","  (7, 0.030450553646429936),\n","  (8, 0.02873234058801069),\n","  (9, 0.05336006109201986)],\n"," [(0, 0.046947234178224134),\n","  (1, 0.18834688346883469),\n","  (2, 0.019926669854933843),\n","  (3, 0.048700781125458316),\n","  (4, 0.5794675593814762),\n","  (5, 0.013231308783676073),\n","  (6, 0.05571496891439503),\n","  (7, 0.011477761836441894),\n","  (8, 0.013390722142515544),\n","  (9, 0.022796110314044316)],\n"," [(0, 0.12233676975945014),\n","  (1, 0.23963344788087051),\n","  (2, 0.03711340206185566),\n","  (3, 0.024742268041237105),\n","  (4, 0.3806414662084764),\n","  (5, 0.0542955326460481),\n","  (6, 0.05830469644902633),\n","  (7, 0.024627720504009156),\n","  (8, 0.020045819014891178),\n","  (9, 0.03825887743413516)],\n"," [(0, 0.015227576974564927),\n","  (1, 0.07463186077643909),\n","  (2, 0.035391566265060244),\n","  (3, 0.03246318607764391),\n","  (4, 0.7354417670682731),\n","  (5, 0.014223560910307898),\n","  (6, 0.020247657295850065),\n","  (7, 0.027024765729585006),\n","  (8, 0.013721552878179385),\n","  (9, 0.03162650602409638)],\n"," [(0, 0.07519738108992875),\n","  (1, 0.012805699980743308),\n","  (2, 0.03764683227421529),\n","  (3, 0.02628538417099942),\n","  (4, 0.4169073753129212),\n","  (5, 0.018005006739842094),\n","  (6, 0.02628538417099942),\n","  (7, 0.009917196225688426),\n","  (8, 0.009628345850182938),\n","  (9, 0.3673213941844791)],\n"," [(0, 0.02810203199308258),\n","  (1, 0.019563337656722874),\n","  (2, 0.02453523562472979),\n","  (3, 0.010376134889753568),\n","  (4, 0.817877215737138),\n","  (5, 0.027129269347168184),\n","  (6, 0.015564202334630352),\n","  (7, 0.01405101599654129),\n","  (8, 0.00972762645914397),\n","  (9, 0.0330739299610895)],\n"," [(0, 0.2400847714199213),\n","  (1, 0.07281259461095974),\n","  (2, 0.0326975476839237),\n","  (3, 0.0246745382985165),\n","  (4, 0.32833787465940056),\n","  (5, 0.05782621858916137),\n","  (6, 0.04526188313654254),\n","  (7, 0.1344232515894641),\n","  (8, 0.0210414774447472),\n","  (9, 0.04283984256736301)],\n"," [(0, 0.22781858458668539),\n","  (1, 0.0479188527457153),\n","  (2, 0.01375772414597179),\n","  (3, 0.04966771598461002),\n","  (4, 0.5260580622595314),\n","  (5, 0.016439314445610358),\n","  (6, 0.028564766235280408),\n","  (7, 0.034627492130115435),\n","  (8, 0.015273405619680545),\n","  (9, 0.03987408184679959)],\n"," [(0, 0.14855875831485588),\n","  (1, 0.01823109140182311),\n","  (2, 0.012564671101256468),\n","  (3, 0.04200542005420054),\n","  (4, 0.513550135501355),\n","  (5, 0.12416851441241686),\n","  (6, 0.016383345651638334),\n","  (7, 0.028455284552845527),\n","  (8, 0.044838630204483865),\n","  (9, 0.051244148805124415)],\n"," [(0, 0.05527876631079478),\n","  (1, 0.012811387900355872),\n","  (2, 0.04286279161724001),\n","  (3, 0.020798734677738236),\n","  (4, 0.008066429418742586),\n","  (5, 0.07694741004349545),\n","  (6, 0.04863582443653618),\n","  (7, 0.08588374851720047),\n","  (8, 0.6404903123764334),\n","  (9, 0.008224594701463028)],\n"," [(0, 0.08763693270735524),\n","  (1, 0.2205368965932346),\n","  (2, 0.10569399301793668),\n","  (3, 0.07860840255206453),\n","  (4, 0.04803178042614662),\n","  (5, 0.21788852774768266),\n","  (6, 0.04574455278680631),\n","  (7, 0.07078367641747924),\n","  (8, 0.10256410256410256),\n","  (9, 0.022511135187191524)],\n"," [(0, 0.10656436487638533),\n","  (1, 0.026617410249123803),\n","  (2, 0.024059865492090556),\n","  (3, 0.05304537273846737),\n","  (4, 0.03372170124088283),\n","  (5, 0.14028606611726815),\n","  (6, 0.09623946196836222),\n","  (7, 0.023302074452969595),\n","  (8, 0.48792270531400966),\n","  (9, 0.008240977550440466)],\n"," [(0, 0.022943843093718202),\n","  (1, 0.02692200943658063),\n","  (2, 0.09473586825793322),\n","  (3, 0.024979184013322234),\n","  (4, 0.009529096123600705),\n","  (5, 0.042742159311684715),\n","  (6, 0.1105560181330373),\n","  (7, 0.03145526875751689),\n","  (8, 0.6236469608659452),\n","  (9, 0.012489592006661117)],\n"," [(0, 0.06576604880092157),\n","  (1, 0.013718714001466124),\n","  (2, 0.046287569378992574),\n","  (3, 0.030579118232275635),\n","  (4, 0.008482563619227146),\n","  (5, 0.07215415226725314),\n","  (6, 0.33731280762383503),\n","  (7, 0.045973400356058236),\n","  (8, 0.36789192585611064),\n","  (9, 0.011833699863860092)],\n"," [(0, 0.06338965693804405),\n","  (1, 0.022529441884280597),\n","  (2, 0.06676907322068613),\n","  (3, 0.0253968253968254),\n","  (4, 0.011879160266257042),\n","  (5, 0.04700460829493088),\n","  (6, 0.31018945212493604),\n","  (7, 0.03553507424475167),\n","  (8, 0.39723502304147473),\n","  (9, 0.020071684587813624)],\n"," [(0, 0.012305757657414892),\n","  (1, 0.05506152878828707),\n","  (2, 0.027036737626875057),\n","  (3, 0.017066379232911166),\n","  (4, 0.012215934608820624),\n","  (5, 0.17686158268211624),\n","  (6, 0.01634779484415701),\n","  (7, 0.055330997934069884),\n","  (8, 0.5964250426659481),\n","  (9, 0.03134824395939998)],\n"," [(0, 0.06785185185185186),\n","  (1, 0.05511111111111111),\n","  (2, 0.01674074074074074),\n","  (3, 0.013777777777777778),\n","  (4, 0.15881481481481483),\n","  (5, 0.07644444444444444),\n","  (6, 0.020444444444444446),\n","  (7, 0.3494814814814815),\n","  (8, 0.12444444444444444),\n","  (9, 0.11688888888888889)],\n"," [(0, 0.013888888888888888),\n","  (1, 0.04450757575757576),\n","  (2, 0.027567340067340067),\n","  (3, 0.012626262626262626),\n","  (4, 0.018308080808080808),\n","  (5, 0.026936026936026935),\n","  (6, 0.01125841750841751),\n","  (7, 0.013362794612794613),\n","  (8, 0.8001893939393939),\n","  (9, 0.03135521885521886)],\n"," [(0, 0.014708805406479827),\n","  (1, 0.02892069171138939),\n","  (2, 0.041144901610017895),\n","  (3, 0.01649771417213278),\n","  (4, 0.01172729079705824),\n","  (5, 0.40717551182667466),\n","  (6, 0.020572450805008947),\n","  (7, 0.03001391373484397),\n","  (8, 0.3952494533889883),\n","  (9, 0.03398926654740609)],\n"," [(0, 0.1278024691358025),\n","  (1, 0.05027160493827161),\n","  (2, 0.10291358024691359),\n","  (3, 0.26548148148148154),\n","  (4, 0.009283950617283952),\n","  (5, 0.3599012345679013),\n","  (6, 0.011160493827160495),\n","  (7, 0.026469135802469138),\n","  (8, 0.012740740740740742),\n","  (9, 0.033975308641975316)],\n"," [(0, 0.02551521099116781),\n","  (1, 0.014611274670155926),\n","  (2, 0.06160724021371715),\n","  (3, 0.12059753571039145),\n","  (4, 0.015919747028677354),\n","  (5, 0.645840148293534),\n","  (6, 0.015701668302257114),\n","  (7, 0.01722821938719878),\n","  (8, 0.06498746047323084),\n","  (9, 0.017991494929669612)],\n"," [(0, 0.009482912864555377),\n","  (1, 0.026033279656468063),\n","  (2, 0.021739130434782608),\n","  (3, 0.02612274109858651),\n","  (4, 0.0352478081946681),\n","  (5, 0.6573626766863482),\n","  (6, 0.011182680264805869),\n","  (7, 0.02325997495079621),\n","  (8, 0.03578457684737878),\n","  (9, 0.1537842190016103)],\n"," [(0, 0.009505080301540477),\n","  (1, 0.02108598273790014),\n","  (2, 0.04697913252485522),\n","  (3, 0.013875232164317707),\n","  (4, 0.030700316836010044),\n","  (5, 0.053425106522451644),\n","  (6, 0.01573254670599803),\n","  (7, 0.011580902436359663),\n","  (8, 0.7790888233366108),\n","  (9, 0.018026876433956077)],\n"," [(0, 0.030033003300330034),\n","  (1, 0.010451045104510451),\n","  (2, 0.043454345434543455),\n","  (3, 0.04312431243124312),\n","  (4, 0.0088008800880088),\n","  (5, 0.2316831683168317),\n","  (6, 0.0352035203520352),\n","  (7, 0.558965896589659),\n","  (8, 0.015621562156215622),\n","  (9, 0.022662266226622662)],\n"," [(0, 0.03054939640305494),\n","  (1, 0.017738359201773836),\n","  (2, 0.14166050751416606),\n","  (3, 0.025539952369220662),\n","  (4, 0.022337193068900387),\n","  (5, 0.15824915824915825),\n","  (6, 0.021762338835509567),\n","  (7, 0.46012975281267965),\n","  (8, 0.07834441980783444),\n","  (9, 0.043688921737702224)],\n"," [(0, 0.02285618952285619),\n","  (1, 0.0547213880547214),\n","  (2, 0.07140473807140475),\n","  (3, 0.11911911911911913),\n","  (4, 0.02085418752085419),\n","  (5, 0.3198198198198199),\n","  (6, 0.01801801801801802),\n","  (7, 0.17600934267600937),\n","  (8, 0.16549883216549885),\n","  (9, 0.03169836503169837)],\n"," [(0, 0.051914893617021285),\n","  (1, 0.034420803782505914),\n","  (2, 0.07612293144208039),\n","  (3, 0.025815602836879437),\n","  (4, 0.043687943262411356),\n","  (5, 0.5698345153664304),\n","  (6, 0.009834515366430261),\n","  (7, 0.06004728132387708),\n","  (8, 0.10382978723404257),\n","  (9, 0.024491725768321516)],\n"," [(0, 0.042385457243554074),\n","  (1, 0.03014283064366537),\n","  (2, 0.026618438137636802),\n","  (3, 0.030699313670933037),\n","  (4, 0.023928770172509738),\n","  (5, 0.4834910035243925),\n","  (6, 0.035522166573919496),\n","  (7, 0.2484696716750139),\n","  (8, 0.06418104247820441),\n","  (9, 0.014561305880170655)],\n"," [(0, 0.11122868900646679),\n","  (1, 0.051499118165784834),\n","  (2, 0.21622574955908289),\n","  (3, 0.03715461493239271),\n","  (4, 0.02739564961787184),\n","  (5, 0.14191651969429747),\n","  (6, 0.1111111111111111),\n","  (7, 0.21763668430335098),\n","  (8, 0.07360376249265138),\n","  (9, 0.012228101116990006)],\n"," [(0, 0.031663187195546276),\n","  (1, 0.010786360473208072),\n","  (2, 0.20296914868939922),\n","  (3, 0.0337508698677801),\n","  (4, 0.02145673857573649),\n","  (5, 0.43354210160055673),\n","  (6, 0.013569937369519834),\n","  (7, 0.11261888192994665),\n","  (8, 0.1264207840408258),\n","  (9, 0.013221990257480862)],\n"," [(0, 0.13103864734299517),\n","  (1, 0.030452035886818496),\n","  (2, 0.1130952380952381),\n","  (3, 0.09057971014492754),\n","  (4, 0.031314699792960664),\n","  (5, 0.38811249137336096),\n","  (6, 0.03140096618357488),\n","  (7, 0.09825741890959282),\n","  (8, 0.07263630089717046),\n","  (9, 0.013112491373360938)],\n"," [(0, 0.06382701576136511),\n","  (1, 0.01406799531066823),\n","  (2, 0.06278494203464895),\n","  (3, 0.12739351309105118),\n","  (4, 0.023186140419434675),\n","  (5, 0.507880682558291),\n","  (6, 0.015240328253223915),\n","  (7, 0.10264426208154227),\n","  (8, 0.04884720593982024),\n","  (9, 0.03412791454995441)],\n"," [(0, 0.461976320582878),\n","  (1, 0.054758652094717676),\n","  (2, 0.06807832422586522),\n","  (3, 0.038592896174863396),\n","  (4, 0.01138433515482696),\n","  (5, 0.16154371584699456),\n","  (6, 0.02800546448087432),\n","  (7, 0.10462204007285976),\n","  (8, 0.05862932604735884),\n","  (9, 0.012408925318761387)],\n"," [(0, 0.0662748938948743),\n","  (1, 0.1095875503319186),\n","  (2, 0.04080966372837088),\n","  (3, 0.01392969855261726),\n","  (4, 0.17749483077592773),\n","  (5, 0.49504842746762434),\n","  (6, 0.010447273914462945),\n","  (7, 0.06094243116770051),\n","  (8, 0.012514963543367069),\n","  (9, 0.012950266623136359)],\n"," [(0, 0.08724513987671884),\n","  (1, 0.14540856646119807),\n","  (2, 0.02023075707286234),\n","  (3, 0.01801801801801802),\n","  (4, 0.3556187766714083),\n","  (5, 0.08503240082187452),\n","  (6, 0.019914651493598865),\n","  (7, 0.10494705231547338),\n","  (8, 0.02497234076181445),\n","  (9, 0.13861229650703338)],\n"," [(0, 0.027049521431543905),\n","  (1, 0.060479955611041754),\n","  (2, 0.06519628242474684),\n","  (3, 0.028852822860313498),\n","  (4, 0.48605909280066584),\n","  (5, 0.08087113330559023),\n","  (6, 0.07324178110694965),\n","  (7, 0.02732695242058538),\n","  (8, 0.0986267166042447),\n","  (9, 0.05229574143431821)],\n"," [(0, 0.17597292724196278),\n","  (1, 0.016215454032712916),\n","  (2, 0.1457980823463057),\n","  (3, 0.10941906373378454),\n","  (4, 0.01551043429216018),\n","  (5, 0.062887760857304),\n","  (6, 0.11336717428087986),\n","  (7, 0.03398195149464185),\n","  (8, 0.014805414551607445),\n","  (9, 0.3120417371686407)],\n"," [(0, 0.01888162672476398),\n","  (1, 0.021682747172943252),\n","  (2, 0.06753812636165578),\n","  (3, 0.016184251478369126),\n","  (4, 0.013486876231974271),\n","  (5, 0.1092436974789916),\n","  (6, 0.19203236850295674),\n","  (7, 0.06670816474738044),\n","  (8, 0.019815333540823737),\n","  (9, 0.47442680776014107)],\n"," [(0, 0.018272914824638966),\n","  (1, 0.06690244621279105),\n","  (2, 0.025542784163473824),\n","  (3, 0.009627664800078595),\n","  (4, 0.15551625896453486),\n","  (5, 0.13213478730720113),\n","  (6, 0.011396011396011398),\n","  (7, 0.10128696335592889),\n","  (8, 0.020728951763434526),\n","  (9, 0.45859121721190693)],\n"," [(0, 0.044062947067238914),\n","  (1, 0.03662374821173105),\n","  (2, 0.11130185979971388),\n","  (3, 0.11483071053886504),\n","  (4, 0.020314735336194562),\n","  (5, 0.03509775870290892),\n","  (6, 0.10510252742012399),\n","  (7, 0.04148783977110158),\n","  (8, 0.023652837386742967),\n","  (9, 0.4675250357653791)],\n"," [(0, 0.055926544240400666),\n","  (1, 0.040437766648117233),\n","  (2, 0.0371916156557225),\n","  (3, 0.20645520311630494),\n","  (4, 0.01966240029679095),\n","  (5, 0.10090892227787053),\n","  (6, 0.17816731589686516),\n","  (7, 0.04155073270265257),\n","  (8, 0.024578000370988683),\n","  (9, 0.2951214987942868)],\n"," [(0, 0.0540487994556236),\n","  (1, 0.08292019053173909),\n","  (2, 0.017303392631476622),\n","  (3, 0.08204529989306893),\n","  (4, 0.01292893943812579),\n","  (5, 0.010984738018858754),\n","  (6, 0.04977155633323612),\n","  (7, 0.02167784582482745),\n","  (8, 0.02410809759891125),\n","  (9, 0.6442111402741324)],\n"," [(0, 0.013643195745172853),\n","  (1, 0.015608740894901144),\n","  (2, 0.02300844028211354),\n","  (3, 0.010752688172043012),\n","  (4, 0.38802173661694994),\n","  (5, 0.13458203260492543),\n","  (6, 0.01919297028558215),\n","  (7, 0.01294947392762169),\n","  (8, 0.022314718464562375),\n","  (9, 0.35992600300612787)],\n"," [(0, 0.00996590611067401),\n","  (1, 0.040213305358860044),\n","  (2, 0.03916426261036804),\n","  (3, 0.026663169857505026),\n","  (4, 0.2126059970277122),\n","  (5, 0.11006206836261911),\n","  (6, 0.025264446192849024),\n","  (7, 0.07046070460704607),\n","  (8, 0.026488329399423027),\n","  (9, 0.43911181047294345)],\n"," [(0, 0.038486312399355876),\n","  (1, 0.030595813204508857),\n","  (2, 0.150402576489533),\n","  (3, 0.06843800322061191),\n","  (4, 0.01320450885668277),\n","  (5, 0.13172302737520128),\n","  (6, 0.1285024154589372),\n","  (7, 0.01964573268921095),\n","  (8, 0.03429951690821256),\n","  (9, 0.3847020933977456)],\n"," [(0, 0.06786102062975027),\n","  (1, 0.01773434672457474),\n","  (2, 0.040535649656170826),\n","  (3, 0.018367716250452408),\n","  (4, 0.1248642779587405),\n","  (5, 0.25615273253709736),\n","  (6, 0.01311979732175172),\n","  (7, 0.02379659790083243),\n","  (8, 0.028411147303655446),\n","  (9, 0.4091567137169743)],\n"," [(0, 0.017595307917888565),\n","  (1, 0.04431410883023786),\n","  (2, 0.04431410883023786),\n","  (3, 0.02845660910177039),\n","  (4, 0.022482893450635387),\n","  (5, 0.12001737808189421),\n","  (6, 0.09905506679700228),\n","  (7, 0.018138372977082654),\n","  (8, 0.2821766047572499),\n","  (9, 0.3234495492560009)],\n"," [(0, 0.02838111786852013),\n","  (1, 0.016217781639154362),\n","  (2, 0.10087846317212087),\n","  (3, 0.04305434887537408),\n","  (4, 0.09692055217685105),\n","  (5, 0.04836374167390675),\n","  (6, 0.018727676416642535),\n","  (7, 0.01119799208417801),\n","  (8, 0.16237088522058116),\n","  (9, 0.47388744087267115)],\n"," [(0, 0.1273251765642097),\n","  (1, 0.017805630160151197),\n","  (2, 0.41161842236148416),\n","  (3, 0.14652342584303193),\n","  (4, 0.012135680891276236),\n","  (5, 0.06157365960409828),\n","  (6, 0.03620809708544713),\n","  (7, 0.014523027951855168),\n","  (8, 0.13369143539242018),\n","  (9, 0.03859544414602606)],\n"," [(0, 0.016582265912024625),\n","  (1, 0.23522986793764275),\n","  (2, 0.03465395690596763),\n","  (3, 0.053718597954522884),\n","  (4, 0.015986495879257274),\n","  (5, 0.044881342468473834),\n","  (6, 0.019759706086783835),\n","  (7, 0.02134842617416344),\n","  (8, 0.17197894945884223),\n","  (9, 0.3858603912223215)],\n"," [(0, 0.027777777777777776),\n","  (1, 0.02646129541864139),\n","  (2, 0.07596103212216956),\n","  (3, 0.02330173775671406),\n","  (4, 0.01685097419694576),\n","  (5, 0.13256977356503422),\n","  (6, 0.020010531858873092),\n","  (7, 0.026724591890468666),\n","  (8, 0.5107951553449184),\n","  (9, 0.1395471300684571)],\n"," [(0, 0.028588807785888078),\n","  (1, 0.028588807785888078),\n","  (2, 0.052311435523114354),\n","  (3, 0.02645985401459854),\n","  (4, 0.023824006488240065),\n","  (5, 0.22465531224655313),\n","  (6, 0.025547445255474453),\n","  (7, 0.0113544201135442),\n","  (8, 0.4097729115977291),\n","  (9, 0.16889699918897)],\n"," [(0, 0.010881577828785174),\n","  (1, 0.03562016492391397),\n","  (2, 0.04055087987758225),\n","  (3, 0.016152342089602992),\n","  (4, 0.0076511094108645756),\n","  (5, 0.11136614809147326),\n","  (6, 0.013176910652044547),\n","  (7, 0.01734251466462637),\n","  (8, 0.491031199523931),\n","  (9, 0.2562271529371759)],\n"," [(0, 0.0337950682778269),\n","  (1, 0.02269378131447097),\n","  (2, 0.11346890657235485),\n","  (3, 0.018764122212398076),\n","  (4, 0.009529423322526771),\n","  (5, 0.0346792415757933),\n","  (6, 0.19068670792808723),\n","  (7, 0.12270360546222615),\n","  (8, 0.18547990961784067),\n","  (9, 0.2681992337164751)],\n"," [(0, 0.055227471566054245),\n","  (1, 0.03849518810148731),\n","  (2, 0.07370953630796151),\n","  (3, 0.027668416447944006),\n","  (4, 0.09000437445319336),\n","  (5, 0.05205599300087489),\n","  (6, 0.04451006124234471),\n","  (7, 0.5244969378827646),\n","  (8, 0.014326334208223972),\n","  (9, 0.07950568678915136)],\n"," [(0, 0.042884990253411304),\n","  (1, 0.01569713758079409),\n","  (2, 0.1523545706371191),\n","  (3, 0.025443726274751204),\n","  (4, 0.013234841489689135),\n","  (5, 0.0682261208576998),\n","  (6, 0.02072432543346671),\n","  (7, 0.5690981840566328),\n","  (8, 0.019903560069765056),\n","  (9, 0.07243254334667078)],\n"," [(0, 0.30936653116531165),\n","  (1, 0.06571815718157181),\n","  (2, 0.04725609756097561),\n","  (3, 0.041666666666666664),\n","  (4, 0.12610094850948508),\n","  (5, 0.019393631436314365),\n","  (6, 0.017869241192411924),\n","  (7, 0.31207655826558267),\n","  (8, 0.019224254742547426),\n","  (9, 0.041327913279132794)],\n"," [(0, 0.0373491468997087),\n","  (1, 0.02174365376612568),\n","  (2, 0.012484394506866418),\n","  (3, 0.008114856429463174),\n","  (4, 0.07823553890969623),\n","  (5, 0.08520599250936331),\n","  (6, 0.10549313358302123),\n","  (7, 0.35237203495630465),\n","  (8, 0.06668747399084479),\n","  (9, 0.23231377444860593)],\n"," [(0, 0.055159672736869884),\n","  (1, 0.0638690947479546),\n","  (2, 0.2664291369754553),\n","  (3, 0.02929532858273951),\n","  (4, 0.03549749274214833),\n","  (5, 0.18171021377672208),\n","  (6, 0.02520453945632093),\n","  (7, 0.25785167590393243),\n","  (8, 0.07363420427553444),\n","  (9, 0.011348640802322512)],\n"," [(0, 0.11238691718858733),\n","  (1, 0.032011134307585246),\n","  (2, 0.15147297610763164),\n","  (3, 0.022500579911853398),\n","  (4, 0.016353514265831592),\n","  (5, 0.05022036650429135),\n","  (6, 0.03549060542797495),\n","  (7, 0.538506147065646),\n","  (8, 0.021920668058455117),\n","  (9, 0.019137091162143353)],\n"," [(0, 0.0574002574002574),\n","  (1, 0.03423423423423423),\n","  (2, 0.2254826254826255),\n","  (3, 0.044015444015444015),\n","  (4, 0.014414414414414415),\n","  (5, 0.02556842556842557),\n","  (6, 0.06108966108966109),\n","  (7, 0.4720720720720721),\n","  (8, 0.01613041613041613),\n","  (9, 0.04959244959244959)],\n"," [(0, 0.10985460420032311),\n","  (1, 0.029886914378029084),\n","  (2, 0.04424699335846348),\n","  (3, 0.014001077005923535),\n","  (4, 0.01373182552504039),\n","  (5, 0.34230838269610486),\n","  (6, 0.018578352180936997),\n","  (7, 0.15598635792496862),\n","  (8, 0.24071082390953152),\n","  (9, 0.030694668820678516)],\n"," [(0, 0.10348221976128953),\n","  (1, 0.014519502891595915),\n","  (2, 0.24203273040482343),\n","  (3, 0.07222837455395595),\n","  (4, 0.019195275009228498),\n","  (5, 0.05167958656330749),\n","  (6, 0.03322259136212625),\n","  (7, 0.25975144579795745),\n","  (8, 0.06066199089454903),\n","  (9, 0.14322628276116647)],\n"," [(0, 0.011859628657528922),\n","  (1, 0.03859239817245067),\n","  (2, 0.023135996889277734),\n","  (3, 0.02148342568290075),\n","  (4, 0.30737824438611844),\n","  (5, 0.06882473024205309),\n","  (6, 0.014970350928356179),\n","  (7, 0.24419169825993975),\n","  (8, 0.051521337610576465),\n","  (9, 0.21804218917079812)],\n"," [(0, 0.021880670295655887),\n","  (1, 0.025402924538371223),\n","  (2, 0.024015369836695485),\n","  (3, 0.019959440708720248),\n","  (4, 0.675525669761981),\n","  (5, 0.1520973422990714),\n","  (6, 0.013768812039705411),\n","  (7, 0.021346995410395985),\n","  (8, 0.01878535596114847),\n","  (9, 0.027217419148254884)],\n"," [(0, 0.024183006535947717),\n","  (1, 0.04403594771241831),\n","  (2, 0.0383169934640523),\n","  (3, 0.013562091503267976),\n","  (4, 0.3490196078431373),\n","  (5, 0.07320261437908498),\n","  (6, 0.013398692810457517),\n","  (7, 0.33129084967320266),\n","  (8, 0.013643790849673204),\n","  (9, 0.09934640522875818)],\n"," [(0, 0.027152256510054675),\n","  (1, 0.012325085719581132),\n","  (2, 0.02307478454267445),\n","  (3, 0.022333426003150775),\n","  (4, 0.1841349272541933),\n","  (5, 0.050505050505050504),\n","  (6, 0.06561023074784543),\n","  (7, 0.2981188027059587),\n","  (8, 0.014178482068390326),\n","  (9, 0.30256695394310074)],\n"," [(0, 0.10940499040307101),\n","  (1, 0.021539773939006183),\n","  (2, 0.0436127105992749),\n","  (3, 0.02036681595222862),\n","  (4, 0.13382384303689487),\n","  (5, 0.013329068031563232),\n","  (6, 0.008530603540200469),\n","  (7, 0.5386009810194071),\n","  (8, 0.014288760929835786),\n","  (9, 0.0965024525485178)],\n"," [(0, 0.05076405076405077),\n","  (1, 0.26948976948976955),\n","  (2, 0.02421652421652422),\n","  (3, 0.0793835793835794),\n","  (4, 0.0271950271950272),\n","  (5, 0.055037555037555044),\n","  (6, 0.015151515151515154),\n","  (7, 0.04260554260554261),\n","  (8, 0.4206164206164207),\n","  (9, 0.015540015540015542)],\n"," [(0, 0.022000369754113513),\n","  (1, 0.06784987982991311),\n","  (2, 0.17933074505453872),\n","  (3, 0.028101312627102977),\n","  (4, 0.07006840451100019),\n","  (5, 0.08929561841375486),\n","  (6, 0.019412090959511925),\n","  (7, 0.04954705121094472),\n","  (8, 0.2673322240709928),\n","  (9, 0.2070623035681272)],\n"," [(0, 0.01971326164874552),\n","  (1, 0.04608294930875576),\n","  (2, 0.10189452124935997),\n","  (3, 0.02227342549923195),\n","  (4, 0.373015873015873),\n","  (5, 0.0812852022529442),\n","  (6, 0.01894521249359959),\n","  (7, 0.03494623655913978),\n","  (8, 0.2717613927291347),\n","  (9, 0.030081925243215565)],\n"," [(0, 0.06050605060506051),\n","  (1, 0.04262926292629263),\n","  (2, 0.3458929226255959),\n","  (3, 0.09332599926659332),\n","  (4, 0.024019068573524018),\n","  (5, 0.11642830949761643),\n","  (6, 0.16492482581591492),\n","  (7, 0.07535753575357536),\n","  (8, 0.05097176384305097),\n","  (9, 0.025944261092775943)],\n"," [(0, 0.024524946176167742),\n","  (1, 0.09107928484508097),\n","  (2, 0.05541514555836376),\n","  (3, 0.055040718899185624),\n","  (4, 0.039595619208087615),\n","  (5, 0.024805766170551344),\n","  (6, 0.2808199943836001),\n","  (7, 0.08237386501918936),\n","  (8, 0.28765328091360104),\n","  (9, 0.058691378826172425)],\n"," [(0, 0.08948458948458948),\n","  (1, 0.08041958041958042),\n","  (2, 0.08404558404558404),\n","  (3, 0.06565656565656566),\n","  (4, 0.02861952861952862),\n","  (5, 0.13817663817663817),\n","  (6, 0.09958559958559958),\n","  (7, 0.09052059052059053),\n","  (8, 0.19295519295519295),\n","  (9, 0.13053613053613053)],\n"," [(0, 0.022826451829472974),\n","  (1, 0.01107754279959718),\n","  (2, 0.5414568647197044),\n","  (3, 0.0832494125545485),\n","  (4, 0.012979747118719924),\n","  (5, 0.10227145574577597),\n","  (6, 0.15777106411547495),\n","  (7, 0.020140986908358506),\n","  (8, 0.03658945955018462),\n","  (9, 0.011637014658162692)],\n"," [(0, 0.049537564930951475),\n","  (1, 0.0534650956543773),\n","  (2, 0.17787913340935005),\n","  (3, 0.025845686050931203),\n","  (4, 0.036361332826555175),\n","  (5, 0.4820727226656531),\n","  (6, 0.042696059799822626),\n","  (7, 0.024325351577347016),\n","  (8, 0.08665906499429875),\n","  (9, 0.02115798809071329)],\n"," [(0, 0.12476928755998525),\n","  (1, 0.052171773102005665),\n","  (2, 0.20253476067429557),\n","  (3, 0.10348221976128955),\n","  (4, 0.03383782453549896),\n","  (5, 0.12218530823181988),\n","  (6, 0.1812476928755999),\n","  (7, 0.03359173126614988),\n","  (8, 0.1287067798695706),\n","  (9, 0.017472622123784916)],\n"," [(0, 0.01628430049482681),\n","  (1, 0.07008547008547009),\n","  (2, 0.08978857399910031),\n","  (3, 0.02249212775528565),\n","  (4, 0.29329734592892487),\n","  (5, 0.1731893837156995),\n","  (6, 0.01790373369320738),\n","  (7, 0.016194331983805668),\n","  (8, 0.2694556905083221),\n","  (9, 0.031309041835357626)],\n"," [(0, 0.06030701754385965),\n","  (1, 0.04922027290448343),\n","  (2, 0.019980506822612085),\n","  (3, 0.033625730994152045),\n","  (4, 0.1695906432748538),\n","  (5, 0.0769980506822612),\n","  (6, 0.023635477582846003),\n","  (7, 0.023757309941520467),\n","  (8, 0.020346003898635476),\n","  (9, 0.5225389863547758)],\n"," [(0, 0.19786566227244193),\n","  (1, 0.20991839296924042),\n","  (2, 0.04758317639673572),\n","  (3, 0.061268047708725674),\n","  (4, 0.018706842435655995),\n","  (5, 0.15969868173258003),\n","  (6, 0.011173885750156936),\n","  (7, 0.24469554300062774),\n","  (8, 0.03465160075329567),\n","  (9, 0.014438166980539862)],\n"," [(0, 0.15861214374225527),\n","  (1, 0.37037037037037035),\n","  (2, 0.07641470466749277),\n","  (3, 0.03318188076552389),\n","  (4, 0.012253889577309652),\n","  (5, 0.042544403139198675),\n","  (6, 0.16604708798017348),\n","  (7, 0.06058102712377805),\n","  (8, 0.06498692000550736),\n","  (9, 0.015007572628390472)],\n"," [(0, 0.026879402041109675),\n","  (1, 0.5769728331177232),\n","  (2, 0.020123616501365533),\n","  (3, 0.05102774184274831),\n","  (4, 0.014230271668822769),\n","  (5, 0.07028891763691246),\n","  (6, 0.01882995544056346),\n","  (7, 0.026016961333908295),\n","  (8, 0.13856547362368837),\n","  (9, 0.05706482679315797)],\n"," [(0, 0.13909292293342662),\n","  (1, 0.6931327970152735),\n","  (2, 0.016089541797831414),\n","  (3, 0.012591815320041975),\n","  (4, 0.025649994170455874),\n","  (5, 0.03101317476973301),\n","  (6, 0.015623178267459487),\n","  (7, 0.01760522327154017),\n","  (8, 0.014340678558936693),\n","  (9, 0.0348606738953014)],\n"," [(0, 0.015575298892179444),\n","  (1, 0.2968081605791379),\n","  (2, 0.03235713502248547),\n","  (3, 0.016562465723373918),\n","  (4, 0.3569156520785346),\n","  (5, 0.017110891740704178),\n","  (6, 0.00767796424262367),\n","  (7, 0.030053745749698366),\n","  (8, 0.010639464736207086),\n","  (9, 0.21629922123505538)],\n"," [(0, 0.03177196804647784),\n","  (1, 0.47467320261437895),\n","  (2, 0.018972403776325343),\n","  (3, 0.03721859114015976),\n","  (4, 0.25835148874364555),\n","  (5, 0.014796659404502538),\n","  (6, 0.016067538126361653),\n","  (7, 0.014524328249818443),\n","  (8, 0.0076252723311546824),\n","  (9, 0.125998547567175)],\n"," [(0, 0.022727272727272728),\n","  (1, 0.5421717171717172),\n","  (2, 0.03068181818181818),\n","  (3, 0.018686868686868686),\n","  (4, 0.015404040404040404),\n","  (5, 0.09318181818181819),\n","  (6, 0.010858585858585859),\n","  (7, 0.0659090909090909),\n","  (8, 0.17563131313131314),\n","  (9, 0.024747474747474747)],\n"," [(0, 0.02893352462936394),\n","  (1, 0.29937828790052606),\n","  (2, 0.024549657261278494),\n","  (3, 0.05157022158456879),\n","  (4, 0.00741272118603539),\n","  (5, 0.302407141718476),\n","  (6, 0.012195121951219513),\n","  (7, 0.04407779371911366),\n","  (8, 0.15893511876295233),\n","  (9, 0.0705404112864658)],\n"," [(0, 0.020191765369430348),\n","  (1, 0.47219402143260014),\n","  (2, 0.25775521714608013),\n","  (3, 0.023124647490129727),\n","  (4, 0.020304568527918784),\n","  (5, 0.01421319796954315),\n","  (6, 0.01319796954314721),\n","  (7, 0.13457416807670616),\n","  (8, 0.019740552735476598),\n","  (9, 0.024703891708967855)],\n"," [(0, 0.16129032258064518),\n","  (1, 0.34556915090841683),\n","  (2, 0.12050426399703376),\n","  (3, 0.05586454084785565),\n","  (4, 0.015202076381164258),\n","  (5, 0.016190829316524536),\n","  (6, 0.039920899765171186),\n","  (7, 0.027685082190087754),\n","  (8, 0.019157088122605366),\n","  (9, 0.19861574589049563)],\n"," [(0, 0.05599300087489064),\n","  (1, 0.5828521434820647),\n","  (2, 0.02799650043744532),\n","  (3, 0.03307086614173228),\n","  (4, 0.04741907261592301),\n","  (5, 0.04251968503937008),\n","  (6, 0.03674540682414698),\n","  (7, 0.033245844269466314),\n","  (8, 0.11023622047244094),\n","  (9, 0.029921259842519685)],\n"," [(0, 0.010137875101378748),\n","  (1, 0.5869153825358204),\n","  (2, 0.015409570154095699),\n","  (3, 0.018518518518518514),\n","  (4, 0.2462827791294944),\n","  (5, 0.02933225195998918),\n","  (6, 0.012976480129764798),\n","  (7, 0.030413625304136247),\n","  (8, 0.027575020275750196),\n","  (9, 0.02243849689105163)],\n"," [(0, 0.14122137404580154),\n","  (1, 0.34488973706530957),\n","  (2, 0.022582697201017812),\n","  (3, 0.014525021204410517),\n","  (4, 0.13857082273112808),\n","  (5, 0.07135284139100934),\n","  (6, 0.02099236641221374),\n","  (7, 0.023642917726887192),\n","  (8, 0.021628498727735368),\n","  (9, 0.20059372349448684)],\n"," [(0, 0.04778156996587031),\n","  (1, 0.02490203514094299),\n","  (2, 0.2782201997219062),\n","  (3, 0.3563392744280116),\n","  (4, 0.026292504108203767),\n","  (5, 0.05738844646694476),\n","  (6, 0.09050688914170142),\n","  (7, 0.02844141069397042),\n","  (8, 0.07217798002780938),\n","  (9, 0.01794969030463911)],\n"," [(0, 0.04005766656369066),\n","  (1, 0.02234579343013078),\n","  (2, 0.2300483987230975),\n","  (3, 0.07661414890330553),\n","  (4, 0.015343424981979199),\n","  (5, 0.03470291422098651),\n","  (6, 0.5024199361548759),\n","  (7, 0.02255174544331171),\n","  (8, 0.04242611471527134),\n","  (9, 0.013489856863350838)],\n"," [(0, 0.08518911276069283),\n","  (1, 0.017909744314834452),\n","  (2, 0.3759868033462943),\n","  (3, 0.14504536349711322),\n","  (4, 0.018498880640980323),\n","  (5, 0.026157652880876636),\n","  (6, 0.2786614822669966),\n","  (7, 0.014139271827500884),\n","  (8, 0.02368328031106398),\n","  (9, 0.014728408153646753)],\n"," [(0, 0.06944444444444445),\n","  (1, 0.018779342723004695),\n","  (2, 0.04107981220657277),\n","  (3, 0.6424100156494522),\n","  (4, 0.01085680751173709),\n","  (5, 0.02269170579029734),\n","  (6, 0.13302034428794993),\n","  (7, 0.030125195618153366),\n","  (8, 0.02190923317683881),\n","  (9, 0.009683098591549295)],\n"," [(0, 0.024967459472251805),\n","  (1, 0.15986273813749852),\n","  (2, 0.09407170749023784),\n","  (3, 0.3520293456395693),\n","  (4, 0.05419476984972193),\n","  (5, 0.19796473790084013),\n","  (6, 0.014436161401017632),\n","  (7, 0.05419476984972193),\n","  (8, 0.01538279493551059),\n","  (9, 0.03289551532363034)],\n"," [(0, 0.19004940825002872),\n","  (1, 0.4550155118924509),\n","  (2, 0.0383775709525451),\n","  (3, 0.02493393082844996),\n","  (4, 0.033436745949672525),\n","  (5, 0.16454096288636103),\n","  (6, 0.021142134896012868),\n","  (7, 0.02987475583132253),\n","  (8, 0.023784901758014478),\n","  (9, 0.018844076755141905)],\n"," [(0, 0.020784313725490194),\n","  (1, 0.017124183006535947),\n","  (2, 0.047058823529411764),\n","  (3, 0.7183006535947712),\n","  (4, 0.01843137254901961),\n","  (5, 0.07751633986928104),\n","  (6, 0.022091503267973857),\n","  (7, 0.02522875816993464),\n","  (8, 0.03712418300653595),\n","  (9, 0.016339869281045753)],\n"," [(0, 0.3958086200079083),\n","  (1, 0.057203110583893504),\n","  (2, 0.27151706867009356),\n","  (3, 0.054435218136285754),\n","  (4, 0.030974034532753394),\n","  (5, 0.05891656781336497),\n","  (6, 0.019507051535521288),\n","  (7, 0.054567022538552785),\n","  (8, 0.01634374588111243),\n","  (9, 0.04072756030051404)],\n"," [(0, 0.14439603001694504),\n","  (1, 0.030501089324618737),\n","  (2, 0.36213991769547327),\n","  (3, 0.02868554829339143),\n","  (4, 0.00980392156862745),\n","  (5, 0.08448317598644396),\n","  (6, 0.021544420237230695),\n","  (7, 0.029895908980876303),\n","  (8, 0.0692326313241346),\n","  (9, 0.21931735657225854)],\n"," [(0, 0.015196811160936722),\n","  (1, 0.01943198804185351),\n","  (2, 0.010089686098654708),\n","  (3, 0.8582461385151968),\n","  (4, 0.035625311410064774),\n","  (5, 0.011709018435475834),\n","  (6, 0.015196811160936722),\n","  (7, 0.01332835077229696),\n","  (8, 0.01332835077229696),\n","  (9, 0.007847533632286996)],\n"," [(0, 0.10179380006966214),\n","  (1, 0.009230233368164403),\n","  (2, 0.08072100313479624),\n","  (3, 0.11590038314176246),\n","  (4, 0.009578544061302681),\n","  (5, 0.17206548241031),\n","  (6, 0.4018634622082898),\n","  (7, 0.019418321142459075),\n","  (8, 0.039968652037617555),\n","  (9, 0.04946011842563567)],\n"," [(0, 0.06504565507335591),\n","  (1, 0.013234841489689139),\n","  (2, 0.03580588899148457),\n","  (3, 0.5326767210423721),\n","  (4, 0.02021134708115318),\n","  (5, 0.04360315994665026),\n","  (6, 0.00964399302349441),\n","  (7, 0.17974761465066177),\n","  (8, 0.04534728634451627),\n","  (9, 0.054683492356622565)],\n"," [(0, 0.06284950158035497),\n","  (1, 0.020058351568198397),\n","  (2, 0.030026744468757596),\n","  (3, 0.6297106734743496),\n","  (4, 0.020423048869438368),\n","  (5, 0.1224167274495502),\n","  (6, 0.04923413566739606),\n","  (7, 0.012642839776318988),\n","  (8, 0.04461463651835643),\n","  (9, 0.008023340627279359)],\n"," [(0, 0.037615046018407365),\n","  (1, 0.013605442176870748),\n","  (2, 0.0221421902094171),\n","  (3, 0.4692543684140323),\n","  (4, 0.022675736961451247),\n","  (5, 0.031746031746031744),\n","  (6, 0.1974122982526344),\n","  (7, 0.024409763905562223),\n","  (8, 0.017073496065092703),\n","  (9, 0.1640656262505002)],\n"," [(0, 0.019007881316643487),\n","  (1, 0.14866326688301654),\n","  (2, 0.016689847009735744),\n","  (3, 0.6961829701746253),\n","  (4, 0.014371812702828002),\n","  (5, 0.0296708391284191),\n","  (6, 0.011590171534538712),\n","  (7, 0.021789522484932777),\n","  (8, 0.019471488178025034),\n","  (9, 0.02256220058723536)],\n"," [(0, 0.30125580125580126),\n","  (1, 0.016516516516516516),\n","  (2, 0.40035490035490034),\n","  (3, 0.09295659295659296),\n","  (4, 0.016516516516516516),\n","  (5, 0.0677040677040677),\n","  (6, 0.036855036855036855),\n","  (7, 0.01365001365001365),\n","  (8, 0.03903903903903904),\n","  (9, 0.015151515151515152)],\n"," [(0, 0.0175485719401916),\n","  (1, 0.0182648401826484),\n","  (2, 0.25373802489032143),\n","  (3, 0.08192318023099651),\n","  (4, 0.032948339153012804),\n","  (5, 0.12194466827827022),\n","  (6, 0.23108604172262512),\n","  (7, 0.054257319366102605),\n","  (8, 0.0530933834721103),\n","  (9, 0.13519563076372101)],\n"," [(0, 0.03981154299175501),\n","  (1, 0.016804083235178645),\n","  (2, 0.060620337652139776),\n","  (3, 0.07067137809187281),\n","  (4, 0.01115037298782882),\n","  (5, 0.5901060070671379),\n","  (6, 0.019238319591676486),\n","  (7, 0.05598743619945034),\n","  (8, 0.10475068708284258),\n","  (9, 0.030859835100117788)],\n"," [(0, 0.013651877133105804),\n","  (1, 0.016021994690936674),\n","  (2, 0.29285172544558213),\n","  (3, 0.06740614334470992),\n","  (4, 0.017064846416382257),\n","  (5, 0.3846226772847934),\n","  (6, 0.03204398938187335),\n","  (7, 0.014031095942358743),\n","  (8, 0.14154342055365948),\n","  (9, 0.02076222980659841)],\n"," [(0, 0.02102102102102102),\n","  (1, 0.013051513051513048),\n","  (2, 0.18861168861168856),\n","  (3, 0.08754908754908754),\n","  (4, 0.02621852621852621),\n","  (5, 0.18064218064218063),\n","  (6, 0.2998382998382998),\n","  (7, 0.049780549780549764),\n","  (8, 0.05116655116655115),\n","  (9, 0.08212058212058211)],\n"," [(0, 0.034018113540976364),\n","  (1, 0.022752374641042634),\n","  (2, 0.3687872763419483),\n","  (3, 0.04749282085266181),\n","  (4, 0.012149326264634415),\n","  (5, 0.16556218246079082),\n","  (6, 0.2775568809366026),\n","  (7, 0.020764303070466093),\n","  (8, 0.039430086149768055),\n","  (9, 0.011486635741108902)],\n"," [(0, 0.04988662131519275),\n","  (1, 0.018807523009203686),\n","  (2, 0.26023742830465524),\n","  (3, 0.03441376550620249),\n","  (4, 0.09870614912631721),\n","  (5, 0.32412965186074433),\n","  (6, 0.026944110977724426),\n","  (7, 0.13965586234493801),\n","  (8, 0.027077497665732964),\n","  (9, 0.02014138988928905)],\n"," [(0, 0.394919824038598),\n","  (1, 0.031928480204342274),\n","  (2, 0.3218390804597701),\n","  (3, 0.06527600397332198),\n","  (4, 0.020008514261387826),\n","  (5, 0.01816375762735916),\n","  (6, 0.04767986377181779),\n","  (7, 0.050943663970483895),\n","  (8, 0.02937420178799489),\n","  (9, 0.01986660990492408)],\n"," [(0, 0.0733569968807142),\n","  (1, 0.012907389480477573),\n","  (2, 0.11347746584919867),\n","  (3, 0.04313219318059589),\n","  (4, 0.20673335484564914),\n","  (5, 0.18629665483489297),\n","  (6, 0.06787135635151124),\n","  (7, 0.012477143164461654),\n","  (8, 0.0404431537054964),\n","  (9, 0.24330429170700227)],\n"," [(0, 0.3594883543337152),\n","  (1, 0.04095074455899198),\n","  (2, 0.349942726231386),\n","  (3, 0.045437189767086675),\n","  (4, 0.02147766323024055),\n","  (5, 0.02567773959526537),\n","  (6, 0.08629247804505537),\n","  (7, 0.02577319587628866),\n","  (8, 0.0143184421534937),\n","  (9, 0.030641466208476516)],\n"," [(0, 0.03708458137212951),\n","  (1, 0.015974896591071173),\n","  (2, 0.2859791755812295),\n","  (3, 0.13165026387105977),\n","  (4, 0.02168021680216802),\n","  (5, 0.0713165026387106),\n","  (6, 0.3304806732277849),\n","  (7, 0.015119098559406647),\n","  (8, 0.07887605191841392),\n","  (9, 0.01183853943802596)],\n"," [(0, 0.02834008097165992),\n","  (1, 0.030889188783925627),\n","  (2, 0.03748687959214275),\n","  (3, 0.6732643574748838),\n","  (4, 0.016194331983805668),\n","  (5, 0.05787974209026841),\n","  (6, 0.07437396911081122),\n","  (7, 0.03478782426150847),\n","  (8, 0.015594541910331383),\n","  (9, 0.031189083820662766)],\n"," [(0, 0.04989775051124743),\n","  (1, 0.020449897750511245),\n","  (2, 0.3282890252215405),\n","  (3, 0.17573278800272663),\n","  (4, 0.04526244035446489),\n","  (5, 0.16687116564417176),\n","  (6, 0.09952283571915471),\n","  (7, 0.03817314246762099),\n","  (8, 0.04062713019768234),\n","  (9, 0.03517382413087934)],\n"," [(0, 0.24988425925925925),\n","  (1, 0.01875),\n","  (2, 0.21944444444444444),\n","  (3, 0.3038194444444444),\n","  (4, 0.009953703703703704),\n","  (5, 0.06539351851851852),\n","  (6, 0.023842592592592592),\n","  (7, 0.0642361111111111),\n","  (8, 0.022569444444444444),\n","  (9, 0.02210648148148148)],\n"," [(0, 0.02091945383940875),\n","  (1, 0.023174245271201308),\n","  (2, 0.21232619316046603),\n","  (3, 0.28773643993486164),\n","  (4, 0.010522360015031944),\n","  (5, 0.14706250782913693),\n","  (6, 0.11499436302142053),\n","  (7, 0.06551421771263938),\n","  (8, 0.023675310033821874),\n","  (9, 0.09407490918201179)],\n"," [(0, 0.06461300009687108),\n","  (1, 0.018696115470309024),\n","  (2, 0.3306209435241694),\n","  (3, 0.029739416836191034),\n","  (4, 0.04320449481739805),\n","  (5, 0.23374987891116927),\n","  (6, 0.11207982175724113),\n","  (7, 0.02809260873777003),\n","  (8, 0.0957086118376441),\n","  (9, 0.04349510801123705)],\n"," [(0, 0.01736111111111111),\n","  (1, 0.019933127572016457),\n","  (2, 0.27006172839506165),\n","  (3, 0.07201646090534979),\n","  (4, 0.013245884773662548),\n","  (5, 0.27970679012345673),\n","  (6, 0.015946502057613166),\n","  (7, 0.025334362139917688),\n","  (8, 0.27417695473251025),\n","  (9, 0.012217078189300408)],\n"," [(0, 0.022413999760278077),\n","  (1, 0.03799592472731631),\n","  (2, 0.3238643173918255),\n","  (3, 0.050581325662231814),\n","  (4, 0.027208438211674457),\n","  (5, 0.1704422869471413),\n","  (6, 0.29102241399976025),\n","  (7, 0.018218866115306244),\n","  (8, 0.046266331055975066),\n","  (9, 0.01198609612849095)],\n"," [(0, 0.04206955046649703),\n","  (1, 0.022561492790500424),\n","  (2, 0.20797285835453774),\n","  (3, 0.10500424088210347),\n","  (4, 0.022900763358778626),\n","  (5, 0.061238337574215436),\n","  (6, 0.2734520780322307),\n","  (7, 0.04071246819338423),\n","  (8, 0.19491094147582697),\n","  (9, 0.02917726887192536)],\n"," [(0, 0.0701525054466231),\n","  (1, 0.030065359477124187),\n","  (2, 0.2561002178649238),\n","  (3, 0.05827886710239652),\n","  (4, 0.0266884531590414),\n","  (5, 0.13453159041394339),\n","  (6, 0.03986928104575164),\n","  (7, 0.025599128540305015),\n","  (8, 0.22004357298474947),\n","  (9, 0.13867102396514164)],\n"," [(0, 0.039836347975882855),\n","  (1, 0.018841515934539185),\n","  (2, 0.40977605512489224),\n","  (3, 0.10960378983634794),\n","  (4, 0.011089577950043065),\n","  (5, 0.05027993109388457),\n","  (6, 0.2088716623600344),\n","  (7, 0.014965546942291125),\n","  (8, 0.09151593453919034),\n","  (9, 0.04521963824289405)],\n"," [(0, 0.017435409732128704),\n","  (1, 0.019020446980504042),\n","  (2, 0.055317799968299254),\n","  (3, 0.022983040101442385),\n","  (4, 0.04533206530353463),\n","  (5, 0.3902361705500079),\n","  (6, 0.01632588365826597),\n","  (7, 0.020605484228879378),\n","  (8, 0.34347757172293547),\n","  (9, 0.06926612775400222)],\n"," [(0, 0.04662885948330183),\n","  (1, 0.06574249107330393),\n","  (2, 0.09073724007561437),\n","  (3, 0.3276622558286074),\n","  (4, 0.008716656164671287),\n","  (5, 0.04725897920604915),\n","  (6, 0.09031716026044949),\n","  (7, 0.07750472589792061),\n","  (8, 0.16876706574249106),\n","  (9, 0.07666456626759084)],\n"," [(0, 0.01276595744680851),\n","  (1, 0.010307328605200946),\n","  (2, 0.4189125295508274),\n","  (3, 0.01739952718676123),\n","  (4, 0.006903073286052009),\n","  (5, 0.17465721040189125),\n","  (6, 0.04094562647754137),\n","  (7, 0.014940898345153664),\n","  (8, 0.24122931442080378),\n","  (9, 0.06193853427895981)],\n"," [(0, 0.14744459520578923),\n","  (1, 0.017789838685361072),\n","  (2, 0.1821197045077642),\n","  (3, 0.07115935474144429),\n","  (4, 0.08397406904869592),\n","  (5, 0.04929895974672094),\n","  (6, 0.08080808080808081),\n","  (7, 0.0236695311322177),\n","  (8, 0.08668777325493743),\n","  (9, 0.2570480928689884)],\n"," [(0, 0.021650326797385624),\n","  (1, 0.01388888888888889),\n","  (2, 0.15740740740740744),\n","  (3, 0.14419934640522877),\n","  (4, 0.021650326797385624),\n","  (5, 0.11996187363834424),\n","  (6, 0.016884531590413948),\n","  (7, 0.025462962962962965),\n","  (8, 0.3596132897603486),\n","  (9, 0.11928104575163401)],\n"," [(0, 0.14447295884421632),\n","  (1, 0.0641573994867408),\n","  (2, 0.19636916642904667),\n","  (3, 0.07993536736051707),\n","  (4, 0.03697367170421063),\n","  (5, 0.09599847923201217),\n","  (6, 0.29597946963216426),\n","  (7, 0.013686911890504704),\n","  (8, 0.052941735576466115),\n","  (9, 0.01948483984412128)],\n"," [(0, 0.09315756434400503),\n","  (1, 0.022222222222222227),\n","  (2, 0.3643440050219712),\n","  (3, 0.056622724419334594),\n","  (4, 0.06541117388575017),\n","  (5, 0.2734463276836159),\n","  (6, 0.04607658505963592),\n","  (7, 0.048587570621468935),\n","  (8, 0.019962335216572508),\n","  (9, 0.01016949152542373)],\n"," [(0, 0.3332577989274114),\n","  (1, 0.2399728076138681),\n","  (2, 0.02787219578518015),\n","  (3, 0.018052723015333484),\n","  (4, 0.24382506231588488),\n","  (5, 0.008308784651408717),\n","  (6, 0.05793488934209533),\n","  (7, 0.029080746279930507),\n","  (8, 0.010952488858675127),\n","  (9, 0.03074250321021225)],\n"," [(0, 0.06485812285625195),\n","  (1, 0.023282403076603262),\n","  (2, 0.020683920590375222),\n","  (3, 0.1384471468662301),\n","  (4, 0.017877559505248934),\n","  (5, 0.035651179711048746),\n","  (6, 0.03336451512316807),\n","  (7, 0.02983057894189793),\n","  (8, 0.04053632678515747),\n","  (9, 0.5954682465440183)],\n"," [(0, 0.02433371958285052),\n","  (1, 0.062057422428221966),\n","  (2, 0.016222479721900347),\n","  (3, 0.020213724732844083),\n","  (4, 0.7404403244495944),\n","  (5, 0.041586198017252475),\n","  (6, 0.011716235354705807),\n","  (7, 0.04004119994850006),\n","  (8, 0.03077121153598558),\n","  (9, 0.012617484228144715)],\n"," [(0, 0.1518345042935207),\n","  (1, 0.038381472807702315),\n","  (2, 0.016653655997918293),\n","  (3, 0.03669008587041374),\n","  (4, 0.44119177725735104),\n","  (5, 0.09692948217538382),\n","  (6, 0.012360135310954984),\n","  (7, 0.16380431954202446),\n","  (8, 0.01795472287275566),\n","  (9, 0.02419984387197502)],\n"," [(0, 0.02159367396593674),\n","  (1, 0.137875101378751),\n","  (2, 0.017842660178426603),\n","  (3, 0.017842660178426603),\n","  (4, 0.7393552311435523),\n","  (5, 0.011050283860502838),\n","  (6, 0.013888888888888888),\n","  (7, 0.01348337388483374),\n","  (8, 0.01267234387672344),\n","  (9, 0.014395782643957826)],\n"," [(0, 0.04813059505002633),\n","  (1, 0.03033175355450237),\n","  (2, 0.24644549763033174),\n","  (3, 0.09015271195365981),\n","  (4, 0.061084781463928386),\n","  (5, 0.07288046340179041),\n","  (6, 0.18862559241706162),\n","  (7, 0.013059505002632964),\n","  (8, 0.1281727224855187),\n","  (9, 0.12111637704054766)],\n"," [(0, 0.018731375053214133),\n","  (1, 0.013793103448275862),\n","  (2, 0.124563644103874),\n","  (3, 0.06853980417198809),\n","  (4, 0.009110259684972328),\n","  (5, 0.14627501064282675),\n","  (6, 0.3424435930183057),\n","  (7, 0.17249893571732652),\n","  (8, 0.09178373776074926),\n","  (9, 0.012260536398467433)],\n"," [(0, 0.01988795518207283),\n","  (1, 0.09140989729225024),\n","  (2, 0.024556489262371622),\n","  (3, 0.742670401493931),\n","  (4, 0.010364145658263307),\n","  (5, 0.053221288515406175),\n","  (6, 0.014752567693744167),\n","  (7, 0.013445378151260508),\n","  (8, 0.016059757236227826),\n","  (9, 0.01363211951447246)],\n"," [(0, 0.011238558683814159),\n","  (1, 0.008805468659483258),\n","  (2, 0.050283860502838604),\n","  (3, 0.026995713127099988),\n","  (4, 0.00926891437840343),\n","  (5, 0.3658903950874754),\n","  (6, 0.01007994438651373),\n","  (7, 0.03475842891901286),\n","  (8, 0.024330900243309004),\n","  (9, 0.4583478160120496)],\n"," [(0, 0.1364091364091364),\n","  (1, 0.03958503958503958),\n","  (2, 0.09045409045409046),\n","  (3, 0.022568022568022567),\n","  (4, 0.008008008008008008),\n","  (5, 0.5328055328055328),\n","  (6, 0.03376103376103376),\n","  (7, 0.05314405314405314),\n","  (8, 0.06406406406406406),\n","  (9, 0.0192010192010192)],\n"," [(0, 0.030495430495430496),\n","  (1, 0.024146224146224145),\n","  (2, 0.24723424723424722),\n","  (3, 0.0962000962000962),\n","  (4, 0.009427609427609427),\n","  (5, 0.035016835016835016),\n","  (6, 0.021933621933621934),\n","  (7, 0.017123617123617124),\n","  (8, 0.4173160173160173),\n","  (9, 0.1011063011063011)],\n"," [(0, 0.07474271759986047),\n","  (1, 0.009680795395081111),\n","  (2, 0.4784580498866214),\n","  (3, 0.029565672422815284),\n","  (4, 0.007936507936507938),\n","  (5, 0.02014652014652015),\n","  (6, 0.015000872143729289),\n","  (7, 0.00976800976800977),\n","  (8, 0.15009593581022154),\n","  (9, 0.2046049188906332)],\n"," [(0, 0.018706842435656),\n","  (1, 0.2479598242310107),\n","  (2, 0.017325800376647837),\n","  (3, 0.5585687382297553),\n","  (4, 0.018706842435656),\n","  (5, 0.07357187696170749),\n","  (6, 0.011676082862523543),\n","  (7, 0.018204645323289394),\n","  (8, 0.022347771500313876),\n","  (9, 0.012931575643440052)],\n"," [(0, 0.04896227496846692),\n","  (1, 0.03210640981538814),\n","  (2, 0.06363949088407293),\n","  (3, 0.04357298474945534),\n","  (4, 0.05125558995528036),\n","  (5, 0.5674807934869854),\n","  (6, 0.015250544662309368),\n","  (7, 0.13370026373122348),\n","  (8, 0.034858387799564274),\n","  (9, 0.009173259947253756)],\n"," [(0, 0.019893749293545832),\n","  (1, 0.023397761953204473),\n","  (2, 0.44354018311291954),\n","  (3, 0.19192946761614102),\n","  (4, 0.014242115971515765),\n","  (5, 0.05990731321351869),\n","  (6, 0.1553068836893862),\n","  (7, 0.04623036057420594),\n","  (8, 0.01898948796202102),\n","  (9, 0.026562676613541305)],\n"," [(0, 0.03863092991065811),\n","  (1, 0.030451742796023657),\n","  (2, 0.14571536428841073),\n","  (3, 0.031710079275198186),\n","  (4, 0.01245753114382786),\n","  (5, 0.25141562853907135),\n","  (6, 0.03737259343148358),\n","  (7, 0.4171385428463571),\n","  (8, 0.013841701270919844),\n","  (9, 0.02126588649804958)],\n"," [(0, 0.03743028743028743),\n","  (1, 0.08365508365508366),\n","  (2, 0.12827112827112827),\n","  (3, 0.09491634491634492),\n","  (4, 0.05062205062205062),\n","  (5, 0.4751179751179751),\n","  (6, 0.052123552123552123),\n","  (7, 0.017374517374517374),\n","  (8, 0.03903903903903904),\n","  (9, 0.02145002145002145)],\n"," [(0, 0.05179952909519004),\n","  (1, 0.04058750981051688),\n","  (2, 0.1966588182531674),\n","  (3, 0.09844152931943043),\n","  (4, 0.16111671712075346),\n","  (5, 0.3013790783720148),\n","  (6, 0.024217961654894045),\n","  (7, 0.07512052920731023),\n","  (8, 0.02253615876219307),\n","  (9, 0.028142168404529655)],\n"," [(0, 0.024342745861733205),\n","  (1, 0.022178946229579142),\n","  (2, 0.16845180136319376),\n","  (3, 0.04652169209131234),\n","  (4, 0.012982797792924375),\n","  (5, 0.4014930217461863),\n","  (6, 0.09585632370442497),\n","  (7, 0.04814454181542789),\n","  (8, 0.16877637130801687),\n","  (9, 0.011251758087201125)],\n"," [(0, 0.0971971971971972),\n","  (1, 0.023923923923923923),\n","  (2, 0.07477477477477477),\n","  (3, 0.016716716716716717),\n","  (4, 0.016616616616616616),\n","  (5, 0.6397397397397397),\n","  (6, 0.011611611611611611),\n","  (7, 0.05775775775775776),\n","  (8, 0.05205205205205205),\n","  (9, 0.00960960960960961)],\n"," [(0, 0.046893046893046894),\n","  (1, 0.09805959805959806),\n","  (2, 0.29521829521829523),\n","  (3, 0.037075537075537075),\n","  (4, 0.09066759066759067),\n","  (5, 0.13317163317163316),\n","  (6, 0.14703164703164703),\n","  (7, 0.015015015015015015),\n","  (8, 0.012705012705012704),\n","  (9, 0.12416262416262416)],\n"," [(0, 0.0545512210593086),\n","  (1, 0.012052013954963527),\n","  (2, 0.43799555978433236),\n","  (3, 0.035838883602917856),\n","  (4, 0.01765514324981499),\n","  (5, 0.22708531557247066),\n","  (6, 0.09831906121154456),\n","  (7, 0.02272967544137858),\n","  (8, 0.03594460302357543),\n","  (9, 0.05782852309969341)],\n"," [(0, 0.013510592304366624),\n","  (1, 0.011132728058798098),\n","  (2, 0.230652831820147),\n","  (3, 0.01232166018158236),\n","  (4, 0.01664504971897968),\n","  (5, 0.025399913532209252),\n","  (6, 0.05717682663207955),\n","  (7, 0.014267185473411154),\n","  (8, 0.6019239083441418),\n","  (9, 0.016969303934284478)],\n"," [(0, 0.04469009826152684),\n","  (1, 0.007275132275132276),\n","  (2, 0.24659863945578234),\n","  (3, 0.3259637188208617),\n","  (4, 0.015495086923658355),\n","  (5, 0.04922524565381709),\n","  (6, 0.05064247921390779),\n","  (7, 0.011999244142101287),\n","  (8, 0.229119425547997),\n","  (9, 0.018990929705215424)],\n"," [(0, 0.28582577309530777),\n","  (1, 0.01268168959125939),\n","  (2, 0.4047410008779631),\n","  (3, 0.013949858550385328),\n","  (4, 0.008584528338698664),\n","  (5, 0.06331089649790264),\n","  (6, 0.08223587942639743),\n","  (7, 0.03736220856501805),\n","  (8, 0.0651643742073944),\n","  (9, 0.026143790849673203)],\n"," [(0, 0.21094877344877344),\n","  (1, 0.015602453102453102),\n","  (2, 0.5118145743145743),\n","  (3, 0.040584415584415584),\n","  (4, 0.015512265512265512),\n","  (5, 0.12346681096681096),\n","  (6, 0.04013347763347763),\n","  (7, 0.012175324675324676),\n","  (8, 0.019570707070707072),\n","  (9, 0.010191197691197692)],\n"," [(0, 0.2583874458874459),\n","  (1, 0.011453823953823954),\n","  (2, 0.037337662337662336),\n","  (3, 0.028589466089466088),\n","  (4, 0.011994949494949494),\n","  (5, 0.25432900432900435),\n","  (6, 0.13392857142857142),\n","  (7, 0.041035353535353536),\n","  (8, 0.014971139971139972),\n","  (9, 0.20797258297258298)],\n"," [(0, 0.2911382308367233),\n","  (1, 0.033261545824359896),\n","  (2, 0.02743878120762543),\n","  (3, 0.01571348807529712),\n","  (4, 0.008375209380234505),\n","  (5, 0.16766371540240887),\n","  (6, 0.03581399058785993),\n","  (7, 0.13512004466778335),\n","  (8, 0.27111749222301984),\n","  (9, 0.014357501794687724)],\n"," [(0, 0.070019284972556),\n","  (1, 0.03426791277258567),\n","  (2, 0.25619344310933095),\n","  (3, 0.0936062898679721),\n","  (4, 0.02091677792612372),\n","  (5, 0.39326509419967365),\n","  (6, 0.05548138258418632),\n","  (7, 0.01943331849873906),\n","  (8, 0.023438658952677643),\n","  (9, 0.03337783711615487)],\n"," [(0, 0.07975527149568448),\n","  (1, 0.01507702392658145),\n","  (2, 0.36621872610073203),\n","  (3, 0.11427947121162461),\n","  (4, 0.012017917622637389),\n","  (5, 0.035507483885065014),\n","  (6, 0.32360974543865406),\n","  (7, 0.01616956189227576),\n","  (8, 0.02600240358352453),\n","  (9, 0.011362394843220804)],\n"," [(0, 0.029823971297230623),\n","  (1, 0.10348693799753333),\n","  (2, 0.2635945733826662),\n","  (3, 0.09754456777665654),\n","  (4, 0.012108980827447019),\n","  (5, 0.17546810180513508),\n","  (6, 0.022648278955039798),\n","  (7, 0.09788092835519674),\n","  (8, 0.1816347124117053),\n","  (9, 0.015808947191389165)],\n"," [(0, 0.3839368616527391),\n","  (1, 0.16264314453729495),\n","  (2, 0.1598576292169607),\n","  (3, 0.022438873413803778),\n","  (4, 0.028938409161250386),\n","  (5, 0.10940885174868462),\n","  (6, 0.020891364902506964),\n","  (7, 0.028783658310120707),\n","  (8, 0.06963788300835655),\n","  (9, 0.013463324048282266)],\n"," [(0, 0.1427402862985685),\n","  (1, 0.014996591683708248),\n","  (2, 0.26598500340831627),\n","  (3, 0.1488752556237219),\n","  (4, 0.021540558963871848),\n","  (5, 0.06639400136332652),\n","  (6, 0.06066802999318337),\n","  (7, 0.03353783231083845),\n","  (8, 0.2027266530334015),\n","  (9, 0.0425357873210634)],\n"," [(0, 0.06670998483863981),\n","  (1, 0.019709768247779945),\n","  (2, 0.5982239549491012),\n","  (3, 0.051332033788174136),\n","  (4, 0.018735109378384233),\n","  (5, 0.09865713666883258),\n","  (6, 0.06909248429716267),\n","  (7, 0.016894087069525665),\n","  (8, 0.050032488628979854),\n","  (9, 0.01061295213341997)],\n"," [(0, 0.24245768947755708),\n","  (1, 0.10294334069168509),\n","  (2, 0.012656364974245772),\n","  (3, 0.013171449595290659),\n","  (4, 0.19190581309786614),\n","  (5, 0.33127299484915385),\n","  (6, 0.010596026490066227),\n","  (7, 0.04576894775570273),\n","  (8, 0.034878587196468),\n","  (9, 0.014348785871964684)],\n"," [(0, 0.4132640435535759),\n","  (1, 0.14303390249938136),\n","  (2, 0.1638208364266271),\n","  (3, 0.029695619896065333),\n","  (4, 0.019384640765487095),\n","  (5, 0.0868596881959911),\n","  (6, 0.046770601336302904),\n","  (7, 0.0725068052462262),\n","  (8, 0.010558442629712119),\n","  (9, 0.014105419450631033)],\n"," [(0, 0.08441746169413125),\n","  (1, 0.010600366194468537),\n","  (2, 0.28254794256528865),\n","  (3, 0.1078346342873663),\n","  (4, 0.01792425556519225),\n","  (5, 0.0199479618386817),\n","  (6, 0.018598824323022067),\n","  (7, 0.23542449648260577),\n","  (8, 0.015515081430085767),\n","  (9, 0.20718897561915775)],\n"," [(0, 0.020443382450550912),\n","  (1, 0.027080844285145365),\n","  (2, 0.07248108323377142),\n","  (3, 0.022700119474313028),\n","  (4, 0.018584893136864466),\n","  (5, 0.042878003451480164),\n","  (6, 0.058940661091198734),\n","  (7, 0.6756936147617152),\n","  (8, 0.028142838178680477),\n","  (9, 0.03305455993628037)],\n"," [(0, 0.015446400988569664),\n","  (1, 0.014519616929255486),\n","  (2, 0.06817011636288746),\n","  (3, 0.03501184224075791),\n","  (4, 0.016270209041293383),\n","  (5, 0.15992173823499128),\n","  (6, 0.04469158686026157),\n","  (7, 0.5859334774997427),\n","  (8, 0.017814849140150347),\n","  (9, 0.04222016270209042)],\n"," [(0, 0.015873015873015872),\n","  (1, 0.027721886876816454),\n","  (2, 0.04974290185557791),\n","  (3, 0.02526268723451822),\n","  (4, 0.04214173932483792),\n","  (5, 0.23731276548177957),\n","  (6, 0.06762799016320144),\n","  (7, 0.04124748490945674),\n","  (8, 0.07288173485356583),\n","  (9, 0.42018779342723006)],\n"," [(0, 0.06334765394659254),\n","  (1, 0.008508129053997138),\n","  (2, 0.18743155589251118),\n","  (3, 0.05938842557493051),\n","  (4, 0.010698340493639965),\n","  (5, 0.07640468368292479),\n","  (6, 0.028809704321455652),\n","  (7, 0.49111279588914164),\n","  (8, 0.046752590346221895),\n","  (9, 0.02754612079858479)],\n"," [(0, 0.1718249733191035),\n","  (1, 0.018854500177872643),\n","  (2, 0.048499940709118936),\n","  (3, 0.09854144432586268),\n","  (4, 0.00984228625637377),\n","  (5, 0.020988971896122376),\n","  (6, 0.036997509782995375),\n","  (7, 0.5661093323846792),\n","  (8, 0.017787264318747775),\n","  (9, 0.01055377682912368)],\n"," [(0, 0.08017988373368432),\n","  (1, 0.07217286388066249),\n","  (2, 0.11549851924975318),\n","  (3, 0.009213557091148402),\n","  (4, 0.01634309531644181),\n","  (5, 0.060436547109794875),\n","  (6, 0.023582318745201267),\n","  (7, 0.5256114950093231),\n","  (8, 0.06899199298014697),\n","  (9, 0.027969726883843363)],\n"," [(0, 0.08467816731589686),\n","  (1, 0.02624744945279169),\n","  (2, 0.0295863476163977),\n","  (3, 0.015581524763494713),\n","  (4, 0.1860508254498238),\n","  (5, 0.10703023557781488),\n","  (6, 0.012149879428677426),\n","  (7, 0.11315154887775923),\n","  (8, 0.02912261176034131),\n","  (9, 0.3964014097570024)],\n"," [(0, 0.17414219640744327),\n","  (1, 0.017640098956652684),\n","  (2, 0.027535764225018824),\n","  (3, 0.1414434763902334),\n","  (4, 0.016564483166612885),\n","  (5, 0.017317414219640743),\n","  (6, 0.11713455953533398),\n","  (7, 0.4535871786597827),\n","  (8, 0.0186081531676885),\n","  (9, 0.01602667527159299)],\n"," [(0, 0.11098354381936472),\n","  (1, 0.03112641918612068),\n","  (2, 0.06531445337415487),\n","  (3, 0.07182038525322107),\n","  (4, 0.032657226687077434),\n","  (5, 0.021686439596887357),\n","  (6, 0.027554535017221583),\n","  (7, 0.5961219543309095),\n","  (8, 0.019645362928945018),\n","  (9, 0.023089679806097718)],\n"," [(0, 0.06481481481481481),\n","  (1, 0.06688453159041394),\n","  (2, 0.11797385620915032),\n","  (3, 0.05032679738562092),\n","  (4, 0.061546840958605666),\n","  (5, 0.13954248366013072),\n","  (6, 0.026252723311546843),\n","  (7, 0.4110021786492375),\n","  (8, 0.03300653594771242),\n","  (9, 0.028649237472766886)],\n"," [(0, 0.024931079947261178),\n","  (1, 0.04374925086899197),\n","  (2, 0.3039673978185305),\n","  (3, 0.04303008510128251),\n","  (4, 0.06772144312597388),\n","  (5, 0.2699268848136162),\n","  (6, 0.051899796236365814),\n","  (7, 0.13520316432937793),\n","  (8, 0.024931079947261178),\n","  (9, 0.03463981781133885)],\n"," [(0, 0.03784000802970992),\n","  (1, 0.012747164508682124),\n","  (2, 0.09153869316470943),\n","  (3, 0.022683930543009132),\n","  (4, 0.0689551339957844),\n","  (5, 0.22443039245207266),\n","  (6, 0.09705911873933554),\n","  (7, 0.06995884773662552),\n","  (8, 0.14895111914082104),\n","  (9, 0.22583559168925021)],\n"," [(0, 0.007332585110362888),\n","  (1, 0.015488215488215488),\n","  (2, 0.040778151889263),\n","  (3, 0.006434717545828657),\n","  (4, 0.1829405162738496),\n","  (5, 0.1405910961466517),\n","  (6, 0.03307145529367751),\n","  (7, 0.029255518144407033),\n","  (8, 0.012345679012345678),\n","  (9, 0.5317620650953985)],\n"," [(0, 0.01247050893158072),\n","  (1, 0.015166835187057633),\n","  (2, 0.01988540613414223),\n","  (3, 0.056735198292326705),\n","  (4, 0.6974497247500281),\n","  (5, 0.020334793843388384),\n","  (6, 0.05123019885406134),\n","  (7, 0.010897651949219189),\n","  (8, 0.010111223458038422),\n","  (9, 0.10571845860015729)],\n"," [(0, 0.017686631944444444),\n","  (1, 0.03331163194444445),\n","  (2, 0.072265625),\n","  (3, 0.054144965277777776),\n","  (4, 0.031575520833333336),\n","  (5, 0.061197916666666664),\n","  (6, 0.05284288194444445),\n","  (7, 0.013454861111111112),\n","  (8, 0.03233506944444445),\n","  (9, 0.6311848958333334)],\n"," [(0, 0.04654928152195912),\n","  (1, 0.009208662214126695),\n","  (2, 0.12021857923497267),\n","  (3, 0.03602509613438575),\n","  (4, 0.007893139040680024),\n","  (5, 0.05585913782635094),\n","  (6, 0.3665249949402955),\n","  (7, 0.012649261283141065),\n","  (8, 0.21281117182756526),\n","  (9, 0.13226067597652297)],\n"," [(0, 0.07933461292386436),\n","  (1, 0.04286628278950736),\n","  (2, 0.1783962465344423),\n","  (3, 0.030283642567711665),\n","  (4, 0.009490296438473022),\n","  (5, 0.09511622947323523),\n","  (6, 0.17029217317125186),\n","  (7, 0.054062699936020475),\n","  (8, 0.03582853486884197),\n","  (9, 0.30432928129665177)],\n"," [(0, 0.0216618170061429),\n","  (1, 0.11175773251427953),\n","  (2, 0.020045263498221787),\n","  (3, 0.01056148291841793),\n","  (4, 0.5226856342278262),\n","  (5, 0.05722599418040736),\n","  (6, 0.02575708589287638),\n","  (7, 0.032115529690699425),\n","  (8, 0.01788985882099364),\n","  (9, 0.1802996012501347)],\n"," [(0, 0.01971326164874552),\n","  (1, 0.02661290322580645),\n","  (2, 0.00949820788530466),\n","  (3, 0.01478494623655914),\n","  (4, 0.49883512544802866),\n","  (5, 0.0671146953405018),\n","  (6, 0.017293906810035842),\n","  (7, 0.09112903225806451),\n","  (8, 0.010663082437275985),\n","  (9, 0.24435483870967742)],\n"," [(0, 0.014763014763014764),\n","  (1, 0.03671328671328671),\n","  (2, 0.033216783216783216),\n","  (3, 0.021756021756021756),\n","  (4, 0.017676767676767676),\n","  (5, 0.12461149961149962),\n","  (6, 0.04040404040404041),\n","  (7, 0.034867909867909865),\n","  (8, 0.2748640248640249),\n","  (9, 0.40112665112665113)],\n"," [(0, 0.15035931453841903),\n","  (1, 0.011387506909894969),\n","  (2, 0.028745163073521283),\n","  (3, 0.043449419568822555),\n","  (4, 0.37965726920950804),\n","  (5, 0.02609176340519624),\n","  (6, 0.022553897180762852),\n","  (7, 0.01205085682697623),\n","  (8, 0.009508015478164732),\n","  (9, 0.3161967938087341)],\n"," [(0, 0.013189107125913275),\n","  (1, 0.018787361229718188),\n","  (2, 0.05047917259702059),\n","  (3, 0.010152765917069932),\n","  (4, 0.010722079893728058),\n","  (5, 0.11680425087769238),\n","  (6, 0.015371477369769428),\n","  (7, 0.02599867160072113),\n","  (8, 0.4359996204573489),\n","  (9, 0.30249549293101813)],\n"," [(0, 0.4933787362845252),\n","  (1, 0.04086265607264473),\n","  (2, 0.15285660234581916),\n","  (3, 0.03846638920418717),\n","  (4, 0.014377601210745368),\n","  (5, 0.020305208727456177),\n","  (6, 0.1653424139235717),\n","  (7, 0.027494009332828858),\n","  (8, 0.016269390843738178),\n","  (9, 0.030646992054483544)],\n"," [(0, 0.3465031789282471),\n","  (1, 0.0174084165909779),\n","  (2, 0.18165304268846505),\n","  (3, 0.24371783227369062),\n","  (4, 0.014834998486224646),\n","  (5, 0.05782621858916138),\n","  (6, 0.0709960641840751),\n","  (7, 0.019981834695731157),\n","  (8, 0.0333030578262186),\n","  (9, 0.0137753557372086)],\n"," [(0, 0.35245594300712413),\n","  (1, 0.014998125234345707),\n","  (2, 0.07611548556430446),\n","  (3, 0.046994125734283215),\n","  (4, 0.0111236095488064),\n","  (5, 0.05561804774403199),\n","  (6, 0.3125859267591551),\n","  (7, 0.01262342207224097),\n","  (8, 0.018997625296837895),\n","  (9, 0.09848768903887015)],\n"," [(0, 0.19480957195820697),\n","  (1, 0.019773059206830697),\n","  (2, 0.2526682395236491),\n","  (3, 0.024941017863161444),\n","  (4, 0.017750814515223014),\n","  (5, 0.03853499606785755),\n","  (6, 0.33793955735310643),\n","  (7, 0.062015503875969),\n","  (8, 0.03707448601280756),\n","  (9, 0.014492753623188408)],\n"," [(0, 0.06334882873028896),\n","  (1, 0.049949867833378916),\n","  (2, 0.26652082763649626),\n","  (3, 0.4477258226232796),\n","  (4, 0.013216662109196975),\n","  (5, 0.021511256950141285),\n","  (6, 0.05578342904019689),\n","  (7, 0.03928538875216481),\n","  (8, 0.017500683620453927),\n","  (9, 0.02515723270440252)],\n"," [(0, 0.11811391223155929),\n","  (1, 0.19339402427637722),\n","  (2, 0.17180205415499533),\n","  (3, 0.02030812324929972),\n","  (4, 0.10515873015873016),\n","  (5, 0.1437908496732026),\n","  (6, 0.046218487394957986),\n","  (7, 0.026143790849673203),\n","  (8, 0.13025210084033614),\n","  (9, 0.04481792717086835)],\n"," [(0, 0.12337337337337337),\n","  (1, 0.03803803803803804),\n","  (2, 0.11136136136136136),\n","  (3, 0.03903903903903904),\n","  (4, 0.0965965965965966),\n","  (5, 0.19857357357357358),\n","  (6, 0.21383883883883884),\n","  (7, 0.02715215215215215),\n","  (8, 0.05555555555555555),\n","  (9, 0.09647147147147148)],\n"," [(0, 0.4196465269214962),\n","  (1, 0.026441978353199072),\n","  (2, 0.14947253048362794),\n","  (3, 0.04329360186326895),\n","  (4, 0.022331826277572273),\n","  (5, 0.045348677901082345),\n","  (6, 0.18029867105082892),\n","  (7, 0.04795177421564598),\n","  (8, 0.02781202904507467),\n","  (9, 0.03740238388820387)],\n"," [(0, 0.21418128654970756),\n","  (1, 0.08289473684210524),\n","  (2, 0.01622807017543859),\n","  (3, 0.04517543859649122),\n","  (4, 0.33479532163742687),\n","  (5, 0.0327485380116959),\n","  (6, 0.20701754385964907),\n","  (7, 0.01944444444444444),\n","  (8, 0.03391812865497075),\n","  (9, 0.013596491228070173)],\n"," [(0, 0.5537958773252891),\n","  (1, 0.015208647561588738),\n","  (2, 0.11802413273001508),\n","  (3, 0.07554047259929612),\n","  (4, 0.019482151835093012),\n","  (5, 0.1297134238310709),\n","  (6, 0.01860231271995978),\n","  (7, 0.030165912518853696),\n","  (8, 0.01897938662644545),\n","  (9, 0.020487682252388135)],\n"," [(0, 0.1330891330891331),\n","  (1, 0.05670872337539004),\n","  (2, 0.05182471849138516),\n","  (3, 0.2805589472256139),\n","  (4, 0.019943019943019943),\n","  (5, 0.039886039886039885),\n","  (6, 0.1803011803011803),\n","  (7, 0.04042870709537376),\n","  (8, 0.17921584588251255),\n","  (9, 0.018043684710351376)],\n"," [(0, 0.08685626441199079),\n","  (1, 0.015372790161414298),\n","  (2, 0.09795883508412334),\n","  (3, 0.26791357075753697),\n","  (4, 0.020240840379195494),\n","  (5, 0.01810573063455462),\n","  (6, 0.45170381757622347),\n","  (7, 0.01691006917755573),\n","  (8, 0.015543598940985568),\n","  (9, 0.009394482876419849)],\n"," [(0, 0.5512510088781275),\n","  (1, 0.036454129674468656),\n","  (2, 0.025692762980898575),\n","  (3, 0.03403282216841539),\n","  (4, 0.19383911756793112),\n","  (5, 0.049771320957761635),\n","  (6, 0.025558245897228948),\n","  (7, 0.03766478342749529),\n","  (8, 0.021791767554479417),\n","  (9, 0.023944040893193436)],\n"," [(0, 0.2993944514857063),\n","  (1, 0.031122377129981695),\n","  (2, 0.037600337980566126),\n","  (3, 0.10928038304464162),\n","  (4, 0.31319532460216876),\n","  (5, 0.05111956062526406),\n","  (6, 0.08027038445289397),\n","  (7, 0.04971130826644135),\n","  (8, 0.01211097028587523),\n","  (9, 0.016194902126461064)],\n"," [(0, 0.20150375939849624),\n","  (1, 0.08170426065162907),\n","  (2, 0.06733500417710944),\n","  (3, 0.30041771094402675),\n","  (4, 0.042439431913116124),\n","  (5, 0.1443609022556391),\n","  (6, 0.06232247284878864),\n","  (7, 0.012698412698412698),\n","  (8, 0.07218045112781955),\n","  (9, 0.015037593984962405)],\n"," [(0, 0.30051318773123287),\n","  (1, 0.047499701635039984),\n","  (2, 0.026136770497672754),\n","  (3, 0.08175199904523213),\n","  (4, 0.26423200859291085),\n","  (5, 0.061940565699964194),\n","  (6, 0.12662608903210407),\n","  (7, 0.023391812865497075),\n","  (8, 0.05215419501133787),\n","  (9, 0.015753669889008236)],\n"," [(0, 0.0681497175141243),\n","  (1, 0.021657250470809793),\n","  (2, 0.17384651600753295),\n","  (3, 0.04719868173258004),\n","  (4, 0.017184557438794726),\n","  (5, 0.2966101694915254),\n","  (6, 0.10769774011299435),\n","  (7, 0.012829566854990584),\n","  (8, 0.05649717514124294),\n","  (9, 0.1983286252354049)],\n"," [(0, 0.05164509169363538),\n","  (1, 0.015911542610571736),\n","  (2, 0.04355447680690399),\n","  (3, 0.6668015102481122),\n","  (4, 0.013888888888888888),\n","  (5, 0.0749730312837109),\n","  (6, 0.0656688241639698),\n","  (7, 0.03033980582524272),\n","  (8, 0.02063106796116505),\n","  (9, 0.016585760517799353)],\n"," [(0, 0.15328679464805117),\n","  (1, 0.01497963932518906),\n","  (2, 0.2465095986038394),\n","  (3, 0.2651250727166957),\n","  (4, 0.01672484002326934),\n","  (5, 0.011198371146015124),\n","  (6, 0.25116346713205345),\n","  (7, 0.012652705061082022),\n","  (8, 0.01774287376381617),\n","  (9, 0.010616637579988363)],\n"," [(0, 0.15372096463973958),\n","  (1, 0.016718449474774372),\n","  (2, 0.15786358928835623),\n","  (3, 0.3389554667850273),\n","  (4, 0.00946885633969522),\n","  (5, 0.034916407752626125),\n","  (6, 0.2168959905311436),\n","  (7, 0.016570498594466635),\n","  (8, 0.04571682201509098),\n","  (9, 0.009172954579079743)],\n"," [(0, 0.01555352241537054),\n","  (1, 0.03517332520077259),\n","  (2, 0.02348276913693199),\n","  (3, 0.6470468638812646),\n","  (4, 0.010064043915827997),\n","  (5, 0.21398800447290842),\n","  (6, 0.02216122801667175),\n","  (7, 0.012300498119345329),\n","  (8, 0.012097184100843754),\n","  (9, 0.008132560740063027)],\n"," [(0, 0.12708420905142218),\n","  (1, 0.019195740507215918),\n","  (2, 0.017934706459296623),\n","  (3, 0.6820793050301247),\n","  (4, 0.01120919153706039),\n","  (5, 0.03797113633179207),\n","  (6, 0.024660221381532857),\n","  (7, 0.023679417122040074),\n","  (8, 0.008406893652795292),\n","  (9, 0.04777917892671991)],\n"," [(0, 0.34282632146709824),\n","  (1, 0.022330097087378643),\n","  (2, 0.09471413160733551),\n","  (3, 0.05954692556634305),\n","  (4, 0.03700107874865157),\n","  (5, 0.26774541531823093),\n","  (6, 0.014023732470334414),\n","  (7, 0.10377562028047466),\n","  (8, 0.04142394822006473),\n","  (9, 0.01661272923408846)],\n"," [(0, 0.0450070323488045),\n","  (1, 0.05426629160806376),\n","  (2, 0.04723394280356306),\n","  (3, 0.5690342240975153),\n","  (4, 0.03375527426160337),\n","  (5, 0.1618612283169245),\n","  (6, 0.018635724331926864),\n","  (7, 0.022620721987810597),\n","  (8, 0.022855133614627286),\n","  (9, 0.024730426629160807)],\n"," [(0, 0.013354700854700854),\n","  (1, 0.019965277777777776),\n","  (2, 0.02063301282051282),\n","  (3, 0.017895299145299144),\n","  (4, 0.006543803418803419),\n","  (5, 0.0763888888888889),\n","  (6, 0.1440304487179487),\n","  (7, 0.060229700854700856),\n","  (8, 0.6313434829059829),\n","  (9, 0.009615384615384616)],\n"," [(0, 0.024261603375527425),\n","  (1, 0.07747304266291608),\n","  (2, 0.023206751054852322),\n","  (3, 0.014299109235818097),\n","  (4, 0.0923581809657759),\n","  (5, 0.04946085325832161),\n","  (6, 0.03563056727613689),\n","  (7, 0.2928973277074543),\n","  (8, 0.2817627754336615),\n","  (9, 0.10864978902953587)],\n"," [(0, 0.02015732546705998),\n","  (1, 0.4437069813176008),\n","  (2, 0.01978859390363815),\n","  (3, 0.028269419862340217),\n","  (4, 0.19837758112094395),\n","  (5, 0.025688298918387413),\n","  (6, 0.04092920353982301),\n","  (7, 0.021878072763028514),\n","  (8, 0.13544739429695182),\n","  (9, 0.06575712881022616)],\n"," [(0, 0.017143593709719),\n","  (1, 0.016627996906419182),\n","  (2, 0.029131219386439804),\n","  (3, 0.01946377932456819),\n","  (4, 0.015725702500644496),\n","  (5, 0.0894560453725187),\n","  (6, 0.06973446764630059),\n","  (7, 0.02616653776746584),\n","  (8, 0.7031451405001289),\n","  (9, 0.013405516885795308)],\n"," [(0, 0.04598686089688661),\n","  (1, 0.20013329524897652),\n","  (2, 0.10139960011425309),\n","  (3, 0.03694182614491098),\n","  (4, 0.15338474721508144),\n","  (5, 0.09454441588117682),\n","  (6, 0.022088926973245743),\n","  (7, 0.04103589450633153),\n","  (8, 0.279729601066362),\n","  (9, 0.0247548319527754)],\n"," [(0, 0.022822243651558983),\n","  (1, 0.01200042858673524),\n","  (2, 0.07350262509375335),\n","  (3, 0.02646523090110361),\n","  (4, 0.023465123754419802),\n","  (5, 0.11175399121397193),\n","  (6, 0.016072002571520413),\n","  (7, 0.04360870031072538),\n","  (8, 0.6599164255866281),\n","  (9, 0.0103932283295832)],\n"," [(0, 0.010718729064982295),\n","  (1, 0.02153316106804479),\n","  (2, 0.07943343860656522),\n","  (3, 0.06048425686668581),\n","  (4, 0.009378887931859509),\n","  (5, 0.044023351516891567),\n","  (6, 0.050435448368264905),\n","  (7, 0.02650971384821514),\n","  (8, 0.6835103837687817),\n","  (9, 0.013972628959709063)],\n"," [(0, 0.03022562792677735),\n","  (1, 0.014261387824606215),\n","  (2, 0.05023414218816517),\n","  (3, 0.02543635589612601),\n","  (4, 0.014793529161345253),\n","  (5, 0.08141762452107279),\n","  (6, 0.02596849723286505),\n","  (7, 0.05214985100042571),\n","  (8, 0.6966794380587484),\n","  (9, 0.008833546189868029)],\n"," [(0, 0.01507770818835537),\n","  (1, 0.033170958014381816),\n","  (2, 0.06947344003711436),\n","  (3, 0.032011134307585246),\n","  (4, 0.017281373231268847),\n","  (5, 0.07573648805381582),\n","  (6, 0.01646949663651125),\n","  (7, 0.03954998840176293),\n","  (8, 0.6520528879610299),\n","  (9, 0.04917652516817444)],\n"," [(0, 0.0726752503576538),\n","  (1, 0.014020028612303292),\n","  (2, 0.04034334763948499),\n","  (3, 0.09403910348116358),\n","  (4, 0.010300429184549358),\n","  (5, 0.023938960419647118),\n","  (6, 0.08288030519790178),\n","  (7, 0.13667143538388177),\n","  (8, 0.3951359084406295),\n","  (9, 0.12999523128278495)],\n"," [(0, 0.03476414039794321),\n","  (1, 0.054102392130561144),\n","  (2, 0.397719651240778),\n","  (3, 0.046054102392130564),\n","  (4, 0.03006930471719204),\n","  (5, 0.09959758551307847),\n","  (6, 0.12687234518220433),\n","  (7, 0.11077576570534317),\n","  (8, 0.06338028169014084),\n","  (9, 0.03666443103062821)],\n"," [(0, 0.07003444316877153),\n","  (1, 0.050899349406812094),\n","  (2, 0.08304630692690394),\n","  (3, 0.11902028319938768),\n","  (4, 0.05217502232427606),\n","  (5, 0.0644214823319301),\n","  (6, 0.1179997448654165),\n","  (7, 0.1012884296466386),\n","  (8, 0.31930093124122977),\n","  (9, 0.021814006888633754)],\n"," [(0, 0.013513513513513514),\n","  (1, 0.03744369369369369),\n","  (2, 0.09309309309309309),\n","  (3, 0.016704204204204206),\n","  (4, 0.09778528528528528),\n","  (5, 0.17192192192192193),\n","  (6, 0.029748498498498498),\n","  (7, 0.03481606606606607),\n","  (8, 0.3298611111111111),\n","  (9, 0.1751126126126126)],\n"," [(0, 0.03298611111111112),\n","  (1, 0.4880952380952382),\n","  (2, 0.13157242063492067),\n","  (3, 0.027901785714285723),\n","  (4, 0.0207093253968254),\n","  (5, 0.03782242063492064),\n","  (6, 0.14422123015873017),\n","  (7, 0.014136904761904765),\n","  (8, 0.04228670634920636),\n","  (9, 0.06026785714285716)],\n"," [(0, 0.02371813797854542),\n","  (1, 0.6282846176557425),\n","  (2, 0.03966145064462159),\n","  (3, 0.044975888199980316),\n","  (4, 0.04950300167306367),\n","  (5, 0.044582226158842636),\n","  (6, 0.028146835941344355),\n","  (7, 0.05216022045074304),\n","  (8, 0.07076075189449857),\n","  (9, 0.018206869402617853)],\n"," [(0, 0.0184108527131783),\n","  (1, 0.4423987941429803),\n","  (2, 0.014319552110249788),\n","  (3, 0.014965546942291132),\n","  (4, 0.12596899224806204),\n","  (5, 0.03520671834625324),\n","  (6, 0.010766580534022397),\n","  (7, 0.029931093884582264),\n","  (8, 0.009151593453919037),\n","  (9, 0.29888027562446173)],\n"," [(0, 0.18658055256548262),\n","  (1, 0.4079655543595264),\n","  (2, 0.12354981461547664),\n","  (3, 0.016026791053701715),\n","  (4, 0.011601483076187061),\n","  (5, 0.02703025953833274),\n","  (6, 0.12390862337041025),\n","  (7, 0.029900729577801703),\n","  (8, 0.01985408443966033),\n","  (9, 0.05358210740342065)],\n"," [(0, 0.020755885997521685),\n","  (1, 0.25154894671623296),\n","  (2, 0.05947955390334572),\n","  (3, 0.03407682775712516),\n","  (4, 0.02137546468401487),\n","  (5, 0.20280875671210244),\n","  (6, 0.02395704254440314),\n","  (7, 0.1570631970260223),\n","  (8, 0.15117719950433706),\n","  (9, 0.07775712515489468)],\n"," [(0, 0.42386446315351844),\n","  (1, 0.2686830890759797),\n","  (2, 0.03533936181270139),\n","  (3, 0.04115996258185221),\n","  (4, 0.07982538197692549),\n","  (5, 0.06974326993036069),\n","  (6, 0.01746180230745245),\n","  (7, 0.0320133042303295),\n","  (8, 0.011953019436649),\n","  (9, 0.01995634549423137)],\n"," [(0, 0.019434392172631015),\n","  (1, 0.3743466023321271),\n","  (2, 0.01407318053880177),\n","  (3, 0.01876424071840236),\n","  (4, 0.4759415627931913),\n","  (5, 0.015815574319796275),\n","  (6, 0.01541348344725908),\n","  (7, 0.032569360675512665),\n","  (8, 0.02104275566277979),\n","  (9, 0.012598847339498728)],\n"," [(0, 0.04758427649993915),\n","  (1, 0.3938176950225143),\n","  (2, 0.014847267859316051),\n","  (3, 0.023487890957770475),\n","  (4, 0.018254837531945966),\n","  (5, 0.03468419131069733),\n","  (6, 0.009005719849093343),\n","  (7, 0.017281246196908847),\n","  (8, 0.01569916027747353),\n","  (9, 0.425337714494341)],\n"," [(0, 0.3239597622313672),\n","  (1, 0.2445130315500686),\n","  (2, 0.10539551897576589),\n","  (3, 0.05178326474622771),\n","  (4, 0.01108824874256973),\n","  (5, 0.1440329218106996),\n","  (6, 0.05201188843164152),\n","  (7, 0.01886145404663923),\n","  (8, 0.014517604023776864),\n","  (9, 0.03383630544124371)],\n"," [(0, 0.04967620995228357),\n","  (1, 0.012610770279481936),\n","  (2, 0.013207225630538513),\n","  (3, 0.19308111792774368),\n","  (4, 0.013462849352419905),\n","  (5, 0.6556748466257669),\n","  (6, 0.02147239263803681),\n","  (7, 0.01525221540558964),\n","  (8, 0.012014314928425357),\n","  (9, 0.0135480572597137)],\n"," [(0, 0.0211401663160004),\n","  (1, 0.01021941689209498),\n","  (2, 0.04789099288648432),\n","  (3, 0.018034265103697024),\n","  (4, 0.04548642420599138),\n","  (5, 0.40066125638713557),\n","  (6, 0.01653140967838894),\n","  (7, 0.029255585612664062),\n","  (8, 0.06672678088367899),\n","  (9, 0.3440537020338643)],\n"," [(0, 0.030648363981697315),\n","  (1, 0.023741690408357077),\n","  (2, 0.04454804454804455),\n","  (3, 0.04411637744971078),\n","  (4, 0.01674868341535008),\n","  (5, 0.7187257187257188),\n","  (6, 0.04057670724337391),\n","  (7, 0.03954070620737287),\n","  (8, 0.023828023828023827),\n","  (9, 0.01752568419235086)],\n"," [(0, 0.06656930753316295),\n","  (1, 0.012656687355482536),\n","  (2, 0.23755628574905682),\n","  (3, 0.04989655592065231),\n","  (4, 0.014725568942436412),\n","  (5, 0.15175854934891078),\n","  (6, 0.32298892539856394),\n","  (7, 0.07362784471218206),\n","  (8, 0.05306072775952294),\n","  (9, 0.01715954728002921)],\n"," [(0, 0.12087445611800912),\n","  (1, 0.04796773851215112),\n","  (2, 0.01920832006791892),\n","  (3, 0.038204393505253106),\n","  (4, 0.13764193993420354),\n","  (5, 0.0725883476599809),\n","  (6, 0.030351268173617742),\n","  (7, 0.05146980791679932),\n","  (8, 0.3163536028865542),\n","  (9, 0.16534012522551206)],\n"," [(0, 0.026892525600535937),\n","  (1, 0.008804670303378314),\n","  (2, 0.06048425686668581),\n","  (3, 0.10000957029380803),\n","  (4, 0.056847545219638244),\n","  (5, 0.4181261364723897),\n","  (6, 0.024117140396210164),\n","  (7, 0.04574600440233515),\n","  (8, 0.13809933964972726),\n","  (9, 0.12087281079529141)],\n"," [(0, 0.026026878667423814),\n","  (1, 0.02025364376301344),\n","  (2, 0.12322544009085747),\n","  (3, 0.06284308158243422),\n","  (4, 0.012587544955517699),\n","  (5, 0.1943971228468673),\n","  (6, 0.02858224493658906),\n","  (7, 0.03274654552337687),\n","  (8, 0.42049971607041453),\n","  (9, 0.07883778156350559)],\n"," [(0, 0.018440905280804696),\n","  (1, 0.0351122287417342),\n","  (2, 0.012387072739126388),\n","  (3, 0.037161218217379166),\n","  (4, 0.2937505820992829),\n","  (5, 0.1499487752631089),\n","  (6, 0.009313588525658938),\n","  (7, 0.026543727298127973),\n","  (8, 0.017137002887212448),\n","  (9, 0.40020489894756456)],\n"," [(0, 0.09852529875413173),\n","  (1, 0.0764047800661073),\n","  (2, 0.019069412662090012),\n","  (3, 0.031782354436816686),\n","  (4, 0.4781337401474702),\n","  (5, 0.024790236460717013),\n","  (6, 0.026697177726926015),\n","  (7, 0.1441647597254005),\n","  (8, 0.018560894991100946),\n","  (9, 0.08187134502923978)],\n"," [(0, 0.017867603983596953),\n","  (1, 0.18570591681312243),\n","  (2, 0.009470806483108768),\n","  (3, 0.01845342706502636),\n","  (4, 0.6350322202694786),\n","  (5, 0.01425502831478227),\n","  (6, 0.014352665495020504),\n","  (7, 0.04881859011911736),\n","  (8, 0.022651825815270456),\n","  (9, 0.033391915641476276)],\n"," [(0, 0.008655846858094049),\n","  (1, 0.016645859342488557),\n","  (2, 0.01531419059508947),\n","  (3, 0.007990012484394507),\n","  (4, 0.5136079900124844),\n","  (5, 0.03803578859758635),\n","  (6, 0.013233458177278402),\n","  (7, 0.26816479400749066),\n","  (8, 0.008739076154806492),\n","  (9, 0.10961298377028714)],\n"," [(0, 0.011220787986163841),\n","  (1, 0.01442672741078208),\n","  (2, 0.03087825866869147),\n","  (3, 0.01282375769847296),\n","  (4, 0.4874715261958998),\n","  (5, 0.2738547203239686),\n","  (6, 0.021935375010545852),\n","  (7, 0.046823588964819036),\n","  (8, 0.06943389859107399),\n","  (9, 0.031131359149582385)],\n"," [(0, 0.026066827697262488),\n","  (1, 0.01982689210950081),\n","  (2, 0.06390901771336556),\n","  (3, 0.03754025764895331),\n","  (4, 0.44504830917874405),\n","  (5, 0.042673107890499204),\n","  (6, 0.14412238325281806),\n","  (7, 0.06582125603864736),\n","  (8, 0.11574074074074077),\n","  (9, 0.03925120772946861)],\n"," [(0, 0.19265654979940694),\n","  (1, 0.029391243676957963),\n","  (2, 0.11625675911390197),\n","  (3, 0.058782487353915926),\n","  (4, 0.12166405023547881),\n","  (5, 0.04657247514390372),\n","  (6, 0.2452468166753881),\n","  (7, 0.013692656549799407),\n","  (8, 0.00898308041165184),\n","  (9, 0.16675388103959532)],\n"," [(0, 0.06929510155316607),\n","  (1, 0.10681003584229391),\n","  (2, 0.032974910394265235),\n","  (3, 0.018399044205495818),\n","  (4, 0.5243727598566308),\n","  (5, 0.07037037037037037),\n","  (6, 0.00967741935483871),\n","  (7, 0.06690561529271206),\n","  (8, 0.016248506571087215),\n","  (9, 0.08494623655913978)],\n"," [(0, 0.022825150732127476),\n","  (1, 0.025322997416020673),\n","  (2, 0.020327304048234282),\n","  (3, 0.008010335917312662),\n","  (4, 0.4373815676141258),\n","  (5, 0.06149870801033592),\n","  (6, 0.024806201550387597),\n","  (7, 0.26503014642549527),\n","  (8, 0.00809646856158484),\n","  (9, 0.12670111972437553)],\n"," [(0, 0.19686800894854586),\n","  (1, 0.0218387131138809),\n","  (2, 0.01257057632896559),\n","  (3, 0.0218387131138809),\n","  (4, 0.5003728560775541),\n","  (5, 0.022264834345371257),\n","  (6, 0.08447853414296368),\n","  (7, 0.10450623202301054),\n","  (8, 0.019814637264301695),\n","  (9, 0.015446894641525514)],\n"," [(0, 0.026336611280660113),\n","  (1, 0.012659658641347351),\n","  (2, 0.035379224595908226),\n","  (3, 0.01932858596134283),\n","  (4, 0.6377303040578728),\n","  (5, 0.10432915112467504),\n","  (6, 0.033457669266418),\n","  (7, 0.013111789307109756),\n","  (8, 0.010625070645416528),\n","  (9, 0.10704193511924948)],\n"," [(0, 0.13581585924319114),\n","  (1, 0.26295492889852984),\n","  (2, 0.03000723065798988),\n","  (3, 0.0457941672692215),\n","  (4, 0.3829838515304893),\n","  (5, 0.019040732706676312),\n","  (6, 0.05145818269462522),\n","  (7, 0.022174017835623044),\n","  (8, 0.017474090142202945),\n","  (9, 0.032296939021450956)],\n"," [(0, 0.09614368726888536),\n","  (1, 0.0919175911251981),\n","  (2, 0.026589188237365734),\n","  (3, 0.023419616129600283),\n","  (4, 0.5180489522803311),\n","  (5, 0.1045958795562599),\n","  (6, 0.025884838880084523),\n","  (7, 0.05229793977812995),\n","  (8, 0.020602218700475437),\n","  (9, 0.04050008804366966)],\n"," [(0, 0.16749018117319148),\n","  (1, 0.0410490307867731),\n","  (2, 0.03990877993158495),\n","  (3, 0.049030786773090085),\n","  (4, 0.4899277841125048),\n","  (5, 0.09514759913847715),\n","  (6, 0.03129355124794122),\n","  (7, 0.014189788420119094),\n","  (8, 0.03509438743190169),\n","  (9, 0.03686811098441658)],\n"," [(0, 0.02035410567405549),\n","  (1, 0.046005855290673366),\n","  (2, 0.012686463125609927),\n","  (3, 0.01561410846228914),\n","  (4, 0.48598912588874954),\n","  (5, 0.08964171197546356),\n","  (6, 0.011571169664017847),\n","  (7, 0.09229053394674475),\n","  (8, 0.01435940331799805),\n","  (9, 0.21148752265439846)],\n"," [(0, 0.04598074218327383),\n","  (1, 0.022395326192794548),\n","  (2, 0.011576328032024234),\n","  (3, 0.041328572974142594),\n","  (4, 0.7156767283349562),\n","  (5, 0.03213242453748783),\n","  (6, 0.08189981607703127),\n","  (7, 0.008871578491831657),\n","  (8, 0.030725954776587686),\n","  (9, 0.009412528399870172)],\n"," [(0, 0.021588433353139237),\n","  (1, 0.06852842146959794),\n","  (2, 0.023073876015052487),\n","  (3, 0.04268171915230739),\n","  (4, 0.5675381263616558),\n","  (5, 0.024658348187759953),\n","  (6, 0.054367201426024955),\n","  (7, 0.08199643493761141),\n","  (8, 0.010596157654981184),\n","  (9, 0.10497128144186968)],\n"," [(0, 0.10939012584704744),\n","  (1, 0.18984618694202432),\n","  (2, 0.12262020006453694),\n","  (3, 0.1265999784876842),\n","  (4, 0.015811552113585026),\n","  (5, 0.05399591265999785),\n","  (6, 0.09207271162740668),\n","  (7, 0.012262020006453695),\n","  (8, 0.2628804990857266),\n","  (9, 0.01452081316553727)],\n"," [(0, 0.02606712284131639),\n","  (1, 0.6348430541978929),\n","  (2, 0.010752688172043012),\n","  (3, 0.05246008471814923),\n","  (4, 0.07678939937004453),\n","  (5, 0.08634734441186054),\n","  (6, 0.03280112957532312),\n","  (7, 0.044096882806560225),\n","  (8, 0.014554143586401651),\n","  (9, 0.021288150320408386)],\n"," [(0, 0.01730505434209138),\n","  (1, 0.04969927192149414),\n","  (2, 0.025218951144877072),\n","  (3, 0.02184235517568851),\n","  (4, 0.01741057296612852),\n","  (5, 0.04938271604938271),\n","  (6, 0.01445605149308853),\n","  (7, 0.6606521050965496),\n","  (8, 0.04579508283211987),\n","  (9, 0.09823783897857973)],\n"," [(0, 0.07047905693503888),\n","  (1, 0.04782208845414263),\n","  (2, 0.14823175319789314),\n","  (3, 0.04288939051918736),\n","  (4, 0.008276899924755455),\n","  (5, 0.20650447287016135),\n","  (6, 0.40431402056684224),\n","  (7, 0.021486497784466183),\n","  (8, 0.014798093804865815),\n","  (9, 0.03519772594264694)],\n"," [(0, 0.016498625114573787),\n","  (1, 0.016702311844383342),\n","  (2, 0.15561666157449844),\n","  (3, 0.020674203075669622),\n","  (4, 0.0498014054384357),\n","  (5, 0.3805886546491497),\n","  (6, 0.12883185660454224),\n","  (7, 0.01273042061309706),\n","  (8, 0.008656686016906001),\n","  (9, 0.20989917506874434)],\n"," [(0, 0.040691046215908085),\n","  (1, 0.011838989739542222),\n","  (2, 0.20722616855213535),\n","  (3, 0.03341225993159694),\n","  (4, 0.009032710690169253),\n","  (5, 0.38086468473208795),\n","  (6, 0.13180741910023674),\n","  (7, 0.1323335964219942),\n","  (8, 0.035604665438919575),\n","  (9, 0.01718845917740945)],\n"," [(0, 0.024208566108007448),\n","  (1, 0.014121663563004346),\n","  (2, 0.17908131595282434),\n","  (3, 0.04795158286778398),\n","  (4, 0.05276225946617008),\n","  (5, 0.20918684047175667),\n","  (6, 0.3015207945375543),\n","  (7, 0.02281191806331471),\n","  (8, 0.04127870887647424),\n","  (9, 0.10707635009310987)],\n"," [(0, 0.09596949891067538),\n","  (1, 0.02178649237472767),\n","  (2, 0.534640522875817),\n","  (3, 0.06666666666666667),\n","  (4, 0.020261437908496733),\n","  (5, 0.09117647058823529),\n","  (6, 0.11394335511982571),\n","  (7, 0.0210239651416122),\n","  (8, 0.01971677559912854),\n","  (9, 0.014814814814814815)],\n"," [(0, 0.02845100105374078),\n","  (1, 0.01838192249151153),\n","  (2, 0.2639035241774968),\n","  (3, 0.02259688561058424),\n","  (4, 0.010771572415408031),\n","  (5, 0.02856808336260391),\n","  (6, 0.08266011005737033),\n","  (7, 0.042266713499590214),\n","  (8, 0.0813722046598759),\n","  (9, 0.4210279826718183)],\n"," [(0, 0.023523942352394234),\n","  (1, 0.02212924221292422),\n","  (2, 0.2699209669920967),\n","  (3, 0.0899116689911669),\n","  (4, 0.014318921431892144),\n","  (5, 0.4382147838214784),\n","  (6, 0.046582984658298465),\n","  (7, 0.05811250581125058),\n","  (8, 0.023709902370990237),\n","  (9, 0.013575081357508135)],\n"," [(0, 0.03271089921864654),\n","  (1, 0.01748112832737386),\n","  (2, 0.41027678453185007),\n","  (3, 0.13931929545755528),\n","  (4, 0.009932459276916964),\n","  (5, 0.05548933916037611),\n","  (6, 0.03006224341146868),\n","  (7, 0.21387895642961197),\n","  (8, 0.0740299298106211),\n","  (9, 0.016818964375579393)],\n"," [(0, 0.16755586484187737),\n","  (1, 0.07747622361401067),\n","  (2, 0.00850537384984149),\n","  (3, 0.012603417613856027),\n","  (4, 0.25516121549524473),\n","  (5, 0.34686460991262663),\n","  (6, 0.0071135854016856105),\n","  (7, 0.07940926312533828),\n","  (8, 0.014072527642465011),\n","  (9, 0.031237918503054202)],\n"," [(0, 0.1769936890418818),\n","  (1, 0.010327022375215147),\n","  (2, 0.13406004972270033),\n","  (3, 0.0535475234270415),\n","  (4, 0.010231401797666857),\n","  (5, 0.21457257601835916),\n","  (6, 0.13855421686746988),\n","  (7, 0.17106521323388793),\n","  (8, 0.07793077070185504),\n","  (9, 0.012717536813922356)],\n"," [(0, 0.21541950113378686),\n","  (1, 0.05813234384662956),\n","  (2, 0.03421974850546279),\n","  (3, 0.03793032364460936),\n","  (4, 0.04225932797361369),\n","  (5, 0.13811585240156668),\n","  (6, 0.14182642754071326),\n","  (7, 0.2481962481962482),\n","  (8, 0.06534735106163678),\n","  (9, 0.01855287569573284)],\n"," [(0, 0.07250912884715702),\n","  (1, 0.030125195618153366),\n","  (2, 0.12284820031298904),\n","  (3, 0.022170057381324985),\n","  (4, 0.04929577464788732),\n","  (5, 0.41875326030255605),\n","  (6, 0.02047470005216484),\n","  (7, 0.2014866979655712),\n","  (8, 0.032733437663015126),\n","  (9, 0.029603547209181014)],\n"," [(0, 0.11906976744186047),\n","  (1, 0.020258397932816537),\n","  (2, 0.4908527131782946),\n","  (3, 0.020878552971576227),\n","  (4, 0.008268733850129198),\n","  (5, 0.020051679586563308),\n","  (6, 0.09291989664082688),\n","  (7, 0.013436692506459949),\n","  (8, 0.016640826873385015),\n","  (9, 0.19762273901808786)],\n"," [(0, 0.132858837485172),\n","  (1, 0.03845393436140767),\n","  (2, 0.4277382364570977),\n","  (3, 0.06128904705417161),\n","  (4, 0.019572953736654804),\n","  (5, 0.018979833926453145),\n","  (6, 0.19276393831553973),\n","  (7, 0.012356662712534598),\n","  (8, 0.015223408461842626),\n","  (9, 0.08076314748912614)],\n"," [(0, 0.022374280438796572),\n","  (1, 0.03356142065819486),\n","  (2, 0.10991636798088412),\n","  (3, 0.10242207016400566),\n","  (4, 0.0337786466818725),\n","  (5, 0.10372542630607148),\n","  (6, 0.08232866297382428),\n","  (7, 0.046594982078853056),\n","  (8, 0.3825350276963181),\n","  (9, 0.08276311502117956)],\n"," [(0, 0.014760147601476014),\n","  (1, 0.00963509635096351),\n","  (2, 0.054223042230422305),\n","  (3, 0.08302583025830258),\n","  (4, 0.007790077900779008),\n","  (5, 0.05576055760557606),\n","  (6, 0.6513940139401394),\n","  (7, 0.05145551455514555),\n","  (8, 0.04202542025420254),\n","  (9, 0.02993029930299303)],\n"," [(0, 0.0793835793835794),\n","  (1, 0.02421652421652422),\n","  (2, 0.12263662263662264),\n","  (3, 0.022533022533022536),\n","  (4, 0.013079513079513081),\n","  (5, 0.06138306138306139),\n","  (6, 0.5506345506345507),\n","  (7, 0.01696451696451697),\n","  (8, 0.07886557886557888),\n","  (9, 0.030303030303030307)],\n"," [(0, 0.2677702044790652),\n","  (1, 0.06837606837606837),\n","  (2, 0.1565509033863464),\n","  (3, 0.026073785567456445),\n","  (4, 0.04565617223845071),\n","  (5, 0.027155685383533478),\n","  (6, 0.08774207508384722),\n","  (7, 0.019906956615817373),\n","  (8, 0.08341447581953909),\n","  (9, 0.21735367304987552)],\n"," [(0, 0.3772136028835606),\n","  (1, 0.0611189468735308),\n","  (2, 0.03886538160162984),\n","  (3, 0.0485817270020373),\n","  (4, 0.027111737972104685),\n","  (5, 0.14887948597398526),\n","  (6, 0.1386929948283968),\n","  (7, 0.11659614480488951),\n","  (8, 0.01723867732330356),\n","  (9, 0.02570130073656167)],\n"," [(0, 0.047836257309941524),\n","  (1, 0.02327485380116959),\n","  (2, 0.40304093567251464),\n","  (3, 0.015087719298245613),\n","  (4, 0.0383625730994152),\n","  (5, 0.13216374269005848),\n","  (6, 0.07239766081871345),\n","  (7, 0.016608187134502923),\n","  (8, 0.08853801169590643),\n","  (9, 0.16269005847953216)],\n"," [(0, 0.037874848327266425),\n","  (1, 0.03839486912809846),\n","  (2, 0.3049921996879875),\n","  (3, 0.06517594037094818),\n","  (4, 0.016987346160513086),\n","  (5, 0.05078869821459525),\n","  (6, 0.14014560582423297),\n","  (7, 0.26443057722308894),\n","  (8, 0.011353787484832727),\n","  (9, 0.06985612757843647)],\n"," [(0, 0.014245014245014245),\n","  (1, 0.027329323625619923),\n","  (2, 0.5212620027434842),\n","  (3, 0.060884246069431255),\n","  (4, 0.011396011396011397),\n","  (5, 0.02859554711406563),\n","  (6, 0.030811438218845626),\n","  (7, 0.016144349477682812),\n","  (8, 0.10150891632373114),\n","  (9, 0.18782315078611375)],\n"," [(0, 0.02193486590038314),\n","  (1, 0.011781609195402299),\n","  (2, 0.054022988505747126),\n","  (3, 0.2768199233716475),\n","  (4, 0.00967432950191571),\n","  (5, 0.19118773946360154),\n","  (6, 0.1409961685823755),\n","  (7, 0.12385057471264367),\n","  (8, 0.15134099616858238),\n","  (9, 0.01839080459770115)],\n"," [(0, 0.014890016920473769),\n","  (1, 0.02019176536943034),\n","  (2, 0.5643542019176536),\n","  (3, 0.04658770445572475),\n","  (4, 0.011731528482797515),\n","  (5, 0.027636773829667224),\n","  (6, 0.017484489565707837),\n","  (7, 0.017597292724196273),\n","  (8, 0.26373378454596724),\n","  (9, 0.015792442188381273)],\n"," [(0, 0.06494341563786009),\n","  (1, 0.015432098765432098),\n","  (2, 0.1332304526748971),\n","  (3, 0.09799382716049383),\n","  (4, 0.019290123456790122),\n","  (5, 0.03935185185185185),\n","  (6, 0.5796039094650206),\n","  (7, 0.012088477366255145),\n","  (8, 0.022505144032921812),\n","  (9, 0.015560699588477367)],\n"," [(0, 0.20084790673025968),\n","  (1, 0.015456633103691927),\n","  (2, 0.19351704645822293),\n","  (3, 0.06094329623741388),\n","  (4, 0.018636283342165695),\n","  (5, 0.19607843137254902),\n","  (6, 0.2640876170287935),\n","  (7, 0.010333863275039745),\n","  (8, 0.018636283342165695),\n","  (9, 0.021462639109697933)],\n"," [(0, 0.027900146842878122),\n","  (1, 0.02373959862946647),\n","  (2, 0.258565834557024),\n","  (3, 0.037322564855604506),\n","  (4, 0.024351443954968185),\n","  (5, 0.14096916299559473),\n","  (6, 0.4287812041116006),\n","  (7, 0.024963289280469897),\n","  (8, 0.021047479197258932),\n","  (9, 0.012359275575134606)],\n"," [(0, 0.037399257448157204),\n","  (1, 0.011409942950285249),\n","  (2, 0.4524132934890881),\n","  (3, 0.08149959250203749),\n","  (4, 0.08611790274381961),\n","  (5, 0.04754142895952187),\n","  (6, 0.038576473784297745),\n","  (7, 0.024178212442271124),\n","  (8, 0.0928189803495427),\n","  (9, 0.1280449153309789)],\n"," [(0, 0.08999736078120876),\n","  (1, 0.013723937714436528),\n","  (2, 0.43481129585642647),\n","  (3, 0.09487991554499868),\n","  (4, 0.07337028239641066),\n","  (5, 0.13262074425969914),\n","  (6, 0.08617049353391396),\n","  (7, 0.0345737661652151),\n","  (8, 0.022697281604645025),\n","  (9, 0.017154922143045658)],\n"," [(0, 0.2140105433901054),\n","  (1, 0.31711273317112726),\n","  (2, 0.11820762368207621),\n","  (3, 0.04734387672343875),\n","  (4, 0.010847526358475261),\n","  (5, 0.15379156528791563),\n","  (6, 0.025851581508515808),\n","  (7, 0.027980535279805346),\n","  (8, 0.06589618815896187),\n","  (9, 0.01895782643957826)],\n"," [(0, 0.03797897592404205),\n","  (1, 0.01853735729625862),\n","  (2, 0.29275460608115744),\n","  (3, 0.08104442183791116),\n","  (4, 0.013789985305753363),\n","  (5, 0.2614445574771109),\n","  (6, 0.07550582118232169),\n","  (7, 0.010964168644738329),\n","  (8, 0.1940770882785125),\n","  (9, 0.013903017972193964)],\n"," [(0, 0.030061278760550345),\n","  (1, 0.013065094230546881),\n","  (2, 0.39669325933633937),\n","  (3, 0.05029483177245923),\n","  (4, 0.014105676956873623),\n","  (5, 0.07862180598913168),\n","  (6, 0.22580645161290316),\n","  (7, 0.038617181177014674),\n","  (8, 0.013874436351023237),\n","  (9, 0.13885998381315756)],\n"," [(0, 0.02466475095785441),\n","  (1, 0.02705938697318008),\n","  (2, 0.28220785440613033),\n","  (3, 0.04262452107279694),\n","  (4, 0.017361111111111115),\n","  (5, 0.3802681992337165),\n","  (6, 0.05974616858237548),\n","  (7, 0.031010536398467438),\n","  (8, 0.05579501915708813),\n","  (9, 0.07926245210727971)],\n"," [(0, 0.11289423827036667),\n","  (1, 0.017719826145101977),\n","  (2, 0.5911066532932131),\n","  (3, 0.059511868940153805),\n","  (4, 0.015156580853672129),\n","  (5, 0.06040343251978157),\n","  (6, 0.08324974924774324),\n","  (7, 0.01939150785690405),\n","  (8, 0.02674690738883317),\n","  (9, 0.01381923548423047)],\n"," [(0, 0.04034690799396682),\n","  (1, 0.02249874308697838),\n","  (2, 0.194947209653092),\n","  (3, 0.0405982905982906),\n","  (4, 0.01872800402212167),\n","  (5, 0.4110105580693816),\n","  (6, 0.18665158371040724),\n","  (7, 0.020236299648064355),\n","  (8, 0.05115635997988939),\n","  (9, 0.013826043237807943)],\n"," [(0, 0.03729878288182176),\n","  (1, 0.03638267242507526),\n","  (2, 0.12838633686690226),\n","  (3, 0.0362517995026829),\n","  (4, 0.03140950137416569),\n","  (5, 0.38764559612616156),\n","  (6, 0.03926187671770711),\n","  (7, 0.0399162413296689),\n","  (8, 0.2516686297605026),\n","  (9, 0.011778563015312134)],\n"," [(0, 0.013814274750575594),\n","  (1, 0.00673659077342884),\n","  (2, 0.08689349364713908),\n","  (3, 0.06182314317387226),\n","  (4, 0.006992410676217277),\n","  (5, 0.22674170717148462),\n","  (6, 0.267758164918564),\n","  (7, 0.02080668542679287),\n","  (8, 0.06020295045621216),\n","  (9, 0.2482305790057133)],\n"," [(0, 0.15978835978835979),\n","  (1, 0.026102292768959437),\n","  (2, 0.26754850088183424),\n","  (3, 0.21604938271604937),\n","  (4, 0.0164021164021164),\n","  (5, 0.04497354497354497),\n","  (6, 0.16684303350970017),\n","  (7, 0.025396825396825397),\n","  (8, 0.026455026455026454),\n","  (9, 0.05044091710758378)],\n"," [(0, 0.18841345307272406),\n","  (1, 0.011357633386159536),\n","  (2, 0.2355168163409051),\n","  (3, 0.4479661912308505),\n","  (4, 0.01734460292304983),\n","  (5, 0.01716851558372953),\n","  (6, 0.05291424546575101),\n","  (7, 0.009948934671597113),\n","  (8, 0.010213065680577567),\n","  (9, 0.00915654164465575)],\n"," [(0, 0.036033229491173414),\n","  (1, 0.06344755970924196),\n","  (2, 0.034994807892004154),\n","  (3, 0.690238836967809),\n","  (4, 0.008515057113187955),\n","  (5, 0.1043613707165109),\n","  (6, 0.017341640706126687),\n","  (7, 0.018276220145379024),\n","  (8, 0.0161993769470405),\n","  (9, 0.01059190031152648)],\n"," [(0, 0.14770138051199572),\n","  (1, 0.014609301702184694),\n","  (2, 0.02613590671491757),\n","  (3, 0.5690926149309744),\n","  (4, 0.03511593620158156),\n","  (5, 0.07813965956306125),\n","  (6, 0.07304650851092347),\n","  (7, 0.023455300898002948),\n","  (8, 0.018496180136710897),\n","  (9, 0.0142072108296475)],\n"," [(0, 0.14568925772245275),\n","  (1, 0.018211157215306594),\n","  (2, 0.3808206546795758),\n","  (3, 0.11606731212540342),\n","  (4, 0.01751959428307976),\n","  (5, 0.015560165975103735),\n","  (6, 0.04495159059474412),\n","  (7, 0.017289073305670817),\n","  (8, 0.2281005071461503),\n","  (9, 0.015790686952512678)],\n"," [(0, 0.1734045258072718),\n","  (1, 0.01906941266209001),\n","  (2, 0.016018306636155607),\n","  (3, 0.45156369183829137),\n","  (4, 0.017289600813628275),\n","  (5, 0.1947622679888126),\n","  (6, 0.01665395372489194),\n","  (7, 0.019450800915331808),\n","  (8, 0.013729977116704805),\n","  (9, 0.07805746249682176)],\n"," [(0, 0.2982217691697814),\n","  (1, 0.0345452486125269),\n","  (2, 0.14395741307056292),\n","  (3, 0.2745497791369351),\n","  (4, 0.008947785706195492),\n","  (5, 0.03624419526560199),\n","  (6, 0.10080416808245554),\n","  (7, 0.06999660210669385),\n","  (8, 0.016876203420545927),\n","  (9, 0.015856835428700873)],\n"," [(0, 0.04803402842618529),\n","  (1, 0.07770515613652869),\n","  (2, 0.023135179997925095),\n","  (3, 0.48303765950824773),\n","  (4, 0.008922087353459902),\n","  (5, 0.24380122419338104),\n","  (6, 0.01971158833903932),\n","  (7, 0.07075422761697271),\n","  (8, 0.015873015873015872),\n","  (9, 0.00902583255524432)],\n"," [(0, 0.08876404494382024),\n","  (1, 0.01585518102372035),\n","  (2, 0.2566791510611736),\n","  (3, 0.3086142322097379),\n","  (4, 0.02084893882646692),\n","  (5, 0.10699126092384521),\n","  (6, 0.17340823970037456),\n","  (7, 0.00923845193508115),\n","  (8, 0.010486891385767793),\n","  (9, 0.009113607990012486)],\n"," [(0, 0.06555933056499308),\n","  (1, 0.035988423304391594),\n","  (2, 0.030829243739776017),\n","  (3, 0.5994714986787467),\n","  (4, 0.040266767333585),\n","  (5, 0.13627784069460175),\n","  (6, 0.02642506606266516),\n","  (7, 0.02617339876683025),\n","  (8, 0.02164338744180194),\n","  (9, 0.01736504341260853)],\n"," [(0, 0.3052937754508435),\n","  (1, 0.04409540430482839),\n","  (2, 0.2136125654450262),\n","  (3, 0.18289703315881325),\n","  (4, 0.010820244328097731),\n","  (5, 0.1733566026759744),\n","  (6, 0.01861547411285631),\n","  (7, 0.024316463059918556),\n","  (8, 0.015590459569517162),\n","  (9, 0.01140197789412449)],\n"," [(0, 0.014860742052947545),\n","  (1, 0.01308926286782797),\n","  (2, 0.3163074500541285),\n","  (3, 0.350556047633107),\n","  (4, 0.010235213069579766),\n","  (5, 0.0241118000196831),\n","  (6, 0.22428894793819507),\n","  (7, 0.01564806613522291),\n","  (8, 0.01210510776498376),\n","  (9, 0.01879736246432438)],\n"," [(0, 0.37490465293668956),\n","  (1, 0.022629036359013477),\n","  (2, 0.13234172387490464),\n","  (3, 0.017543859649122806),\n","  (4, 0.10806000508517671),\n","  (5, 0.12420544113907958),\n","  (6, 0.15026697177726925),\n","  (7, 0.019832189168573607),\n","  (8, 0.010297482837528604),\n","  (9, 0.03991863717264175)],\n"," [(0, 0.034024988844265955),\n","  (1, 0.011713520749665328),\n","  (2, 0.23360107095046853),\n","  (3, 0.21999107541276217),\n","  (4, 0.01494868362338242),\n","  (5, 0.0107095046854083),\n","  (6, 0.3814145470771977),\n","  (7, 0.025658188308790717),\n","  (8, 0.035809906291834004),\n","  (9, 0.0321285140562249)],\n"," [(0, 0.10785226567349472),\n","  (1, 0.053538175046554934),\n","  (2, 0.0361576660459342),\n","  (3, 0.3947858472998138),\n","  (4, 0.027312228429546864),\n","  (5, 0.2839851024208566),\n","  (6, 0.023743016759776536),\n","  (7, 0.035381750465549346),\n","  (8, 0.018156424581005588),\n","  (9, 0.019087523277467412)],\n"," [(0, 0.2970126799914034),\n","  (1, 0.03169997850848914),\n","  (2, 0.15387921770900495),\n","  (3, 0.27724049000644746),\n","  (4, 0.047388781431334626),\n","  (5, 0.039759295078444015),\n","  (6, 0.035890823124865676),\n","  (7, 0.039651837524177946),\n","  (8, 0.029120997206103588),\n","  (9, 0.048355899419729204)],\n"," [(0, 0.6896501244327332),\n","  (1, 0.025179329527155618),\n","  (2, 0.027082418386766217),\n","  (3, 0.02327624066754502),\n","  (4, 0.10715854194115065),\n","  (5, 0.015078319426145514),\n","  (6, 0.046406089884350764),\n","  (7, 0.029424681598594647),\n","  (8, 0.022105109061630806),\n","  (9, 0.014639145073927684)],\n"," [(0, 0.3333333333333333),\n","  (1, 0.06020558002936858),\n","  (2, 0.01595692608908468),\n","  (3, 0.33245227606461086),\n","  (4, 0.02956436612824278),\n","  (5, 0.0926089084679393),\n","  (6, 0.013705335291238374),\n","  (7, 0.049437102300538424),\n","  (8, 0.011062163485070975),\n","  (9, 0.06167400881057269)],\n"," [(0, 0.2240650406504065),\n","  (1, 0.023631436314363145),\n","  (2, 0.018211382113821138),\n","  (3, 0.04791327913279133),\n","  (4, 0.5506775067750678),\n","  (5, 0.04097560975609756),\n","  (6, 0.035555555555555556),\n","  (7, 0.026449864498644986),\n","  (8, 0.023956639566395665),\n","  (9, 0.00856368563685637)],\n"," [(0, 0.4329125759700795),\n","  (1, 0.015895278167367927),\n","  (2, 0.014960261804581581),\n","  (3, 0.021505376344086023),\n","  (4, 0.42200405173757205),\n","  (5, 0.015583606046439146),\n","  (6, 0.022752064827801153),\n","  (7, 0.02851799906498364),\n","  (8, 0.011843540595293751),\n","  (9, 0.014025245441795231)],\n"," [(0, 0.4968817614865725),\n","  (1, 0.02303678248695431),\n","  (2, 0.25035000636375204),\n","  (3, 0.04110983836069747),\n","  (4, 0.011200203640066183),\n","  (5, 0.02800050910016546),\n","  (6, 0.08998345424462263),\n","  (7, 0.03449153620974927),\n","  (8, 0.012472954053710067),\n","  (9, 0.012472954053710067)],\n"," [(0, 0.31752873563218387),\n","  (1, 0.08153735632183906),\n","  (2, 0.03795498084291187),\n","  (3, 0.05806992337164749),\n","  (4, 0.2703544061302681),\n","  (5, 0.0725574712643678),\n","  (6, 0.05112547892720305),\n","  (7, 0.04657567049808428),\n","  (8, 0.043103448275862065),\n","  (9, 0.02119252873563218)],\n"," [(0, 0.6110044785668586),\n","  (1, 0.07357645553422905),\n","  (2, 0.013009170398805716),\n","  (3, 0.18415440392407764),\n","  (4, 0.03508210705907443),\n","  (5, 0.02217956920452122),\n","  (6, 0.01652804435913841),\n","  (7, 0.023245894647046277),\n","  (8, 0.009490296438473022),\n","  (9, 0.011729579867775645)],\n"," [(0, 0.12996755879967561),\n","  (1, 0.01997161394971614),\n","  (2, 0.09286293592862938),\n","  (3, 0.06194241686942418),\n","  (4, 0.037206001622060024),\n","  (5, 0.3419505271695053),\n","  (6, 0.02088402270884023),\n","  (7, 0.17862935928629362),\n","  (8, 0.044099756690997576),\n","  (9, 0.07248580697485808)],\n"," [(0, 0.022149202872094438),\n","  (1, 0.12717536813922356),\n","  (2, 0.009370816599732263),\n","  (3, 0.02117561153705732),\n","  (4, 0.704758427649994),\n","  (5, 0.01095290251916758),\n","  (6, 0.01764634294754777),\n","  (7, 0.06133625410733844),\n","  (8, 0.010101010101010102),\n","  (9, 0.01533406352683461)],\n"," [(0, 0.028791926387652125),\n","  (1, 0.15335905807855943),\n","  (2, 0.014346492529929753),\n","  (3, 0.0338379341050757),\n","  (4, 0.673889383595528),\n","  (5, 0.019293558919560704),\n","  (6, 0.026021569209458795),\n","  (7, 0.014148609874344515),\n","  (8, 0.010289898090432376),\n","  (9, 0.026021569209458795)],\n"," [(0, 0.023167848699763592),\n","  (1, 0.019858156028368795),\n","  (2, 0.018912529550827423),\n","  (3, 0.010023640661938534),\n","  (4, 0.7332387706855792),\n","  (5, 0.06969267139479905),\n","  (6, 0.03404255319148936),\n","  (7, 0.019196217494089833),\n","  (8, 0.01872340425531915),\n","  (9, 0.05314420803782506)],\n"," [(0, 0.02702702702702703),\n","  (1, 0.10066066066066066),\n","  (2, 0.02990990990990991),\n","  (3, 0.015255255255255255),\n","  (4, 0.67003003003003),\n","  (5, 0.03927927927927928),\n","  (6, 0.017897897897897898),\n","  (7, 0.07099099099099099),\n","  (8, 0.014894894894894895),\n","  (9, 0.014054054054054054)],\n"," [(0, 0.032416293452761785),\n","  (1, 0.026124973341863936),\n","  (2, 0.04638515674984005),\n","  (3, 0.01652804435913841),\n","  (4, 0.5944764342077202),\n","  (5, 0.016101514182128386),\n","  (6, 0.020473448496481125),\n","  (7, 0.01108978460226061),\n","  (8, 0.012689272766048199),\n","  (9, 0.22371507784175731)],\n"," [(0, 0.06463241920947599),\n","  (1, 0.10802111497360628),\n","  (2, 0.030513711857860178),\n","  (3, 0.065919917600103),\n","  (4, 0.18205227243465946),\n","  (5, 0.019441225698467877),\n","  (6, 0.16183854770181538),\n","  (7, 0.018411226985966266),\n","  (8, 0.015063731170336037),\n","  (9, 0.3341058323677095)],\n"," [(0, 0.01849551414768806),\n","  (1, 0.13526570048309178),\n","  (2, 0.020979986197377502),\n","  (3, 0.009937888198757764),\n","  (4, 0.4692891649413389),\n","  (5, 0.062663906142167),\n","  (6, 0.015044858523119393),\n","  (7, 0.03395445134575569),\n","  (8, 0.013388543823326432),\n","  (9, 0.2209799861973775)],\n"," [(0, 0.17767170511800584),\n","  (1, 0.14465658976398832),\n","  (2, 0.06815168390347388),\n","  (3, 0.0196234420578096),\n","  (4, 0.38782816229116945),\n","  (5, 0.045876425351365685),\n","  (6, 0.06178732431715725),\n","  (7, 0.05422964730840626),\n","  (8, 0.021479713603818614),\n","  (9, 0.01869530628480509)],\n"," [(0, 0.024621212121212127),\n","  (1, 0.1043771043771044),\n","  (2, 0.04997895622895624),\n","  (3, 0.027356902356902364),\n","  (4, 0.4300294612794614),\n","  (5, 0.17403198653198657),\n","  (6, 0.01830808080808081),\n","  (7, 0.021780303030303035),\n","  (8, 0.04198232323232324),\n","  (9, 0.10753367003367006)],\n"," [(0, 0.01557285873192436),\n","  (1, 0.3686812507724632),\n","  (2, 0.014954888147324187),\n","  (3, 0.011370658756643184),\n","  (4, 0.24026696329254726),\n","  (5, 0.012730194042763564),\n","  (6, 0.008775182301322458),\n","  (7, 0.10196514645902854),\n","  (8, 0.014831294030404153),\n","  (9, 0.21085156346557904)],\n"," [(0, 0.021912587828986547),\n","  (1, 0.05085149458139813),\n","  (2, 0.09169941645825892),\n","  (3, 0.05097058473264262),\n","  (4, 0.2991544599261642),\n","  (5, 0.015124449208050495),\n","  (6, 0.3035607955222104),\n","  (7, 0.020364415862808148),\n","  (8, 0.11730379897582471),\n","  (9, 0.029057996903656073)],\n"," [(0, 0.02855064557192217),\n","  (1, 0.025277323149663574),\n","  (2, 0.047826877614111654),\n","  (3, 0.038461538461538464),\n","  (4, 0.3042371340243681),\n","  (5, 0.06482996908528824),\n","  (6, 0.05555555555555555),\n","  (7, 0.35551918530641935),\n","  (8, 0.012184033460629205),\n","  (9, 0.06755773777050372)],\n"," [(0, 0.024956597222222224),\n","  (1, 0.083984375),\n","  (2, 0.047417534722222224),\n","  (3, 0.047526041666666664),\n","  (4, 0.3274739583333333),\n","  (5, 0.1812065972222222),\n","  (6, 0.10286458333333333),\n","  (7, 0.05544704861111111),\n","  (8, 0.031141493055555556),\n","  (9, 0.09798177083333333)],\n"," [(0, 0.10821815064823744),\n","  (1, 0.040715739847851706),\n","  (2, 0.023572270438229938),\n","  (3, 0.05625200900032144),\n","  (4, 0.19222115075538412),\n","  (5, 0.07682417229186757),\n","  (6, 0.05453766205935926),\n","  (7, 0.25940212150433944),\n","  (8, 0.018107789563912995),\n","  (9, 0.1701489338904961)],\n"," [(0, 0.015534842432312472),\n","  (1, 0.2994525817428614),\n","  (2, 0.018937712679390442),\n","  (3, 0.009172954579079745),\n","  (4, 0.550229323864477),\n","  (5, 0.025447551412930907),\n","  (6, 0.019677467080929132),\n","  (7, 0.02248853380677615),\n","  (8, 0.01227992306554224),\n","  (9, 0.026779109335700546)],\n"," [(0, 0.13371537726838587),\n","  (1, 0.05422901411440093),\n","  (2, 0.061445399554282076),\n","  (3, 0.024938979093706886),\n","  (4, 0.5983232516183805),\n","  (5, 0.034383954154727794),\n","  (6, 0.026106335561922954),\n","  (7, 0.017298100392656265),\n","  (8, 0.010824578159821713),\n","  (9, 0.038735010081714955)],\n"," [(0, 0.015749730312837114),\n","  (1, 0.01887810140237325),\n","  (2, 0.11585760517799354),\n","  (3, 0.028155339805825245),\n","  (4, 0.013484358144552322),\n","  (5, 0.03473570658036678),\n","  (6, 0.010679611650485438),\n","  (7, 0.7234088457389429),\n","  (8, 0.022437971952535062),\n","  (9, 0.01661272923408846)],\n"," [(0, 0.21856287425149704),\n","  (1, 0.030161898425371484),\n","  (2, 0.17254380128631627),\n","  (3, 0.01874029718341096),\n","  (4, 0.02295409181636727),\n","  (5, 0.0919272565979153),\n","  (6, 0.038811266356176544),\n","  (7, 0.3184741627855401),\n","  (8, 0.05089820359281438),\n","  (9, 0.03692614770459082)],\n"," [(0, 0.044722719141323794),\n","  (1, 0.015006956867421983),\n","  (2, 0.023255813953488372),\n","  (3, 0.02623732856290996),\n","  (4, 0.01282051282051282),\n","  (5, 0.020969986086265158),\n","  (6, 0.04283442655535679),\n","  (7, 0.719837010534685),\n","  (8, 0.011926058437686345),\n","  (9, 0.08238918704034984)],\n"," [(0, 0.030924128011506652),\n","  (1, 0.015342203044468417),\n","  (2, 0.16708618003116385),\n","  (3, 0.01881817092173079),\n","  (4, 0.03428023492748412),\n","  (5, 0.01737983938631188),\n","  (6, 0.01881817092173079),\n","  (7, 0.6034999400695193),\n","  (8, 0.011866235167206042),\n","  (9, 0.08198489751887811)],\n"," [(0, 0.02048131080389145),\n","  (1, 0.04467485919098822),\n","  (2, 0.06387608806963646),\n","  (3, 0.013568868407578085),\n","  (4, 0.019329237071172557),\n","  (5, 0.17319508448540707),\n","  (6, 0.017665130568356373),\n","  (7, 0.6099590373783922),\n","  (8, 0.019073220686123913),\n","  (9, 0.01817716333845366)],\n"," [(0, 0.02184415038857383),\n","  (1, 0.0330812854442344),\n","  (2, 0.3027725267800882),\n","  (3, 0.0330812854442344),\n","  (4, 0.012287334593572778),\n","  (5, 0.03371140516698173),\n","  (6, 0.13190506196177273),\n","  (7, 0.3837429111531191),\n","  (8, 0.026990128124343624),\n","  (9, 0.020583910943079185)],\n"," [(0, 0.07407407407407407),\n","  (1, 0.011199822577068086),\n","  (2, 0.019738301175426923),\n","  (3, 0.13628298957640275),\n","  (4, 0.007873142603681526),\n","  (5, 0.014415613218008428),\n","  (6, 0.03881126635617654),\n","  (7, 0.6807496118873364),\n","  (8, 0.008871146595697495),\n","  (9, 0.007984031936127744)],\n"," [(0, 0.1422981145499822),\n","  (1, 0.01007944978062374),\n","  (2, 0.014466974979248192),\n","  (3, 0.3055852009960868),\n","  (4, 0.015415629076248072),\n","  (5, 0.01577137436262303),\n","  (6, 0.05881655401399265),\n","  (7, 0.41788212972844774),\n","  (8, 0.009012213921498874),\n","  (9, 0.010672358591248666)],\n"," [(0, 0.01527001862197393),\n","  (1, 0.02991930477963998),\n","  (2, 0.16561142147734328),\n","  (3, 0.04481688392302918),\n","  (4, 0.014525139664804471),\n","  (5, 0.023960273122284298),\n","  (6, 0.025574177529484796),\n","  (7, 0.5888268156424582),\n","  (8, 0.02358783364369957),\n","  (9, 0.06790813159528244)],\n"," [(0, 0.05707403916640486),\n","  (1, 0.022201277620693266),\n","  (2, 0.052675672845324116),\n","  (3, 0.011624253848570531),\n","  (4, 0.01413760603204524),\n","  (5, 0.09089957063566866),\n","  (6, 0.24913603518693056),\n","  (7, 0.41606450937270917),\n","  (8, 0.03319719342339512),\n","  (9, 0.052989841868258454)],\n"," [(0, 0.07240213717151892),\n","  (1, 0.017882455566459492),\n","  (2, 0.05735470504852252),\n","  (3, 0.02300730563733508),\n","  (4, 0.07948969578017664),\n","  (5, 0.05342928797295824),\n","  (6, 0.06204339766655763),\n","  (7, 0.3035655871769709),\n","  (8, 0.21382619125504307),\n","  (9, 0.11699923672445753)],\n"," [(0, 0.02863049095607235),\n","  (1, 0.04361757105943152),\n","  (2, 0.1075968992248062),\n","  (3, 0.016537467700258397),\n","  (4, 0.013540051679586563),\n","  (5, 0.09219638242894057),\n","  (6, 0.028940568475452195),\n","  (7, 0.05136950904392765),\n","  (8, 0.5029457364341086),\n","  (9, 0.11462532299741603)],\n"," [(0, 0.012312713247292686),\n","  (1, 0.09909508974929536),\n","  (2, 0.10933095979824951),\n","  (3, 0.024477080551846907),\n","  (4, 0.021065123868862185),\n","  (5, 0.07506304702566385),\n","  (6, 0.024922118380062305),\n","  (7, 0.029965880433170153),\n","  (8, 0.28378578845868563),\n","  (9, 0.31998219848687137)],\n"," [(0, 0.01096727795756922),\n","  (1, 0.03460985257101762),\n","  (2, 0.0593311758360302),\n","  (3, 0.01438331535418914),\n","  (4, 0.015551959726717008),\n","  (5, 0.1272024451636102),\n","  (6, 0.01393383674937073),\n","  (7, 0.030564545127651922),\n","  (8, 0.33162531463502337),\n","  (9, 0.3618302768788206)],\n"," [(0, 0.031435134808366585),\n","  (1, 0.03808487486398259),\n","  (2, 0.1755531374682626),\n","  (3, 0.04546004110748398),\n","  (4, 0.13239027928908234),\n","  (5, 0.11159472856970136),\n","  (6, 0.03675492685285939),\n","  (7, 0.041349292709466814),\n","  (8, 0.3586023455446742),\n","  (9, 0.028775238786120177)],\n"," [(0, 0.02272523129435013),\n","  (1, 0.13033324351028475),\n","  (2, 0.14398634689661366),\n","  (3, 0.012395580706009162),\n","  (4, 0.09925446869666757),\n","  (5, 0.010958411928500854),\n","  (6, 0.010329650588340969),\n","  (7, 0.039791610527261295),\n","  (8, 0.12206952303961197),\n","  (9, 0.4081559328123597)],\n"," [(0, 0.03452991452991453),\n","  (1, 0.03339031339031339),\n","  (2, 0.062336182336182336),\n","  (3, 0.018803418803418803),\n","  (4, 0.01150997150997151),\n","  (5, 0.04752136752136752),\n","  (6, 0.01584045584045584),\n","  (7, 0.03111111111111111),\n","  (8, 0.6348717948717949),\n","  (9, 0.11008547008547008)],\n"," [(0, 0.01878064575816449),\n","  (1, 0.24044777500231287),\n","  (2, 0.04375982977148672),\n","  (3, 0.034508280136922934),\n","  (4, 0.007678786196687945),\n","  (5, 0.022481265611990008),\n","  (6, 0.011656952539550375),\n","  (7, 0.035340919604033676),\n","  (8, 0.2561754093810713),\n","  (9, 0.3291701359977796)],\n"," [(0, 0.016567621105708627),\n","  (1, 0.034936070592472536),\n","  (2, 0.19511975508734017),\n","  (3, 0.07653520619484963),\n","  (4, 0.026922384296776517),\n","  (5, 0.21555915721231766),\n","  (6, 0.03439582207815595),\n","  (7, 0.008914100486223663),\n","  (8, 0.3788042499549793),\n","  (9, 0.012245632991175941)],\n"," [(0, 0.14738491202699447),\n","  (1, 0.01735357917570499),\n","  (2, 0.2675343456254519),\n","  (3, 0.033622559652928416),\n","  (4, 0.030850807423475537),\n","  (5, 0.12051096649795132),\n","  (6, 0.03603277898288744),\n","  (7, 0.22631959508315258),\n","  (8, 0.09472161966738973),\n","  (9, 0.02566883586406363)],\n"," [(0, 0.0694689398393102),\n","  (1, 0.012639623750734859),\n","  (2, 0.15755437977660197),\n","  (3, 0.08602782676856749),\n","  (4, 0.02312365275328238),\n","  (5, 0.2452478933960415),\n","  (6, 0.04801097393689985),\n","  (7, 0.02998236331569664),\n","  (8, 0.2844405251812659),\n","  (9, 0.043503821281599055)],\n"," [(0, 0.021812678062678063),\n","  (1, 0.030448717948717948),\n","  (2, 0.1014957264957265),\n","  (3, 0.02546296296296296),\n","  (4, 0.04291310541310541),\n","  (5, 0.11280270655270655),\n","  (6, 0.017984330484330485),\n","  (7, 0.426994301994302),\n","  (8, 0.06258903133903133),\n","  (9, 0.15749643874643873)],\n"," [(0, 0.044786032830439325),\n","  (1, 0.06366827972293387),\n","  (2, 0.02780149919347187),\n","  (3, 0.03691052282000191),\n","  (4, 0.021633931113008827),\n","  (5, 0.036246323180567426),\n","  (6, 0.22364550716386758),\n","  (7, 0.18369864313502232),\n","  (8, 0.3457633551570358),\n","  (9, 0.015845905683651205)],\n"," [(0, 0.04244196349459507),\n","  (1, 0.07930178982810562),\n","  (2, 0.013822434875066455),\n","  (3, 0.04093567251461988),\n","  (4, 0.04961899698741804),\n","  (5, 0.12103491050859472),\n","  (6, 0.13379408116250222),\n","  (7, 0.16950203792309054),\n","  (8, 0.2571327308169413),\n","  (9, 0.0924153818890661)],\n"," [(0, 0.02613621732972834),\n","  (1, 0.44998068752414067),\n","  (2, 0.13531608085489896),\n","  (3, 0.03952620059224927),\n","  (4, 0.01969872537659328),\n","  (5, 0.13235483455645683),\n","  (6, 0.02484871893910133),\n","  (7, 0.04428994463756921),\n","  (8, 0.09102613621732975),\n","  (9, 0.03682245397193254)],\n"," [(0, 0.023126248291811206),\n","  (1, 0.47156522653211397),\n","  (2, 0.09040260695889835),\n","  (3, 0.01061705035214969),\n","  (4, 0.015557657941763902),\n","  (5, 0.034584253127299486),\n","  (6, 0.03321770209187428),\n","  (7, 0.04541154210028382),\n","  (8, 0.2634289919058131),\n","  (9, 0.012088720697992222)],\n"," [(0, 0.2241183162684869),\n","  (1, 0.420838073568449),\n","  (2, 0.07167235494880546),\n","  (3, 0.02104664391353811),\n","  (4, 0.09139173302995829),\n","  (5, 0.07318923018581722),\n","  (6, 0.01943496397421312),\n","  (7, 0.04294653014789534),\n","  (8, 0.018960940462646948),\n","  (9, 0.01640121350018961)],\n"," [(0, 0.03876898481215027),\n","  (1, 0.6069810818012256),\n","  (2, 0.05608846256328269),\n","  (3, 0.023581135091926452),\n","  (4, 0.023581135091926452),\n","  (5, 0.07447375432986943),\n","  (6, 0.01438848920863309),\n","  (7, 0.026512123634425785),\n","  (8, 0.12150279776179054),\n","  (9, 0.014122035704769515)],\n"," [(0, 0.17107843137254902),\n","  (1, 0.37777777777777777),\n","  (2, 0.09313725490196079),\n","  (3, 0.09052287581699346),\n","  (4, 0.018300653594771243),\n","  (5, 0.041666666666666664),\n","  (6, 0.07777777777777778),\n","  (7, 0.03643790849673203),\n","  (8, 0.011928104575163398),\n","  (9, 0.08137254901960785)],\n"," [(0, 0.012589413447782546),\n","  (1, 0.47076776347162613),\n","  (2, 0.061707200762994754),\n","  (3, 0.03261802575107296),\n","  (4, 0.015069146399618502),\n","  (5, 0.23328564616118264),\n","  (6, 0.01754887935145446),\n","  (7, 0.07715784453981879),\n","  (8, 0.052169766332856464),\n","  (9, 0.027086313781592753)],\n"," [(0, 0.26988382484361034),\n","  (1, 0.4022440671234237),\n","  (2, 0.022341376228775692),\n","  (3, 0.015887200873796047),\n","  (4, 0.015490020851951147),\n","  (5, 0.18965346043094033),\n","  (6, 0.0082414854532817),\n","  (7, 0.03276735180220435),\n","  (8, 0.03177440174759209),\n","  (9, 0.011716810644424586)],\n"," [(0, 0.048168667282240694),\n","  (1, 0.32163742690058483),\n","  (2, 0.09202831640504772),\n","  (3, 0.04616805170821792),\n","  (4, 0.31040320098491847),\n","  (5, 0.07294552169898431),\n","  (6, 0.02800861803631887),\n","  (7, 0.03431825176977532),\n","  (8, 0.02831640504770699),\n","  (9, 0.01800554016620499)],\n"," [(0, 0.013636902644373292),\n","  (1, 0.0990157713743626),\n","  (2, 0.1892564923514763),\n","  (3, 0.20538361200047428),\n","  (4, 0.010790940353373649),\n","  (5, 0.1566465077671054),\n","  (6, 0.07304636546899086),\n","  (7, 0.049922921854618746),\n","  (8, 0.15629076248073043),\n","  (9, 0.04600972370449424)],\n"," [(0, 0.018664333624963548),\n","  (1, 0.6570428696412949),\n","  (2, 0.012540099154272382),\n","  (3, 0.04150870030135122),\n","  (4, 0.11694371536891222),\n","  (5, 0.07076893166132012),\n","  (6, 0.022455526392534265),\n","  (7, 0.0242053076698746),\n","  (8, 0.012734519296199086),\n","  (9, 0.02313599688927773)],\n"," [(0, 0.020606060606060603),\n","  (1, 0.38612794612794604),\n","  (2, 0.012929292929292926),\n","  (3, 0.02141414141414141),\n","  (4, 0.38585858585858573),\n","  (5, 0.05535353535353534),\n","  (6, 0.023164983164983163),\n","  (7, 0.04888888888888888),\n","  (8, 0.021818181818181816),\n","  (9, 0.023838383838383832)],\n"," [(0, 0.03370695256660169),\n","  (1, 0.023473034437946717),\n","  (2, 0.11102988953866147),\n","  (3, 0.022498375568551006),\n","  (4, 0.0099090318388564),\n","  (5, 0.0687946718648473),\n","  (6, 0.07066276803118908),\n","  (7, 0.0489766081871345),\n","  (8, 0.4841617933723197),\n","  (9, 0.12678687459389215)],\n"," [(0, 0.01625025391021735),\n","  (1, 0.03016453382084095),\n","  (2, 0.07708714198659354),\n","  (3, 0.019601868779199675),\n","  (4, 0.036461507211050175),\n","  (5, 0.044789762340036565),\n","  (6, 0.017062766605728214),\n","  (7, 0.05393053016453382),\n","  (8, 0.20840950639853748),\n","  (9, 0.49624212878326224)],\n"," [(0, 0.03862495171881035),\n","  (1, 0.07068366164542295),\n","  (2, 0.021243723445345693),\n","  (3, 0.01892622634221707),\n","  (4, 0.255053431183211),\n","  (5, 0.01532123084846144),\n","  (6, 0.03617870477661903),\n","  (7, 0.012488734389082014),\n","  (8, 0.02420496974378782),\n","  (9, 0.5072743659070427)],\n"," [(0, 0.020888888888888887),\n","  (1, 0.008333333333333333),\n","  (2, 0.14377777777777778),\n","  (3, 0.07766666666666666),\n","  (4, 0.01),\n","  (5, 0.059666666666666666),\n","  (6, 0.23055555555555557),\n","  (7, 0.02311111111111111),\n","  (8, 0.13833333333333334),\n","  (9, 0.2876666666666667)],\n"," [(0, 0.10707502374169041),\n","  (1, 0.025522317188983856),\n","  (2, 0.016144349477682812),\n","  (3, 0.0584045584045584),\n","  (4, 0.25047483380816715),\n","  (5, 0.008665716999050332),\n","  (6, 0.028252611585944918),\n","  (7, 0.014245014245014245),\n","  (8, 0.009852801519468187),\n","  (9, 0.4813627730294397)],\n"," [(0, 0.11254789272030652),\n","  (1, 0.011374521072796935),\n","  (2, 0.05399904214559387),\n","  (3, 0.11613984674329501),\n","  (4, 0.010177203065134099),\n","  (5, 0.016283524904214558),\n","  (6, 0.10572318007662836),\n","  (7, 0.020114942528735632),\n","  (8, 0.010656130268199233),\n","  (9, 0.5429837164750958)],\n"," [(0, 0.18646723646723648),\n","  (1, 0.025213675213675214),\n","  (2, 0.1294871794871795),\n","  (3, 0.07022792022792022),\n","  (4, 0.009544159544159544),\n","  (5, 0.017378917378917377),\n","  (6, 0.07792022792022792),\n","  (7, 0.014387464387464388),\n","  (8, 0.017806267806267807),\n","  (9, 0.4515669515669516)],\n"," [(0, 0.013309134906231096),\n","  (1, 0.062310949788263764),\n","  (2, 0.014115749142972374),\n","  (3, 0.034180278281911676),\n","  (4, 0.2656785642266586),\n","  (5, 0.1038515829804396),\n","  (6, 0.022887678967533777),\n","  (7, 0.019257914902198022),\n","  (8, 0.015023190159306312),\n","  (9, 0.4493849566444848)],\n"," [(0, 0.07371691846137363),\n","  (1, 0.051537554912675455),\n","  (2, 0.03717989928211722),\n","  (3, 0.08871745419479267),\n","  (4, 0.0600021429336762),\n","  (5, 0.03567984570877531),\n","  (6, 0.3846565948783885),\n","  (7, 0.01885781635058395),\n","  (8, 0.011250401800064288),\n","  (9, 0.23840137147755278)],\n"," [(0, 0.15995705850778313),\n","  (1, 0.2020397208803006),\n","  (2, 0.07053140096618357),\n","  (3, 0.10585077831454644),\n","  (4, 0.013633923778851314),\n","  (5, 0.240150295222759),\n","  (6, 0.02877079978529254),\n","  (7, 0.0632313472893183),\n","  (8, 0.05603864734299517),\n","  (9, 0.05979602791196994)],\n"," [(0, 0.07063956186763204),\n","  (1, 0.030168012624152973),\n","  (2, 0.1010860484544695),\n","  (3, 0.011510257124292211),\n","  (4, 0.012809802283486493),\n","  (5, 0.0371298616912652),\n","  (6, 0.051332033788174136),\n","  (7, 0.07658033973823447),\n","  (8, 0.5843311983662861),\n","  (9, 0.024412884062006868)],\n"," [(0, 0.01893719806763285),\n","  (1, 0.06357487922705314),\n","  (2, 0.02714975845410628),\n","  (3, 0.028985507246376812),\n","  (4, 0.03729468599033817),\n","  (5, 0.07140096618357487),\n","  (6, 0.08106280193236715),\n","  (7, 0.015845410628019325),\n","  (8, 0.338743961352657),\n","  (9, 0.3170048309178744)],\n"," [(0, 0.01459245361684386),\n","  (1, 0.02689180737961226),\n","  (2, 0.12924744632061705),\n","  (3, 0.1050656660412758),\n","  (4, 0.013550135501355014),\n","  (5, 0.02074213049822806),\n","  (6, 0.03960808838857619),\n","  (7, 0.012716281008963937),\n","  (8, 0.6212215968313529),\n","  (9, 0.0163643944131749)],\n"," [(0, 0.04682890855457227),\n","  (1, 0.11246312684365782),\n","  (2, 0.02667158308751229),\n","  (3, 0.013028515240904622),\n","  (4, 0.056293018682399214),\n","  (5, 0.025811209439528023),\n","  (6, 0.02077187807276303),\n","  (7, 0.372173058013766),\n","  (8, 0.24029006882989185),\n","  (9, 0.08566863323500491)],\n"," [(0, 0.02470265324794145),\n","  (1, 0.12849445969299586),\n","  (2, 0.032530242960252115),\n","  (3, 0.06739859713327236),\n","  (4, 0.015451865406119755),\n","  (5, 0.10948459896309852),\n","  (6, 0.09077970926095355),\n","  (7, 0.026634136423706416),\n","  (8, 0.4586764257395548),\n","  (9, 0.045847311172105325)],\n"," [(0, 0.018952854773750295),\n","  (1, 0.018715944089078417),\n","  (2, 0.1558872305140962),\n","  (3, 0.06124141198768064),\n","  (4, 0.022743425728500355),\n","  (5, 0.099620942904525),\n","  (6, 0.10814972755271263),\n","  (7, 0.018005212035062782),\n","  (8, 0.47879649372186683),\n","  (9, 0.01788675669272684)],\n"," [(0, 0.015916463909164638),\n","  (1, 0.27220194647201945),\n","  (2, 0.02159367396593674),\n","  (3, 0.029805352798053526),\n","  (4, 0.20508921330089214),\n","  (5, 0.11709245742092457),\n","  (6, 0.024330900243309004),\n","  (7, 0.060523114355231146),\n","  (8, 0.1030008110300081),\n","  (9, 0.15044606650446066)],\n"," [(0, 0.059763008758371976),\n","  (1, 0.04190279924437575),\n","  (2, 0.04052893697406835),\n","  (3, 0.021895929933024215),\n","  (4, 0.1374720934226344),\n","  (5, 0.0197492701356689),\n","  (6, 0.013395157135497167),\n","  (7, 0.4436716469173965),\n","  (8, 0.019148205392409412),\n","  (9, 0.20247295208655333)],\n"," [(0, 0.01557632398753894),\n","  (1, 0.018880392712168414),\n","  (2, 0.021995657509676202),\n","  (3, 0.0392712168413103),\n","  (4, 0.19626168224299065),\n","  (5, 0.4536014349098461),\n","  (6, 0.022656471254602097),\n","  (7, 0.06475974700273766),\n","  (8, 0.1041253658076088),\n","  (9, 0.06287170773152082)],\n"," [(0, 0.06239787581699345),\n","  (1, 0.024101307189542478),\n","  (2, 0.23468137254901955),\n","  (3, 0.11836192810457513),\n","  (4, 0.038092320261437905),\n","  (5, 0.30708741830065356),\n","  (6, 0.02532679738562091),\n","  (7, 0.030433006535947705),\n","  (8, 0.14930555555555552),\n","  (9, 0.010212418300653593)],\n"," [(0, 0.05667480667480669),\n","  (1, 0.023402523402523405),\n","  (2, 0.09564509564509567),\n","  (3, 0.015873015873015876),\n","  (4, 0.08516483516483518),\n","  (5, 0.39275539275539284),\n","  (6, 0.022792022792022797),\n","  (7, 0.09106634106634108),\n","  (8, 0.01434676434676435),\n","  (9, 0.20227920227920235)],\n"," [(0, 0.04044981537428667),\n","  (1, 0.020224907687143336),\n","  (2, 0.16163141993957703),\n","  (3, 0.0355824102047667),\n","  (4, 0.014853977844914401),\n","  (5, 0.3065626049009735),\n","  (6, 0.04472977509231286),\n","  (7, 0.016532393420610945),\n","  (8, 0.07124874118831823),\n","  (9, 0.2881839543470963)],\n"," [(0, 0.041064365336209996),\n","  (1, 0.018554476806903992),\n","  (2, 0.05084501977705861),\n","  (3, 0.016468896080546566),\n","  (4, 0.016181229773462782),\n","  (5, 0.6769507371449119),\n","  (6, 0.05393743257820928),\n","  (7, 0.009564904710535779),\n","  (8, 0.037612369651204604),\n","  (9, 0.07882056814095649)],\n"," [(0, 0.13730803974706413),\n","  (1, 0.019572417946401688),\n","  (2, 0.03382515306634548),\n","  (3, 0.13560172638763424),\n","  (4, 0.007327110308140119),\n","  (5, 0.5450165612767239),\n","  (6, 0.016862390846130684),\n","  (7, 0.017765733212887685),\n","  (8, 0.05399979925725183),\n","  (9, 0.032721067951420255)],\n"," [(0, 0.027014127529591446),\n","  (1, 0.017563955708285606),\n","  (2, 0.6485299732722413),\n","  (3, 0.03865979381443299),\n","  (4, 0.010500190912562046),\n","  (5, 0.11006109201985491),\n","  (6, 0.05956471935853379),\n","  (7, 0.05526918671248568),\n","  (8, 0.020427644138984347),\n","  (9, 0.012409316533027874)],\n"," [(0, 0.08529411764705884),\n","  (1, 0.10228758169934642),\n","  (2, 0.0224400871459695),\n","  (3, 0.038344226579520704),\n","  (4, 0.16056644880174295),\n","  (5, 0.03398692810457517),\n","  (6, 0.07549019607843138),\n","  (7, 0.02276688453159042),\n","  (8, 0.4367102396514162),\n","  (9, 0.022113289760348587)],\n"," [(0, 0.016389491443721378),\n","  (1, 0.3065798987707881),\n","  (2, 0.03386358158592432),\n","  (3, 0.01482284887924801),\n","  (4, 0.3541817305374789),\n","  (5, 0.02554832489756568),\n","  (6, 0.012894673415280791),\n","  (7, 0.01193058568329718),\n","  (8, 0.015063870812243915),\n","  (9, 0.20872499397445168)],\n"," [(0, 0.02022058823529412),\n","  (1, 0.04411764705882353),\n","  (2, 0.016339869281045753),\n","  (3, 0.0352328431372549),\n","  (4, 0.103656045751634),\n","  (5, 0.19475081699346405),\n","  (6, 0.013174019607843137),\n","  (7, 0.022263071895424837),\n","  (8, 0.38092320261437906),\n","  (9, 0.1693218954248366)],\n"," [(0, 0.08896481903315616),\n","  (1, 0.014806378132118452),\n","  (2, 0.07706909643128322),\n","  (3, 0.15021513540875728),\n","  (4, 0.013540875727663883),\n","  (5, 0.015818780055682106),\n","  (6, 0.21159200202480385),\n","  (7, 0.4006580612503164),\n","  (8, 0.013034674765882055),\n","  (9, 0.014300177170336624)],\n"," [(0, 0.30378434421980305),\n","  (1, 0.011404872991187146),\n","  (2, 0.14359771902540178),\n","  (3, 0.2179021945740453),\n","  (4, 0.012614480732676691),\n","  (5, 0.08484534301019528),\n","  (6, 0.17504752030412998),\n","  (7, 0.013996889580093314),\n","  (8, 0.0235009504060826),\n","  (9, 0.013305685156385003)],\n"," [(0, 0.5134340890008396),\n","  (1, 0.012594458438287154),\n","  (2, 0.17828155611530927),\n","  (3, 0.06031346207668626),\n","  (4, 0.01581304226140498),\n","  (5, 0.11978729359082003),\n","  (6, 0.028967254408060455),\n","  (7, 0.02155051777218024),\n","  (8, 0.02966694654352085),\n","  (9, 0.019591379792891127)],\n"," [(0, 0.06063867958378184),\n","  (1, 0.018418849419925845),\n","  (2, 0.3828489415141729),\n","  (3, 0.10022724554479129),\n","  (4, 0.013873938524099988),\n","  (5, 0.12271259418729817),\n","  (6, 0.08192799904317666),\n","  (7, 0.024398995335486187),\n","  (8, 0.18514531754574812),\n","  (9, 0.009807439301518957)],\n"," [(0, 0.02666196852243364),\n","  (1, 0.1020671834625323),\n","  (2, 0.01949729856706601),\n","  (3, 0.013859525487432463),\n","  (4, 0.7327930467465351),\n","  (5, 0.027953958186516328),\n","  (6, 0.009396288466055908),\n","  (7, 0.012802443035001174),\n","  (8, 0.012802443035001174),\n","  (9, 0.04216584449142589)],\n"," [(0, 0.06740481740481741),\n","  (1, 0.010878010878010878),\n","  (2, 0.025349650349650348),\n","  (3, 0.027777777777777776),\n","  (4, 0.09770784770784771),\n","  (5, 0.37334887334887334),\n","  (6, 0.010101010101010102),\n","  (7, 0.2631118881118881),\n","  (8, 0.08022533022533022),\n","  (9, 0.044094794094794096)],\n"," [(0, 0.040334271339643554),\n","  (1, 0.03837298541826554),\n","  (2, 0.034962053381086385),\n","  (3, 0.012194082032915493),\n","  (4, 0.10906455188880361),\n","  (5, 0.031210028140189305),\n","  (6, 0.043915749978681674),\n","  (7, 0.4636309371535772),\n","  (8, 0.029845655325317643),\n","  (9, 0.19646968534151957)],\n"," [(0, 0.12696571281258057),\n","  (1, 0.024619747357566384),\n","  (2, 0.014436710492394948),\n","  (3, 0.06767208043310131),\n","  (4, 0.03596287703016241),\n","  (5, 0.42523846352152617),\n","  (6, 0.015725702500644496),\n","  (7, 0.06071152358855375),\n","  (8, 0.20005155968032998),\n","  (9, 0.028615622583139984)],\n"," [(0, 0.09645793801391524),\n","  (1, 0.025722116803710733),\n","  (2, 0.04891418933164664),\n","  (3, 0.054606788952139995),\n","  (4, 0.5065359477124183),\n","  (5, 0.12586970271979758),\n","  (6, 0.015601939700611427),\n","  (7, 0.01686696183849884),\n","  (8, 0.0367910605102256),\n","  (9, 0.07263335441703563)],\n"," [(0, 0.029831768388106417),\n","  (1, 0.042155712050078245),\n","  (2, 0.022007042253521125),\n","  (3, 0.009976525821596244),\n","  (4, 0.1456377151799687),\n","  (5, 0.01633411580594679),\n","  (6, 0.13575899843505476),\n","  (7, 0.4180359937402191),\n","  (8, 0.012030516431924883),\n","  (9, 0.16823161189358374)],\n"," [(0, 0.07858422939068102),\n","  (1, 0.031989247311827965),\n","  (2, 0.0199820788530466),\n","  (3, 0.015501792114695342),\n","  (4, 0.16272401433691758),\n","  (5, 0.43458781362007176),\n","  (6, 0.050896057347670255),\n","  (7, 0.029390681003584232),\n","  (8, 0.1584229390681004),\n","  (9, 0.01792114695340502)],\n"," [(0, 0.04851896528615468),\n","  (1, 0.026403967296609036),\n","  (2, 0.06192199437072779),\n","  (3, 0.06982978153062594),\n","  (4, 0.5394719206540679),\n","  (5, 0.08055220479828443),\n","  (6, 0.07358262967430641),\n","  (7, 0.02211499798954564),\n","  (8, 0.06406647902425949),\n","  (9, 0.013537059375418846)],\n"," [(0, 0.07732008959002824),\n","  (1, 0.2513389814003311),\n","  (2, 0.04800856948096212),\n","  (3, 0.06719252118025124),\n","  (4, 0.4060765410458662),\n","  (5, 0.043042165741552244),\n","  (6, 0.013146362839614373),\n","  (7, 0.022397507060083748),\n","  (8, 0.013535884701528873),\n","  (9, 0.05794137695978187)],\n"," [(0, 0.05737704918032788),\n","  (1, 0.02975106253794779),\n","  (2, 0.00971463266545234),\n","  (3, 0.01851851851851852),\n","  (4, 0.6513863590366323),\n","  (5, 0.1631248735073872),\n","  (6, 0.014268366727383122),\n","  (7, 0.026006881198138033),\n","  (8, 0.01315523173446671),\n","  (9, 0.01669702489374621)],\n"," [(0, 0.05002491280518186),\n","  (1, 0.11798704534130543),\n","  (2, 0.03139013452914798),\n","  (3, 0.019033383158943696),\n","  (4, 0.3552566018933732),\n","  (5, 0.2844045839561535),\n","  (6, 0.01574489287493772),\n","  (7, 0.0408570004982561),\n","  (8, 0.0661684105630294),\n","  (9, 0.019133034379671152)],\n"," [(0, 0.07608161258603736),\n","  (1, 0.018436578171091445),\n","  (2, 0.07890855457227139),\n","  (3, 0.016224188790560472),\n","  (4, 0.4950835791543756),\n","  (5, 0.038102261553588986),\n","  (6, 0.01733038348082596),\n","  (7, 0.014380530973451327),\n","  (8, 0.03638151425762045),\n","  (9, 0.20907079646017698)],\n"," [(0, 0.03528019791330536),\n","  (1, 0.09809616005162954),\n","  (2, 0.01731741421964074),\n","  (3, 0.02226524685382381),\n","  (4, 0.44207808970635676),\n","  (5, 0.11670431321931803),\n","  (6, 0.026567710013982997),\n","  (7, 0.1742497579864472),\n","  (8, 0.025276971065935242),\n","  (9, 0.04216413896956007)],\n"," [(0, 0.03009430094300943),\n","  (1, 0.015498154981549815),\n","  (2, 0.01836818368183682),\n","  (3, 0.015088150881508815),\n","  (4, 0.7517835178351784),\n","  (5, 0.036490364903649035),\n","  (6, 0.01845018450184502),\n","  (7, 0.014596145961459615),\n","  (8, 0.011726117261172612),\n","  (9, 0.08790487904879049)],\n"," [(0, 0.02623316114656133),\n","  (1, 0.03443735440089132),\n","  (2, 0.027246024511293425),\n","  (3, 0.06563354603463993),\n","  (4, 0.5304365441101996),\n","  (5, 0.03372835004557885),\n","  (6, 0.07474931631722881),\n","  (7, 0.012255646713258381),\n","  (8, 0.014078800769776157),\n","  (9, 0.18120125595057227)],\n"," [(0, 0.0719088319088319),\n","  (1, 0.04831908831908832),\n","  (2, 0.05299145299145299),\n","  (3, 0.015042735042735043),\n","  (4, 0.41766381766381766),\n","  (5, 0.12695156695156695),\n","  (6, 0.015384615384615385),\n","  (7, 0.12296296296296297),\n","  (8, 0.013675213675213675),\n","  (9, 0.1150997150997151)],\n"," [(0, 0.12395178197064989),\n","  (1, 0.014412997903563941),\n","  (2, 0.01729559748427673),\n","  (3, 0.037124388539482876),\n","  (4, 0.5707547169811321),\n","  (5, 0.039919636617749825),\n","  (6, 0.020877009084556253),\n","  (7, 0.03843466107617051),\n","  (8, 0.008735150244584208),\n","  (9, 0.12849406009783368)],\n"," [(0, 0.03806146572104018),\n","  (1, 0.022931442080378246),\n","  (2, 0.03002364066193853),\n","  (3, 0.012529550827423165),\n","  (4, 0.4799054373522458),\n","  (5, 0.22237982663514572),\n","  (6, 0.02143420015760441),\n","  (7, 0.06784869976359337),\n","  (8, 0.07683215130023639),\n","  (9, 0.028053585500394003)],\n"," [(0, 0.20427681065978942),\n","  (1, 0.02578981302385558),\n","  (2, 0.01568880292284548),\n","  (3, 0.020202020202020207),\n","  (4, 0.49570169782935747),\n","  (5, 0.12282398452611219),\n","  (6, 0.01901998710509349),\n","  (7, 0.04126370083816893),\n","  (8, 0.029873200085966046),\n","  (9, 0.02535998280679132)],\n"," [(0, 0.029930299302993034),\n","  (1, 0.024190241902419026),\n","  (2, 0.01629766297662977),\n","  (3, 0.03167281672816729),\n","  (4, 0.3857113571135712),\n","  (5, 0.04510045100451005),\n","  (6, 0.10660106601066012),\n","  (7, 0.019885198851988525),\n","  (8, 0.00881508815088151),\n","  (9, 0.33179581795817964)],\n"," [(0, 0.013048350944604548),\n","  (1, 0.00984630163304515),\n","  (2, 0.029058597502401536),\n","  (3, 0.012087736151136728),\n","  (4, 0.3045148895292987),\n","  (5, 0.14745437079731027),\n","  (6, 0.04010566762728146),\n","  (7, 0.030339417227025295),\n","  (8, 0.01440922190201729),\n","  (9, 0.39913544668587897)],\n"," [(0, 0.03400982300064869),\n","  (1, 0.017792604948568252),\n","  (2, 0.017143916226485034),\n","  (3, 0.017143916226485034),\n","  (4, 0.6494300806227412),\n","  (5, 0.05013437123528867),\n","  (6, 0.014085812250949866),\n","  (7, 0.0316930775646372),\n","  (8, 0.010842368640533779),\n","  (9, 0.1577240292836623)],\n"," [(0, 0.03490540900612843),\n","  (1, 0.10054178879118927),\n","  (2, 0.011191047162270184),\n","  (3, 0.46558308908428814),\n","  (4, 0.2671640465405453),\n","  (5, 0.03783639754862776),\n","  (6, 0.01794120259348077),\n","  (7, 0.026467714717115197),\n","  (8, 0.02069455546673772),\n","  (9, 0.017674749089617196)],\n"," [(0, 0.32135618205269945),\n","  (1, 0.018518518518518517),\n","  (2, 0.11184816657453474),\n","  (3, 0.21162704993550766),\n","  (4, 0.0073705546342362266),\n","  (5, 0.02791597567716971),\n","  (6, 0.23272526257600884),\n","  (7, 0.040353786622443336),\n","  (8, 0.014648977335544499),\n","  (9, 0.013635526073337019)],\n"," [(0, 0.12456866804692891),\n","  (1, 0.03735334713595583),\n","  (2, 0.025534851621808144),\n","  (3, 0.03347135955831608),\n","  (4, 0.44565217391304346),\n","  (5, 0.04520358868184955),\n","  (6, 0.013285024154589372),\n","  (7, 0.012077294685990338),\n","  (8, 0.027432712215320912),\n","  (9, 0.23542097998619738)],\n"," [(0, 0.5568743818001978),\n","  (1, 0.013298164633476207),\n","  (2, 0.12243103637762391),\n","  (3, 0.142433234421365),\n","  (4, 0.04165292889328498),\n","  (5, 0.01208924057588746),\n","  (6, 0.07011759534014726),\n","  (7, 0.008902077151335312),\n","  (8, 0.021540828662490383),\n","  (9, 0.01066051214419167)],\n"," [(0, 0.07295550584141189),\n","  (1, 0.020755654983842905),\n","  (2, 0.059781257767834946),\n","  (3, 0.6792194879443202),\n","  (4, 0.024857071836937607),\n","  (5, 0.01901565995525727),\n","  (6, 0.040517027094208305),\n","  (7, 0.021749937857320406),\n","  (8, 0.03616703952274422),\n","  (9, 0.024981357196122298)],\n"," [(0, 0.18877614931729927),\n","  (1, 0.014530878115996492),\n","  (2, 0.45183514969309785),\n","  (3, 0.10935738444193913),\n","  (4, 0.015157209069272204),\n","  (5, 0.021545784792684455),\n","  (6, 0.09482650632594262),\n","  (7, 0.014155079544031066),\n","  (8, 0.02304897908054616),\n","  (9, 0.06676687961919078)],\n"," [(0, 0.08811425804889857),\n","  (1, 0.012587751149842653),\n","  (2, 0.6429435971919633),\n","  (3, 0.06560154926167998),\n","  (4, 0.01440329218106996),\n","  (5, 0.022754780924715567),\n","  (6, 0.0841200677801985),\n","  (7, 0.01476640038731542),\n","  (8, 0.0336480271120794),\n","  (9, 0.021060275962236745)],\n"," [(0, 0.06076829153752229),\n","  (1, 0.07109984033060955),\n","  (2, 0.27002911618296227),\n","  (3, 0.09871325255940638),\n","  (4, 0.025641025641025633),\n","  (5, 0.0818070818070818),\n","  (6, 0.020193481731943266),\n","  (7, 0.13008359162205313),\n","  (8, 0.23076923076923073),\n","  (9, 0.01089508781816474)],\n"," [(0, 0.30102527420123987),\n","  (1, 0.02145922746781116),\n","  (2, 0.06878874582737243),\n","  (3, 0.08309489747257988),\n","  (4, 0.03922269909394373),\n","  (5, 0.14043872198378637),\n","  (6, 0.29339532665712925),\n","  (7, 0.013233190271816882),\n","  (8, 0.017405817835002384),\n","  (9, 0.021936099189318072)],\n"," [(0, 0.29882243208581094),\n","  (1, 0.00827795266410167),\n","  (2, 0.18654541214877),\n","  (3, 0.05922816835723447),\n","  (4, 0.016905677975982282),\n","  (5, 0.018421359449691036),\n","  (6, 0.3622478722163927),\n","  (7, 0.015856360032645452),\n","  (8, 0.020869767984143642),\n","  (9, 0.012824997085227937)],\n"," [(0, 0.20450421603159355),\n","  (1, 0.011954317429821752),\n","  (2, 0.18582559504749707),\n","  (3, 0.45885366634646174),\n","  (4, 0.01387554701675739),\n","  (5, 0.04397481054541573),\n","  (6, 0.04002561639449247),\n","  (7, 0.01707759632831679),\n","  (8, 0.013341872131497491),\n","  (9, 0.010566762728146013)],\n"," [(0, 0.49025803054239075),\n","  (1, 0.026856240126382307),\n","  (2, 0.036334913112164295),\n","  (3, 0.07512725996138318),\n","  (4, 0.06266456029489205),\n","  (5, 0.06406880814463753),\n","  (6, 0.0538880112339828),\n","  (7, 0.13779182025627523),\n","  (8, 0.017377567140600316),\n","  (9, 0.035632789187291555)],\n"," [(0, 0.04683195592286502),\n","  (1, 0.016018773594531174),\n","  (2, 0.023058871543720032),\n","  (3, 0.6421793694520969),\n","  (4, 0.012753800632588514),\n","  (5, 0.04397510458116519),\n","  (6, 0.15937149270482606),\n","  (7, 0.023364962758902156),\n","  (8, 0.013774104683195594),\n","  (9, 0.018671564126109583)],\n"," [(0, 0.35758106923751093),\n","  (1, 0.007887817703768623),\n","  (2, 0.3725776609212192),\n","  (3, 0.11286395948972636),\n","  (4, 0.010711851202648748),\n","  (5, 0.019281332164767746),\n","  (6, 0.08959002824033499),\n","  (7, 0.010906612133605999),\n","  (8, 0.007985198169247249),\n","  (9, 0.010614470737170124)],\n"," [(0, 0.18719163212946516),\n","  (1, 0.00996644957568581),\n","  (2, 0.21788040260509178),\n","  (3, 0.1377540951253207),\n","  (4, 0.015591079534241168),\n","  (5, 0.014604302348529701),\n","  (6, 0.3707321886717979),\n","  (7, 0.018156700217090982),\n","  (8, 0.018156700217090982),\n","  (9, 0.00996644957568581)],\n"," [(0, 0.5759306864705322),\n","  (1, 0.01104446348662287),\n","  (2, 0.0744549176425783),\n","  (3, 0.22155574597733982),\n","  (4, 0.01104446348662287),\n","  (5, 0.021327239836237265),\n","  (6, 0.02951537655907836),\n","  (7, 0.01942302199371608),\n","  (8, 0.017709225935447014),\n","  (9, 0.017994858611825194)],\n"," [(0, 0.22032374100719423),\n","  (1, 0.03597122302158273),\n","  (2, 0.17515987210231815),\n","  (3, 0.028277378097521982),\n","  (4, 0.0192845723421263),\n","  (5, 0.06574740207833733),\n","  (6, 0.19874100719424462),\n","  (7, 0.15837330135891287),\n","  (8, 0.07883693045563549),\n","  (9, 0.0192845723421263)],\n"," [(0, 0.5107323232323233),\n","  (1, 0.010416666666666668),\n","  (2, 0.015361952861952863),\n","  (3, 0.05492424242424243),\n","  (4, 0.25515572390572394),\n","  (5, 0.07375841750841752),\n","  (6, 0.03251262626262627),\n","  (7, 0.012731481481481483),\n","  (8, 0.012626262626262628),\n","  (9, 0.021780303030303035)],\n"," [(0, 0.10802469135802469),\n","  (1, 0.016689529035208046),\n","  (2, 0.1210562414266118),\n","  (3, 0.35230909922267944),\n","  (4, 0.03372199359853681),\n","  (5, 0.05715592135345222),\n","  (6, 0.21342021033379058),\n","  (7, 0.024805669867398263),\n","  (8, 0.03429355281207133),\n","  (9, 0.0385230909922268)],\n"," [(0, 0.23232323232323232),\n","  (1, 0.015448603683897801),\n","  (2, 0.11931075460487225),\n","  (3, 0.35496137849079024),\n","  (4, 0.011764705882352941),\n","  (5, 0.04289958407605467),\n","  (6, 0.14022578728461083),\n","  (7, 0.043969102792632206),\n","  (8, 0.031016042780748664),\n","  (9, 0.00808080808080808)],\n"," [(0, 0.06336275175379046),\n","  (1, 0.01482235799954741),\n","  (2, 0.025005657388549447),\n","  (3, 0.14109527042317266),\n","  (4, 0.011654220411857886),\n","  (5, 0.016632722335369995),\n","  (6, 0.11371350984385607),\n","  (7, 0.5866711925775062),\n","  (8, 0.01787734781624802),\n","  (9, 0.009164969450101833)],\n"," [(0, 0.18047452353169974),\n","  (1, 0.014131984960456373),\n","  (2, 0.12226111759367302),\n","  (3, 0.040062232594321276),\n","  (4, 0.014002333722287048),\n","  (5, 0.021651756774277196),\n","  (6, 0.019706988201737326),\n","  (7, 0.543757292882147),\n","  (8, 0.021911059250615844),\n","  (9, 0.022040710488785167)],\n"," [(0, 0.03996958835668513),\n","  (1, 0.009992397089171283),\n","  (2, 0.12729444987509503),\n","  (3, 0.24481372868469642),\n","  (4, 0.007494297816878462),\n","  (5, 0.018138372977082654),\n","  (6, 0.039643749321168674),\n","  (7, 0.49625285109156075),\n","  (8, 0.007820136852394917),\n","  (9, 0.008580427935266645)],\n"," [(0, 0.031225374310480693),\n","  (1, 0.00975177304964539),\n","  (2, 0.11997635933806147),\n","  (3, 0.12894011032308905),\n","  (4, 0.008372734436564224),\n","  (5, 0.04186367218282112),\n","  (6, 0.05476753349093775),\n","  (7, 0.5674743892829),\n","  (8, 0.01842001576044129),\n","  (9, 0.0192080378250591)],\n"," [(0, 0.01819706498951782),\n","  (1, 0.013081761006289308),\n","  (2, 0.10306079664570231),\n","  (3, 0.021383647798742137),\n","  (4, 0.00930817610062893),\n","  (5, 0.12268343815513627),\n","  (6, 0.01610062893081761),\n","  (7, 0.62020964360587),\n","  (8, 0.03287211740041929),\n","  (9, 0.04310272536687631)],\n"," [(0, 0.051695906432748545),\n","  (1, 0.007953216374269007),\n","  (2, 0.06869395711500977),\n","  (3, 0.06534113060428852),\n","  (4, 0.010370370370370372),\n","  (5, 0.0523196881091618),\n","  (6, 0.013333333333333336),\n","  (7, 0.6904483430799221),\n","  (8, 0.013411306042884992),\n","  (9, 0.0264327485380117)],\n"," [(0, 0.1333596629805161),\n","  (1, 0.014218009478672987),\n","  (2, 0.039362822538177994),\n","  (3, 0.08293838862559243),\n","  (4, 0.01500789889415482),\n","  (5, 0.01961558715113218),\n","  (6, 0.03238546603475514),\n","  (7, 0.6332280147446026),\n","  (8, 0.01816745655608215),\n","  (9, 0.011716692996313852)],\n"," [(0, 0.05956072351421189),\n","  (1, 0.026485788113695095),\n","  (2, 0.019379844961240313),\n","  (3, 0.10813953488372094),\n","  (4, 0.010852713178294575),\n","  (5, 0.029198966408268738),\n","  (6, 0.05232558139534885),\n","  (7, 0.6614987080103361),\n","  (8, 0.01679586563307494),\n","  (9, 0.015762273901808788)],\n"," [(0, 0.035325754797701996),\n","  (1, 0.03324776922136658),\n","  (2, 0.464979831316465),\n","  (3, 0.11086664221977753),\n","  (4, 0.02016868353502017),\n","  (5, 0.08275271971641608),\n","  (6, 0.02224666911135558),\n","  (7, 0.13506906246180173),\n","  (8, 0.06906246180173573),\n","  (9, 0.026280405818359612)],\n"," [(0, 0.04715560190012746),\n","  (1, 0.014250955856795275),\n","  (2, 0.21596570501679993),\n","  (3, 0.021897810218978107),\n","  (4, 0.016220600162206004),\n","  (5, 0.10033599814621713),\n","  (6, 0.42196732707681617),\n","  (7, 0.026648128837909863),\n","  (8, 0.09686015525431585),\n","  (9, 0.03869771752983432)],\n"," [(0, 0.07035952497153083),\n","  (1, 0.007646006181877338),\n","  (2, 0.041158288596063124),\n","  (3, 0.06881405563689605),\n","  (4, 0.00886611355132585),\n","  (5, 0.24019847079876364),\n","  (6, 0.2475191150154547),\n","  (7, 0.1843175532780218),\n","  (8, 0.121848055962258),\n","  (9, 0.009272816007808687)],\n"," [(0, 0.22308271485425407),\n","  (1, 0.032483596859201894),\n","  (2, 0.03237603528019791),\n","  (3, 0.03291384317521781),\n","  (4, 0.023878670538883512),\n","  (5, 0.1376788211250941),\n","  (6, 0.29590190383994835),\n","  (7, 0.0734645584597182),\n","  (8, 0.11982359901043348),\n","  (9, 0.02839625685705066)],\n"," [(0, 0.04763521158633174),\n","  (1, 0.031455080334917396),\n","  (2, 0.25933469110658514),\n","  (3, 0.14448970355284),\n","  (4, 0.022855849739760122),\n","  (5, 0.03484951346458474),\n","  (6, 0.31443765557818504),\n","  (7, 0.10386965376782074),\n","  (8, 0.023987327449649234),\n","  (9, 0.017085313419325635)],\n"," [(0, 0.014814814814814815),\n","  (1, 0.06774691358024691),\n","  (2, 0.08649691358024691),\n","  (3, 0.007253086419753086),\n","  (4, 0.016898148148148148),\n","  (5, 0.2695216049382716),\n","  (6, 0.02662037037037037),\n","  (7, 0.062191358024691355),\n","  (8, 0.11666666666666667),\n","  (9, 0.3317901234567901)],\n"," [(0, 0.15913715913715915),\n","  (1, 0.010785510785510785),\n","  (2, 0.3667073667073667),\n","  (3, 0.08648758648758649),\n","  (4, 0.01221001221001221),\n","  (5, 0.02543752543752544),\n","  (6, 0.2505087505087505),\n","  (7, 0.01790801790801791),\n","  (8, 0.023809523809523808),\n","  (9, 0.04700854700854701)],\n"," [(0, 0.2682030728122913),\n","  (1, 0.060008906702293474),\n","  (2, 0.025829436651079937),\n","  (3, 0.05021153417947005),\n","  (4, 0.05477621910487642),\n","  (5, 0.1261411712313516),\n","  (6, 0.36027610777109775),\n","  (7, 0.016143397906924963),\n","  (8, 0.012803384546871521),\n","  (9, 0.025606769093743043)],\n"," [(0, 0.05674433349183706),\n","  (1, 0.03629735298779521),\n","  (2, 0.11412268188302425),\n","  (3, 0.04010144238389602),\n","  (4, 0.24219369155175147),\n","  (5, 0.24076715802821366),\n","  (6, 0.1393247741321921),\n","  (7, 0.028055159296243463),\n","  (8, 0.04977016959898558),\n","  (9, 0.05262323664606118)],\n"," [(0, 0.34268895196047516),\n","  (1, 0.061074319352465045),\n","  (2, 0.09145380006307159),\n","  (3, 0.036896877956480605),\n","  (4, 0.02407232208556712),\n","  (5, 0.2630085146641438),\n","  (6, 0.013350152423000106),\n","  (7, 0.10249132765689058),\n","  (8, 0.014191106906338695),\n","  (9, 0.05077262693156733)],\n"," [(0, 0.10503408064383674),\n","  (1, 0.02406175576907284),\n","  (2, 0.3114067504311407),\n","  (3, 0.02061263036872793),\n","  (4, 0.019955654101995565),\n","  (5, 0.1684322903835099),\n","  (6, 0.2870986285620432),\n","  (7, 0.01970928800197093),\n","  (8, 0.013550135501355014),\n","  (9, 0.030138786236347213)],\n"," [(0, 0.06176841347019629),\n","  (1, 0.024581307401404646),\n","  (2, 0.2485143165856294),\n","  (3, 0.0476319106789123),\n","  (4, 0.07176301098505312),\n","  (5, 0.3703403565640194),\n","  (6, 0.02278047902034936),\n","  (7, 0.01980911219160814),\n","  (8, 0.05753646677471637),\n","  (9, 0.07527462632811092)],\n"," [(0, 0.13765062467657277),\n","  (1, 0.008575441709174243),\n","  (2, 0.10852369335403267),\n","  (3, 0.17675759591927256),\n","  (4, 0.011976047904191617),\n","  (5, 0.33052413691136245),\n","  (6, 0.013454572336807866),\n","  (7, 0.1175426923929918),\n","  (8, 0.06313299327271384),\n","  (9, 0.03186220152288016)],\n"," [(0, 0.14426196269261962),\n","  (1, 0.13463098134630982),\n","  (2, 0.027879156528791565),\n","  (3, 0.024128142741281427),\n","  (4, 0.1744728304947283),\n","  (5, 0.39263990267639903),\n","  (6, 0.04399837793998378),\n","  (7, 0.020884022708840228),\n","  (8, 0.021695052716950526),\n","  (9, 0.015409570154095702)],\n"," [(0, 0.06985161056822296),\n","  (1, 0.024007721076124987),\n","  (2, 0.23561346362649296),\n","  (3, 0.07262637230063941),\n","  (4, 0.030281095427675236),\n","  (5, 0.20159247195077817),\n","  (6, 0.03136687175775124),\n","  (7, 0.01387380866208228),\n","  (8, 0.2919531909759923),\n","  (9, 0.028833393654240563)],\n"," [(0, 0.09327846364883402),\n","  (1, 0.024874256973022407),\n","  (2, 0.3914037494284408),\n","  (3, 0.1296753543667124),\n","  (4, 0.016095107453132144),\n","  (5, 0.16954732510288065),\n","  (6, 0.13461362597165066),\n","  (7, 0.015089163237311385),\n","  (8, 0.013351623228166439),\n","  (9, 0.012071330589849109)],\n"," [(0, 0.0900067456875783),\n","  (1, 0.02650091548617134),\n","  (2, 0.30644694998554495),\n","  (3, 0.04191962995085285),\n","  (4, 0.01310590729497928),\n","  (5, 0.44087886672448684),\n","  (6, 0.02158620025055411),\n","  (7, 0.01310590729497928),\n","  (8, 0.0342102727185121),\n","  (9, 0.012238604606340946)],\n"," [(0, 0.20783373301358915),\n","  (1, 0.013589128697042367),\n","  (2, 0.12976285638156146),\n","  (3, 0.022648547828403948),\n","  (4, 0.011457500666133762),\n","  (5, 0.02797761790567546),\n","  (6, 0.4992006394884093),\n","  (7, 0.03783639754862777),\n","  (8, 0.014255262456701308),\n","  (9, 0.03543831601385559)],\n"," [(0, 0.32101656691820624),\n","  (1, 0.013010668748373666),\n","  (2, 0.03721051262034868),\n","  (3, 0.10798855061150144),\n","  (4, 0.008933992540549917),\n","  (5, 0.09818718015439327),\n","  (6, 0.10486599011189175),\n","  (7, 0.23462572642900512),\n","  (8, 0.060803191950732936),\n","  (9, 0.013357619914996965)],\n"," [(0, 0.05127411534096771),\n","  (1, 0.014856081708449397),\n","  (2, 0.17548746518105848),\n","  (3, 0.028783658310120707),\n","  (4, 0.02414113277623027),\n","  (5, 0.139482100484886),\n","  (6, 0.231713607758176),\n","  (7, 0.026307644692045808),\n","  (8, 0.039719385123284844),\n","  (9, 0.2682348086247808)],\n"," [(0, 0.22130563193807923),\n","  (1, 0.02933088909257562),\n","  (2, 0.1725226601486913),\n","  (3, 0.27793054282513496),\n","  (4, 0.01863733577757409),\n","  (5, 0.02016498625114574),\n","  (6, 0.09634382319991852),\n","  (7, 0.12384153172420817),\n","  (8, 0.02525715449638456),\n","  (9, 0.01466544454628781)],\n"," [(0, 0.053095530955309556),\n","  (1, 0.030647806478064782),\n","  (2, 0.021935219352193523),\n","  (3, 0.03023780237802378),\n","  (4, 0.030647806478064782),\n","  (5, 0.4203567035670357),\n","  (6, 0.013120131201312012),\n","  (7, 0.017015170151701516),\n","  (8, 0.22345223452234522),\n","  (9, 0.15949159491594916)],\n"," [(0, 0.20986490732013824),\n","  (1, 0.014975390093203476),\n","  (2, 0.3353230704785841),\n","  (3, 0.1459838726568227),\n","  (4, 0.014765944077913918),\n","  (5, 0.03372080846161902),\n","  (6, 0.15792229552832757),\n","  (7, 0.02565713687297099),\n","  (8, 0.043355325164938736),\n","  (9, 0.018431249345481203)],\n"," [(0, 0.01665669259924197),\n","  (1, 0.017354877318970677),\n","  (2, 0.25763016157989227),\n","  (3, 0.21524037502493518),\n","  (4, 0.019848394175144623),\n","  (5, 0.023937761819269897),\n","  (6, 0.2824655894673848),\n","  (7, 0.0895671254737682),\n","  (8, 0.03401156991821265),\n","  (9, 0.043287452623179734)],\n"," [(0, 0.036290633927882024),\n","  (1, 0.01420739711219211),\n","  (2, 0.04401204540189947),\n","  (3, 0.011504903096286002),\n","  (4, 0.016678248783877695),\n","  (5, 0.550845494556405),\n","  (6, 0.1518029495791831),\n","  (7, 0.010655547834144083),\n","  (8, 0.04686896764728593),\n","  (9, 0.11713381206084474)],\n"," [(0, 0.03577981651376147),\n","  (1, 0.021916411824668705),\n","  (2, 0.21743119266055047),\n","  (3, 0.12110091743119267),\n","  (4, 0.012640163098878695),\n","  (5, 0.4017329255861366),\n","  (6, 0.017533129459734963),\n","  (7, 0.018144750254842),\n","  (8, 0.1346585117227319),\n","  (9, 0.01906218144750255)],\n"," [(0, 0.10422364023093282),\n","  (1, 0.01569938215334751),\n","  (2, 0.06492454167932744),\n","  (3, 0.04172997062696242),\n","  (4, 0.014585232452142202),\n","  (5, 0.09612073331307604),\n","  (6, 0.5574799959485465),\n","  (7, 0.04314797933758735),\n","  (8, 0.013774941760356525),\n","  (9, 0.048313582497721046)],\n"," [(0, 0.033125534775699794),\n","  (1, 0.08556411196675223),\n","  (2, 0.3701259014790368),\n","  (3, 0.042782055983376116),\n","  (4, 0.039237257059039236),\n","  (5, 0.20902090209020902),\n","  (6, 0.05451656276738785),\n","  (7, 0.04363769710304364),\n","  (8, 0.01674611905635008),\n","  (9, 0.10524385771910524)],\n"," [(0, 0.13390313390313388),\n","  (1, 0.015954415954415952),\n","  (2, 0.042849002849002844),\n","  (3, 0.029401709401709396),\n","  (4, 0.007749287749287748),\n","  (5, 0.08068376068376067),\n","  (6, 0.6092307692307691),\n","  (7, 0.01789173789173789),\n","  (8, 0.05470085470085469),\n","  (9, 0.007635327635327633)],\n"," [(0, 0.11351427763641504),\n","  (1, 0.0453774385072095),\n","  (2, 0.07958722080859486),\n","  (3, 0.04170200735086231),\n","  (4, 0.019225332202431437),\n","  (5, 0.02813118461973424),\n","  (6, 0.5248798416737348),\n","  (7, 0.015408538309301669),\n","  (8, 0.030817076618603337),\n","  (9, 0.10135708227311281)],\n"," [(0, 0.32633819951338194),\n","  (1, 0.09489051094890508),\n","  (2, 0.186536901865369),\n","  (3, 0.018653690186536898),\n","  (4, 0.018552311435523113),\n","  (5, 0.20366991078669905),\n","  (6, 0.03213706407137063),\n","  (7, 0.07147201946472018),\n","  (8, 0.038523925385239244),\n","  (9, 0.009225466342254662)],\n"," [(0, 0.010108420966821553),\n","  (1, 0.02649384527594359),\n","  (2, 0.22817314746881878),\n","  (3, 0.04043368386728621),\n","  (4, 0.027716638134833292),\n","  (5, 0.3892557267465558),\n","  (6, 0.009293225727561751),\n","  (7, 0.04760740197277248),\n","  (8, 0.09097578870139399),\n","  (9, 0.12994212113801257)],\n"," [(0, 0.03352403894409314),\n","  (1, 0.027903241995382917),\n","  (2, 0.5130984643179766),\n","  (3, 0.06483990765833585),\n","  (4, 0.0107397370269999),\n","  (5, 0.08611863896416742),\n","  (6, 0.022985044665261467),\n","  (7, 0.021880959550336244),\n","  (8, 0.1315868714242698),\n","  (9, 0.08732309545317675)],\n"," [(0, 0.1563605317721673),\n","  (1, 0.030491523356506883),\n","  (2, 0.3727283815099402),\n","  (3, 0.03841931942919868),\n","  (4, 0.03354067569215757),\n","  (5, 0.25661666056836197),\n","  (6, 0.05366508110745211),\n","  (7, 0.025978777899743864),\n","  (8, 0.023295523844371262),\n","  (9, 0.00890352482010001)],\n"," [(0, 0.02529158993247391),\n","  (1, 0.051442602823818295),\n","  (2, 0.2143646408839779),\n","  (3, 0.031062001227747084),\n","  (4, 0.01792510742786986),\n","  (5, 0.01682013505217925),\n","  (6, 0.32756292203806014),\n","  (7, 0.013996316758747698),\n","  (8, 0.2842234499693063),\n","  (9, 0.01731123388581952)],\n"," [(0, 0.08199141170155663),\n","  (1, 0.01637144390767579),\n","  (2, 0.21457326892109502),\n","  (3, 0.05743424584004294),\n","  (4, 0.014224369296833066),\n","  (5, 0.031937734836285564),\n","  (6, 0.4322329575952764),\n","  (7, 0.019860440150295224),\n","  (8, 0.08132045088566828),\n","  (9, 0.05005367686527107)],\n"," [(0, 0.10092395167022032),\n","  (1, 0.06574271499644634),\n","  (2, 0.19071310116086235),\n","  (3, 0.041814735844586594),\n","  (4, 0.014333096422648661),\n","  (5, 0.31355129116323144),\n","  (6, 0.0222696043591566),\n","  (7, 0.10068704098554845),\n","  (8, 0.13598673300165837),\n","  (9, 0.013977730395640844)],\n"," [(0, 0.017275225755791126),\n","  (1, 0.01753697160057584),\n","  (2, 0.2726082973432797),\n","  (3, 0.05614448370632116),\n","  (4, 0.016097369454259915),\n","  (5, 0.031409501374165684),\n","  (6, 0.4075382803297998),\n","  (7, 0.145530689700301),\n","  (8, 0.020154430048422982),\n","  (9, 0.015704750687082842)],\n"," [(0, 0.04829138530795989),\n","  (1, 0.013607530182115817),\n","  (2, 0.47790055248618785),\n","  (3, 0.05688561489666462),\n","  (4, 0.010845099242889298),\n","  (5, 0.060671168405975036),\n","  (6, 0.0737671373030489),\n","  (7, 0.013402905668099038),\n","  (8, 0.017290771434417842),\n","  (9, 0.22733783507264171)],\n"," [(0, 0.05007541478129714),\n","  (1, 0.022322775263951735),\n","  (2, 0.27310206133735543),\n","  (3, 0.015082956259426848),\n","  (4, 0.010658622423328306),\n","  (5, 0.1283056812468577),\n","  (6, 0.07199597787833081),\n","  (7, 0.3911513323278029),\n","  (8, 0.02091503267973856),\n","  (9, 0.01639014580191051)],\n"," [(0, 0.045155221072436504),\n","  (1, 0.5710253998118533),\n","  (2, 0.023413818333855963),\n","  (3, 0.051113201630605204),\n","  (4, 0.1938956830772447),\n","  (5, 0.03543430542489809),\n","  (6, 0.009825441622243128),\n","  (7, 0.023204766384446535),\n","  (8, 0.008989233824605414),\n","  (9, 0.03794292881781123)],\n"," [(0, 0.07013888888888889),\n","  (1, 0.08611111111111111),\n","  (2, 0.32060185185185186),\n","  (3, 0.10694444444444444),\n","  (4, 0.029050925925925924),\n","  (5, 0.12291666666666666),\n","  (6, 0.08564814814814815),\n","  (7, 0.018055555555555554),\n","  (8, 0.1310185185185185),\n","  (9, 0.029513888888888888)],\n"," [(0, 0.07448257080610021),\n","  (1, 0.5727124183006536),\n","  (2, 0.05923202614379085),\n","  (3, 0.046432461873638343),\n","  (4, 0.02627995642701525),\n","  (5, 0.08782679738562091),\n","  (6, 0.012799564270152506),\n","  (7, 0.03281590413943355),\n","  (8, 0.05841503267973856),\n","  (9, 0.02900326797385621)],\n"," [(0, 0.04036770583533174),\n","  (1, 0.735544897415401),\n","  (2, 0.021449507061017854),\n","  (3, 0.02651212363442579),\n","  (4, 0.029443112176925128),\n","  (5, 0.02411404209965361),\n","  (6, 0.016786570743405275),\n","  (7, 0.056088462563282704),\n","  (8, 0.0374367172928324),\n","  (9, 0.012256861177724487)],\n"," [(0, 0.16713947990543734),\n","  (1, 0.3159574468085106),\n","  (2, 0.0983451536643026),\n","  (3, 0.029196217494089835),\n","  (4, 0.030141843971631204),\n","  (5, 0.16749408983451536),\n","  (6, 0.02304964539007092),\n","  (7, 0.028368794326241134),\n","  (8, 0.06418439716312056),\n","  (9, 0.07612293144208038)],\n"," [(0, 0.06951377334072842),\n","  (1, 0.5651691625069329),\n","  (2, 0.1408763172490294),\n","  (3, 0.029395452024403773),\n","  (4, 0.057774080236642635),\n","  (5, 0.0258827879460159),\n","  (6, 0.04649657977444999),\n","  (7, 0.009983361064891847),\n","  (8, 0.01858014420410427),\n","  (9, 0.03632834165280089)],\n"," [(0, 0.014322916666666666),\n","  (1, 0.40769675925925924),\n","  (2, 0.015335648148148149),\n","  (3, 0.021701388888888888),\n","  (4, 0.16652199074074073),\n","  (5, 0.06438078703703703),\n","  (6, 0.019386574074074073),\n","  (7, 0.016927083333333332),\n","  (8, 0.01765046296296296),\n","  (9, 0.2560763888888889)],\n"," [(0, 0.3636826560749567),\n","  (1, 0.3579794276402892),\n","  (2, 0.03269172013443324),\n","  (3, 0.038700478663815055),\n","  (4, 0.024544250942051125),\n","  (5, 0.052754863020674204),\n","  (6, 0.026071901415622774),\n","  (7, 0.0367654547306243),\n","  (8, 0.027701395254099197),\n","  (9, 0.03910785212343416)],\n"," [(0, 0.4020926756352765),\n","  (1, 0.10936721474838067),\n","  (2, 0.19045839561534628),\n","  (3, 0.03761833582461385),\n","  (4, 0.015072247135027405),\n","  (5, 0.11733931240657698),\n","  (6, 0.02852516193323368),\n","  (7, 0.05318883906327852),\n","  (8, 0.03188839063278525),\n","  (9, 0.014449427005480818)],\n"," [(0, 0.023456790123456795),\n","  (1, 0.552204585537919),\n","  (2, 0.03650793650793652),\n","  (3, 0.035978835978835985),\n","  (4, 0.041269841269841276),\n","  (5, 0.060493827160493834),\n","  (6, 0.017283950617283952),\n","  (7, 0.023280423280423283),\n","  (8, 0.1955908289241623),\n","  (9, 0.013932980599647269)],\n"," [(0, 0.0523989898989899),\n","  (1, 0.7530513468013469),\n","  (2, 0.031355218855218865),\n","  (3, 0.02398989898989899),\n","  (4, 0.04156144781144782),\n","  (5, 0.021569865319865322),\n","  (6, 0.009785353535353538),\n","  (7, 0.027462121212121215),\n","  (8, 0.018097643097643102),\n","  (9, 0.02072811447811448)],\n"," [(0, 0.03243616287094548),\n","  (1, 0.4727398205659075),\n","  (2, 0.050293305728088336),\n","  (3, 0.017339544513457556),\n","  (4, 0.044513457556935816),\n","  (5, 0.04218426501035197),\n","  (6, 0.06625258799171843),\n","  (7, 0.025793650793650792),\n","  (8, 0.11499309868875086),\n","  (9, 0.13345410628019325)],\n"," [(0, 0.03146813136514476),\n","  (1, 0.04268222908799635),\n","  (2, 0.1114543998169127),\n","  (3, 0.03032383567913949),\n","  (4, 0.017050005721478433),\n","  (5, 0.05938894610367319),\n","  (6, 0.46023572491131715),\n","  (7, 0.03043826524774002),\n","  (8, 0.20139604073692646),\n","  (9, 0.015562421329671589)],\n"," [(0, 0.025381611643592473),\n","  (1, 0.11093361732339369),\n","  (2, 0.039226127085552),\n","  (3, 0.012513312034078808),\n","  (4, 0.01828186013489528),\n","  (5, 0.09637912673056442),\n","  (6, 0.02547035853745119),\n","  (7, 0.12255946041888534),\n","  (8, 0.43432729854455093),\n","  (9, 0.11492722754703585)],\n"," [(0, 0.03558529849160824),\n","  (1, 0.03739111960909284),\n","  (2, 0.040684087529211814),\n","  (3, 0.010728701933290844),\n","  (4, 0.0419587847886127),\n","  (5, 0.023050775440832803),\n","  (6, 0.09687699171446781),\n","  (7, 0.059804546420225195),\n","  (8, 0.35224134268111323),\n","  (9, 0.30167835139154453)],\n"," [(0, 0.031064707398814128),\n","  (1, 0.01211652487754576),\n","  (2, 0.06380510440835267),\n","  (3, 0.06870327403970096),\n","  (4, 0.00979633926269657),\n","  (5, 0.025522041763341066),\n","  (6, 0.23433874709976799),\n","  (7, 0.05839133797370456),\n","  (8, 0.4756380510440835),\n","  (9, 0.020623872131992783)],\n"," [(0, 0.02872448508358733),\n","  (1, 0.01773344416735938),\n","  (2, 0.08996028447400019),\n","  (3, 0.06483790523690773),\n","  (4, 0.00997506234413965),\n","  (5, 0.05356977925556479),\n","  (6, 0.12422647085988732),\n","  (7, 0.01773344416735938),\n","  (8, 0.5701487023182784),\n","  (9, 0.02309042209291586)],\n"," [(0, 0.01468103084072666),\n","  (1, 0.06590621039290243),\n","  (2, 0.0934727503168568),\n","  (3, 0.012991128010139418),\n","  (4, 0.013519222644697931),\n","  (5, 0.012146176594845797),\n","  (6, 0.017743979721166037),\n","  (7, 0.014575411913814958),\n","  (8, 0.7221166032953106),\n","  (9, 0.03284748626953951)],\n"," [(0, 0.2896015549076773),\n","  (1, 0.15505884893639993),\n","  (2, 0.036605118237771295),\n","  (3, 0.16175359032501888),\n","  (4, 0.01468523917503509),\n","  (5, 0.06133246949573479),\n","  (6, 0.10430839002267571),\n","  (7, 0.028938559550804443),\n","  (8, 0.12666018788467764),\n","  (9, 0.021056041464204728)],\n"," [(0, 0.03473762010347377),\n","  (1, 0.04927322000492733),\n","  (2, 0.06734006734006735),\n","  (3, 0.032273959103227404),\n","  (4, 0.09912129424324548),\n","  (5, 0.05329719963866306),\n","  (6, 0.030385152336371853),\n","  (7, 0.11694177547836086),\n","  (8, 0.015192576168185926),\n","  (9, 0.5014371355834771)],\n"," [(0, 0.012411347517730497),\n","  (1, 0.0485618597320725),\n","  (2, 0.04659180457052797),\n","  (3, 0.013494877856579985),\n","  (4, 0.025512214342001575),\n","  (5, 0.02846729708431836),\n","  (6, 0.01103230890464933),\n","  (7, 0.04393223010244287),\n","  (8, 0.3537234042553192),\n","  (9, 0.4162726556343578)],\n"," [(0, 0.014109347442680775),\n","  (1, 0.03317376333249349),\n","  (2, 0.037876879146720416),\n","  (3, 0.015369110607205845),\n","  (4, 0.024943310657596373),\n","  (5, 0.055513563450071385),\n","  (6, 0.022843705383387924),\n","  (7, 0.15587469555723524),\n","  (8, 0.11833375325438818),\n","  (9, 0.5219618711682203)],\n"," [(0, 0.04311774461028192),\n","  (1, 0.04007739082365948),\n","  (2, 0.11682329095264418),\n","  (3, 0.019716233646581906),\n","  (4, 0.27998894416804865),\n","  (5, 0.050304035378662244),\n","  (6, 0.017689331122166942),\n","  (7, 0.08310300350101345),\n","  (8, 0.02865303114059333),\n","  (9, 0.3205269946563479)],\n"," [(0, 0.1383067896060352),\n","  (1, 0.02738195026543727),\n","  (2, 0.02915153208531247),\n","  (3, 0.02048989475644966),\n","  (4, 0.5698053459998137),\n","  (5, 0.010896898575020955),\n","  (6, 0.03520536462699078),\n","  (7, 0.03269069572506287),\n","  (8, 0.010524355033994598),\n","  (9, 0.12554717332588247)],\n"," [(0, 0.030806182121971594),\n","  (1, 0.008145363408521303),\n","  (2, 0.11079782790309106),\n","  (3, 0.01733500417710944),\n","  (4, 0.041040100250626564),\n","  (5, 0.2686925647451963),\n","  (6, 0.18201754385964913),\n","  (7, 0.020885547201336674),\n","  (8, 0.05419799498746867),\n","  (9, 0.26608187134502925)],\n"," [(0, 0.07530491216291821),\n","  (1, 0.0237216068031778),\n","  (2, 0.1387490209242475),\n","  (3, 0.0436388049681101),\n","  (4, 0.020364775651784715),\n","  (5, 0.1667226138525232),\n","  (6, 0.051135727872888),\n","  (7, 0.01734362761553094),\n","  (8, 0.04889784043862594),\n","  (9, 0.4141210697101936)],\n"," [(0, 0.032634795896092555),\n","  (1, 0.014843920541366514),\n","  (2, 0.0848068107400131),\n","  (3, 0.02150185549006767),\n","  (4, 0.011351233355162629),\n","  (5, 0.23488321327221132),\n","  (6, 0.0435494433529797),\n","  (7, 0.027723204540493342),\n","  (8, 0.14276358873608383),\n","  (9, 0.38594193407552935)],\n"," [(0, 0.09798775153105861),\n","  (1, 0.02524684414448194),\n","  (2, 0.3064616922884639),\n","  (3, 0.06074240719910011),\n","  (4, 0.0196225471816023),\n","  (5, 0.021747281589801274),\n","  (6, 0.05961754780652419),\n","  (7, 0.014873140857392825),\n","  (8, 0.05261842269716285),\n","  (9, 0.34108236470441194)],\n"," [(0, 0.023892773892773892),\n","  (1, 0.01554001554001554),\n","  (2, 0.25777000777000775),\n","  (3, 0.016608391608391608),\n","  (4, 0.011752136752136752),\n","  (5, 0.11965811965811966),\n","  (6, 0.1660839160839161),\n","  (7, 0.04331779331779332),\n","  (8, 0.13335275835275837),\n","  (9, 0.21202408702408704)],\n"," [(0, 0.046587926509186355),\n","  (1, 0.023950131233595802),\n","  (2, 0.35159667541557305),\n","  (3, 0.03237095363079615),\n","  (4, 0.016076115485564306),\n","  (5, 0.019138232720909885),\n","  (6, 0.04669728783902012),\n","  (7, 0.023293963254593177),\n","  (8, 0.045275590551181105),\n","  (9, 0.39501312335958005)],\n"," [(0, 0.011616564689190388),\n","  (1, 0.020750199521149242),\n","  (2, 0.13682717034672343),\n","  (3, 0.026336791699920193),\n","  (4, 0.00860157843398067),\n","  (5, 0.08663651680411456),\n","  (6, 0.014276846679081317),\n","  (7, 0.019508734592533477),\n","  (8, 0.23286335018178594),\n","  (9, 0.44258224705152077)],\n"," [(0, 0.08069329660238751),\n","  (1, 0.025826446280991736),\n","  (2, 0.31714876033057854),\n","  (3, 0.040748393021120294),\n","  (4, 0.008838383838383838),\n","  (5, 0.09710743801652892),\n","  (6, 0.026515151515151516),\n","  (7, 0.03696051423324151),\n","  (8, 0.28868227731864093),\n","  (9, 0.07747933884297521)],\n"," [(0, 0.09490497843068672),\n","  (1, 0.02448408534452606),\n","  (2, 0.41529672379619914),\n","  (3, 0.11320974699778477),\n","  (4, 0.011076133846333217),\n","  (5, 0.019703859158213828),\n","  (6, 0.02576658505304885),\n","  (7, 0.015273405619680542),\n","  (8, 0.1086627025766585),\n","  (9, 0.17162177917686836)],\n"," [(0, 0.13691528219630025),\n","  (1, 0.3292093790503123),\n","  (2, 0.08530694002592201),\n","  (3, 0.013550135501355016),\n","  (4, 0.018734535171438676),\n","  (5, 0.05738187816660776),\n","  (6, 0.06775067750677508),\n","  (7, 0.10062448450571464),\n","  (8, 0.06940025921998352),\n","  (9, 0.12112642865559092)],\n"," [(0, 0.01433389544688027),\n","  (1, 0.012085441259134345),\n","  (2, 0.2761851227281244),\n","  (3, 0.026513022297170695),\n","  (4, 0.011898070076822184),\n","  (5, 0.06248828930110549),\n","  (6, 0.30625819748922617),\n","  (7, 0.01424020985572419),\n","  (8, 0.1736930860033727),\n","  (9, 0.10230466554243957)],\n"," [(0, 0.09570957095709573),\n","  (1, 0.013751375137513754),\n","  (2, 0.03355335533553356),\n","  (3, 0.08099559955995601),\n","  (4, 0.07123212321232125),\n","  (5, 0.03781628162816282),\n","  (6, 0.06476897689768979),\n","  (7, 0.3674367436743675),\n","  (8, 0.09887238723872388),\n","  (9, 0.13586358635863588)],\n"," [(0, 0.14758269720101783),\n","  (1, 0.1520120629535388),\n","  (2, 0.04749787955894827),\n","  (3, 0.009706907925737444),\n","  (4, 0.010743567995476395),\n","  (5, 0.096880595608331),\n","  (6, 0.014513240976345303),\n","  (7, 0.45000471209122617),\n","  (8, 0.055508434643294705),\n","  (9, 0.015549901046084254)],\n"," [(0, 0.0062960007094085306),\n","  (1, 0.03121397534805356),\n","  (2, 0.06881262747184536),\n","  (3, 0.018799326061895893),\n","  (4, 0.0148975791433892),\n","  (5, 0.19632881085395051),\n","  (6, 0.015429635541367386),\n","  (7, 0.051520794537554315),\n","  (8, 0.5736454730868139),\n","  (9, 0.02305577724572138)],\n"," [(0, 0.01941859129035965),\n","  (1, 0.08813052970240148),\n","  (2, 0.052395725611857985),\n","  (3, 0.08204067562909342),\n","  (4, 0.013673445938182238),\n","  (5, 0.25060324026197867),\n","  (6, 0.12570378030564175),\n","  (7, 0.029759852924278987),\n","  (8, 0.32781799379524307),\n","  (9, 0.010456164540962888)],\n"," [(0, 0.009092562284051647),\n","  (1, 0.023549736315693764),\n","  (2, 0.3014184397163121),\n","  (3, 0.039916348426986735),\n","  (4, 0.014548099654482635),\n","  (5, 0.055737406801236594),\n","  (6, 0.04328059647208584),\n","  (7, 0.028368794326241138),\n","  (8, 0.47354064375340976),\n","  (9, 0.010547372249499911)],\n"," [(0, 0.0352514256091239),\n","  (1, 0.024537757041645066),\n","  (2, 0.2953170900293762),\n","  (3, 0.06894764126490409),\n","  (4, 0.07499567997235182),\n","  (5, 0.05045792293070676),\n","  (6, 0.09106618282357007),\n","  (7, 0.0368066355624676),\n","  (8, 0.290478659063418),\n","  (9, 0.0321410057024365)],\n"," [(0, 0.027345102111457253),\n","  (1, 0.028902734510211147),\n","  (2, 0.17661820699203876),\n","  (3, 0.03435444790584977),\n","  (4, 0.01704742125302873),\n","  (5, 0.12002422983731395),\n","  (6, 0.017480096919349257),\n","  (7, 0.5025960539979232),\n","  (8, 0.04820006922810661),\n","  (9, 0.027431637244721356)],\n"," [(0, 0.030913580246913583),\n","  (1, 0.010962962962962964),\n","  (2, 0.05876543209876544),\n","  (3, 0.04474074074074075),\n","  (4, 0.026074074074074076),\n","  (5, 0.0657777777777778),\n","  (6, 0.45748148148148154),\n","  (7, 0.0971851851851852),\n","  (8, 0.1940740740740741),\n","  (9, 0.014024691358024694)],\n"," [(0, 0.02718556119571348),\n","  (1, 0.028764805414551606),\n","  (2, 0.032148900169204735),\n","  (3, 0.0604624929498026),\n","  (4, 0.019063733784545966),\n","  (5, 0.017146080090242526),\n","  (6, 0.24376762549351383),\n","  (7, 0.01849971799210378),\n","  (8, 0.3748448956570784),\n","  (9, 0.1781161872532431)],\n"," [(0, 0.06636838180462341),\n","  (1, 0.010120379247896027),\n","  (2, 0.02514115265793118),\n","  (3, 0.0270586981996378),\n","  (4, 0.011185682326621925),\n","  (5, 0.03781825929476936),\n","  (6, 0.061148396718866516),\n","  (7, 0.06892510919356556),\n","  (8, 0.6259720890593374),\n","  (9, 0.06626185149675083)],\n"," [(0, 0.17447293447293444),\n","  (1, 0.025982905982905976),\n","  (2, 0.11521367521367519),\n","  (3, 0.03635327635327635),\n","  (4, 0.09173789173789172),\n","  (5, 0.06985754985754984),\n","  (6, 0.02575498575498575),\n","  (7, 0.036011396011396),\n","  (8, 0.24843304843304836),\n","  (9, 0.17618233618233614)],\n"," [(0, 0.019576719576719578),\n","  (1, 0.010687830687830687),\n","  (2, 0.06814814814814815),\n","  (3, 0.08518518518518518),\n","  (4, 0.017354497354497355),\n","  (5, 0.042433862433862435),\n","  (6, 0.21015873015873016),\n","  (7, 0.01566137566137566),\n","  (8, 0.5097354497354497),\n","  (9, 0.021058201058201057)],\n"," [(0, 0.04863700389427459),\n","  (1, 0.024028502775706354),\n","  (2, 0.023365647526721354),\n","  (3, 0.015825669069516945),\n","  (4, 0.1577595492584307),\n","  (5, 0.04109702543707018),\n","  (6, 0.013505675698069434),\n","  (7, 0.4576186925180214),\n","  (8, 0.043748446433010194),\n","  (9, 0.1744137873891789)],\n"," [(0, 0.038710855473386284),\n","  (1, 0.014516570802519857),\n","  (2, 0.07075687026385465),\n","  (3, 0.01159499680452844),\n","  (4, 0.20633616360814389),\n","  (5, 0.05185793846434767),\n","  (6, 0.04847986852917009),\n","  (7, 0.12298000547795125),\n","  (8, 0.03852825709851182),\n","  (9, 0.39623847347758606)],\n"," [(0, 0.021727438958803916),\n","  (1, 0.5084435839518124),\n","  (2, 0.04410024739163171),\n","  (3, 0.04141120791653222),\n","  (4, 0.03323652791222975),\n","  (5, 0.02409379369689147),\n","  (6, 0.015703990534581047),\n","  (7, 0.06615037108744756),\n","  (8, 0.23674303538775948),\n","  (9, 0.008389803162310423)],\n"," [(0, 0.05517005054399041),\n","  (1, 0.022873297352865588),\n","  (2, 0.20971472629144178),\n","  (3, 0.07890002570033411),\n","  (4, 0.05405636939946886),\n","  (5, 0.08078471686798595),\n","  (6, 0.2984665467317742),\n","  (7, 0.013449841514606356),\n","  (8, 0.08977983380450613),\n","  (9, 0.09680459179302664)],\n"," [(0, 0.015737580583996966),\n","  (1, 0.011376564277588168),\n","  (2, 0.042567311338642395),\n","  (3, 0.017444065225635193),\n","  (4, 0.4879598028062192),\n","  (5, 0.05716723549488054),\n","  (6, 0.04010238907849829),\n","  (7, 0.017349260523321957),\n","  (8, 0.02844141069397042),\n","  (9, 0.2818543799772469)],\n"," [(0, 0.020500918673242434),\n","  (1, 0.011120781355768302),\n","  (2, 0.0894497630790059),\n","  (3, 0.0557973116719853),\n","  (4, 0.03974470554105019),\n","  (5, 0.11381877961512427),\n","  (6, 0.01286142539406247),\n","  (7, 0.029784353544144666),\n","  (8, 0.4746156077748767),\n","  (9, 0.15230635335073978)],\n"," [(0, 0.007577832557290241),\n","  (1, 0.009677713868346573),\n","  (2, 0.24742079795489824),\n","  (3, 0.24166894914635262),\n","  (4, 0.008856021181411487),\n","  (5, 0.20323199123527802),\n","  (6, 0.01707294805076235),\n","  (7, 0.030859125353784354),\n","  (8, 0.20852734410663748),\n","  (9, 0.02510727654523875)],\n"," [(0, 0.35544833132287895),\n","  (1, 0.011392574721887146),\n","  (2, 0.02479560380646026),\n","  (3, 0.14837153196622438),\n","  (4, 0.012330786757807265),\n","  (5, 0.36898539069829783),\n","  (6, 0.02720814904168342),\n","  (7, 0.016083634901487735),\n","  (8, 0.014341241120493231),\n","  (9, 0.02104275566277979)],\n"," [(0, 0.2526997840172786),\n","  (1, 0.017758579313654908),\n","  (2, 0.03659707223422126),\n","  (3, 0.02327813774898008),\n","  (4, 0.1615070794336453),\n","  (5, 0.2203023758099352),\n","  (6, 0.03179745620350372),\n","  (7, 0.18970482361411087),\n","  (8, 0.011519078473722102),\n","  (9, 0.054835613150947926)],\n"," [(0, 0.0082338038364097),\n","  (1, 0.015110387260224394),\n","  (2, 0.41757148027506336),\n","  (3, 0.16965255157437567),\n","  (4, 0.006152732537097358),\n","  (5, 0.11961635903003981),\n","  (6, 0.030311255881288454),\n","  (7, 0.016648570394498734),\n","  (8, 0.1615997104596453),\n","  (9, 0.05510314875135722)],\n"," [(0, 0.031236055332440876),\n","  (1, 0.13208389112003568),\n","  (2, 0.02175368139223561),\n","  (3, 0.025992860330209727),\n","  (4, 0.5109326193663543),\n","  (5, 0.05823293172690763),\n","  (6, 0.02431950022311468),\n","  (7, 0.04071842927264614),\n","  (8, 0.03781793842034806),\n","  (9, 0.11691209281570727)],\n"," [(0, 0.022222222222222223),\n","  (1, 0.017136539524599224),\n","  (2, 0.02310668877833057),\n","  (3, 0.015478164731896076),\n","  (4, 0.33112216694306246),\n","  (5, 0.136318407960199),\n","  (6, 0.027418463239358763),\n","  (7, 0.1663902708678828),\n","  (8, 0.013488114980652294),\n","  (9, 0.24731896075179657)],\n"," [(0, 0.018744354110207768),\n","  (1, 0.03116531165311653),\n","  (2, 0.018970189701897018),\n","  (3, 0.027439024390243903),\n","  (4, 0.7428861788617886),\n","  (5, 0.013211382113821139),\n","  (6, 0.035456187895212286),\n","  (7, 0.015356820234869015),\n","  (8, 0.02687443541102078),\n","  (9, 0.06989611562782294)],\n"," [(0, 0.11313298410072604),\n","  (1, 0.11984192629353921),\n","  (2, 0.014612627515853325),\n","  (3, 0.08822718500137856),\n","  (4, 0.44729344729344733),\n","  (5, 0.015807370646080326),\n","  (6, 0.16956162117452442),\n","  (7, 0.00983365499494532),\n","  (8, 0.009374138406396473),\n","  (9, 0.01231504457310909)],\n"," [(0, 0.1671794871794872),\n","  (1, 0.015954415954415956),\n","  (2, 0.061766381766381766),\n","  (3, 0.060626780626780626),\n","  (4, 0.529002849002849),\n","  (5, 0.027236467236467235),\n","  (6, 0.03133903133903134),\n","  (7, 0.020626780626780628),\n","  (8, 0.022905982905982905),\n","  (9, 0.06336182336182336)],\n"," [(0, 0.11170848267622462),\n","  (1, 0.05714854639585822),\n","  (2, 0.0160294703305456),\n","  (3, 0.020708880923934688),\n","  (4, 0.3515531660692951),\n","  (5, 0.012843488649940263),\n","  (6, 0.028076463560334528),\n","  (7, 0.03195937873357228),\n","  (8, 0.020111509358821188),\n","  (9, 0.3498606133014735)],\n"," [(0, 0.048846960167714885),\n","  (1, 0.022431865828092243),\n","  (2, 0.03417190775681342),\n","  (3, 0.040880503144654086),\n","  (4, 0.6723270440251572),\n","  (5, 0.05607966457023061),\n","  (6, 0.0279874213836478),\n","  (7, 0.057651991614255764),\n","  (8, 0.017924528301886792),\n","  (9, 0.02169811320754717)],\n"," [(0, 0.19128540305010897),\n","  (1, 0.11328976034858389),\n","  (2, 0.012055192447349311),\n","  (3, 0.08017429193899783),\n","  (4, 0.42425562817719686),\n","  (5, 0.017138707334785768),\n","  (6, 0.1131445170660857),\n","  (7, 0.02076978939724038),\n","  (8, 0.013071895424836603),\n","  (9, 0.014814814814814817)],\n"," [(0, 0.0152885443583118),\n","  (1, 0.10669681309216193),\n","  (2, 0.040267011197243754),\n","  (3, 0.0229328165374677),\n","  (4, 0.45650301464254955),\n","  (5, 0.22717484926787251),\n","  (6, 0.01442721791559001),\n","  (7, 0.0409130060292851),\n","  (8, 0.035637381567614125),\n","  (9, 0.04015934539190353)],\n"," ...]"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"klTqCABhbdzK","colab_type":"text"},"source":["You can clearly see that the model from MALLET is much better based on these\n","metrics as compared to the default LDA model from Gensim. Can we find the optimal number of topics that maximizes the coherence? This is a tough problem, but we can try doing it iteratively."]},{"cell_type":"markdown","metadata":{"id":"8MYYdImIqs3F","colab_type":"text"},"source":["### LDA Tuning: Finding the Optimal Number of Topics\n","\n","Finding the optimal number of topics in a topic model is tough, given that it is like a model hyperparameter that you always have to set before training the model. We can use an iterative approach and build several models with differing numbers of topics and select the one that has the highest coherence score. To implement this method, we build the following function."]},{"cell_type":"code","metadata":{"id":"tm-R0skIZ-TC","colab_type":"code","colab":{}},"source":["from tqdm import tqdm\n","\n","def topic_model_coherence_generator(corpus, texts, dictionary, start_topic_count=2, end_topic_count=10, step=1, cpus=1):\n","  models = []\n","  coherence_scores = []\n","  for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):\n","    mallet_lda_model = gensim.models.wrappers. LdaMallet (mallet_path=MALLET_PATH, corpus=corpus, num_topics=topic_nums,\n","                                                            id2word=dictionary, iterations=500, workers=cpus)\n","      \n","    cv_coherence_model_mallet_lda = gensim. models.CoherenceModel (model=mallet_lda_model, corpus=corpus, texts=texts,\n","                                                                     dictionary=dictionary, coherence='c_v')\n","      \n","    coherence_score = cv_coherence_model_mallet_lda.get_coherence()\n","    coherence_scores.append(coherence_score)\n","    models.append(mallet_lda_model)\n","  return models, coherence_scores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qMBpTPZ4rnhU","colab_type":"text"},"source":["* Let’s put this function into action now and build several topic models, with the number of topics ranging from 2 to 30."]},{"cell_type":"code","metadata":{"id":"j3YtnuMlZ-Pc","colab_type":"code","colab":{}},"source":["# lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_corpus, texts=norm_corpus_bigrams, dictionary=dictionary,\n","#                                                                start_topic_count=2, end_topic_count=30, step=1, cpus=16)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gp3l3jTxIPSr","colab_type":"code","colab":{}},"source":["# pickle.dump(lda_models, open(\"drive/My Drive/nipstxt/lda_models.pkl\", \"wb\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuhoDNx7IlXK","colab_type":"code","colab":{}},"source":["# pickle.dump(coherence_scores, open(\"drive/My Drive/nipstxt/coherence_scores.pkl\", \"wb\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"psSC4xnSI85h","colab_type":"code","colab":{}},"source":["lda_models = pickle.load(open(\"drive/My Drive/nipstxt/lda_models.pkl\", \"rb\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wu9aFVHEeZQZ","colab_type":"code","colab":{}},"source":["coherence_scores = pickle.load(open(\"drive/My Drive/nipstxt/coherence_scores.pkl\", \"rb\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5KUjERPZ-Nz","colab_type":"code","outputId":"fd59c36f-6d8e-4439-88e6-5a7c67c300cc","executionInfo":{"status":"ok","timestamp":1590836750366,"user_tz":-180,"elapsed":459970,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":343}},"source":["coherence_df = pd.DataFrame({'Number of Topics': range(2, 31, 1), 'Coherence Score': np.round(coherence_scores, 4)})\n","\n","coherence_df.sort_values(by=['Coherence Score'], ascending=False).head(10)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Number of Topics</th>\n","      <th>Coherence Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>15</th>\n","      <td>17</td>\n","      <td>0.5466</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>23</td>\n","      <td>0.5427</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>28</td>\n","      <td>0.5414</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>30</td>\n","      <td>0.5407</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>22</td>\n","      <td>0.5372</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>29</td>\n","      <td>0.5367</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>20</td>\n","      <td>0.5363</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>24</td>\n","      <td>0.5355</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>19</td>\n","      <td>0.5330</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>26</td>\n","      <td>0.5317</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Number of Topics  Coherence Score\n","15                17           0.5466\n","21                23           0.5427\n","26                28           0.5414\n","28                30           0.5407\n","20                22           0.5372\n","27                29           0.5367\n","18                20           0.5363\n","22                24           0.5355\n","17                19           0.5330\n","24                26           0.5317"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"HGVn8R-KZ-I0","colab_type":"code","outputId":"4bf5fefa-909a-4468-836b-717203721118","executionInfo":{"status":"ok","timestamp":1590836750367,"user_tz":-180,"elapsed":459956,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":420}},"source":["import matplotlib.pyplot as plt\n","plt.style.use('fivethirtyeight')\n","\n","\n","x_ax = range(2, 31, 1)\n","y_ax = coherence_scores\n","\n","plt.figure(figsize=(12, 6))\n","plt.plot(x_ax, y_ax, c='r')\n","\n","plt.axhline(y=0.535, c='k', linestyle='--', linewidth=2)\n","plt.rcParams['figure.facecolor'] = 'white'\n","\n","xl = plt.xlabel('Number of Topics')\n","yl = plt.ylabel('Coherence Score')\n","\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"7EbSBB6Ns3dT","colab_type":"text"},"source":["* it looks like the score starts increasing rapidly when the number of\n","topics is five and gradually starts plateauing at 19 or 20."]},{"cell_type":"code","metadata":{"id":"6--dZELtZ-Fu","colab_type":"code","outputId":"d46fa90d-0782-43f6-a148-3300f81fdffb","executionInfo":{"status":"ok","timestamp":1590836750368,"user_tz":-180,"elapsed":459939,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["best_model_idx = coherence_df[coherence_df['Number of Topics'] == 20].index[0]\n","best_lda_model = lda_models[best_model_idx]\n","best_lda_model.num_topics"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"dMEHZwK6tK4M","colab_type":"text"},"source":["* Let’s view all the 20 topics generated by our selected best model, similar to our\n","previous models."]},{"cell_type":"code","metadata":{"id":"OxtniEE-tCqY","colab_type":"code","outputId":"6e2837a5-3802-4a0e-b036-4b8d6d91b006","executionInfo":{"status":"ok","timestamp":1590836750368,"user_tz":-180,"elapsed":459926,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["topics = [[(term, round(wt, 3)) for term, wt in best_lda_model.show_topic(n, topn=20)] for n in range(0, best_lda_model.num_topics)]\n","\n","for idx, topic in enumerate(topics):\n","  print('Topic #'+str(idx+1)+':')\n","  print([term for term, wt in topic])\n","  print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Topic #1:\n","['distribution', 'probability', 'prior', 'variable', 'gaussian', 'mixture', 'estimate', 'density', 'bayesian', 'approximation', 'likelihood', 'sample', 'log', 'expert', 'em', 'estimation', 'posterior', 'step', 'component', 'probabilistic']\n","\n","Topic #2:\n","['training', 'prediction', 'kernel', 'test', 'training_set', 'regression', 'estimate', 'selection', 'machine', 'experiment', 'sample', 'test_set', 'cross_validation', 'measure', 'ensemble', 'regularization', 'variance', 'margin', 'risk', 'pruning']\n","\n","Topic #3:\n","['circuit', 'chip', 'current', 'analog', 'voltage', 'neuron', 'implementation', 'processor', 'bit', 'design', 'device', 'computation', 'array', 'parallel', 'neural', 'digital', 'synapse', 'operation', 'hardware', 'transistor']\n","\n","Topic #4:\n","['image', 'feature', 'object', 'pixel', 'face', 'view', 'recognition', 'representation', 'scale', 'contour', 'surface', 'edge', 'shape', 'visual', 'part', 'scene', 'vision', 'digit', 'local', 'texture']\n","\n","Topic #5:\n","['cell', 'response', 'activity', 'stimulus', 'neuron', 'pattern', 'cortical', 'layer', 'receptive_field', 'cortex', 'connection', 'orientation', 'unit', 'visual', 'spatial', 'contrast', 'simulation', 'mechanism', 'population', 'synaptic']\n","\n","Topic #6:\n","['motion', 'direction', 'position', 'visual', 'target', 'control', 'field', 'velocity', 'movement', 'motor', 'trajectory', 'hand', 'location', 'arm', 'moving', 'human', 'response', 'spatial', 'feedback', 'sensory']\n","\n","Topic #7:\n","['equation', 'solution', 'convergence', 'gradient', 'vector', 'constraint', 'energy', 'iteration', 'rate', 'optimization', 'update', 'minimum', 'optimal', 'constant', 'gradient_descent', 'eq', 'step', 'matrix', 'condition', 'derivative']\n","\n","Topic #8:\n","['bound', 'theorem', 'class', 'threshold', 'approximation', 'proof', 'size', 'probability', 'loss', 'complexity', 'polynomial', 'theory', 'assume', 'linear', 'hypothesis', 'defined', 'definition', 'define', 'xi', 'constant']\n","\n","Topic #9:\n","['vector', 'linear', 'matrix', 'component', 'nonlinear', 'signal', 'source', 'filter', 'coefficient', 'operator', 'basis', 'transformation', 'pca', 'ica', 'projection', 'gaussian', 'representation', 'principal_component', 'rule', 'independent']\n","\n","Topic #10:\n","['map', 'region', 'subject', 'location', 'effect', 'change', 'study', 'condition', 'light', 'experiment', 'et_al', 'brain', 'pair', 'normal', 'correlation', 'trial', 'eeg', 'site', 'left', 'theory']\n","\n","Topic #11:\n","['state', 'dynamic', 'neuron', 'memory', 'pattern', 'recurrent', 'attractor', 'module', 'capacity', 'connection', 'phase', 'hopfield', 'fixed_point', 'delay', 'behavior', 'neural', 'oscillator', 'stable', 'oscillation', 'sequence']\n","\n","Topic #12:\n","['unit', 'training', 'layer', 'hidden_unit', 'net', 'architecture', 'pattern', 'trained', 'task', 'activation', 'back_propagation', 'hidden_layer', 'training_set', 'hidden', 'connection', 'learn', 'backpropagation', 'generalization', 'epoch', 'train']\n","\n","Topic #13:\n","['rule', 'representation', 'structure', 'sequence', 'symbol', 'connectionist', 'language', 'level', 'unit', 'string', 'activation', 'context', 'pattern', 'role', 'similarity', 'represented', 'task', 'learned', 'note', 'part']\n","\n","Topic #14:\n","['node', 'class', 'classification', 'classifier', 'pattern', 'tree', 'vector', 'code', 'probability', 'feature', 'bit', 'sample', 'label', 'binary', 'decision', 'stage', 'labeled', 'decision_tree', 'technique', 'error_rate']\n","\n","Topic #15:\n","['neuron', 'signal', 'frequency', 'spike', 'channel', 'response', 'firing', 'temporal', 'stimulus', 'threshold', 'current', 'rate', 'neural', 'synaptic', 'auditory', 'event', 'amplitude', 'sound', 'phase', 'firing_rate']\n","\n","Topic #16:\n","['search', 'rate', 'experiment', 'task', 'strategy', 'table', 'application', 'user', 'instance', 'test', 'run', 'random', 'call', 'technique', 'average', 'block', 'feature', 'high', 'good', 'program']\n","\n","Topic #17:\n","['word', 'recognition', 'speech', 'training', 'character', 'sequence', 'hmm', 'context', 'speaker', 'feature', 'frame', 'mlp', 'letter', 'trained', 'state', 'experiment', 'speech_recognition', 'phoneme', 'window', 'segmentation']\n","\n","Topic #18:\n","['noise', 'average', 'distribution', 'curve', 'equation', 'correlation', 'theory', 'rate', 'limit', 'stochastic', 'optimal', 'random', 'solution', 'size', 'teacher', 'effect', 'simulation', 'student', 'eq', 'obtained']\n","\n","Topic #19:\n","['state', 'control', 'action', 'step', 'policy', 'controller', 'reinforcement_learning', 'environment', 'task', 'optimal', 'goal', 'robot', 'reward', 'td', 'agent', 'trial', 'transition', 'current', 'reinforcement', 'rl']\n","\n","Topic #20:\n","['local', 'distance', 'cluster', 'structure', 'graph', 'clustering', 'variable', 'dimensional', 'mapping', 'global', 'dimension', 'center', 'vector', 'cost', 'partition', 'neighborhood', 'prototype', 'constraint', 'interpolation', 'manifold']\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wtRr1f4-tww0","colab_type":"text"},"source":["* A better way of visualizing the topics is to build a term-topic dataframe, as depicted."]},{"cell_type":"code","metadata":{"id":"5zAadDaftCmu","colab_type":"code","outputId":"03546d5f-1ca1-40db-de91-ea8be79ef178","executionInfo":{"status":"ok","timestamp":1590836750909,"user_tz":-180,"elapsed":460458,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":659}},"source":["topics_df = pd.DataFrame([[term for term, wt in topic] for topic in topics], columns = ['Term'+str(i) for i in range(1, 21)],\n","                         index=['Topic '+str(t) for t in range(1, best_lda_model.num_topics+1)]).T\n","topics_df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Topic 1</th>\n","      <th>Topic 2</th>\n","      <th>Topic 3</th>\n","      <th>Topic 4</th>\n","      <th>Topic 5</th>\n","      <th>Topic 6</th>\n","      <th>Topic 7</th>\n","      <th>Topic 8</th>\n","      <th>Topic 9</th>\n","      <th>Topic 10</th>\n","      <th>Topic 11</th>\n","      <th>Topic 12</th>\n","      <th>Topic 13</th>\n","      <th>Topic 14</th>\n","      <th>Topic 15</th>\n","      <th>Topic 16</th>\n","      <th>Topic 17</th>\n","      <th>Topic 18</th>\n","      <th>Topic 19</th>\n","      <th>Topic 20</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Term1</th>\n","      <td>distribution</td>\n","      <td>training</td>\n","      <td>circuit</td>\n","      <td>image</td>\n","      <td>cell</td>\n","      <td>motion</td>\n","      <td>equation</td>\n","      <td>bound</td>\n","      <td>vector</td>\n","      <td>map</td>\n","      <td>state</td>\n","      <td>unit</td>\n","      <td>rule</td>\n","      <td>node</td>\n","      <td>neuron</td>\n","      <td>search</td>\n","      <td>word</td>\n","      <td>noise</td>\n","      <td>state</td>\n","      <td>local</td>\n","    </tr>\n","    <tr>\n","      <th>Term2</th>\n","      <td>probability</td>\n","      <td>prediction</td>\n","      <td>chip</td>\n","      <td>feature</td>\n","      <td>response</td>\n","      <td>direction</td>\n","      <td>solution</td>\n","      <td>theorem</td>\n","      <td>linear</td>\n","      <td>region</td>\n","      <td>dynamic</td>\n","      <td>training</td>\n","      <td>representation</td>\n","      <td>class</td>\n","      <td>signal</td>\n","      <td>rate</td>\n","      <td>recognition</td>\n","      <td>average</td>\n","      <td>control</td>\n","      <td>distance</td>\n","    </tr>\n","    <tr>\n","      <th>Term3</th>\n","      <td>prior</td>\n","      <td>kernel</td>\n","      <td>current</td>\n","      <td>object</td>\n","      <td>activity</td>\n","      <td>position</td>\n","      <td>convergence</td>\n","      <td>class</td>\n","      <td>matrix</td>\n","      <td>subject</td>\n","      <td>neuron</td>\n","      <td>layer</td>\n","      <td>structure</td>\n","      <td>classification</td>\n","      <td>frequency</td>\n","      <td>experiment</td>\n","      <td>speech</td>\n","      <td>distribution</td>\n","      <td>action</td>\n","      <td>cluster</td>\n","    </tr>\n","    <tr>\n","      <th>Term4</th>\n","      <td>variable</td>\n","      <td>test</td>\n","      <td>analog</td>\n","      <td>pixel</td>\n","      <td>stimulus</td>\n","      <td>visual</td>\n","      <td>gradient</td>\n","      <td>threshold</td>\n","      <td>component</td>\n","      <td>location</td>\n","      <td>memory</td>\n","      <td>hidden_unit</td>\n","      <td>sequence</td>\n","      <td>classifier</td>\n","      <td>spike</td>\n","      <td>task</td>\n","      <td>training</td>\n","      <td>curve</td>\n","      <td>step</td>\n","      <td>structure</td>\n","    </tr>\n","    <tr>\n","      <th>Term5</th>\n","      <td>gaussian</td>\n","      <td>training_set</td>\n","      <td>voltage</td>\n","      <td>face</td>\n","      <td>neuron</td>\n","      <td>target</td>\n","      <td>vector</td>\n","      <td>approximation</td>\n","      <td>nonlinear</td>\n","      <td>effect</td>\n","      <td>pattern</td>\n","      <td>net</td>\n","      <td>symbol</td>\n","      <td>pattern</td>\n","      <td>channel</td>\n","      <td>strategy</td>\n","      <td>character</td>\n","      <td>equation</td>\n","      <td>policy</td>\n","      <td>graph</td>\n","    </tr>\n","    <tr>\n","      <th>Term6</th>\n","      <td>mixture</td>\n","      <td>regression</td>\n","      <td>neuron</td>\n","      <td>view</td>\n","      <td>pattern</td>\n","      <td>control</td>\n","      <td>constraint</td>\n","      <td>proof</td>\n","      <td>signal</td>\n","      <td>change</td>\n","      <td>recurrent</td>\n","      <td>architecture</td>\n","      <td>connectionist</td>\n","      <td>tree</td>\n","      <td>response</td>\n","      <td>table</td>\n","      <td>sequence</td>\n","      <td>correlation</td>\n","      <td>controller</td>\n","      <td>clustering</td>\n","    </tr>\n","    <tr>\n","      <th>Term7</th>\n","      <td>estimate</td>\n","      <td>estimate</td>\n","      <td>implementation</td>\n","      <td>recognition</td>\n","      <td>cortical</td>\n","      <td>field</td>\n","      <td>energy</td>\n","      <td>size</td>\n","      <td>source</td>\n","      <td>study</td>\n","      <td>attractor</td>\n","      <td>pattern</td>\n","      <td>language</td>\n","      <td>vector</td>\n","      <td>firing</td>\n","      <td>application</td>\n","      <td>hmm</td>\n","      <td>theory</td>\n","      <td>reinforcement_learning</td>\n","      <td>variable</td>\n","    </tr>\n","    <tr>\n","      <th>Term8</th>\n","      <td>density</td>\n","      <td>selection</td>\n","      <td>processor</td>\n","      <td>representation</td>\n","      <td>layer</td>\n","      <td>velocity</td>\n","      <td>iteration</td>\n","      <td>probability</td>\n","      <td>filter</td>\n","      <td>condition</td>\n","      <td>module</td>\n","      <td>trained</td>\n","      <td>level</td>\n","      <td>code</td>\n","      <td>temporal</td>\n","      <td>user</td>\n","      <td>context</td>\n","      <td>rate</td>\n","      <td>environment</td>\n","      <td>dimensional</td>\n","    </tr>\n","    <tr>\n","      <th>Term9</th>\n","      <td>bayesian</td>\n","      <td>machine</td>\n","      <td>bit</td>\n","      <td>scale</td>\n","      <td>receptive_field</td>\n","      <td>movement</td>\n","      <td>rate</td>\n","      <td>loss</td>\n","      <td>coefficient</td>\n","      <td>light</td>\n","      <td>capacity</td>\n","      <td>task</td>\n","      <td>unit</td>\n","      <td>probability</td>\n","      <td>stimulus</td>\n","      <td>instance</td>\n","      <td>speaker</td>\n","      <td>limit</td>\n","      <td>task</td>\n","      <td>mapping</td>\n","    </tr>\n","    <tr>\n","      <th>Term10</th>\n","      <td>approximation</td>\n","      <td>experiment</td>\n","      <td>design</td>\n","      <td>contour</td>\n","      <td>cortex</td>\n","      <td>motor</td>\n","      <td>optimization</td>\n","      <td>complexity</td>\n","      <td>operator</td>\n","      <td>experiment</td>\n","      <td>connection</td>\n","      <td>activation</td>\n","      <td>string</td>\n","      <td>feature</td>\n","      <td>threshold</td>\n","      <td>test</td>\n","      <td>feature</td>\n","      <td>stochastic</td>\n","      <td>optimal</td>\n","      <td>global</td>\n","    </tr>\n","    <tr>\n","      <th>Term11</th>\n","      <td>likelihood</td>\n","      <td>sample</td>\n","      <td>device</td>\n","      <td>surface</td>\n","      <td>connection</td>\n","      <td>trajectory</td>\n","      <td>update</td>\n","      <td>polynomial</td>\n","      <td>basis</td>\n","      <td>et_al</td>\n","      <td>phase</td>\n","      <td>back_propagation</td>\n","      <td>activation</td>\n","      <td>bit</td>\n","      <td>current</td>\n","      <td>run</td>\n","      <td>frame</td>\n","      <td>optimal</td>\n","      <td>goal</td>\n","      <td>dimension</td>\n","    </tr>\n","    <tr>\n","      <th>Term12</th>\n","      <td>sample</td>\n","      <td>test_set</td>\n","      <td>computation</td>\n","      <td>edge</td>\n","      <td>orientation</td>\n","      <td>hand</td>\n","      <td>minimum</td>\n","      <td>theory</td>\n","      <td>transformation</td>\n","      <td>brain</td>\n","      <td>hopfield</td>\n","      <td>hidden_layer</td>\n","      <td>context</td>\n","      <td>sample</td>\n","      <td>rate</td>\n","      <td>random</td>\n","      <td>mlp</td>\n","      <td>random</td>\n","      <td>robot</td>\n","      <td>center</td>\n","    </tr>\n","    <tr>\n","      <th>Term13</th>\n","      <td>log</td>\n","      <td>cross_validation</td>\n","      <td>array</td>\n","      <td>shape</td>\n","      <td>unit</td>\n","      <td>location</td>\n","      <td>optimal</td>\n","      <td>assume</td>\n","      <td>pca</td>\n","      <td>pair</td>\n","      <td>fixed_point</td>\n","      <td>training_set</td>\n","      <td>pattern</td>\n","      <td>label</td>\n","      <td>neural</td>\n","      <td>call</td>\n","      <td>letter</td>\n","      <td>solution</td>\n","      <td>reward</td>\n","      <td>vector</td>\n","    </tr>\n","    <tr>\n","      <th>Term14</th>\n","      <td>expert</td>\n","      <td>measure</td>\n","      <td>parallel</td>\n","      <td>visual</td>\n","      <td>visual</td>\n","      <td>arm</td>\n","      <td>constant</td>\n","      <td>linear</td>\n","      <td>ica</td>\n","      <td>normal</td>\n","      <td>delay</td>\n","      <td>hidden</td>\n","      <td>role</td>\n","      <td>binary</td>\n","      <td>synaptic</td>\n","      <td>technique</td>\n","      <td>trained</td>\n","      <td>size</td>\n","      <td>td</td>\n","      <td>cost</td>\n","    </tr>\n","    <tr>\n","      <th>Term15</th>\n","      <td>em</td>\n","      <td>ensemble</td>\n","      <td>neural</td>\n","      <td>part</td>\n","      <td>spatial</td>\n","      <td>moving</td>\n","      <td>gradient_descent</td>\n","      <td>hypothesis</td>\n","      <td>projection</td>\n","      <td>correlation</td>\n","      <td>behavior</td>\n","      <td>connection</td>\n","      <td>similarity</td>\n","      <td>decision</td>\n","      <td>auditory</td>\n","      <td>average</td>\n","      <td>state</td>\n","      <td>teacher</td>\n","      <td>agent</td>\n","      <td>partition</td>\n","    </tr>\n","    <tr>\n","      <th>Term16</th>\n","      <td>estimation</td>\n","      <td>regularization</td>\n","      <td>digital</td>\n","      <td>scene</td>\n","      <td>contrast</td>\n","      <td>human</td>\n","      <td>eq</td>\n","      <td>defined</td>\n","      <td>gaussian</td>\n","      <td>trial</td>\n","      <td>neural</td>\n","      <td>learn</td>\n","      <td>represented</td>\n","      <td>stage</td>\n","      <td>event</td>\n","      <td>block</td>\n","      <td>experiment</td>\n","      <td>effect</td>\n","      <td>trial</td>\n","      <td>neighborhood</td>\n","    </tr>\n","    <tr>\n","      <th>Term17</th>\n","      <td>posterior</td>\n","      <td>variance</td>\n","      <td>synapse</td>\n","      <td>vision</td>\n","      <td>simulation</td>\n","      <td>response</td>\n","      <td>step</td>\n","      <td>definition</td>\n","      <td>representation</td>\n","      <td>eeg</td>\n","      <td>oscillator</td>\n","      <td>backpropagation</td>\n","      <td>task</td>\n","      <td>labeled</td>\n","      <td>amplitude</td>\n","      <td>feature</td>\n","      <td>speech_recognition</td>\n","      <td>simulation</td>\n","      <td>transition</td>\n","      <td>prototype</td>\n","    </tr>\n","    <tr>\n","      <th>Term18</th>\n","      <td>step</td>\n","      <td>margin</td>\n","      <td>operation</td>\n","      <td>digit</td>\n","      <td>mechanism</td>\n","      <td>spatial</td>\n","      <td>matrix</td>\n","      <td>define</td>\n","      <td>principal_component</td>\n","      <td>site</td>\n","      <td>stable</td>\n","      <td>generalization</td>\n","      <td>learned</td>\n","      <td>decision_tree</td>\n","      <td>sound</td>\n","      <td>high</td>\n","      <td>phoneme</td>\n","      <td>student</td>\n","      <td>current</td>\n","      <td>constraint</td>\n","    </tr>\n","    <tr>\n","      <th>Term19</th>\n","      <td>component</td>\n","      <td>risk</td>\n","      <td>hardware</td>\n","      <td>local</td>\n","      <td>population</td>\n","      <td>feedback</td>\n","      <td>condition</td>\n","      <td>xi</td>\n","      <td>rule</td>\n","      <td>left</td>\n","      <td>oscillation</td>\n","      <td>epoch</td>\n","      <td>note</td>\n","      <td>technique</td>\n","      <td>phase</td>\n","      <td>good</td>\n","      <td>window</td>\n","      <td>eq</td>\n","      <td>reinforcement</td>\n","      <td>interpolation</td>\n","    </tr>\n","    <tr>\n","      <th>Term20</th>\n","      <td>probabilistic</td>\n","      <td>pruning</td>\n","      <td>transistor</td>\n","      <td>texture</td>\n","      <td>synaptic</td>\n","      <td>sensory</td>\n","      <td>derivative</td>\n","      <td>constant</td>\n","      <td>independent</td>\n","      <td>theory</td>\n","      <td>sequence</td>\n","      <td>train</td>\n","      <td>part</td>\n","      <td>error_rate</td>\n","      <td>firing_rate</td>\n","      <td>program</td>\n","      <td>segmentation</td>\n","      <td>obtained</td>\n","      <td>rl</td>\n","      <td>manifold</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              Topic 1           Topic 2  ...                Topic 19       Topic 20\n","Term1    distribution          training  ...                   state          local\n","Term2     probability        prediction  ...                 control       distance\n","Term3           prior            kernel  ...                  action        cluster\n","Term4        variable              test  ...                    step      structure\n","Term5        gaussian      training_set  ...                  policy          graph\n","Term6         mixture        regression  ...              controller     clustering\n","Term7        estimate          estimate  ...  reinforcement_learning       variable\n","Term8         density         selection  ...             environment    dimensional\n","Term9        bayesian           machine  ...                    task        mapping\n","Term10  approximation        experiment  ...                 optimal         global\n","Term11     likelihood            sample  ...                    goal      dimension\n","Term12         sample          test_set  ...                   robot         center\n","Term13            log  cross_validation  ...                  reward         vector\n","Term14         expert           measure  ...                      td           cost\n","Term15             em          ensemble  ...                   agent      partition\n","Term16     estimation    regularization  ...                   trial   neighborhood\n","Term17      posterior          variance  ...              transition      prototype\n","Term18           step            margin  ...                 current     constraint\n","Term19      component              risk  ...           reinforcement  interpolation\n","Term20  probabilistic           pruning  ...                      rl       manifold\n","\n","[20 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"e3SW05MSuA2M","colab_type":"text"},"source":["* Another easy way to view the topics is to create a topic-term dataframe, whereby each topic is represented in a row with the terms of the topic being represented as a comma-separated string."]},{"cell_type":"code","metadata":{"id":"cN6bHjnVtCkC","colab_type":"code","outputId":"8cfdf791-c3a6-48c9-942c-e9cb02728778","executionInfo":{"status":"ok","timestamp":1590836750911,"user_tz":-180,"elapsed":460452,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":693}},"source":["pd.set_option('display.max_colwidth', -1)\n","\n","topics_df = pd.DataFrame([', '.join([term for term, wt in topic]) for topic in topics], columns = ['Terms per Topic'],\n","                         index=['Topic'+str(t) for t in range(1, best_lda_model.num_topics+1)] )\n","\n","topics_df"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Terms per Topic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Topic1</th>\n","      <td>distribution, probability, prior, variable, gaussian, mixture, estimate, density, bayesian, approximation, likelihood, sample, log, expert, em, estimation, posterior, step, component, probabilistic</td>\n","    </tr>\n","    <tr>\n","      <th>Topic2</th>\n","      <td>training, prediction, kernel, test, training_set, regression, estimate, selection, machine, experiment, sample, test_set, cross_validation, measure, ensemble, regularization, variance, margin, risk, pruning</td>\n","    </tr>\n","    <tr>\n","      <th>Topic3</th>\n","      <td>circuit, chip, current, analog, voltage, neuron, implementation, processor, bit, design, device, computation, array, parallel, neural, digital, synapse, operation, hardware, transistor</td>\n","    </tr>\n","    <tr>\n","      <th>Topic4</th>\n","      <td>image, feature, object, pixel, face, view, recognition, representation, scale, contour, surface, edge, shape, visual, part, scene, vision, digit, local, texture</td>\n","    </tr>\n","    <tr>\n","      <th>Topic5</th>\n","      <td>cell, response, activity, stimulus, neuron, pattern, cortical, layer, receptive_field, cortex, connection, orientation, unit, visual, spatial, contrast, simulation, mechanism, population, synaptic</td>\n","    </tr>\n","    <tr>\n","      <th>Topic6</th>\n","      <td>motion, direction, position, visual, target, control, field, velocity, movement, motor, trajectory, hand, location, arm, moving, human, response, spatial, feedback, sensory</td>\n","    </tr>\n","    <tr>\n","      <th>Topic7</th>\n","      <td>equation, solution, convergence, gradient, vector, constraint, energy, iteration, rate, optimization, update, minimum, optimal, constant, gradient_descent, eq, step, matrix, condition, derivative</td>\n","    </tr>\n","    <tr>\n","      <th>Topic8</th>\n","      <td>bound, theorem, class, threshold, approximation, proof, size, probability, loss, complexity, polynomial, theory, assume, linear, hypothesis, defined, definition, define, xi, constant</td>\n","    </tr>\n","    <tr>\n","      <th>Topic9</th>\n","      <td>vector, linear, matrix, component, nonlinear, signal, source, filter, coefficient, operator, basis, transformation, pca, ica, projection, gaussian, representation, principal_component, rule, independent</td>\n","    </tr>\n","    <tr>\n","      <th>Topic10</th>\n","      <td>map, region, subject, location, effect, change, study, condition, light, experiment, et_al, brain, pair, normal, correlation, trial, eeg, site, left, theory</td>\n","    </tr>\n","    <tr>\n","      <th>Topic11</th>\n","      <td>state, dynamic, neuron, memory, pattern, recurrent, attractor, module, capacity, connection, phase, hopfield, fixed_point, delay, behavior, neural, oscillator, stable, oscillation, sequence</td>\n","    </tr>\n","    <tr>\n","      <th>Topic12</th>\n","      <td>unit, training, layer, hidden_unit, net, architecture, pattern, trained, task, activation, back_propagation, hidden_layer, training_set, hidden, connection, learn, backpropagation, generalization, epoch, train</td>\n","    </tr>\n","    <tr>\n","      <th>Topic13</th>\n","      <td>rule, representation, structure, sequence, symbol, connectionist, language, level, unit, string, activation, context, pattern, role, similarity, represented, task, learned, note, part</td>\n","    </tr>\n","    <tr>\n","      <th>Topic14</th>\n","      <td>node, class, classification, classifier, pattern, tree, vector, code, probability, feature, bit, sample, label, binary, decision, stage, labeled, decision_tree, technique, error_rate</td>\n","    </tr>\n","    <tr>\n","      <th>Topic15</th>\n","      <td>neuron, signal, frequency, spike, channel, response, firing, temporal, stimulus, threshold, current, rate, neural, synaptic, auditory, event, amplitude, sound, phase, firing_rate</td>\n","    </tr>\n","    <tr>\n","      <th>Topic16</th>\n","      <td>search, rate, experiment, task, strategy, table, application, user, instance, test, run, random, call, technique, average, block, feature, high, good, program</td>\n","    </tr>\n","    <tr>\n","      <th>Topic17</th>\n","      <td>word, recognition, speech, training, character, sequence, hmm, context, speaker, feature, frame, mlp, letter, trained, state, experiment, speech_recognition, phoneme, window, segmentation</td>\n","    </tr>\n","    <tr>\n","      <th>Topic18</th>\n","      <td>noise, average, distribution, curve, equation, correlation, theory, rate, limit, stochastic, optimal, random, solution, size, teacher, effect, simulation, student, eq, obtained</td>\n","    </tr>\n","    <tr>\n","      <th>Topic19</th>\n","      <td>state, control, action, step, policy, controller, reinforcement_learning, environment, task, optimal, goal, robot, reward, td, agent, trial, transition, current, reinforcement, rl</td>\n","    </tr>\n","    <tr>\n","      <th>Topic20</th>\n","      <td>local, distance, cluster, structure, graph, clustering, variable, dimensional, mapping, global, dimension, center, vector, cost, partition, neighborhood, prototype, constraint, interpolation, manifold</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                                                                                                           Terms per Topic\n","Topic1   distribution, probability, prior, variable, gaussian, mixture, estimate, density, bayesian, approximation, likelihood, sample, log, expert, em, estimation, posterior, step, component, probabilistic            \n","Topic2   training, prediction, kernel, test, training_set, regression, estimate, selection, machine, experiment, sample, test_set, cross_validation, measure, ensemble, regularization, variance, margin, risk, pruning   \n","Topic3   circuit, chip, current, analog, voltage, neuron, implementation, processor, bit, design, device, computation, array, parallel, neural, digital, synapse, operation, hardware, transistor                         \n","Topic4   image, feature, object, pixel, face, view, recognition, representation, scale, contour, surface, edge, shape, visual, part, scene, vision, digit, local, texture                                                 \n","Topic5   cell, response, activity, stimulus, neuron, pattern, cortical, layer, receptive_field, cortex, connection, orientation, unit, visual, spatial, contrast, simulation, mechanism, population, synaptic             \n","Topic6   motion, direction, position, visual, target, control, field, velocity, movement, motor, trajectory, hand, location, arm, moving, human, response, spatial, feedback, sensory                                     \n","Topic7   equation, solution, convergence, gradient, vector, constraint, energy, iteration, rate, optimization, update, minimum, optimal, constant, gradient_descent, eq, step, matrix, condition, derivative              \n","Topic8   bound, theorem, class, threshold, approximation, proof, size, probability, loss, complexity, polynomial, theory, assume, linear, hypothesis, defined, definition, define, xi, constant                           \n","Topic9   vector, linear, matrix, component, nonlinear, signal, source, filter, coefficient, operator, basis, transformation, pca, ica, projection, gaussian, representation, principal_component, rule, independent       \n","Topic10  map, region, subject, location, effect, change, study, condition, light, experiment, et_al, brain, pair, normal, correlation, trial, eeg, site, left, theory                                                     \n","Topic11  state, dynamic, neuron, memory, pattern, recurrent, attractor, module, capacity, connection, phase, hopfield, fixed_point, delay, behavior, neural, oscillator, stable, oscillation, sequence                    \n","Topic12  unit, training, layer, hidden_unit, net, architecture, pattern, trained, task, activation, back_propagation, hidden_layer, training_set, hidden, connection, learn, backpropagation, generalization, epoch, train\n","Topic13  rule, representation, structure, sequence, symbol, connectionist, language, level, unit, string, activation, context, pattern, role, similarity, represented, task, learned, note, part                          \n","Topic14  node, class, classification, classifier, pattern, tree, vector, code, probability, feature, bit, sample, label, binary, decision, stage, labeled, decision_tree, technique, error_rate                           \n","Topic15  neuron, signal, frequency, spike, channel, response, firing, temporal, stimulus, threshold, current, rate, neural, synaptic, auditory, event, amplitude, sound, phase, firing_rate                               \n","Topic16  search, rate, experiment, task, strategy, table, application, user, instance, test, run, random, call, technique, average, block, feature, high, good, program                                                   \n","Topic17  word, recognition, speech, training, character, sequence, hmm, context, speaker, feature, frame, mlp, letter, trained, state, experiment, speech_recognition, phoneme, window, segmentation                      \n","Topic18  noise, average, distribution, curve, equation, correlation, theory, rate, limit, stochastic, optimal, random, solution, size, teacher, effect, simulation, student, eq, obtained                                 \n","Topic19  state, control, action, step, policy, controller, reinforcement_learning, environment, task, optimal, goal, robot, reward, td, agent, trial, transition, current, reinforcement, rl                              \n","Topic20  local, distance, cluster, structure, graph, clustering, variable, dimensional, mapping, global, dimension, center, vector, cost, partition, neighborhood, prototype, constraint, interpolation, manifold         "]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"t8O5dMlifOpG","colab_type":"text"},"source":["### Interpreting Topic Model Results\n","\n","An interesting point to remember is, given a corpus of documents (in the form of\n","features, e.g., Bag of Words) and a trained topic model, you can predict the distribution of topics in each document (research paper in this case) with the following code."]},{"cell_type":"code","metadata":{"id":"1v49CrZqvvq_","colab_type":"code","outputId":"ce7625e9-1040-483c-9f0d-0789311a5fab","executionInfo":{"status":"error","timestamp":1590836753848,"user_tz":-180,"elapsed":463381,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["tm_results = best_lda_model[bow_corpus]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"error","ename":"CalledProcessError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-2565cfa22815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtm_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_lda_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, bow, iterations)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmallet_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' infer-topics --input %s --inferencer %s '\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mconvert_input\u001b[0;34m(self, corpus, infer, serialize_corpus)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcorpusmallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"converting temporary corpus to MALLET format with %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m   1877\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1879\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1880\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCalledProcessError\u001b[0m: Command '/content/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input /tmp/9367bc_corpus.txt --output /tmp/9367bc_corpus.mallet.infer --use-pipe-from /tmp/9367bc_corpus.mallet' returned non-zero exit status 1."]}]},{"cell_type":"code","metadata":{"id":"kfYREv_tw09l","colab_type":"code","outputId":"ee82fb45-f09d-411b-e18e-755f8349c6f6","executionInfo":{"status":"ok","timestamp":1590836794459,"user_tz":-180,"elapsed":686,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":709}},"source":["best_lda_model.show_topics(num_topics=20)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0,\n","  '0.023*\"distribution\" + 0.017*\"probability\" + 0.016*\"prior\" + 0.014*\"variable\" + 0.013*\"gaussian\" + 0.013*\"mixture\" + 0.012*\"estimate\" + 0.012*\"density\" + 0.011*\"bayesian\" + 0.009*\"approximation\"'),\n"," (1,\n","  '0.035*\"training\" + 0.026*\"prediction\" + 0.013*\"kernel\" + 0.013*\"test\" + 0.013*\"training_set\" + 0.012*\"regression\" + 0.010*\"estimate\" + 0.008*\"selection\" + 0.007*\"machine\" + 0.007*\"experiment\"'),\n"," (2,\n","  '0.029*\"circuit\" + 0.021*\"chip\" + 0.017*\"current\" + 0.016*\"analog\" + 0.014*\"voltage\" + 0.014*\"neuron\" + 0.010*\"implementation\" + 0.009*\"processor\" + 0.009*\"bit\" + 0.008*\"design\"'),\n"," (3,\n","  '0.078*\"image\" + 0.031*\"feature\" + 0.030*\"object\" + 0.018*\"pixel\" + 0.013*\"face\" + 0.012*\"view\" + 0.009*\"recognition\" + 0.008*\"representation\" + 0.008*\"scale\" + 0.007*\"contour\"'),\n"," (4,\n","  '0.052*\"cell\" + 0.020*\"response\" + 0.019*\"activity\" + 0.019*\"stimulus\" + 0.017*\"neuron\" + 0.015*\"pattern\" + 0.011*\"cortical\" + 0.011*\"layer\" + 0.011*\"receptive_field\" + 0.010*\"cortex\"'),\n"," (5,\n","  '0.027*\"motion\" + 0.021*\"direction\" + 0.020*\"position\" + 0.017*\"visual\" + 0.017*\"target\" + 0.015*\"control\" + 0.014*\"field\" + 0.013*\"velocity\" + 0.012*\"movement\" + 0.011*\"motor\"'),\n"," (6,\n","  '0.021*\"equation\" + 0.021*\"solution\" + 0.018*\"convergence\" + 0.016*\"gradient\" + 0.013*\"vector\" + 0.013*\"constraint\" + 0.011*\"energy\" + 0.010*\"iteration\" + 0.010*\"rate\" + 0.010*\"optimization\"'),\n"," (7,\n","  '0.016*\"bound\" + 0.013*\"theorem\" + 0.011*\"class\" + 0.009*\"threshold\" + 0.009*\"approximation\" + 0.008*\"proof\" + 0.008*\"size\" + 0.008*\"probability\" + 0.008*\"loss\" + 0.008*\"complexity\"'),\n"," (8,\n","  '0.040*\"vector\" + 0.029*\"linear\" + 0.029*\"matrix\" + 0.019*\"component\" + 0.015*\"nonlinear\" + 0.014*\"signal\" + 0.013*\"source\" + 0.011*\"filter\" + 0.010*\"coefficient\" + 0.008*\"operator\"'),\n"," (9,\n","  '0.033*\"map\" + 0.022*\"region\" + 0.011*\"subject\" + 0.011*\"location\" + 0.010*\"effect\" + 0.010*\"change\" + 0.009*\"study\" + 0.007*\"condition\" + 0.007*\"light\" + 0.006*\"experiment\"'),\n"," (10,\n","  '0.041*\"state\" + 0.031*\"dynamic\" + 0.031*\"neuron\" + 0.024*\"memory\" + 0.018*\"pattern\" + 0.017*\"recurrent\" + 0.010*\"attractor\" + 0.010*\"module\" + 0.009*\"capacity\" + 0.009*\"connection\"'),\n"," (11,\n","  '0.074*\"unit\" + 0.041*\"training\" + 0.036*\"layer\" + 0.027*\"hidden_unit\" + 0.027*\"net\" + 0.021*\"architecture\" + 0.020*\"pattern\" + 0.017*\"trained\" + 0.014*\"task\" + 0.012*\"activation\"'),\n"," (12,\n","  '0.033*\"rule\" + 0.031*\"representation\" + 0.017*\"structure\" + 0.014*\"sequence\" + 0.009*\"symbol\" + 0.009*\"connectionist\" + 0.008*\"language\" + 0.008*\"level\" + 0.008*\"unit\" + 0.008*\"string\"'),\n"," (13,\n","  '0.052*\"node\" + 0.043*\"class\" + 0.035*\"classification\" + 0.033*\"classifier\" + 0.022*\"pattern\" + 0.020*\"tree\" + 0.017*\"vector\" + 0.015*\"code\" + 0.012*\"probability\" + 0.012*\"feature\"'),\n"," (14,\n","  '0.034*\"neuron\" + 0.023*\"signal\" + 0.017*\"frequency\" + 0.017*\"spike\" + 0.014*\"channel\" + 0.012*\"response\" + 0.011*\"firing\" + 0.009*\"temporal\" + 0.009*\"stimulus\" + 0.008*\"threshold\"'),\n"," (15,\n","  '0.016*\"search\" + 0.010*\"rate\" + 0.008*\"task\" + 0.008*\"experiment\" + 0.007*\"strategy\" + 0.007*\"table\" + 0.007*\"application\" + 0.007*\"user\" + 0.006*\"instance\" + 0.006*\"test\"'),\n"," (16,\n","  '0.036*\"word\" + 0.026*\"recognition\" + 0.020*\"speech\" + 0.020*\"training\" + 0.015*\"character\" + 0.013*\"sequence\" + 0.013*\"hmm\" + 0.011*\"context\" + 0.010*\"speaker\" + 0.010*\"feature\"'),\n"," (17,\n","  '0.034*\"noise\" + 0.013*\"average\" + 0.012*\"distribution\" + 0.011*\"curve\" + 0.010*\"equation\" + 0.009*\"correlation\" + 0.009*\"theory\" + 0.008*\"rate\" + 0.008*\"limit\" + 0.007*\"stochastic\"'),\n"," (18,\n","  '0.065*\"state\" + 0.028*\"control\" + 0.027*\"action\" + 0.016*\"step\" + 0.015*\"policy\" + 0.013*\"controller\" + 0.012*\"reinforcement_learning\" + 0.011*\"environment\" + 0.010*\"task\" + 0.009*\"optimal\"'),\n"," (19,\n","  '0.024*\"local\" + 0.020*\"distance\" + 0.018*\"cluster\" + 0.016*\"structure\" + 0.013*\"graph\" + 0.013*\"clustering\" + 0.008*\"variable\" + 0.008*\"dimensional\" + 0.008*\"mapping\" + 0.007*\"global\"')]"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"s_ZSLra-XyDH","colab_type":"text"},"source":["### overcome the problem"]},{"cell_type":"code","metadata":{"id":"dd9etgptWsW0","colab_type":"code","outputId":"8df1f049-e92d-47ab-f49b-92cf9f378c63","executionInfo":{"status":"ok","timestamp":1590836935819,"user_tz":-180,"elapsed":141598,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["# opt_lda_model = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, corpus=bow_corpus,\n","#                                               num_topics=20, id2word=dictionary,\n","#                                                iterations=500, workers=16)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"fHBSgldLQ-x1","colab_type":"code","colab":{}},"source":["# pickle.dump(opt_lda_model, open(\"drive/My Drive/nipstxt/opt_lda_model.pkl\", \"wb\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aVPGON2JRBwR","colab_type":"code","colab":{}},"source":["opt_lda_model = pickle.load(open(\"drive/My Drive/nipstxt/opt_lda_model.pkl\", \"rb\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EoFYom6kX_Sg","colab_type":"code","outputId":"8c650f60-7c64-4cfd-c1c1-f745d026d3d9","executionInfo":{"status":"ok","timestamp":1590837016481,"user_tz":-180,"elapsed":21481,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["tm_results = opt_lda_model[bow_corpus]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"V4Wkf8bZYPS1","colab_type":"text"},"source":["We can now get the most dominant topic per research paper with some intelligent sorting and indexing using the following code."]},{"cell_type":"code","metadata":{"id":"zDANfSSAZ4vc","colab_type":"code","outputId":"a266cbe9-941b-425b-c9eb-00171365618f","executionInfo":{"status":"ok","timestamp":1590837051501,"user_tz":-180,"elapsed":1824,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["tm_results"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[(0, 0.38040123456790126),\n","  (1, 0.023748285322359397),\n","  (2, 0.02409122085048011),\n","  (3, 0.008830589849108368),\n","  (4, 0.07827503429355281),\n","  (5, 0.010373799725651578),\n","  (6, 0.01637517146776406),\n","  (7, 0.009516460905349794),\n","  (8, 0.10502400548696846),\n","  (9, 0.03746570644718793),\n","  (10, 0.015174897119341564),\n","  (11, 0.015517832647462277),\n","  (12, 0.01637517146776406),\n","  (13, 0.020661865569272977),\n","  (14, 0.01688957475994513),\n","  (15, 0.008144718792866941),\n","  (16, 0.06695816186556927),\n","  (17, 0.01997599451303155),\n","  (18, 0.08239026063100137),\n","  (19, 0.043810013717421124)],\n"," [(0, 0.06344586728754367),\n","  (1, 0.019337731212003627),\n","  (2, 0.07275902211874274),\n","  (3, 0.005756047083171648),\n","  (4, 0.01907903246669254),\n","  (5, 0.007308239555038159),\n","  (6, 0.009636528262837927),\n","  (7, 0.013905057560470833),\n","  (8, 0.03912818522830165),\n","  (9, 0.010153925753460098),\n","  (10, 0.09798214978657355),\n","  (11, 0.01222351571594878),\n","  (12, 0.38552580519984486),\n","  (13, 0.08323632130384169),\n","  (14, 0.06344586728754367),\n","  (15, 0.008860432026904671),\n","  (16, 0.027745440434613897),\n","  (17, 0.021019273056525678),\n","  (18, 0.03253136722286898),\n","  (19, 0.006920191437071531)],\n"," [(0, 0.029479396615158204),\n","  (1, 0.007312362030905077),\n","  (2, 0.04870309050772627),\n","  (3, 0.11281272994849154),\n","  (4, 0.006024650478292862),\n","  (5, 0.012003311258278146),\n","  (6, 0.0591887417218543),\n","  (7, 0.03315857247976453),\n","  (8, 0.08843818984547461),\n","  (9, 0.005380794701986755),\n","  (10, 0.05210632818248712),\n","  (11, 0.006576526857983812),\n","  (12, 0.13461184694628403),\n","  (13, 0.044655997056659305),\n","  (14, 0.13102465047829287),\n","  (15, 0.07583701250919794),\n","  (16, 0.03297461368653422),\n","  (17, 0.024236571008094187),\n","  (18, 0.07905629139072848),\n","  (19, 0.01641832229580574)],\n"," [(0, 0.3781515460414543),\n","  (1, 0.0036357458375807),\n","  (2, 0.021576622494053687),\n","  (3, 0.014237172952769283),\n","  (4, 0.007917091403329935),\n","  (5, 0.021916411824668705),\n","  (6, 0.010839279646619095),\n","  (7, 0.0056065239551478085),\n","  (8, 0.020149507305470608),\n","  (9, 0.004791029561671763),\n","  (10, 0.018314644920149507),\n","  (11, 0.011382942575603126),\n","  (12, 0.02619775739041794),\n","  (13, 0.055351681957186545),\n","  (14, 0.1251444104655114),\n","  (15, 0.003431872239211689),\n","  (16, 0.06969079170914033),\n","  (17, 0.023003737682636766),\n","  (18, 0.13581379544682298),\n","  (19, 0.04284743459055386)],\n"," [(0, 0.04616249197174053),\n","  (1, 0.011319845857418112),\n","  (2, 0.08598265895953758),\n","  (3, 0.0098747591522158),\n","  (4, 0.015494540783558124),\n","  (5, 0.012443802183686577),\n","  (6, 0.053387925497752085),\n","  (7, 0.04086384071933205),\n","  (8, 0.019348105330764292),\n","  (9, 0.0821290944123314),\n","  (10, 0.03556518946692357),\n","  (11, 0.009232498394348105),\n","  (12, 0.11841682723185613),\n","  (13, 0.21812780989081568),\n","  (14, 0.01164097623635196),\n","  (15, 0.019990366088631986),\n","  (16, 0.044556840077071294),\n","  (17, 0.044556840077071294),\n","  (18, 0.026252408477842003),\n","  (19, 0.09465317919075145)],\n"," [(0, 0.009512442129629631),\n","  (1, 0.05523003472222224),\n","  (2, 0.023618344907407416),\n","  (3, 0.05754484953703705),\n","  (4, 0.005895543981481483),\n","  (5, 0.5503110532407408),\n","  (6, 0.012695312500000003),\n","  (7, 0.011610243055555558),\n","  (8, 0.017180266203703706),\n","  (9, 0.00509982638888889),\n","  (10, 0.04727285879629631),\n","  (11, 0.05132378472222224),\n","  (12, 0.04850260416666668),\n","  (13, 0.008861400462962965),\n","  (14, 0.004593460648148149),\n","  (15, 0.004014756944444445),\n","  (16, 0.013129340277777781),\n","  (17, 0.02868200231481482),\n","  (18, 0.007993344907407409),\n","  (19, 0.0369285300925926)],\n"," [(0, 0.009332331203808095),\n","  (1, 0.004321683577602406),\n","  (2, 0.0188525616935989),\n","  (3, 0.010334460729049231),\n","  (4, 0.01634723788049606),\n","  (5, 0.009332331203808095),\n","  (6, 0.005073280721533259),\n","  (7, 0.02386320931980459),\n","  (8, 0.015971439308530632),\n","  (9, 0.009457597394463236),\n","  (10, 0.03989728172366279),\n","  (11, 0.00745333834398096),\n","  (12, 0.09551547037454593),\n","  (13, 0.3700989602906176),\n","  (14, 0.0868721032193411),\n","  (15, 0.004697482149567832),\n","  (16, 0.23042715771013406),\n","  (17, 0.01196292120756608),\n","  (18, 0.008706000250532383),\n","  (19, 0.021483151697356886)],\n"," [(0, 0.01287663237761728),\n","  (1, 0.01440869628656891),\n","  (2, 0.025570876194645077),\n","  (3, 0.046873860071496326),\n","  (4, 0.0117823010140804),\n","  (5, 0.022506748376741815),\n","  (6, 0.008207485226526595),\n","  (7, 0.01404391916538995),\n","  (8, 0.689757058437295),\n","  (9, 0.004340847742029621),\n","  (10, 0.05613919894944191),\n","  (11, 0.007842708105347634),\n","  (12, 0.006675421317574964),\n","  (13, 0.009301816590063473),\n","  (14, 0.007259064711461299),\n","  (15, 0.004194936893558037),\n","  (16, 0.020682862770847014),\n","  (17, 0.006456555044867588),\n","  (18, 0.019807397680017512),\n","  (19, 0.011271613044429855)],\n"," [(0, 0.03692865105908584),\n","  (1, 0.009150873281308064),\n","  (2, 0.0302396878483835),\n","  (3, 0.23704013377926422),\n","  (4, 0.012774061687105166),\n","  (5, 0.004134150873281308),\n","  (6, 0.145066889632107),\n","  (7, 0.00812894834633965),\n","  (8, 0.28962281679672985),\n","  (9, 0.08403010033444816),\n","  (10, 0.015746934225195096),\n","  (11, 0.01407469342251951),\n","  (12, 0.04417502787068005),\n","  (13, 0.005434782608695652),\n","  (14, 0.005527684875510962),\n","  (15, 0.005713489409141583),\n","  (16, 0.025873281308063916),\n","  (17, 0.011937941285767372),\n","  (18, 0.00775733927907841),\n","  (19, 0.006642512077294686)],\n"," [(0, 0.005936379928315412),\n","  (1, 0.011312724014336917),\n","  (2, 0.037223715651135005),\n","  (3, 0.33202658303464755),\n","  (4, 0.0055630227001194745),\n","  (5, 0.008624551971326165),\n","  (6, 0.08023446833930704),\n","  (7, 0.011760752688172043),\n","  (8, 0.14012096774193547),\n","  (9, 0.00571236559139785),\n","  (10, 0.05253136200716846),\n","  (11, 0.007429808841099164),\n","  (12, 0.07142323775388291),\n","  (13, 0.011462066905615293),\n","  (14, 0.01840651135005974),\n","  (15, 0.004890979689366786),\n","  (16, 0.039613201911589006),\n","  (17, 0.00817652329749104),\n","  (18, 0.13608870967741934),\n","  (19, 0.011462066905615293)],\n"," [(0, 0.010273391068093057),\n","  (1, 0.014235580460746026),\n","  (2, 0.06223467481745628),\n","  (3, 0.041914303503707485),\n","  (4, 0.012480896586856853),\n","  (5, 0.19440199241523748),\n","  (6, 0.01904681043753892),\n","  (7, 0.007330050376407994),\n","  (8, 0.47356653648044383),\n","  (9, 0.006933831437142697),\n","  (10, 0.045536876662704485),\n","  (11, 0.004839531329597556),\n","  (12, 0.009141336955906494),\n","  (13, 0.028782475802343357),\n","  (14, 0.031386200260372456),\n","  (15, 0.005292352974472181),\n","  (16, 0.004386709684722931),\n","  (17, 0.01287711552612215),\n","  (18, 0.007047036848361353),\n","  (19, 0.008292296371766572)],\n"," [(0, 0.004303106633081444),\n","  (1, 0.015148334732717605),\n","  (2, 0.023124825076966136),\n","  (3, 0.007591659669745312),\n","  (4, 0.00829135180520571),\n","  (5, 0.007941505737475511),\n","  (6, 0.015148334732717605),\n","  (7, 0.009200951581304226),\n","  (8, 0.011160089560593339),\n","  (9, 0.0028337531486146094),\n","  (10, 0.026203470472991885),\n","  (11, 0.008711167086481947),\n","  (12, 0.03383011474951021),\n","  (13, 0.13612510495382033),\n","  (14, 0.044325496781416174),\n","  (15, 0.00549258326336412),\n","  (16, 0.009550797649034425),\n","  (17, 0.0061223061852784776),\n","  (18, 0.3535194514413658),\n","  (19, 0.27137559473831513)],\n"," [(0, 0.3613519588953115),\n","  (1, 0.006663455362877328),\n","  (2, 0.023522800256904303),\n","  (3, 0.00537893384714194),\n","  (4, 0.012764932562620424),\n","  (5, 0.0058606294155427105),\n","  (6, 0.00762684649967887),\n","  (7, 0.006663455362877328),\n","  (8, 0.009071933204881182),\n","  (9, 0.016618497109826588),\n","  (10, 0.01854527938342967),\n","  (11, 0.00939306358381503),\n","  (12, 0.1256422607578677),\n","  (13, 0.006984585741811176),\n","  (14, 0.05515414258188825),\n","  (15, 0.006342324983943481),\n","  (16, 0.0098747591522158),\n","  (17, 0.012764932562620424),\n","  (18, 0.07988118175979447),\n","  (19, 0.21989402697495183)],\n"," [(0, 0.01883561643835616),\n","  (1, 0.010464231354642311),\n","  (2, 0.053462709284627076),\n","  (3, 0.15823186199898526),\n","  (4, 0.019977168949771685),\n","  (5, 0.01883561643835616),\n","  (6, 0.09861745306950784),\n","  (7, 0.019850329781836627),\n","  (8, 0.021372399797057327),\n","  (9, 0.006659056316590561),\n","  (10, 0.01731354642313546),\n","  (11, 0.016045154743784876),\n","  (12, 0.011098427194317603),\n","  (13, 0.15239726027397257),\n","  (14, 0.012239979705733126),\n","  (15, 0.30802891933028914),\n","  (16, 0.010464231354642311),\n","  (17, 0.009322678843226786),\n","  (18, 0.018201420598680868),\n","  (19, 0.018581938102486043)],\n"," [(0, 0.012266742445770583),\n","  (1, 0.011224779766979257),\n","  (2, 0.01785545135928768),\n","  (3, 0.06086009282940231),\n","  (4, 0.015960973761485274),\n","  (5, 0.008477787250165769),\n","  (6, 0.01444539168324335),\n","  (7, 0.006204414132802882),\n","  (8, 0.5985128350857252),\n","  (9, 0.005730794733352279),\n","  (10, 0.07942597328786588),\n","  (11, 0.0058255186132424),\n","  (12, 0.004120488775220234),\n","  (13, 0.006488585772473242),\n","  (14, 0.008477787250165769),\n","  (15, 0.005162451454011557),\n","  (16, 0.008004167850715166),\n","  (17, 0.003457421615989392),\n","  (18, 0.05868144359192954),\n","  (19, 0.06881689874017241)],\n"," [(0, 0.07134589947089946),\n","  (1, 0.03348214285714285),\n","  (2, 0.022404100529100527),\n","  (3, 0.012814153439153436),\n","  (4, 0.07002314814814814),\n","  (5, 0.015294312169312166),\n","  (6, 0.013971560846560843),\n","  (7, 0.01165674603174603),\n","  (8, 0.15054563492063489),\n","  (9, 0.009837962962962962),\n","  (10, 0.11665013227513225),\n","  (11, 0.01231812169312169),\n","  (12, 0.3125826719576719),\n","  (13, 0.007688492063492061),\n","  (14, 0.015790343915343914),\n","  (15, 0.018270502645502642),\n","  (16, 0.055142195767195756),\n","  (17, 0.022734788359788358),\n","  (18, 0.01975859788359788),\n","  (19, 0.007688492063492061)],\n"," [(0, 0.006264288980338363),\n","  (1, 0.003703703703703704),\n","  (2, 0.026657521719250114),\n","  (3, 0.05793324188385917),\n","  (4, 0.020804755372656608),\n","  (5, 0.01156835848193873),\n","  (6, 0.003703703703703704),\n","  (7, 0.010470964791952446),\n","  (8, 0.21678097850937358),\n","  (9, 0.005441243712848651),\n","  (10, 0.07110196616369456),\n","  (11, 0.026748971193415638),\n","  (12, 0.0063557384545038864),\n","  (13, 0.007178783721993599),\n","  (14, 0.027663465935070873),\n","  (15, 0.01723822588020119),\n","  (16, 0.015043438500228623),\n","  (17, 0.01696387745770462),\n","  (18, 0.4283036122542295),\n","  (19, 0.02007315957933242)],\n"," [(0, 0.19260004471272082),\n","  (1, 0.011513525598032643),\n","  (2, 0.043930248155600274),\n","  (3, 0.020903196959534992),\n","  (4, 0.014419852448021466),\n","  (5, 0.009054325955734409),\n","  (6, 0.07567627990163203),\n","  (7, 0.013972725240330877),\n","  (8, 0.012631343617259113),\n","  (9, 0.04482450257098145),\n","  (10, 0.025821596244131464),\n","  (11, 0.014196288844176171),\n","  (12, 0.021350324167225578),\n","  (13, 0.22725240330874139),\n","  (14, 0.013749161636485584),\n","  (15, 0.01956181533646323),\n","  (16, 0.1291079812206573),\n","  (17, 0.009948580371115585),\n","  (18, 0.042141739324837925),\n","  (19, 0.05734406438631792)],\n"," [(0, 0.038915407053831876),\n","  (1, 0.004706974277380005),\n","  (2, 0.02631927870591355),\n","  (3, 0.005237337576239724),\n","  (4, 0.0065632458233890216),\n","  (5, 0.011203924688411563),\n","  (6, 0.027910368602492706),\n","  (7, 0.004972155926809865),\n","  (8, 0.08704587642535136),\n","  (9, 0.007756563245823389),\n","  (10, 0.025391142932909044),\n","  (11, 0.005767700875099443),\n","  (12, 0.06503579952267304),\n","  (13, 0.04448422169185892),\n","  (14, 0.33194112967382655),\n","  (15, 0.014120922832140016),\n","  (16, 0.10123309466984885),\n","  (17, 0.14352956775391143),\n","  (18, 0.037722089631397504),\n","  (19, 0.010143198090692125)],\n"," [(0, 0.017605633802816902),\n","  (1, 0.014140397943214844),\n","  (2, 0.02062374245472837),\n","  (3, 0.16716968477531857),\n","  (4, 0.010116253073999553),\n","  (5, 0.04845741113346747),\n","  (6, 0.04622177509501453),\n","  (7, 0.02207690587972278),\n","  (8, 0.3744131455399061),\n","  (9, 0.026101050748938072),\n","  (10, 0.0844511513525598),\n","  (11, 0.008327744243237201),\n","  (12, 0.020064833445115136),\n","  (13, 0.02509501453163425),\n","  (14, 0.021517996870109544),\n","  (15, 0.006651017214397496),\n","  (16, 0.03135479543930248),\n","  (17, 0.009221998658618377),\n","  (18, 0.036161412921976305),\n","  (19, 0.0102280348759222)],\n"," [(0, 0.011627906976744186),\n","  (1, 0.08330524660150544),\n","  (2, 0.07521626783507471),\n","  (3, 0.011515560049432648),\n","  (4, 0.05016290304460173),\n","  (5, 0.06566677901359398),\n","  (6, 0.013537804741040332),\n","  (7, 0.022188518144028762),\n","  (8, 0.011066172340186496),\n","  (9, 0.003763622064936524),\n","  (10, 0.3359734861251545),\n","  (11, 0.0302774969104595),\n","  (12, 0.07971014492753623),\n","  (13, 0.004774744410740366),\n","  (14, 0.028592293000786428),\n","  (15, 0.007695764520840355),\n","  (16, 0.010616784630940344),\n","  (17, 0.06645320750477475),\n","  (18, 0.05544320862824402),\n","  (19, 0.03241208852937872)],\n"," [(0, 0.007438618393674573),\n","  (1, 0.031054931335830212),\n","  (2, 0.017114024136496046),\n","  (3, 0.08380149812734082),\n","  (4, 0.005253849354972951),\n","  (5, 0.031991260923845195),\n","  (6, 0.005253849354972951),\n","  (7, 0.008791094465251769),\n","  (8, 0.5487411568872244),\n","  (9, 0.01420099875156055),\n","  (10, 0.04926133999167707),\n","  (11, 0.009207240948813982),\n","  (12, 0.011704119850187267),\n","  (13, 0.019402829796088223),\n","  (14, 0.007854764877236787),\n","  (15, 0.005253849354972951),\n","  (16, 0.03781731169371619),\n","  (17, 0.009519350811485644),\n","  (18, 0.020339159384103203),\n","  (19, 0.07599875156054932)],\n"," [(0, 0.012237237237237238),\n","  (1, 0.007732732732732734),\n","  (2, 0.08115615615615616),\n","  (3, 0.019294294294294297),\n","  (4, 0.011636636636636638),\n","  (5, 0.11974474474474475),\n","  (6, 0.028003003003003006),\n","  (7, 0.010285285285285287),\n","  (8, 0.16944444444444448),\n","  (9, 0.007432432432432434),\n","  (10, 0.06463963963963966),\n","  (11, 0.009384384384384386),\n","  (12, 0.01388888888888889),\n","  (13, 0.09542042042042044),\n","  (14, 0.23701201201201205),\n","  (15, 0.056231231231231235),\n","  (16, 0.017942942942942946),\n","  (17, 0.006081081081081082),\n","  (18, 0.018843843843843848),\n","  (19, 0.01358858858858859)],\n"," [(0, 0.0600669099756691),\n","  (1, 0.020225060827250608),\n","  (2, 0.02458434712084347),\n","  (3, 0.03198499594484996),\n","  (4, 0.00674168694241687),\n","  (5, 0.011303730738037308),\n","  (6, 0.020427818329278185),\n","  (7, 0.005930656934306569),\n","  (8, 0.011810624493106246),\n","  (9, 0.006538929440389294),\n","  (10, 0.03066707218167072),\n","  (11, 0.14177818329278183),\n","  (12, 0.3213199513381995),\n","  (13, 0.17280008110300082),\n","  (14, 0.025192619626926198),\n","  (15, 0.009073398215733983),\n","  (16, 0.0341139497161395),\n","  (17, 0.033505677210056775),\n","  (18, 0.01525750202757502),\n","  (19, 0.016676804541768047)],\n"," [(0, 0.06324910916747653),\n","  (1, 0.007855523161645611),\n","  (2, 0.26198574667962427),\n","  (3, 0.009961127308066085),\n","  (4, 0.01676384839650146),\n","  (5, 0.022594752186588924),\n","  (6, 0.011742792355037255),\n","  (7, 0.019679300291545195),\n","  (8, 0.020003239390994496),\n","  (9, 0.15265630061548432),\n","  (10, 0.056770327178490454),\n","  (11, 0.012714609653385165),\n","  (12, 0.08770651117589895),\n","  (13, 0.11880466472303208),\n","  (14, 0.03830579850988015),\n","  (15, 0.009313249109167478),\n","  (16, 0.007369614512471656),\n","  (17, 0.03198898607061873),\n","  (18, 0.027939747327502432),\n","  (19, 0.022594752186588924)],\n"," [(0, 0.22680672875604652),\n","  (1, 0.0037903400476499895),\n","  (2, 0.03411306042884991),\n","  (3, 0.03331889394267563),\n","  (4, 0.011732004909392826),\n","  (5, 0.06140350877192983),\n","  (6, 0.025593819940798502),\n","  (7, 0.0055952638798642705),\n","  (8, 0.030936394484152772),\n","  (9, 0.00732799075878998),\n","  (10, 0.011010035376507113),\n","  (11, 0.007400187712078551),\n","  (12, 0.011804201862681397),\n","  (13, 0.022200563136235654),\n","  (14, 0.01144321709623854),\n","  (15, 0.023572305248718506),\n","  (16, 0.05266767742401272),\n","  (17, 0.1844993141289438),\n","  (18, 0.08472312468413834),\n","  (19, 0.15006136741029533)],\n"," [(0, 0.021062674299134006),\n","  (1, 0.010494642595038895),\n","  (2, 0.04337296345222368),\n","  (3, 0.034566270365477755),\n","  (4, 0.007559078232790252),\n","  (5, 0.015338323792749152),\n","  (6, 0.054528108028768514),\n","  (7, 0.046455306032584756),\n","  (8, 0.010054307940701598),\n","  (9, 0.0452810802876853),\n","  (10, 0.05907823279025391),\n","  (11, 0.10546014971378245),\n","  (12, 0.11059738734771757),\n","  (13, 0.20893879348304706),\n","  (14, 0.004770292088654043),\n","  (15, 0.026199911933069125),\n","  (16, 0.1075150447673565),\n","  (17, 0.05467488624688095),\n","  (18, 0.016072214883311314),\n","  (19, 0.017980331718772932)],\n"," [(0, 0.02355630189383338),\n","  (1, 0.022809963616008955),\n","  (2, 0.029060546692788507),\n","  (3, 0.004338091239854464),\n","  (4, 0.014413657990484188),\n","  (5, 0.008349659483160743),\n","  (6, 0.0038716298162141992),\n","  (7, 0.028780669838604347),\n","  (8, 0.014040488851571975),\n","  (9, 0.004897844948222782),\n","  (10, 0.03736356003358522),\n","  (11, 0.03139285381098983),\n","  (12, 0.08270361041141898),\n","  (13, 0.46958671517865475),\n","  (14, 0.023089840470193114),\n","  (15, 0.005924060080231365),\n","  (16, 0.046226327082750256),\n","  (17, 0.12048698572628044),\n","  (18, 0.012267935441738969),\n","  (19, 0.016839257393413565)],\n"," [(0, 0.006113342257920569),\n","  (1, 0.1333779562695225),\n","  (2, 0.01691209281570727),\n","  (3, 0.03556448014279339),\n","  (4, 0.028335564480142787),\n","  (5, 0.07626059794734492),\n","  (6, 0.005845604640785363),\n","  (7, 0.005220883534136546),\n","  (8, 0.28232931726907623),\n","  (9, 0.006113342257920569),\n","  (10, 0.31695671575189643),\n","  (11, 0.00539937527889335),\n","  (12, 0.006738063364569387),\n","  (13, 0.003525211958946898),\n","  (14, 0.029852744310575628),\n","  (15, 0.003346720214190093),\n","  (16, 0.004506916555109325),\n","  (17, 0.00539937527889335),\n","  (18, 0.016733601070950465),\n","  (19, 0.01146809460062472)],\n"," [(0, 0.04563345633456335),\n","  (1, 0.022427224272242727),\n","  (2, 0.007257072570725708),\n","  (3, 0.11123411234112343),\n","  (4, 0.005207052070520706),\n","  (5, 0.010127101271012712),\n","  (6, 0.02628126281262813),\n","  (7, 0.006109061090610907),\n","  (8, 0.04399343993439935),\n","  (9, 0.0034030340303403037),\n","  (10, 0.12673226732267326),\n","  (11, 0.1651906519065191),\n","  (12, 0.011521115211152113),\n","  (13, 0.014227142271422716),\n","  (14, 0.0836818368183682),\n","  (15, 0.004305043050430505),\n","  (16, 0.02595325953259533),\n","  (17, 0.01775317753177532),\n","  (18, 0.24874948749487497),\n","  (19, 0.020213202132021324)],\n"," [(0, 0.03398058252427184),\n","  (1, 0.013915857605177993),\n","  (2, 0.01197411003236246),\n","  (3, 0.007874865156418555),\n","  (4, 0.011326860841423949),\n","  (5, 0.00895361380798274),\n","  (6, 0.011758360302049622),\n","  (7, 0.018878101402373247),\n","  (8, 0.0447680690399137),\n","  (9, 0.00895361380798274),\n","  (10, 0.027076591154261056),\n","  (11, 0.01413160733549083),\n","  (12, 0.3925566343042071),\n","  (13, 0.01197411003236246),\n","  (14, 0.04865156418554477),\n","  (15, 0.009600862998921251),\n","  (16, 0.015857605177993526),\n","  (17, 0.03247033441208198),\n","  (18, 0.24757281553398058),\n","  (19, 0.02772384034519957)],\n"," [(0, 0.007729017532939101),\n","  (1, 0.10950306048345264),\n","  (2, 0.059394128021579),\n","  (3, 0.018207282913165267),\n","  (4, 0.023602033405954976),\n","  (5, 0.13398692810457516),\n","  (6, 0.012812532420375557),\n","  (7, 0.0055503682954663345),\n","  (8, 0.1691565515094927),\n","  (9, 0.008455233945430024),\n","  (10, 0.2760141093474427),\n","  (11, 0.058979147214441335),\n","  (12, 0.012293806411453471),\n","  (13, 0.009492685963274199),\n","  (14, 0.006276584707957257),\n","  (15, 0.008973959954352111),\n","  (16, 0.006172839506172839),\n","  (17, 0.008040253138292354),\n","  (18, 0.013435003631082063),\n","  (19, 0.05192447349310094)],\n"," [(0, 0.08244934040527675),\n","  (1, 0.00465796273629811),\n","  (2, 0.016353869169046648),\n","  (3, 0.04566163470692235),\n","  (4, 0.005201958384332925),\n","  (5, 0.008805929552563579),\n","  (6, 0.005133958928328573),\n","  (7, 0.013429892560859514),\n","  (8, 0.03920168638650891),\n","  (9, 0.012749898000815994),\n","  (10, 0.010777913776689787),\n","  (11, 0.003229974160206718),\n","  (12, 0.08231334149326805),\n","  (13, 0.06946144430844553),\n","  (14, 0.3564871481028152),\n","  (15, 0.016557867537059703),\n","  (16, 0.10162518699850401),\n","  (17, 0.049197606419148644),\n","  (18, 0.05715354277165783),\n","  (19, 0.01954984360125119)],\n"," [(0, 0.12017043104161496),\n","  (1, 0.005915446347315298),\n","  (2, 0.011044924298833457),\n","  (3, 0.025936957061305534),\n","  (4, 0.020393811533052038),\n","  (5, 0.006660047985438901),\n","  (6, 0.015098866550839744),\n","  (7, 0.009721188053280384),\n","  (8, 0.06862745098039216),\n","  (9, 0.00839745180772731),\n","  (10, 0.05439728634069661),\n","  (11, 0.007570116654256639),\n","  (12, 0.14664515595267644),\n","  (13, 0.00897658641515678),\n","  (14, 0.3356085050053777),\n","  (15, 0.005170844709191693),\n","  (16, 0.10114172251178953),\n","  (17, 0.02345495160089352),\n","  (18, 0.018739141226110697),\n","  (19, 0.006329113924050633)],\n"," [(0, 0.06469555035128804),\n","  (1, 0.004195940671350507),\n","  (2, 0.06332943013270881),\n","  (3, 0.01408404892011449),\n","  (4, 0.004781420765027322),\n","  (5, 0.007773874577153264),\n","  (6, 0.020914650013010665),\n","  (7, 0.21718058808222737),\n","  (8, 0.02800546448087431),\n","  (9, 0.0028948737965131402),\n","  (10, 0.02234582357533177),\n","  (11, 0.03776346604215456),\n","  (12, 0.03730809263596148),\n","  (13, 0.2111956804579755),\n","  (14, 0.09286364819151703),\n","  (15, 0.00751366120218579),\n","  (16, 0.09988940931563879),\n","  (17, 0.016751236013531092),\n","  (18, 0.02527322404371584),\n","  (19, 0.02123991673172001)],\n"," [(0, 0.01369949494949495),\n","  (1, 0.0591540404040404),\n","  (2, 0.03731060606060606),\n","  (3, 0.1672348484848485),\n","  (4, 0.018623737373737372),\n","  (5, 0.259280303030303),\n","  (6, 0.02847222222222222),\n","  (7, 0.020896464646464646),\n","  (8, 0.07809343434343434),\n","  (9, 0.011047979797979798),\n","  (10, 0.08592171717171718),\n","  (11, 0.01698232323232323),\n","  (12, 0.011553030303030303),\n","  (13, 0.007891414141414142),\n","  (14, 0.11433080808080807),\n","  (15, 0.014078282828282829),\n","  (16, 0.010669191919191919),\n","  (17, 0.008522727272727272),\n","  (18, 0.014078282828282829),\n","  (19, 0.02215909090909091)],\n"," [(0, 0.031639128352490414),\n","  (1, 0.10641163793103446),\n","  (2, 0.028645833333333325),\n","  (3, 0.006794779693486588),\n","  (4, 0.007453304597701147),\n","  (5, 0.015176005747126433),\n","  (6, 0.022359913793103446),\n","  (7, 0.016193726053639845),\n","  (8, 0.017630507662835245),\n","  (9, 0.006675047892720305),\n","  (10, 0.05235272988505746),\n","  (11, 0.01302083333333333),\n","  (12, 0.33072916666666663),\n","  (13, 0.0654633620689655),\n","  (14, 0.05887811302681991),\n","  (15, 0.0033824233716475086),\n","  (16, 0.027209051724137925),\n","  (17, 0.06157207854406129),\n","  (18, 0.040798611111111105),\n","  (19, 0.08761374521072796)],\n"," [(0, 0.039797754203114336),\n","  (1, 0.017774055462497673),\n","  (2, 0.03105031329486941),\n","  (3, 0.03768844221105527),\n","  (4, 0.010453502078292699),\n","  (5, 0.3196538246789503),\n","  (6, 0.009274768906259693),\n","  (7, 0.00635895527017805),\n","  (8, 0.01616105217445251),\n","  (9, 0.015354550530429927),\n","  (10, 0.021496370742601897),\n","  (11, 0.007785842794218003),\n","  (12, 0.04060425584713692),\n","  (13, 0.007413611266207581),\n","  (14, 0.060208449655685836),\n","  (15, 0.006048762330169365),\n","  (16, 0.10189838079285315),\n","  (17, 0.15128109684223587),\n","  (18, 0.013431354302376079),\n","  (19, 0.08626465661641541)],\n"," [(0, 0.020114326545194714),\n","  (1, 0.009824937477670596),\n","  (2, 0.10721686316541622),\n","  (3, 0.009682029296177206),\n","  (4, 0.06784565916398713),\n","  (5, 0.004465880671668453),\n","  (6, 0.004680242943908538),\n","  (7, 0.03483386923901393),\n","  (8, 0.024901750625223294),\n","  (9, 0.019899964272954627),\n","  (10, 0.08756698821007503),\n","  (11, 0.0026795284030010718),\n","  (12, 0.00918185066095034),\n","  (13, 0.2865666309396213),\n","  (14, 0.20317970703822794),\n","  (15, 0.045980707395498394),\n","  (16, 0.0059664165773490535),\n","  (17, 0.0038227938549481957),\n","  (18, 0.047338335119685604),\n","  (19, 0.004251518399428367)],\n"," [(0, 0.0036452565522332963),\n","  (1, 0.008259505352528609),\n","  (2, 0.007705795496493171),\n","  (3, 0.011674049464747139),\n","  (4, 0.006229235880398671),\n","  (5, 0.007521225544481359),\n","  (6, 0.03520671834625323),\n","  (7, 0.0033684016242155777),\n","  (8, 0.008167220376522701),\n","  (9, 0.0029069767441860465),\n","  (10, 0.014904023624953858),\n","  (11, 0.01905684754521964),\n","  (12, 0.009090070136581764),\n","  (13, 0.013519748984865264),\n","  (14, 0.024778516057585825),\n","  (15, 0.03834440753045404),\n","  (16, 0.015457733480989295),\n","  (17, 0.09426910299003323),\n","  (18, 0.6143872277593208),\n","  (19, 0.061507936507936505)],\n"," [(0, 0.007684089414858644),\n","  (1, 0.013519066403681786),\n","  (2, 0.004396778435239973),\n","  (3, 0.0047255095332018405),\n","  (4, 0.002917488494411571),\n","  (5, 0.01187541091387245),\n","  (6, 0.004232412886259039),\n","  (7, 0.0038214990138067052),\n","  (8, 0.11756245890861272),\n","  (9, 0.0038214990138067052),\n","  (10, 0.046803090072320835),\n","  (11, 0.021490795529257065),\n","  (12, 0.03102399737015121),\n","  (13, 0.008177186061801445),\n","  (14, 0.32466305062458906),\n","  (15, 0.0033284023668639045),\n","  (16, 0.1302186061801446),\n","  (17, 0.051816239316239306),\n","  (18, 0.1532297830374753),\n","  (19, 0.05469263642340564)],\n"," [(0, 0.07711059789581731),\n","  (1, 0.005089384997006245),\n","  (2, 0.008083140877598155),\n","  (3, 0.016807800872466003),\n","  (4, 0.010734753228979559),\n","  (5, 0.0044050979385852375),\n","  (6, 0.026473355572662737),\n","  (7, 0.004234026173979985),\n","  (8, 0.09361902318022412),\n","  (9, 0.006543494996150886),\n","  (10, 0.026473355572662737),\n","  (11, 0.01834744675391327),\n","  (12, 0.21259943546317683),\n","  (13, 0.15392182020357542),\n","  (14, 0.08651954494910616),\n","  (15, 0.00688563852536139),\n","  (16, 0.022025489692926187),\n","  (17, 0.10875887434778891),\n","  (18, 0.08703276024292192),\n","  (19, 0.024334958515097087)],\n"," [(0, 0.013510425592687805),\n","  (1, 0.3074835761211083),\n","  (2, 0.12107969151670953),\n","  (3, 0.002485004284490146),\n","  (4, 0.09171665238503286),\n","  (5, 0.07869180234218796),\n","  (6, 0.005055698371893746),\n","  (7, 0.004598686089688661),\n","  (8, 0.014767209368751788),\n","  (9, 0.003227649243073408),\n","  (10, 0.03778920308483291),\n","  (11, 0.07560696943730365),\n","  (12, 0.004655812624964297),\n","  (13, 0.16112539274493004),\n","  (14, 0.019794344473007715),\n","  (15, 0.005912596401028278),\n","  (16, 0.009568694658668954),\n","  (17, 0.02076549557269352),\n","  (18, 0.010711225364181664),\n","  (19, 0.011453870322764926)],\n"," [(0, 0.18982318665862968),\n","  (1, 0.007986738999397227),\n","  (2, 0.08453887884267632),\n","  (3, 0.00547518585493269),\n","  (4, 0.006077958609604179),\n","  (5, 0.008589511754068717),\n","  (6, 0.00617842073538276),\n","  (7, 0.021448663853727144),\n","  (8, 0.019238497086598354),\n","  (9, 0.012206148282097649),\n","  (10, 0.06464737793851717),\n","  (11, 0.007082579867389994),\n","  (12, 0.46890697207152904),\n","  (13, 0.004571026722925457),\n","  (14, 0.029083785412899337),\n","  (15, 0.00477195097448262),\n","  (16, 0.0074844283705043195),\n","  (17, 0.01421539079766928),\n","  (18, 0.02094635322483424),\n","  (19, 0.016726943942133816)],\n"," [(0, 0.006831342743394861),\n","  (1, 0.004659790083242852),\n","  (2, 0.008821932681867536),\n","  (3, 0.010541078537821208),\n","  (4, 0.006740861382555194),\n","  (5, 0.00900289540354687),\n","  (6, 0.011898298950416214),\n","  (7, 0.005655085052479189),\n","  (8, 0.05342924357582338),\n","  (9, 0.010993485342019544),\n","  (10, 0.04673362287368802),\n","  (11, 0.009545783568584872),\n","  (12, 0.02755157437567861),\n","  (13, 0.008640969960188202),\n","  (14, 0.03669019182048498),\n","  (15, 0.00502171552660152),\n","  (16, 0.00655989866087586),\n","  (17, 0.010722041259500542),\n","  (18, 0.70923814694173),\n","  (19, 0.010722041259500542)],\n"," [(0, 0.013464731788318533),\n","  (1, 0.009205111544292831),\n","  (2, 0.03432965128871562),\n","  (3, 0.008194354198252833),\n","  (4, 0.010649050610064255),\n","  (5, 0.011443217096238538),\n","  (6, 0.005667460833152841),\n","  (7, 0.00790556638509855),\n","  (8, 0.08544509421702404),\n","  (9, 0.0050176882535557),\n","  (10, 0.030286621904555628),\n","  (11, 0.007255793805501408),\n","  (12, 0.030070031044689916),\n","  (13, 0.04761389069381272),\n","  (14, 0.12573099415204678),\n","  (15, 0.007688975525232835),\n","  (16, 0.020756624070464226),\n","  (17, 0.0074001877120785505),\n","  (18, 0.5158833297234857),\n","  (19, 0.015991625153418527)],\n"," [(0, 0.0737349070682404),\n","  (1, 0.022452855786189118),\n","  (2, 0.013770180436847104),\n","  (3, 0.0387328720662054),\n","  (4, 0.005494505494505495),\n","  (5, 0.03439153439153439),\n","  (6, 0.01010717677384344),\n","  (7, 0.009021842355175688),\n","  (8, 0.0951702618369285),\n","  (9, 0.026387193053859722),\n","  (10, 0.012277845611178945),\n","  (11, 0.021231854565187898),\n","  (12, 0.06450956450956451),\n","  (13, 0.03289919956586623),\n","  (14, 0.1800976800976801),\n","  (15, 0.005901505901505902),\n","  (16, 0.16096866096866097),\n","  (17, 0.033306199972866636),\n","  (18, 0.07617690951024285),\n","  (19, 0.0833672500339167)],\n"," [(0, 0.0505974152645696),\n","  (1, 0.0031293180525075177),\n","  (2, 0.011338697878566202),\n","  (3, 0.0292205153214663),\n","  (4, 0.006299276599203444),\n","  (5, 0.03556043241485816),\n","  (6, 0.010444607006421197),\n","  (7, 0.0036170039827684297),\n","  (8, 0.04572055596196049),\n","  (9, 0.04385109322929366),\n","  (10, 0.009794359099406647),\n","  (11, 0.02036088758839307),\n","  (12, 0.19088840120295858),\n","  (13, 0.015240185320653495),\n","  (14, 0.167398195562058),\n","  (15, 0.019954482646508977),\n","  (16, 0.028976672356335845),\n","  (17, 0.01377712752987076),\n","  (18, 0.09050638055758757),\n","  (19, 0.20332439242461184)],\n"," [(0, 0.016649546310563257),\n","  (1, 0.004322889916110254),\n","  (2, 0.024952919020715624),\n","  (3, 0.006120527306967983),\n","  (4, 0.005435713062831706),\n","  (5, 0.004408491696627289),\n","  (6, 0.01382468755350111),\n","  (7, 0.006206129087485018),\n","  (8, 0.01759116589625064),\n","  (9, 0.004322889916110254),\n","  (10, 0.025808936825885972),\n","  (11, 0.0068053415511042615),\n","  (12, 0.452191405581236),\n","  (13, 0.10464817668207496),\n","  (14, 0.22012497859955482),\n","  (15, 0.006377332648519088),\n","  (16, 0.0396764252696456),\n","  (17, 0.016307139188495117),\n","  (18, 0.018361581920903952),\n","  (19, 0.005863721965416879)],\n"," [(0, 0.019129782445611403),\n","  (1, 0.024631157789447362),\n","  (2, 0.005959823289155623),\n","  (3, 0.02104692839876636),\n","  (4, 0.022547303492539802),\n","  (5, 0.006793365007918647),\n","  (6, 0.004209385679753271),\n","  (7, 0.021963824289405683),\n","  (8, 0.02629824122697341),\n","  (9, 0.004876219054763691),\n","  (10, 0.021547053430024173),\n","  (11, 0.009544052679836625),\n","  (12, 0.5619321497040927),\n","  (13, 0.012544802867383513),\n","  (14, 0.028298741352004667),\n","  (15, 0.008210385929815787),\n","  (16, 0.007710260898557973),\n","  (17, 0.037467700258397935),\n","  (18, 0.055472201383679254),\n","  (19, 0.09981662082187213)],\n"," [(0, 0.0181241718720424),\n","  (1, 0.006956274843838728),\n","  (2, 0.010931289040318001),\n","  (3, 0.07897974635623699),\n","  (4, 0.023992049971607043),\n","  (5, 0.06468862388794246),\n","  (6, 0.008186636380844218),\n","  (7, 0.07850653038046565),\n","  (8, 0.012256293772477759),\n","  (9, 0.013013439333711907),\n","  (10, 0.01689381033503691),\n","  (11, 0.33792352829831535),\n","  (12, 0.12166382737081204),\n","  (13, 0.03894567480598145),\n","  (14, 0.004306265379519212),\n","  (15, 0.006293772477758849),\n","  (16, 0.05143857656634488),\n","  (17, 0.07358508423244368),\n","  (18, 0.012256293772477759),\n","  (19, 0.021058110921824722)],\n"," [(0, 0.026829083181542198),\n","  (1, 0.006489071038251366),\n","  (2, 0.00869004250151791),\n","  (3, 0.1475789313904068),\n","  (4, 0.00853825136612022),\n","  (5, 0.0023907103825136613),\n","  (6, 0.006944444444444444),\n","  (7, 0.008765938069216758),\n","  (8, 0.003149666059502125),\n","  (9, 0.0057301153612629024),\n","  (10, 0.040490285367334546),\n","  (11, 0.006868548876745598),\n","  (12, 0.08435792349726776),\n","  (13, 0.31485276259866424),\n","  (14, 0.1513737097753491),\n","  (15, 0.06181693989071038),\n","  (16, 0.027588038858530662),\n","  (17, 0.036164238008500306),\n","  (18, 0.03859289617486339),\n","  (19, 0.012788403157255617)],\n"," [(0, 0.01596869712351946),\n","  (1, 0.17586717428087986),\n","  (2, 0.004124365482233503),\n","  (3, 0.1102298364354202),\n","  (4, 0.004547377326565144),\n","  (5, 0.1392766497461929),\n","  (6, 0.01103355893965031),\n","  (7, 0.035004230118443315),\n","  (8, 0.09048928369994359),\n","  (9, 0.004265369430344049),\n","  (10, 0.11636350817822899),\n","  (11, 0.09225183305132544),\n","  (12, 0.04515651438240271),\n","  (13, 0.0059574168076706145),\n","  (14, 0.008072476029328821),\n","  (15, 0.009200507614213198),\n","  (16, 0.055308798646362095),\n","  (17, 0.00447687535250987),\n","  (18, 0.05834038353073886),\n","  (19, 0.014065143824027073)],\n"," [(0, 0.06438645050744696),\n","  (1, 0.019441149334387772),\n","  (2, 0.02695400026360881),\n","  (3, 0.008105970739422699),\n","  (4, 0.012455516014234879),\n","  (5, 0.09575589824700148),\n","  (6, 0.09232898378805854),\n","  (7, 0.045011203374192704),\n","  (8, 0.04843811783313564),\n","  (9, 0.042243310926584954),\n","  (10, 0.047120073810465284),\n","  (11, 0.029062870699881383),\n","  (12, 0.06689073415052064),\n","  (13, 0.017200474495848165),\n","  (14, 0.23217345459338348),\n","  (15, 0.007578753130354556),\n","  (16, 0.012587320416501915),\n","  (17, 0.0593778832212996),\n","  (18, 0.05423751153288521),\n","  (19, 0.01865032292078556)],\n"," [(0, 0.009359903381642514),\n","  (1, 0.15187198067632854),\n","  (2, 0.10766908212560387),\n","  (3, 0.013224637681159421),\n","  (4, 0.0053743961352657015),\n","  (5, 0.4286835748792271),\n","  (6, 0.01733091787439614),\n","  (7, 0.009239130434782611),\n","  (8, 0.036775362318840585),\n","  (9, 0.006944444444444445),\n","  (10, 0.01745169082125604),\n","  (11, 0.05706521739130435),\n","  (12, 0.014915458937198069),\n","  (13, 0.026751207729468603),\n","  (14, 0.012016908212560387),\n","  (15, 0.003925120772946861),\n","  (16, 0.01636473429951691),\n","  (17, 0.020712560386473435),\n","  (18, 0.010446859903381645),\n","  (19, 0.03387681159420291)],\n"," [(0, 0.009451924384604923),\n","  (1, 0.014483884128926969),\n","  (2, 0.02835577315381477),\n","  (3, 0.017475860193118455),\n","  (4, 0.008907928736570108),\n","  (5, 0.23521011831905345),\n","  (6, 0.013531891744866041),\n","  (7, 0.005371957024343805),\n","  (8, 0.19264245886032913),\n","  (9, 0.00727594179246566),\n","  (10, 0.08710730314157487),\n","  (11, 0.009587923296613628),\n","  (12, 0.011627906976744186),\n","  (13, 0.16054671562627498),\n","  (14, 0.03678770569835441),\n","  (15, 0.012307901536787706),\n","  (16, 0.007411940704474364),\n","  (17, 0.021011831905344756),\n","  (18, 0.03841969264245886),\n","  (19, 0.08248334013327893)],\n"," [(0, 0.028208218549127643),\n","  (1, 0.002611340679522498),\n","  (2, 0.41910583103764926),\n","  (3, 0.0023243801652892567),\n","  (4, 0.002381772268135905),\n","  (5, 0.023100321395775943),\n","  (6, 0.002381772268135905),\n","  (7, 0.004562672176308541),\n","  (8, 0.01684458218549128),\n","  (9, 0.004390495867768596),\n","  (10, 0.003357438016528926),\n","  (11, 0.0245925160697888),\n","  (12, 0.008293158861340681),\n","  (13, 0.10396579430670341),\n","  (14, 0.06247130394857668),\n","  (15, 0.019140266299357212),\n","  (16, 0.0059400826446281),\n","  (17, 0.049500688705234164),\n","  (18, 0.012195821854912766),\n","  (19, 0.20463154269972456)],\n"," [(0, 0.10143165856293893),\n","  (1, 0.009139204033855573),\n","  (2, 0.014091482081757605),\n","  (3, 0.006528002881325408),\n","  (4, 0.06118314424635331),\n","  (5, 0.11097604898253194),\n","  (6, 0.005177381595533945),\n","  (7, 0.006528002881325408),\n","  (8, 0.17427516657662523),\n","  (9, 0.04767693138843867),\n","  (10, 0.056771114712767856),\n","  (11, 0.028047902034936064),\n","  (12, 0.01607239330091842),\n","  (13, 0.012650819376913376),\n","  (14, 0.08207275346659462),\n","  (15, 0.007788582748064108),\n","  (16, 0.054159913560237694),\n","  (17, 0.010219701062488743),\n","  (18, 0.03777237529263461),\n","  (19, 0.1574374212137583)],\n"," [(0, 0.01636455186304129),\n","  (1, 0.008308157099697885),\n","  (2, 0.06285666330983551),\n","  (3, 0.007804632426988922),\n","  (4, 0.013679086941926821),\n","  (5, 0.005119167505874454),\n","  (6, 0.21055723397113124),\n","  (7, 0.00931520644511581),\n","  (8, 0.007972473984558576),\n","  (9, 0.06285666330983551),\n","  (10, 0.014686136287344746),\n","  (11, 0.00730110775427996),\n","  (12, 0.060506881503860355),\n","  (13, 0.21106075864384022),\n","  (14, 0.026938569989929505),\n","  (15, 0.005454850621013763),\n","  (16, 0.032141658274588786),\n","  (17, 0.00931520644511581),\n","  (18, 0.02391742195367573),\n","  (19, 0.20384357166834507)],\n"," [(0, 0.12157136215713621),\n","  (1, 0.13709902370990237),\n","  (2, 0.06652719665271967),\n","  (3, 0.019665271966527197),\n","  (4, 0.058716875871687585),\n","  (5, 0.012784751278475127),\n","  (6, 0.045234774523477454),\n","  (7, 0.0200371920037192),\n","  (8, 0.009716410971641097),\n","  (9, 0.013156671315667131),\n","  (10, 0.06624825662482567),\n","  (11, 0.08642491864249187),\n","  (12, 0.02812645281264528),\n","  (13, 0.01390051139005114),\n","  (14, 0.02682473268247327),\n","  (15, 0.10595072059507206),\n","  (16, 0.014365411436541143),\n","  (17, 0.022361692236169222),\n","  (18, 0.021710832171083216),\n","  (19, 0.1095769409576941)],\n"," [(0, 0.013425307817831182),\n","  (1, 0.011051772734015725),\n","  (2, 0.006008010680907877),\n","  (3, 0.009271621421154131),\n","  (4, 0.014167037531523513),\n","  (5, 0.008233199821984869),\n","  (6, 0.009123275478415665),\n","  (7, 0.004524551253523216),\n","  (8, 0.17022696929238984),\n","  (9, 0.0070464322800771395),\n","  (10, 0.019952529298323692),\n","  (11, 0.006304702566384809),\n","  (12, 0.16859516392226673),\n","  (13, 0.0089749295356772),\n","  (14, 0.06445631211986352),\n","  (15, 0.0055629728526924785),\n","  (16, 0.0177273401572467),\n","  (17, 0.025886367007862336),\n","  (18, 0.13521732680611184),\n","  (19, 0.2942441774217475)],\n"," [(0, 0.012395864699387736),\n","  (1, 0.014804777677406405),\n","  (2, 0.11106092542406908),\n","  (3, 0.2072167017966476),\n","  (4, 0.010388437217705512),\n","  (5, 0.034979423868312765),\n","  (6, 0.06017263876342468),\n","  (7, 0.012897721569808292),\n","  (8, 0.10473752885677007),\n","  (9, 0.006875439124761619),\n","  (10, 0.1361537689450969),\n","  (11, 0.008983237980527956),\n","  (12, 0.006473953628425174),\n","  (13, 0.015005520425574627),\n","  (14, 0.007578038743350397),\n","  (15, 0.05154070059219112),\n","  (16, 0.006473953628425174),\n","  (17, 0.00617283950617284),\n","  (18, 0.16827260865201246),\n","  (19, 0.017815918899929742)],\n"," [(0, 0.006960335621662853),\n","  (1, 0.051709890668700735),\n","  (2, 0.3041253496058988),\n","  (3, 0.014206712433257055),\n","  (4, 0.18481439105008898),\n","  (5, 0.020626748029494026),\n","  (6, 0.09010297482837529),\n","  (7, 0.00956648868548182),\n","  (8, 0.00842232392575642),\n","  (9, 0.0058797355708110855),\n","  (10, 0.029335113145181797),\n","  (11, 0.011918382913806255),\n","  (12, 0.0032100177981184844),\n","  (13, 0.13828502415458938),\n","  (14, 0.003654970760233918),\n","  (15, 0.0040999237223493515),\n","  (16, 0.0377256547165014),\n","  (17, 0.02164378337147216),\n","  (18, 0.018020594965675058),\n","  (19, 0.03569158403254513)],\n"," [(0, 0.029560754515763513),\n","  (1, 0.03109378124375125),\n","  (2, 0.0454242484836366),\n","  (3, 0.0034993001399720057),\n","  (4, 0.03709258148370326),\n","  (5, 0.011764313803905885),\n","  (6, 0.006698660267946411),\n","  (7, 0.003365993467973072),\n","  (8, 0.024561754315803506),\n","  (9, 0.003432646803972539),\n","  (10, 0.021695660867826434),\n","  (11, 0.003899220155968806),\n","  (12, 0.007098580283943211),\n","  (13, 0.015963473971872293),\n","  (14, 0.006898620275944811),\n","  (15, 0.004299140171965607),\n","  (16, 0.005365593547957076),\n","  (17, 0.014030527227887757),\n","  (18, 0.3510297940411918),\n","  (19, 0.3732253549290142)],\n"," [(0, 0.08454519532363844),\n","  (1, 0.0083166999334664),\n","  (2, 0.018676931850584545),\n","  (3, 0.06563064347495486),\n","  (4, 0.003849443969204448),\n","  (5, 0.0058454519532363846),\n","  (6, 0.011833475905332192),\n","  (7, 0.0062256439501948485),\n","  (8, 0.013639387890884896),\n","  (9, 0.005750403953996768),\n","  (10, 0.020292747837658016),\n","  (11, 0.0154452998764376),\n","  (12, 0.051563539587491686),\n","  (13, 0.006795931945632544),\n","  (14, 0.15469061876247506),\n","  (15, 0.00499001996007984),\n","  (16, 0.008221651934226784),\n","  (17, 0.1953711624370307),\n","  (18, 0.25325539397395685),\n","  (19, 0.06506035547951716)],\n"," [(0, 0.051856884057971),\n","  (1, 0.005057367149758453),\n","  (2, 0.019701086956521736),\n","  (3, 0.04400664251207729),\n","  (4, 0.02302234299516908),\n","  (5, 0.01683272946859903),\n","  (6, 0.007170893719806761),\n","  (7, 0.009586352657004828),\n","  (8, 0.4005887681159419),\n","  (9, 0.008831521739130432),\n","  (10, 0.07389794685990336),\n","  (11, 0.011699879227053138),\n","  (12, 0.09125905797101448),\n","  (13, 0.04430857487922705),\n","  (14, 0.05880132850241544),\n","  (15, 0.006717995169082124),\n","  (16, 0.019399154589371977),\n","  (17, 0.007623792270531399),\n","  (18, 0.02453200483091787),\n","  (19, 0.0751056763285024)],\n"," [(0, 0.11946004993757804),\n","  (1, 0.047830836454431966),\n","  (2, 0.04236891385767791),\n","  (3, 0.008192883895131087),\n","  (4, 0.014122971285892636),\n","  (5, 0.015527465667915108),\n","  (6, 0.1032303370786517),\n","  (7, 0.009597378277153559),\n","  (8, 0.0061641697877652946),\n","  (9, 0.009129213483146069),\n","  (10, 0.04424157303370787),\n","  (11, 0.03191323345817729),\n","  (12, 0.19171348314606745),\n","  (13, 0.16237515605493136),\n","  (14, 0.00990948813982522),\n","  (15, 0.020677278401997505),\n","  (16, 0.05063982521847691),\n","  (17, 0.08450374531835207),\n","  (18, 0.011313982521847692),\n","  (19, 0.017088014981273412)],\n"," [(0, 0.19781402197304337),\n","  (1, 0.027579567334919015),\n","  (2, 0.0052667346245327895),\n","  (3, 0.022482727375693737),\n","  (4, 0.008098312379657946),\n","  (5, 0.07628270472307169),\n","  (6, 0.007305470608222901),\n","  (7, 0.011156416355193113),\n","  (8, 0.040831351228904744),\n","  (9, 0.0045871559633027525),\n","  (10, 0.1145656359723638),\n","  (11, 0.028825461547174084),\n","  (12, 0.11071469022539358),\n","  (13, 0.00764525993883792),\n","  (14, 0.01908483406954355),\n","  (15, 0.005719787065352815),\n","  (16, 0.009683995922528032),\n","  (17, 0.013987994110318269),\n","  (18, 0.2692830445124023),\n","  (19, 0.01908483406954355)],\n"," [(0, 0.02238984674329502),\n","  (1, 0.007143997445721584),\n","  (2, 0.010496487867177523),\n","  (3, 0.013050766283524905),\n","  (4, 0.010656130268199233),\n","  (5, 0.006744891443167305),\n","  (6, 0.3368853767560664),\n","  (7, 0.004669540229885058),\n","  (8, 0.004270434227330779),\n","  (9, 0.022230204342273307),\n","  (10, 0.04026979565772669),\n","  (11, 0.02015485312899106),\n","  (12, 0.03468231162196679),\n","  (13, 0.2236190932311622),\n","  (14, 0.005946679438058748),\n","  (15, 0.056872605363984675),\n","  (16, 0.02071360153256705),\n","  (17, 0.019755747126436782),\n","  (18, 0.005946679438058748),\n","  (19, 0.13350095785440613)],\n"," [(0, 0.15430720899470898),\n","  (1, 0.003761574074074074),\n","  (2, 0.15108300264550265),\n","  (3, 0.014839616402116403),\n","  (4, 0.005580357142857143),\n","  (5, 0.02161871693121693),\n","  (6, 0.014839616402116403),\n","  (7, 0.010209986772486773),\n","  (8, 0.014095568783068783),\n","  (9, 0.018725198412698412),\n","  (10, 0.0078125),\n","  (11, 0.006985780423280423),\n","  (12, 0.04245205026455026),\n","  (13, 0.11429398148148148),\n","  (14, 0.2559937169312169),\n","  (15, 0.004505621693121693),\n","  (16, 0.060474537037037035),\n","  (17, 0.015997023809523808),\n","  (18, 0.00888723544973545),\n","  (19, 0.07353670634920635)],\n"," [(0, 0.026671494086410806),\n","  (1, 0.025464639150374117),\n","  (2, 0.06842867487328022),\n","  (3, 0.009453696998954058),\n","  (4, 0.006476788156730226),\n","  (5, 0.1296564486282082),\n","  (6, 0.029165660954219963),\n","  (7, 0.010901922922198083),\n","  (8, 0.44142730710435263),\n","  (9, 0.005430847212165097),\n","  (10, 0.07494569152787833),\n","  (11, 0.03962507039987126),\n","  (12, 0.005350390216429318),\n","  (13, 0.024901440180223663),\n","  (14, 0.03874004344677769),\n","  (15, 0.003982621288921071),\n","  (16, 0.012269691849706329),\n","  (17, 0.004626277254807304),\n","  (18, 0.02393595623139431),\n","  (19, 0.018545337517097107)],\n"," [(0, 0.01914828431372549),\n","  (1, 0.006484885620915033),\n","  (2, 0.004850898692810458),\n","  (3, 0.010671977124183007),\n","  (4, 0.018024918300653593),\n","  (5, 0.010876225490196078),\n","  (6, 0.020986519607843136),\n","  (7, 0.00883374183006536),\n","  (8, 0.016288807189542485),\n","  (9, 0.009242238562091503),\n","  (10, 0.09543504901960784),\n","  (11, 0.010978349673202614),\n","  (12, 0.01108047385620915),\n","  (13, 0.04907066993464052),\n","  (14, 0.2667994281045752),\n","  (15, 0.036100898692810454),\n","  (16, 0.02619485294117647),\n","  (17, 0.2376940359477124),\n","  (18, 0.1254595588235294),\n","  (19, 0.015778186274509803)],\n"," [(0, 0.029107077374404107),\n","  (1, 0.13004217088375505),\n","  (2, 0.007654932159882655),\n","  (3, 0.017097543087642098),\n","  (4, 0.01178034470113678),\n","  (5, 0.018747708104143748),\n","  (6, 0.00407957462412908),\n","  (7, 0.02323982398239824),\n","  (8, 0.01278877887788779),\n","  (9, 0.019756142280894756),\n","  (10, 0.01398056472313898),\n","  (11, 0.21713421342134212),\n","  (12, 0.15956178951228456),\n","  (13, 0.0038962229556288963),\n","  (14, 0.01608910891089109),\n","  (15, 0.005088008800880088),\n","  (16, 0.05092592592592592),\n","  (17, 0.00848001466813348),\n","  (18, 0.20209937660432709),\n","  (19, 0.04845067840117345)],\n"," [(0, 0.008699863073172361),\n","  (1, 0.003990916073873693),\n","  (2, 0.15243963530708346),\n","  (3, 0.01574658517850583),\n","  (4, 0.007631165881842167),\n","  (5, 0.13450556056507365),\n","  (6, 0.005493771499181779),\n","  (7, 0.0062952943926794245),\n","  (8, 0.4636642954947734),\n","  (9, 0.014544300838259359),\n","  (10, 0.04152890491934676),\n","  (11, 0.005894532945930601),\n","  (12, 0.004258090371706242),\n","  (13, 0.03899074908993755),\n","  (14, 0.01988778679491033),\n","  (15, 0.0040577096483318306),\n","  (16, 0.006796246201115453),\n","  (17, 0.029572855091340212),\n","  (18, 0.01497845907223725),\n","  (19, 0.02102327756069866)],\n"," [(0, 0.005018765820022694),\n","  (1, 0.014183468621803268),\n","  (2, 0.036877018416688494),\n","  (3, 0.004931482936196213),\n","  (4, 0.04516889238020425),\n","  (5, 0.011652264990835298),\n","  (6, 0.008597364056908441),\n","  (7, 0.004931482936196213),\n","  (8, 0.007899100986296589),\n","  (9, 0.01531814611154753),\n","  (10, 0.02919612463995811),\n","  (11, 0.008946495592214368),\n","  (12, 0.1943353408396614),\n","  (13, 0.1760932181199267),\n","  (14, 0.0573012132320852),\n","  (15, 0.004320502749410841),\n","  (16, 0.14973378720432926),\n","  (17, 0.007200837915684736),\n","  (18, 0.016016409182159384),\n","  (19, 0.20227808326787122)],\n"," [(0, 0.18298472036789795),\n","  (1, 0.04739652870493992),\n","  (2, 0.011200118676754191),\n","  (3, 0.007491470108292538),\n","  (4, 0.01045838896306186),\n","  (5, 0.014167037531523513),\n","  (6, 0.01238688621866192),\n","  (7, 0.006749740394600207),\n","  (8, 0.02840824803441626),\n","  (9, 0.02870493991989319),\n","  (10, 0.0803293279928794),\n","  (11, 0.023216140038569946),\n","  (12, 0.1709686990060822),\n","  (13, 0.029446669633585523),\n","  (14, 0.15020026702269693),\n","  (15, 0.00778816199376947),\n","  (16, 0.02870493991989319),\n","  (17, 0.0540720961281709),\n","  (18, 0.08107105770657172),\n","  (19, 0.02425456163773921)],\n"," [(0, 0.007076092680358083),\n","  (1, 0.004772248551869405),\n","  (2, 0.037750131648235916),\n","  (3, 0.025177725118483412),\n","  (4, 0.061644286466561345),\n","  (5, 0.017607951553449182),\n","  (6, 0.010104002106371775),\n","  (7, 0.006615323854660348),\n","  (8, 0.06138098999473407),\n","  (9, 0.011354660347551343),\n","  (10, 0.05973538704581358),\n","  (11, 0.008787519747235387),\n","  (12, 0.019319378620326486),\n","  (13, 0.15333728278041076),\n","  (14, 0.022018167456556083),\n","  (15, 0.004114007372301211),\n","  (16, 0.14293707214323328),\n","  (17, 0.019714323328067405),\n","  (18, 0.27971958925750395),\n","  (19, 0.04683385992627699)],\n"," [(0, 0.07816785518376303),\n","  (1, 0.020997135369049794),\n","  (2, 0.029347229840921557),\n","  (3, 0.030017675382458708),\n","  (4, 0.2148777960626562),\n","  (5, 0.003626500883769123),\n","  (6, 0.01740110928262327),\n","  (7, 0.00996525873102944),\n","  (8, 0.008014871701103187),\n","  (9, 0.01033095629914061),\n","  (10, 0.31221429877491313),\n","  (11, 0.0035655512890839275),\n","  (12, 0.007039678186140062),\n","  (13, 0.15788992503199853),\n","  (14, 0.055616505150240754),\n","  (15, 0.011488998598159323),\n","  (16, 0.014048881574937527),\n","  (17, 0.0031998537209727554),\n","  (18, 0.007405375754251234),\n","  (19, 0.004784543182787835)],\n"," [(0, 0.022388059701492536),\n","  (1, 0.030192176373036776),\n","  (2, 0.053019217637303676),\n","  (3, 0.19495658960101453),\n","  (4, 0.010876987610964783),\n","  (5, 0.01858355282411472),\n","  (6, 0.013608428446005268),\n","  (7, 0.006974929275192664),\n","  (8, 0.015657009072285632),\n","  (9, 0.010291678860598966),\n","  (10, 0.0212174422007609),\n","  (11, 0.008340649692712906),\n","  (12, 0.04560530679933665),\n","  (13, 0.16286215978928886),\n","  (14, 0.09672227099795142),\n","  (15, 0.11964686372061262),\n","  (16, 0.05711637888986441),\n","  (17, 0.010681884694176177),\n","  (18, 0.008925958443078724),\n","  (19, 0.09233245537020779)],\n"," [(0, 0.018394149690321618),\n","  (1, 0.01205133945227968),\n","  (2, 0.03436310723080367),\n","  (3, 0.07413625848817253),\n","  (4, 0.005857771808074025),\n","  (5, 0.04966047309902246),\n","  (6, 0.026005521975971942),\n","  (7, 0.020185060816356987),\n","  (8, 0.006454742183419148),\n","  (9, 0.02764719050817103),\n","  (10, 0.05771957316618163),\n","  (11, 0.29897022610252966),\n","  (12, 0.009514215357062906),\n","  (13, 0.006230878292664726),\n","  (14, 0.12629654503395268),\n","  (15, 0.006081635698828446),\n","  (16, 0.024886202522199835),\n","  (17, 0.17166629356018206),\n","  (18, 0.014066114469069472),\n","  (19, 0.009812700544735467)],\n"," [(0, 0.008360102133267442),\n","  (1, 0.11716497817313236),\n","  (2, 0.1211185240095544),\n","  (3, 0.022444609175520963),\n","  (4, 0.09756198006753974),\n","  (5, 0.020962029486862696),\n","  (6, 0.01577300057655877),\n","  (7, 0.01289020673750103),\n","  (8, 0.007289350135903138),\n","  (9, 0.004324190758586607),\n","  (10, 0.1841281607775307),\n","  (11, 0.0363643851412569),\n","  (12, 0.036776212832550864),\n","  (13, 0.1618894654476567),\n","  (14, 0.013219668890536199),\n","  (15, 0.009266123054114159),\n","  (16, 0.03166954946050572),\n","  (17, 0.016514290420887902),\n","  (18, 0.04394201466106581),\n","  (19, 0.03834115805946792)],\n"," [(0, 0.0360801531014297),\n","  (1, 0.007261060452549815),\n","  (2, 0.007261060452549815),\n","  (3, 0.014803557356748848),\n","  (4, 0.009625126646403245),\n","  (5, 0.2981537768771812),\n","  (6, 0.007711358775188563),\n","  (7, 0.00669818754925138),\n","  (8, 0.21079590228526401),\n","  (9, 0.09563210627040415),\n","  (10, 0.030676573229764723),\n","  (11, 0.005459867161994822),\n","  (12, 0.05746932342677024),\n","  (13, 0.007373635033209502),\n","  (14, 0.06861420691207927),\n","  (15, 0.0037712484520995163),\n","  (16, 0.06433637284701116),\n","  (17, 0.03653045142406845),\n","  (18, 0.026398739164696615),\n","  (19, 0.005347292581335135)],\n"," [(0, 0.029253530598520518),\n","  (1, 0.02125831278487634),\n","  (2, 0.04046177986998432),\n","  (3, 0.009153403571695437),\n","  (4, 0.4654038705820819),\n","  (5, 0.00556676380482702),\n","  (6, 0.010199506837032057),\n","  (7, 0.029552417245759553),\n","  (8, 0.015056414854666371),\n","  (9, 0.008256743629978332),\n","  (10, 0.1753343794365987),\n","  (11, 0.005417320481207503),\n","  (12, 0.01744750803257865),\n","  (13, 0.0064634237465441245),\n","  (14, 0.09269222147500562),\n","  (15, 0.004670103863109916),\n","  (16, 0.023873570948217897),\n","  (17, 0.0064634237465441245),\n","  (18, 0.005342598819397745),\n","  (19, 0.028132705671374138)],\n"," [(0, 0.014142834817940304),\n","  (1, 0.06321300203156743),\n","  (2, 0.03414596030629786),\n","  (3, 0.03305203938115331),\n","  (4, 0.008360681356461947),\n","  (5, 0.11431473667760587),\n","  (6, 0.008516955774339741),\n","  (7, 0.009767151117362087),\n","  (8, 0.4956243162994218),\n","  (9, 0.007266760431317393),\n","  (10, 0.0972808251289264),\n","  (11, 0.0236755743084857),\n","  (12, 0.015236755743084857),\n","  (13, 0.017580872011251757),\n","  (14, 0.0064853883419284265),\n","  (15, 0.007735583684950774),\n","  (16, 0.007735583684950774),\n","  (17, 0.01492420690732927),\n","  (18, 0.01258009063916237),\n","  (19, 0.008360681356461947)],\n"," [(0, 0.00825524319500223),\n","  (1, 0.12605979473449352),\n","  (2, 0.02922802320392682),\n","  (3, 0.015692399226535772),\n","  (4, 0.04677971143834598),\n","  (5, 0.017923546035995837),\n","  (6, 0.018072289156626505),\n","  (7, 0.007362784471218206),\n","  (8, 0.16741038226982002),\n","  (9, 0.01494868362338242),\n","  (10, 0.06819872080916257),\n","  (11, 0.013907481778967723),\n","  (12, 0.006470325747434181),\n","  (13, 0.042763647181317865),\n","  (14, 0.14733006098467946),\n","  (15, 0.007660270712479548),\n","  (16, 0.009445188160047599),\n","  (17, 0.009742674401308939),\n","  (18, 0.15491596013684367),\n","  (19, 0.08783281273241113)],\n"," [(0, 0.3516540572615339),\n","  (1, 0.004969589081738615),\n","  (2, 0.01016169707758493),\n","  (3, 0.014463729417000446),\n","  (4, 0.023661177866785347),\n","  (5, 0.00630470256638481),\n","  (6, 0.0094199673638926),\n","  (7, 0.008974929535677202),\n","  (8, 0.09991099243435693),\n","  (9, 0.014167037531523515),\n","  (10, 0.007936507936507938),\n","  (11, 0.008826583592938735),\n","  (12, 0.18313306631063644),\n","  (13, 0.009123275478415667),\n","  (14, 0.13595905651980422),\n","  (15, 0.007194778222815607),\n","  (16, 0.03538050734312417),\n","  (17, 0.04798991247589379),\n","  (18, 0.013276961875092717),\n","  (19, 0.007491470108292539)],\n"," [(0, 0.01988348530901722),\n","  (1, 0.03423674434312732),\n","  (2, 0.04267983789260384),\n","  (3, 0.007894292468760552),\n","  (4, 0.08016717325227962),\n","  (5, 0.05500675447483957),\n","  (6, 0.008738601823708204),\n","  (7, 0.020558932792975344),\n","  (8, 0.041244511989192836),\n","  (9, 0.005108071597433299),\n","  (10, 0.38935325903411),\n","  (11, 0.007303275920297195),\n","  (12, 0.020896656534954403),\n","  (13, 0.02047450185748058),\n","  (14, 0.07636778115501518),\n","  (15, 0.005530226274907125),\n","  (16, 0.027482269503546094),\n","  (17, 0.015746369469773723),\n","  (18, 0.02131881121242823),\n","  (19, 0.10000844309354945)],\n"," [(0, 0.05008110300081103),\n","  (1, 0.010205460935387942),\n","  (2, 0.029129494457961612),\n","  (3, 0.007231684238983509),\n","  (4, 0.026020546093538793),\n","  (5, 0.04859421465260881),\n","  (6, 0.009124087591240875),\n","  (7, 0.005474452554744526),\n","  (8, 0.39666125979994593),\n","  (9, 0.02534468775344688),\n","  (10, 0.021019194376858612),\n","  (11, 0.004798594214652609),\n","  (12, 0.016828872668288728),\n","  (13, 0.020343336036766694),\n","  (14, 0.046972154636388215),\n","  (15, 0.005068937550689375),\n","  (16, 0.13064341713976751),\n","  (17, 0.012368207623682076),\n","  (18, 0.0690051365233847),\n","  (19, 0.06508515815085159)],\n"," [(0, 0.3484126984126984),\n","  (1, 0.030586080586080585),\n","  (2, 0.03034188034188034),\n","  (3, 0.012026862026862028),\n","  (4, 0.01105006105006105),\n","  (5, 0.01752136752136752),\n","  (6, 0.006166056166056166),\n","  (7, 0.007142857142857143),\n","  (8, 0.0057997557997558),\n","  (9, 0.0068986568986568984),\n","  (10, 0.01214896214896215),\n","  (11, 0.0068986568986568984),\n","  (12, 0.017277167277167278),\n","  (13, 0.017643467643467643),\n","  (14, 0.2688034188034188),\n","  (15, 0.03376068376068376),\n","  (16, 0.05793650793650794),\n","  (17, 0.033394383394383394),\n","  (18, 0.046825396825396826),\n","  (19, 0.029365079365079365)],\n"," [(0, 0.13050055005500552),\n","  (1, 0.003987898789878989),\n","  (2, 0.007379904657132381),\n","  (3, 0.004171250458379172),\n","  (4, 0.006554822148881556),\n","  (5, 0.004812981298129814),\n","  (6, 0.20090759075907594),\n","  (7, 0.006188118811881189),\n","  (8, 0.016822515584891826),\n","  (9, 0.005913091309130914),\n","  (10, 0.010221855518885223),\n","  (11, 0.013338833883388341),\n","  (12, 0.04478364503116979),\n","  (13, 0.028923725705903926),\n","  (14, 0.14425192519251928),\n","  (15, 0.008846718005133848),\n","  (16, 0.018197653098643202),\n","  (17, 0.2112669600293363),\n","  (18, 0.052026035936927034),\n","  (19, 0.08090392372570591)],\n"," [(0, 0.04876195584160187),\n","  (1, 0.009341199606686335),\n","  (2, 0.27411280951103967),\n","  (3, 0.005765620809868598),\n","  (4, 0.013542504692947172),\n","  (5, 0.03973361937963708),\n","  (6, 0.06297488155895237),\n","  (7, 0.01506212568159471),\n","  (8, 0.004603557700902834),\n","  (9, 0.0052292839903459384),\n","  (10, 0.06976848127290607),\n","  (11, 0.017565030839367127),\n","  (12, 0.08129972289264326),\n","  (13, 0.18803074997765265),\n","  (14, 0.01658174667024225),\n","  (15, 0.005855010279789042),\n","  (16, 0.06646107088584965),\n","  (17, 0.029721998748547425),\n","  (18, 0.003888441941539287),\n","  (19, 0.04170018771788684)],\n"," [(0, 0.022820270695624804),\n","  (1, 0.004668974923932431),\n","  (2, 0.01925296401217081),\n","  (3, 0.01589549889833176),\n","  (4, 0.012957716923722589),\n","  (5, 0.145997272059595),\n","  (6, 0.018413597733711047),\n","  (7, 0.05691952575805267),\n","  (8, 0.012433112999685237),\n","  (9, 0.016944706746406465),\n","  (10, 0.02481376560696674),\n","  (11, 0.12648200608540552),\n","  (12, 0.05020459553037457),\n","  (13, 0.13088867904731927),\n","  (14, 0.007082152974504249),\n","  (15, 0.007396915328926661),\n","  (16, 0.0217710628475501),\n","  (17, 0.28554191585353056),\n","  (18, 0.009495331025076067),\n","  (19, 0.01001993494911342)],\n"," [(0, 0.02484314916008905),\n","  (1, 0.22702894150981584),\n","  (2, 0.06552317344667072),\n","  (3, 0.014318963772515686),\n","  (4, 0.12917425622343656),\n","  (5, 0.022819267354786482),\n","  (6, 0.004806719287593604),\n","  (7, 0.006223436551305404),\n","  (8, 0.03263509411050395),\n","  (9, 0.004705525197328476),\n","  (10, 0.11025096134385752),\n","  (11, 0.012092693786682858),\n","  (12, 0.03182554138838292),\n","  (13, 0.1705626391418741),\n","  (14, 0.018872697834446468),\n","  (15, 0.0037947783849423193),\n","  (16, 0.019075086014976726),\n","  (17, 0.03293867638129933),\n","  (18, 0.010271200161910545),\n","  (19, 0.05823719894758146)],\n"," [(0, 0.012888581101877503),\n","  (1, 0.021968297937827026),\n","  (2, 0.022353031702062178),\n","  (3, 0.014581409664512159),\n","  (4, 0.019506001846722073),\n","  (5, 0.011657433056325025),\n","  (6, 0.05205447830101571),\n","  (7, 0.13631117266851342),\n","  (8, 0.005578639581409665),\n","  (9, 0.005501692828562636),\n","  (10, 0.11484302862419207),\n","  (11, 0.041897506925207766),\n","  (12, 0.06221144967682365),\n","  (13, 0.10299322868574948),\n","  (14, 0.04335949522930133),\n","  (15, 0.007963988919667592),\n","  (16, 0.04089719913819637),\n","  (17, 0.06298091720529395),\n","  (18, 0.022122191443521087),\n","  (19, 0.19833025546321947)],\n"," [(0, 0.034828101644245144),\n","  (1, 0.01340308918784255),\n","  (2, 0.028450423517688093),\n","  (3, 0.004534130543099153),\n","  (4, 0.05844544095665172),\n","  (5, 0.005430991529646238),\n","  (6, 0.10119581464872944),\n","  (7, 0.01649227703039362),\n","  (8, 0.0034379671150971598),\n","  (9, 0.0074240159441953165),\n","  (10, 0.009118086696562033),\n","  (11, 0.019581464872944692),\n","  (12, 0.24977578475336323),\n","  (13, 0.23263577478824116),\n","  (14, 0.008719481813652218),\n","  (15, 0.03413054309915296),\n","  (16, 0.05137020428500249),\n","  (17, 0.03891380169407075),\n","  (18, 0.04070752366716492),\n","  (19, 0.041405082212257104)],\n"," [(0, 0.05235042735042735),\n","  (1, 0.010739991003148899),\n","  (2, 0.036156095366621684),\n","  (3, 0.01748762932973459),\n","  (4, 0.02344804318488529),\n","  (5, 0.018837156995051733),\n","  (6, 0.051563202878992354),\n","  (7, 0.006016644174538912),\n","  (8, 0.013663967611336033),\n","  (9, 0.006016644174538912),\n","  (10, 0.010065227170490328),\n","  (11, 0.005904183535762483),\n","  (12, 0.07236842105263158),\n","  (13, 0.43640350877192985),\n","  (14, 0.051000899685110214),\n","  (15, 0.005566801619433198),\n","  (16, 0.07799145299145299),\n","  (17, 0.023335582546108864),\n","  (18, 0.020411605937921726),\n","  (19, 0.06067251461988304)],\n"," [(0, 0.03555346109540265),\n","  (1, 0.02675672845324117),\n","  (2, 0.3639648130694313),\n","  (3, 0.01335218347470939),\n","  (4, 0.15629908890983346),\n","  (5, 0.005812126924285265),\n","  (6, 0.09880615771284948),\n","  (7, 0.02686145146088595),\n","  (8, 0.010943554298879462),\n","  (9, 0.004869619855482249),\n","  (10, 0.02612839040737249),\n","  (11, 0.005393234893706147),\n","  (12, 0.02633783642266205),\n","  (13, 0.11650434600481722),\n","  (14, 0.0069640800083778385),\n","  (15, 0.014504136558801966),\n","  (16, 0.03125981778196669),\n","  (17, 0.009686878207142107),\n","  (18, 0.008220756100115193),\n","  (19, 0.011781338360037696)],\n"," [(0, 0.02045779685264664),\n","  (1, 0.004625655698617073),\n","  (2, 0.14701955174058182),\n","  (3, 0.005388650453028136),\n","  (4, 0.01168335717691941),\n","  (5, 0.011492608488316644),\n","  (6, 0.08187887458273725),\n","  (7, 0.025035765379113024),\n","  (8, 0.004053409632808775),\n","  (9, 0.010824988078206964),\n","  (10, 0.03819742489270387),\n","  (11, 0.006056270863137817),\n","  (12, 0.030281354315689082),\n","  (13, 0.5076299475441107),\n","  (14, 0.00453028135431569),\n","  (15, 0.005484024797329519),\n","  (16, 0.029518359561278018),\n","  (17, 0.03180734382451121),\n","  (18, 0.007200762994754412),\n","  (19, 0.01683357176919409)],\n"," [(0, 0.10509691934925577),\n","  (1, 0.0037642782969885763),\n","  (2, 0.0207251644167532),\n","  (3, 0.005321910695742471),\n","  (4, 0.01709068881966078),\n","  (5, 0.010081343025268258),\n","  (6, 0.0939338871581862),\n","  (7, 0.0042834890965732075),\n","  (8, 0.007658359293873311),\n","  (9, 0.13149013499480786),\n","  (10, 0.02340775354794046),\n","  (11, 0.0036777431637244713),\n","  (12, 0.12967289719626166),\n","  (13, 0.14239356178608512),\n","  (14, 0.080607476635514),\n","  (15, 0.005581516095534787),\n","  (16, 0.0788767739702319),\n","  (17, 0.05352197992384907),\n","  (18, 0.036907234337140875),\n","  (19, 0.04590688819660781)],\n"," [(0, 0.03872222222222223),\n","  (1, 0.007055555555555556),\n","  (2, 0.007055555555555556),\n","  (3, 0.31450000000000006),\n","  (4, 0.00838888888888889),\n","  (5, 0.003944444444444446),\n","  (6, 0.04972222222222223),\n","  (7, 0.060611111111111116),\n","  (8, 0.0058333333333333345),\n","  (9, 0.010500000000000002),\n","  (10, 0.0673888888888889),\n","  (11, 0.007722222222222223),\n","  (12, 0.011722222222222224),\n","  (13, 0.2061666666666667),\n","  (14, 0.007833333333333335),\n","  (15, 0.026500000000000003),\n","  (16, 0.0792777777777778),\n","  (17, 0.007611111111111112),\n","  (18, 0.018611111111111113),\n","  (19, 0.06083333333333334)],\n"," [(0, 0.028393351800554016),\n","  (1, 0.0057710064635272396),\n","  (2, 0.013004001231148046),\n","  (3, 0.025623268698060944),\n","  (4, 0.025777162203755002),\n","  (5, 0.007463835026161896),\n","  (6, 0.018236380424746075),\n","  (7, 0.14212065250846415),\n","  (8, 0.006232686980609419),\n","  (9, 0.03177900892582333),\n","  (10, 0.027316097260695598),\n","  (11, 0.011465066174207448),\n","  (12, 0.180440135426285),\n","  (13, 0.2492305324715297),\n","  (14, 0.02162203755001539),\n","  (15, 0.040089258233302554),\n","  (16, 0.025777162203755002),\n","  (17, 0.044398276392736226),\n","  (18, 0.009156663588796552),\n","  (19, 0.0861034164358264)],\n"," [(0, 0.00887452549502411),\n","  (1, 0.040473991997537706),\n","  (2, 0.0076433774494716325),\n","  (3, 0.027136554837385862),\n","  (4, 0.04478301015697138),\n","  (5, 0.013901713347696727),\n","  (6, 0.013696522006771314),\n","  (7, 0.05801785164666051),\n","  (8, 0.0037447419718887862),\n","  (9, 0.008464142813173285),\n","  (10, 0.014209500359084847),\n","  (11, 0.014209500359084847),\n","  (12, 0.19795834615779215),\n","  (13, 0.38355391402482814),\n","  (14, 0.00538627269929209),\n","  (15, 0.019544475223145584),\n","  (16, 0.0435518621114189),\n","  (17, 0.05555555555555555),\n","  (18, 0.024776854416743613),\n","  (19, 0.014517287370472967)],\n"," [(0, 0.0099953509995351),\n","  (1, 0.01706183170618317),\n","  (2, 0.024314272431427243),\n","  (3, 0.03091585309158531),\n","  (4, 0.021524872152487214),\n","  (5, 0.009251510925151092),\n","  (6, 0.012226871222687122),\n","  (7, 0.00701999070199907),\n","  (8, 0.005811250581125058),\n","  (9, 0.010181311018131101),\n","  (10, 0.03119479311947931),\n","  (11, 0.004509530450953045),\n","  (12, 0.1901906090190609),\n","  (13, 0.32687122268712226),\n","  (14, 0.2193863319386332),\n","  (15, 0.005625290562529056),\n","  (16, 0.03472803347280335),\n","  (17, 0.0099953509995351),\n","  (18, 0.008786610878661089),\n","  (19, 0.020409112040911205)],\n"," [(0, 0.10213763145861911),\n","  (1, 0.005201188843164152),\n","  (2, 0.08190443529949702),\n","  (3, 0.01343164151806127),\n","  (4, 0.023376771833561957),\n","  (5, 0.021319158664837676),\n","  (6, 0.1300297210791038),\n","  (7, 0.0063443072702331965),\n","  (8, 0.006801554641060814),\n","  (9, 0.004629629629629629),\n","  (10, 0.03720850480109739),\n","  (11, 0.016060813900320072),\n","  (12, 0.00851623228166438),\n","  (13, 0.32241655235482397),\n","  (14, 0.007944673068129859),\n","  (15, 0.005086877000457247),\n","  (16, 0.17552583447645176),\n","  (17, 0.006001371742112483),\n","  (18, 0.006687242798353909),\n","  (19, 0.0193758573388203)],\n"," [(0, 0.024140594824256463),\n","  (1, 0.006630616711729109),\n","  (2, 0.08027552465559416),\n","  (3, 0.009205613492983131),\n","  (4, 0.017316853353933303),\n","  (5, 0.006373117033603706),\n","  (6, 0.028518089352388303),\n","  (7, 0.12005922492596882),\n","  (8, 0.005471868160164799),\n","  (9, 0.006501866872666407),\n","  (10, 0.014355607055491177),\n","  (11, 0.017703102871121406),\n","  (12, 0.08787176516029353),\n","  (13, 0.30224024719969095),\n","  (14, 0.005729367838290201),\n","  (15, 0.03714432856958928),\n","  (16, 0.15070168662289168),\n","  (17, 0.020535599330500833),\n","  (18, 0.01126561091798635),\n","  (19, 0.047959315050856176)],\n"," [(0, 0.056678835533089965),\n","  (1, 0.004727136572123936),\n","  (2, 0.004165496583356736),\n","  (3, 0.1055415145558364),\n","  (4, 0.008939436487877938),\n","  (5, 0.1576804268463915),\n","  (6, 0.004259103248151269),\n","  (7, 0.02691191612842835),\n","  (8, 0.03131142937377142),\n","  (9, 0.05602358887952823),\n","  (10, 0.041233735841991956),\n","  (11, 0.008377796499110739),\n","  (12, 0.09140690817186185),\n","  (13, 0.006973696527192737),\n","  (14, 0.008096976504727139),\n","  (15, 0.007722549845549004),\n","  (16, 0.239024618552841),\n","  (17, 0.118459234297482),\n","  (18, 0.006880089862398204),\n","  (19, 0.015585509688289808)],\n"," [(0, 0.00581953443724502),\n","  (1, 0.010259179265658747),\n","  (2, 0.04901607871370291),\n","  (3, 0.04517638588912887),\n","  (4, 0.008579313654907607),\n","  (5, 0.011219102471802255),\n","  (6, 0.08069354451643869),\n","  (7, 0.018658507319414448),\n","  (8, 0.009539236861051115),\n","  (9, 0.043256539476841854),\n","  (10, 0.08249340052795777),\n","  (11, 0.007619390448764099),\n","  (12, 0.011339092872570195),\n","  (13, 0.15004799616030717),\n","  (14, 0.00665946724262059),\n","  (15, 0.3440724742020638),\n","  (16, 0.04565634749220063),\n","  (17, 0.05345572354211663),\n","  (18, 0.00581953443724502),\n","  (19, 0.010619150467962564)],\n"," [(0, 0.008597364056908441),\n","  (1, 0.006851706380378809),\n","  (2, 0.058610456489482425),\n","  (3, 0.007026272148031773),\n","  (4, 0.007200837915684736),\n","  (5, 0.011564982107008818),\n","  (6, 0.06838613947804836),\n","  (7, 0.005367897355328621),\n","  (8, 0.004495068517063805),\n","  (9, 0.006502574845072883),\n","  (10, 0.021078816444095316),\n","  (11, 0.005106048703849176),\n","  (12, 0.006240726193593438),\n","  (13, 0.29610718338133896),\n","  (14, 0.0036222396787989886),\n","  (15, 0.41917604957667814),\n","  (16, 0.013310639783538452),\n","  (17, 0.007899100986296589),\n","  (18, 0.006066160425940475),\n","  (19, 0.036789735532862015)],\n"," [(0, 0.013421678559293234),\n","  (1, 0.022369464265488727),\n","  (2, 0.06642881413523614),\n","  (3, 0.19634160154037825),\n","  (4, 0.024294937138973828),\n","  (5, 0.01840525540831351),\n","  (6, 0.01387473100011326),\n","  (7, 0.013648204779703247),\n","  (8, 0.06552270925359609),\n","  (9, 0.010929890134783099),\n","  (10, 0.02508777891040887),\n","  (11, 0.013648204779703247),\n","  (12, 0.01749915052667346),\n","  (13, 0.07673575716389171),\n","  (14, 0.01296862611847321),\n","  (15, 0.34188469815381123),\n","  (16, 0.009230943481708005),\n","  (17, 0.008551364820477969),\n","  (18, 0.011156416355193112),\n","  (19, 0.037999773473779586)],\n"," [(0, 0.012753999135322092),\n","  (1, 0.005980688860066292),\n","  (2, 0.03768554546764664),\n","  (3, 0.09244847960801268),\n","  (4, 0.004827784983427006),\n","  (5, 0.019239083441418073),\n","  (6, 0.007277705721285488),\n","  (7, 0.02802997550079262),\n","  (8, 0.005836575875486381),\n","  (9, 0.014483354950281021),\n","  (10, 0.0345150598068886),\n","  (11, 0.00554834990632656),\n","  (12, 0.0804871018878801),\n","  (13, 0.2305087188355671),\n","  (14, 0.08783686410145554),\n","  (15, 0.266104626026805),\n","  (16, 0.021833117163856462),\n","  (17, 0.009583513474564058),\n","  (18, 0.02730941057789307),\n","  (19, 0.00771004467502522)],\n"," [(0, 0.003979977022813065),\n","  (1, 0.02597242737567701),\n","  (2, 0.08825701624815363),\n","  (3, 0.13552437223042837),\n","  (4, 0.013827342852453637),\n","  (5, 0.0435335631052027),\n","  (6, 0.014976202199245036),\n","  (7, 0.005785327424913837),\n","  (8, 0.07717872968980799),\n","  (9, 0.0035696701132447075),\n","  (10, 0.06010996225176433),\n","  (11, 0.008411291646151323),\n","  (12, 0.01858690300344658),\n","  (13, 0.07972263252913181),\n","  (14, 0.140365993763335),\n","  (15, 0.12543082225504681),\n","  (16, 0.0063597570983095365),\n","  (17, 0.011857869686525522),\n","  (18, 0.013909404234367309),\n","  (19, 0.12264073526998195)],\n"," [(0, 0.020357686453576867),\n","  (1, 0.010591070522577373),\n","  (2, 0.357496194824962),\n","  (3, 0.045979198376458655),\n","  (4, 0.009703196347031965),\n","  (5, 0.02162607813292745),\n","  (6, 0.04534500253678336),\n","  (7, 0.04572552004058854),\n","  (8, 0.014903602232369357),\n","  (9, 0.014523084728564182),\n","  (10, 0.06246829020801624),\n","  (11, 0.01858193810248605),\n","  (12, 0.011225266362252665),\n","  (13, 0.22025621511922885),\n","  (14, 0.007927447995941148),\n","  (15, 0.022260273972602742),\n","  (16, 0.01617199391171994),\n","  (17, 0.009576357179096906),\n","  (18, 0.0299974632166413),\n","  (19, 0.015284119736174532)],\n"," [(0, 0.04092400461115546),\n","  (1, 0.004921521681298218),\n","  (2, 0.0804735301942006),\n","  (3, 0.3466790813159529),\n","  (4, 0.018134255564423166),\n","  (5, 0.004300789216990336),\n","  (6, 0.059102598208743466),\n","  (7, 0.008734592533475216),\n","  (8, 0.005276225946617009),\n","  (9, 0.04509177972865124),\n","  (10, 0.009887381395761285),\n","  (11, 0.004389465283320033),\n","  (12, 0.025228340870798976),\n","  (13, 0.09749933492950254),\n","  (14, 0.03418462357009844),\n","  (15, 0.032145074044515394),\n","  (16, 0.02017380509000621),\n","  (17, 0.019198368360379536),\n","  (18, 0.024873636605480184),\n","  (19, 0.11878159084862996)],\n"," [(0, 0.019497471039321264),\n","  (1, 0.013460597161037691),\n","  (2, 0.011829009626366457),\n","  (3, 0.215940610213738),\n","  (4, 0.015581660956110297),\n","  (5, 0.010523739598629468),\n","  (6, 0.2661935062816121),\n","  (7, 0.019986947299722634),\n","  (8, 0.008892152063958233),\n","  (9, 0.08361886115190081),\n","  (10, 0.056045031815956936),\n","  (11, 0.01753956599771578),\n","  (12, 0.010197422091695222),\n","  (13, 0.06926089084679395),\n","  (14, 0.015907978463044547),\n","  (15, 0.013460597161037691),\n","  (16, 0.03206069505628978),\n","  (17, 0.0751346059716104),\n","  (18, 0.03401860009789526),\n","  (19, 0.010850057105563716)],\n"," [(0, 0.005529715762273902),\n","  (1, 0.10403100775193798),\n","  (2, 0.02496124031007752),\n","  (3, 0.01359173126614987),\n","  (4, 0.006356589147286822),\n","  (5, 0.322015503875969),\n","  (6, 0.0071834625322997414),\n","  (7, 0.003359173126614987),\n","  (8, 0.018036175710594314),\n","  (9, 0.0071834625322997414),\n","  (10, 0.07302325581395348),\n","  (11, 0.22940568475452197),\n","  (12, 0.014315245478036176),\n","  (13, 0.007286821705426357),\n","  (14, 0.028062015503875968),\n","  (15, 0.017002583979328165),\n","  (16, 0.007906976744186046),\n","  (17, 0.02186046511627907),\n","  (18, 0.03591731266149871),\n","  (19, 0.05297157622739018)],\n"," [(0, 0.005738332058148432),\n","  (1, 0.1450735356626711),\n","  (2, 0.09381110260987843),\n","  (3, 0.01041401003145456),\n","  (4, 0.01245430587435178),\n","  (5, 0.027331463062144012),\n","  (6, 0.017385020828020063),\n","  (7, 0.0052282580974241264),\n","  (8, 0.009478874436793336),\n","  (9, 0.0027629006205899856),\n","  (10, 0.08905041230978492),\n","  (11, 0.27369718609198335),\n","  (12, 0.003528011561676443),\n","  (13, 0.2169939641247981),\n","  (14, 0.00386806086882598),\n","  (15, 0.030051857519340305),\n","  (16, 0.008458726515344725),\n","  (17, 0.006418430672447505),\n","  (18, 0.033537362917623055),\n","  (19, 0.004718184136699822)],\n"," [(0, 0.01266416510318949),\n","  (1, 0.141494684177611),\n","  (2, 0.05331457160725452),\n","  (3, 0.04997915363769021),\n","  (4, 0.016520742130498225),\n","  (5, 0.031217427558890967),\n","  (6, 0.06332082551594745),\n","  (7, 0.009224515322076295),\n","  (8, 0.011934542422347297),\n","  (9, 0.01089222430685845),\n","  (10, 0.057588075880758795),\n","  (11, 0.06550969355847404),\n","  (12, 0.008286429018136334),\n","  (13, 0.024546591619762345),\n","  (14, 0.038722117990410664),\n","  (15, 0.01151761517615176),\n","  (16, 0.010058369814467373),\n","  (17, 0.031425891181988734),\n","  (18, 0.3381801125703564),\n","  (19, 0.013602251407129453)],\n"," [(0, 0.00797602581834947),\n","  (1, 0.00834485938220378),\n","  (2, 0.06274781005071461),\n","  (3, 0.02946058091286307),\n","  (4, 0.015260488704472107),\n","  (5, 0.02512678653757492),\n","  (6, 0.15126786537574918),\n","  (7, 0.0077916090364223145),\n","  (8, 0.006685108344859382),\n","  (9, 0.005855232826187183),\n","  (10, 0.033241124942369756),\n","  (11, 0.2669893960350392),\n","  (12, 0.011572153065929),\n","  (13, 0.02863070539419087),\n","  (14, 0.008898109727985246),\n","  (15, 0.15053019824804056),\n","  (16, 0.024850161364684185),\n","  (17, 0.020424158598432458),\n","  (18, 0.10322729368372521),\n","  (19, 0.03112033195020747)],\n"," [(0, 0.0057454890788224125),\n","  (1, 0.4405033238366572),\n","  (2, 0.06424501424501425),\n","  (3, 0.006315289648622982),\n","  (4, 0.04363722697056031),\n","  (5, 0.05389363722697056),\n","  (6, 0.021225071225071224),\n","  (7, 0.0070750237416904085),\n","  (8, 0.03347578347578348),\n","  (9, 0.006885090218423552),\n","  (10, 0.07288698955365622),\n","  (11, 0.05854700854700855),\n","  (12, 0.004131054131054131),\n","  (13, 0.057122507122507125),\n","  (14, 0.016761633428300095),\n","  (15, 0.0057454890788224125),\n","  (16, 0.004890788224121557),\n","  (17, 0.020275403608736944),\n","  (18, 0.02663817663817664),\n","  (19, 0.05)],\n"," [(0, 0.036049076371657024),\n","  (1, 0.28349875930521096),\n","  (2, 0.019644334160463198),\n","  (3, 0.07699200441135927),\n","  (4, 0.01468155500413565),\n","  (5, 0.008753791011855529),\n","  (6, 0.031637717121588096),\n","  (7, 0.04638819961400607),\n","  (8, 0.008202371105596913),\n","  (9, 0.009167355941549491),\n","  (10, 0.08829611248966089),\n","  (11, 0.01013234077750207),\n","  (12, 0.014405845051006344),\n","  (13, 0.1983043837882548),\n","  (14, 0.043768955059277644),\n","  (15, 0.033016266887234635),\n","  (16, 0.024744968293355395),\n","  (17, 0.009994485800937416),\n","  (18, 0.01661152467604081),\n","  (19, 0.02570995312930797)],\n"," [(0, 0.015640418393526742),\n","  (1, 0.08866193013617525),\n","  (2, 0.040408525754884544),\n","  (3, 0.13375764752318928),\n","  (4, 0.010311821590684823),\n","  (5, 0.2685514110913756),\n","  (6, 0.006167357410696665),\n","  (7, 0.010015788434971383),\n","  (8, 0.23618511940003947),\n","  (9, 0.011495954213538584),\n","  (10, 0.043664890467732385),\n","  (11, 0.029850009867771858),\n","  (12, 0.01218669824353661),\n","  (13, 0.011199921057825144),\n","  (14, 0.007844878626406157),\n","  (15, 0.0050819025064140514),\n","  (16, 0.01218669824353661),\n","  (17, 0.00922636668640221),\n","  (18, 0.01070653246496941),\n","  (19, 0.03685612788632327)],\n"," [(0, 0.010786172037187472),\n","  (1, 0.01087643289105515),\n","  (2, 0.011508258868128892),\n","  (3, 0.017104431807924904),\n","  (4, 0.0029334777506995215),\n","  (5, 0.5413394710713963),\n","  (6, 0.005370520805126816),\n","  (7, 0.0082588681288925),\n","  (8, 0.14301832295333514),\n","  (9, 0.005099738243523784),\n","  (10, 0.008890694105966242),\n","  (11, 0.005099738243523784),\n","  (12, 0.05853416373318891),\n","  (13, 0.005731564220597527),\n","  (14, 0.006814694467009658),\n","  (15, 0.0025724343352288115),\n","  (16, 0.11079519812257424),\n","  (17, 0.024957126094412852),\n","  (18, 0.009432259229172308),\n","  (19, 0.01087643289105515)],\n"," [(0, 0.007936507936507938),\n","  (1, 0.01248923341946598),\n","  (2, 0.016672818998400396),\n","  (3, 0.007567368032484313),\n","  (4, 0.0051064353389934794),\n","  (5, 0.02442475698289652),\n","  (6, 0.006582994955087979),\n","  (7, 0.004860342069644396),\n","  (8, 0.5273163528977484),\n","  (9, 0.0038759689922480624),\n","  (10, 0.13492063492063494),\n","  (11, 0.004614248800295313),\n","  (12, 0.006829088224437062),\n","  (13, 0.007936507936507938),\n","  (14, 0.010274393995324229),\n","  (15, 0.0051064353389934794),\n","  (16, 0.0071982281284606875),\n","  (17, 0.004983388704318937),\n","  (18, 0.1949673926418113),\n","  (19, 0.006336901685738896)],\n"," [(0, 0.0053863960113960125),\n","  (1, 0.004050925925925927),\n","  (2, 0.006009615384615387),\n","  (3, 0.0072560541310541325),\n","  (4, 0.007879273504273506),\n","  (5, 0.060941951566951584),\n","  (6, 0.009481837606837608),\n","  (7, 0.006009615384615387),\n","  (8, 0.7259170227920229),\n","  (9, 0.006365740740740742),\n","  (10, 0.061298076923076934),\n","  (11, 0.004407051282051283),\n","  (12, 0.003961894586894588),\n","  (13, 0.010016025641025644),\n","  (14, 0.03441061253561254),\n","  (15, 0.004228988603988605),\n","  (16, 0.004228988603988605),\n","  (17, 0.012775997150997155),\n","  (18, 0.018474002849002854),\n","  (19, 0.006899928774928777)],\n"," [(0, 0.0065487032700147435),\n","  (1, 0.1849683407060456),\n","  (2, 0.05781073813860698),\n","  (3, 0.13379304362910918),\n","  (4, 0.0206002255182583),\n","  (5, 0.039422326307572204),\n","  (6, 0.00706913001994969),\n","  (7, 0.09311301934252753),\n","  (8, 0.06240784109636567),\n","  (9, 0.008890623644722003),\n","  (10, 0.2848035389018995),\n","  (11, 0.021033914476537425),\n","  (12, 0.016436811518778727),\n","  (13, 0.005160898603521554),\n","  (14, 0.021467603434816546),\n","  (15, 0.0042067828953074845),\n","  (16, 0.00941105039465695),\n","  (17, 0.006895654436638042),\n","  (18, 0.01175297076936421),\n","  (19, 0.0042067828953074845)],\n"," [(0, 0.024896923564858866),\n","  (1, 0.13357648800084576),\n","  (2, 0.012844909609895337),\n","  (3, 0.13833386193043662),\n","  (4, 0.03166296648694365),\n","  (5, 0.028279945025901258),\n","  (6, 0.007030341473728724),\n","  (7, 0.009039010466222646),\n","  (8, 0.0599957712231737),\n","  (9, 0.017390844698171055),\n","  (10, 0.2450047573739296),\n","  (11, 0.0591500158579131),\n","  (12, 0.04223490855270113),\n","  (13, 0.004281636536631779),\n","  (14, 0.00650174437044085),\n","  (15, 0.0032244423300560312),\n","  (16, 0.049106670895443495),\n","  (17, 0.03367163547943757),\n","  (18, 0.055449836134897984),\n","  (19, 0.03832328998837086)],\n"," [(0, 0.15569774527726998),\n","  (1, 0.02325817590899858),\n","  (2, 0.0271176112126752),\n","  (3, 0.02264879138736543),\n","  (4, 0.008023562868169817),\n","  (5, 0.19672963640056879),\n","  (6, 0.007414178346536665),\n","  (7, 0.07200893763965063),\n","  (8, 0.20485476335567745),\n","  (9, 0.010461100954702419),\n","  (10, 0.014523664432256755),\n","  (11, 0.10938452163315053),\n","  (12, 0.017976843388177944),\n","  (13, 0.01228925451960187),\n","  (14, 0.021633150517976846),\n","  (15, 0.006195409303270364),\n","  (16, 0.017367458866544793),\n","  (17, 0.030164533820840955),\n","  (18, 0.013508023562868171),\n","  (19, 0.028742636603696935)],\n"," [(0, 0.0171957671957672),\n","  (1, 0.020258980785296577),\n","  (2, 0.012879420774157617),\n","  (3, 0.00800612642717906),\n","  (4, 0.008145363408521305),\n","  (5, 0.4428432191590087),\n","  (6, 0.006196045669729881),\n","  (7, 0.006196045669729881),\n","  (8, 0.21449456975772768),\n","  (9, 0.018031189083820665),\n","  (10, 0.030423280423280425),\n","  (11, 0.009120022277917016),\n","  (12, 0.028613199665831247),\n","  (13, 0.005917571707045392),\n","  (14, 0.01482873851294904),\n","  (15, 0.006196045669729881),\n","  (16, 0.1278891673628516),\n","  (17, 0.010233918128654972),\n","  (18, 0.005778334725703147),\n","  (19, 0.006752993595098859)],\n"," [(0, 0.04121193223157676),\n","  (1, 0.02426982642136992),\n","  (2, 0.012628624883068288),\n","  (3, 0.041107992932127635),\n","  (4, 0.006184388317222742),\n","  (5, 0.03965284273983993),\n","  (6, 0.005560752520528012),\n","  (7, 0.006911963413366594),\n","  (8, 0.06906766448394137),\n","  (9, 0.014707410872050722),\n","  (10, 0.014707410872050722),\n","  (11, 0.010341960295187611),\n","  (12, 0.23391539341024842),\n","  (13, 0.009302567300696393),\n","  (14, 0.24326993036066938),\n","  (15, 0.003585905830994699),\n","  (16, 0.022710736929633095),\n","  (17, 0.18059453279284898),\n","  (18, 0.006600145515019229),\n","  (19, 0.013668017877559505)],\n"," [(0, 0.006888888888888889),\n","  (1, 0.20342222222222223),\n","  (2, 0.011244444444444444),\n","  (3, 0.011422222222222222),\n","  (4, 0.008755555555555556),\n","  (5, 0.4244888888888889),\n","  (6, 0.013644444444444445),\n","  (7, 0.006888888888888889),\n","  (8, 0.050444444444444445),\n","  (9, 0.0060888888888888885),\n","  (10, 0.007333333333333333),\n","  (11, 0.053288888888888886),\n","  (12, 0.020933333333333335),\n","  (13, 0.10724444444444445),\n","  (14, 0.008044444444444444),\n","  (15, 0.0060888888888888885),\n","  (16, 0.018533333333333332),\n","  (17, 0.019866666666666668),\n","  (18, 0.009644444444444445),\n","  (19, 0.005733333333333333)],\n"," [(0, 0.02507489904910772),\n","  (1, 0.11925231210108113),\n","  (2, 0.005536016673179627),\n","  (3, 0.02520515826494724),\n","  (4, 0.007489904910772435),\n","  (5, 0.1865963266901133),\n","  (6, 0.006708349615735311),\n","  (7, 0.014523902566106549),\n","  (8, 0.36882896964960266),\n","  (9, 0.011267422170118535),\n","  (10, 0.031327341409404706),\n","  (11, 0.02572619512830532),\n","  (12, 0.028331379445095735),\n","  (13, 0.007229386479093394),\n","  (14, 0.008662237853328121),\n","  (15, 0.010876644522599972),\n","  (16, 0.07496417871564412),\n","  (17, 0.01257001432851374),\n","  (18, 0.02103686335808258),\n","  (19, 0.008792497069167642)],\n"," [(0, 0.005902397468662529),\n","  (1, 0.006024096385542169),\n","  (2, 0.09778507971279056),\n","  (3, 0.006997687720579287),\n","  (4, 0.0055373007180236095),\n","  (5, 0.009553364975051722),\n","  (6, 0.011500547645125958),\n","  (7, 0.006875988803699647),\n","  (8, 0.2585493489107947),\n","  (9, 0.008092977972496045),\n","  (10, 0.12687112084702445),\n","  (11, 0.008336375806255325),\n","  (12, 0.006510892053060728),\n","  (13, 0.018193988073506147),\n","  (14, 0.05920652306194475),\n","  (15, 0.009553364975051722),\n","  (16, 0.014421321650237313),\n","  (17, 0.006510892053060728),\n","  (18, 0.21193866374589265),\n","  (19, 0.12163806742119995)],\n"," [(0, 0.40418781725888325),\n","  (1, 0.015580936266215454),\n","  (2, 0.018401015228426396),\n","  (3, 0.00655668358714044),\n","  (4, 0.009094754653130288),\n","  (5, 0.007120699379582629),\n","  (6, 0.022208121827411168),\n","  (7, 0.0062746756909193456),\n","  (8, 0.006697687535250987),\n","  (9, 0.00796672306824591),\n","  (10, 0.018401015228426396),\n","  (11, 0.009235758601240835),\n","  (12, 0.011491821771009589),\n","  (13, 0.17181331077270162),\n","  (14, 0.0062746756909193456),\n","  (15, 0.013465877044557248),\n","  (16, 0.006133671742808798),\n","  (17, 0.02009306260575296),\n","  (18, 0.007120699379582629),\n","  (19, 0.2318809926677947)],\n"," [(0, 0.042661179698216735),\n","  (1, 0.005624142661179698),\n","  (2, 0.01083676268861454),\n","  (3, 0.003795153177869227),\n","  (4, 0.014220393232738911),\n","  (5, 0.004801097393689987),\n","  (6, 0.03305898491083676),\n","  (7, 0.016415180612711477),\n","  (8, 0.004526748971193415),\n","  (9, 0.06460905349794238),\n","  (10, 0.014311842706904436),\n","  (11, 0.005075445816186557),\n","  (12, 0.09862825788751714),\n","  (13, 0.06790123456790123),\n","  (14, 0.10832190214906265),\n","  (15, 0.3736168267032465),\n","  (16, 0.03251028806584362),\n","  (17, 0.06534064929126658),\n","  (18, 0.006264288980338363),\n","  (19, 0.027480566986739827)],\n"," [(0, 0.14586599770042855),\n","  (1, 0.007369081216682345),\n","  (2, 0.023675133270617748),\n","  (3, 0.006846451343158775),\n","  (4, 0.03339604891815616),\n","  (5, 0.004964983798473921),\n","  (6, 0.015731159193059476),\n","  (7, 0.007264555241977631),\n","  (8, 0.03674088010870701),\n","  (9, 0.06872582836834953),\n","  (10, 0.03130552942406188),\n","  (11, 0.008309814989024772),\n","  (12, 0.1623811017037734),\n","  (13, 0.04186265286923801),\n","  (14, 0.2785094596007108),\n","  (15, 0.006428347444339918),\n","  (16, 0.023779659245322462),\n","  (17, 0.019285042333019756),\n","  (18, 0.019180516358315042),\n","  (19, 0.058377756872582835)],\n"," [(0, 0.01783177038931515),\n","  (1, 0.007317419721511795),\n","  (2, 0.024083546462063093),\n","  (3, 0.020247229326513218),\n","  (4, 0.00888036373969878),\n","  (5, 0.008027848820687697),\n","  (6, 0.009874964478545042),\n","  (7, 0.02976697925547031),\n","  (8, 0.006606990622335893),\n","  (9, 0.010159136118215404),\n","  (10, 0.01811594202898551),\n","  (11, 0.007317419721511795),\n","  (12, 0.26577152600170506),\n","  (13, 0.44167377095765853),\n","  (14, 0.012432509235578293),\n","  (15, 0.03147200909349248),\n","  (16, 0.044685990338164255),\n","  (17, 0.006464904802500712),\n","  (18, 0.00930662119920432),\n","  (19, 0.019963057686842856)],\n"," [(0, 0.019437799043062202),\n","  (1, 0.010333599149388624),\n","  (2, 0.32665470494417864),\n","  (3, 0.015517012227538544),\n","  (4, 0.01890616693248272),\n","  (5, 0.007143806485911749),\n","  (6, 0.006014088250930356),\n","  (7, 0.011596225412014886),\n","  (8, 0.0031565656565656565),\n","  (9, 0.005482456140350877),\n","  (10, 0.020168793195108985),\n","  (11, 0.0381113769271664),\n","  (12, 0.012393673577884104),\n","  (13, 0.058512759170653905),\n","  (14, 0.23175837320574164),\n","  (15, 0.01737772461456672),\n","  (16, 0.024421850079744817),\n","  (17, 0.04136762360446571),\n","  (18, 0.014055023923444977),\n","  (19, 0.11759037745879851)],\n"," [(0, 0.0114148540074466),\n","  (1, 0.10665294924554183),\n","  (2, 0.2988928081520674),\n","  (3, 0.006025867136978248),\n","  (4, 0.01886145404663923),\n","  (5, 0.007103664511071918),\n","  (6, 0.004262198706643151),\n","  (7, 0.004066235547717029),\n","  (8, 0.06099353321575544),\n","  (9, 0.011904761904761904),\n","  (10, 0.0194493435234176),\n","  (11, 0.05903390162649422),\n","  (12, 0.004654125024495395),\n","  (13, 0.09793258867332942),\n","  (14, 0.02513227513227513),\n","  (15, 0.011120909269057417),\n","  (16, 0.003968253968253968),\n","  (17, 0.003968253968253968),\n","  (18, 0.017881638252008624),\n","  (19, 0.2266803840877915)],\n"," [(0, 0.01857506361323155),\n","  (1, 0.009414758269720101),\n","  (2, 0.4094147582697201),\n","  (3, 0.010093299406276506),\n","  (4, 0.018744698897370654),\n","  (5, 0.01603053435114504),\n","  (6, 0.0090754877014419),\n","  (7, 0.009584393553859203),\n","  (8, 0.018066157760814248),\n","  (9, 0.017217981340118746),\n","  (10, 0.02044105173876166),\n","  (11, 0.013994910941475827),\n","  (12, 0.03214588634435963),\n","  (13, 0.10135708227311281),\n","  (14, 0.032315521628498725),\n","  (15, 0.035029686174724343),\n","  (16, 0.009584393553859203),\n","  (17, 0.017387616624257845),\n","  (18, 0.01077184054283291),\n","  (19, 0.190754877014419)],\n"," [(0, 0.01523582405935347),\n","  (1, 0.005961844197138315),\n","  (2, 0.003930400989224519),\n","  (3, 0.007021727609962904),\n","  (4, 0.0043720190779014305),\n","  (5, 0.023714891361950187),\n","  (6, 0.005255255255255256),\n","  (7, 0.0041070482246952835),\n","  (8, 0.1397721250662427),\n","  (9, 0.004018724606959901),\n","  (10, 0.10294117647058823),\n","  (11, 0.005520226108461403),\n","  (12, 0.1937378555025614),\n","  (13, 0.006668433139021374),\n","  (14, 0.26837131248895957),\n","  (15, 0.003400459282812224),\n","  (16, 0.12943826179120296),\n","  (17, 0.015412471294824237),\n","  (18, 0.056129659070835544),\n","  (19, 0.004990284402049108)],\n"," [(0, 0.007266234844710181),\n","  (1, 0.07594253446271383),\n","  (2, 0.02611692409898688),\n","  (3, 0.012082710513203787),\n","  (4, 0.015155289818966949),\n","  (5, 0.0183939544926092),\n","  (6, 0.004110612855007474),\n","  (7, 0.009840558046836074),\n","  (8, 0.19627138349111442),\n","  (9, 0.008844045839561535),\n","  (10, 0.5481232353429663),\n","  (11, 0.007349277528649726),\n","  (12, 0.004691911642584289),\n","  (13, 0.009259259259259259),\n","  (14, 0.018144826440790567),\n","  (15, 0.0051901677462215575),\n","  (16, 0.005356253114100647),\n","  (17, 0.003861484803188839),\n","  (18, 0.012913137352599236),\n","  (19, 0.011086198305929247)],\n"," [(0, 0.01661111111111111),\n","  (1, 0.05383333333333333),\n","  (2, 0.15816666666666668),\n","  (3, 0.02238888888888889),\n","  (4, 0.009722222222222222),\n","  (5, 0.018833333333333334),\n","  (6, 0.028833333333333332),\n","  (7, 0.0865),\n","  (8, 0.0055),\n","  (9, 0.03372222222222222),\n","  (10, 0.06661111111111111),\n","  (11, 0.027166666666666665),\n","  (12, 0.005388888888888889),\n","  (13, 0.01538888888888889),\n","  (14, 0.2788333333333333),\n","  (15, 0.006055555555555555),\n","  (16, 0.035611111111111114),\n","  (17, 0.012611111111111111),\n","  (18, 0.050277777777777775),\n","  (19, 0.06794444444444445)],\n"," [(0, 0.00952563610791459),\n","  (1, 0.016430124025060736),\n","  (2, 0.04353663214422708),\n","  (3, 0.013361462728551337),\n","  (4, 0.09685462217107786),\n","  (5, 0.012850019179133103),\n","  (6, 0.024485359928397902),\n","  (7, 0.006073392149341516),\n","  (8, 0.012466436517069428),\n","  (9, 0.009781357882623706),\n","  (10, 0.018731619997442783),\n","  (11, 0.010804244981460171),\n","  (12, 0.09992328346758726),\n","  (13, 0.2348165196266462),\n","  (14, 0.02704257767548907),\n","  (15, 0.007096279248177982),\n","  (16, 0.1004347270170055),\n","  (17, 0.004411200613732259),\n","  (18, 0.03420278736734433),\n","  (19, 0.21717171717171718)],\n"," [(0, 0.023668981481481485),\n","  (1, 0.00607638888888889),\n","  (2, 0.07471064814814816),\n","  (3, 0.005960648148148149),\n","  (4, 0.01059027777777778),\n","  (5, 0.022164351851851855),\n","  (6, 0.008275462962962965),\n","  (7, 0.013946759259259261),\n","  (8, 0.016261574074074078),\n","  (9, 0.004803240740740742),\n","  (10, 0.029687500000000002),\n","  (11, 0.3198495370370371),\n","  (12, 0.139525462962963),\n","  (13, 0.009780092592592594),\n","  (14, 0.026909722222222227),\n","  (15, 0.014872685185185187),\n","  (16, 0.008043981481481484),\n","  (17, 0.010706018518518521),\n","  (18, 0.04843750000000001),\n","  (19, 0.20572916666666669)],\n"," [(0, 0.023408598035463704),\n","  (1, 0.022388059701492533),\n","  (2, 0.11857379767827526),\n","  (3, 0.012820512820512817),\n","  (4, 0.017668069906875875),\n","  (5, 0.057724199515244275),\n","  (6, 0.012310243653527233),\n","  (7, 0.009631330526852912),\n","  (8, 0.024811838244674058),\n","  (9, 0.012437810945273629),\n","  (10, 0.037313432835820885),\n","  (11, 0.05236637326189564),\n","  (12, 0.052876642428881224),\n","  (13, 0.20008929710422244),\n","  (14, 0.11563974996810815),\n","  (15, 0.005294042607475442),\n","  (16, 0.06078581451715779),\n","  (17, 0.006187013649700215),\n","  (18, 0.07137389973210867),\n","  (19, 0.08629927286643703)],\n"," [(0, 0.2167755991285403),\n","  (1, 0.010306686777275012),\n","  (2, 0.020194402547343723),\n","  (3, 0.009133567957097369),\n","  (4, 0.008798391151332328),\n","  (5, 0.013323278029160382),\n","  (6, 0.00980392156862745),\n","  (7, 0.0057817998994469585),\n","  (8, 0.014496396849338025),\n","  (9, 0.008295625942684766),\n","  (10, 0.023210993799229092),\n","  (11, 0.010977040388805094),\n","  (12, 0.23906485671191555),\n","  (13, 0.013826043237807943),\n","  (14, 0.18929110105580693),\n","  (15, 0.004943857885034356),\n","  (16, 0.0191888721300486),\n","  (17, 0.02287581699346405),\n","  (18, 0.10884866767219709),\n","  (19, 0.05086308027484498)],\n"," [(0, 0.004858657243816254),\n","  (1, 0.010453474676089517),\n","  (2, 0.09094032194738909),\n","  (3, 0.01428150765606596),\n","  (4, 0.008392226148409895),\n","  (5, 0.010846093443266588),\n","  (6, 0.011336866902237928),\n","  (7, 0.012612877895563408),\n","  (8, 0.005349430702787593),\n","  (9, 0.004073419709462112),\n","  (10, 0.007508833922261484),\n","  (11, 0.005840204161758932),\n","  (12, 0.007018060463290145),\n","  (13, 0.23228307813113466),\n","  (14, 0.3496760895170789),\n","  (15, 0.09182371417353749),\n","  (16, 0.003975265017667844),\n","  (17, 0.03636631330977621),\n","  (18, 0.003778955634079309),\n","  (19, 0.08858460934432666)],\n"," [(0, 0.018348197696023784),\n","  (1, 0.008314752879970272),\n","  (2, 0.038600891861761424),\n","  (3, 0.00775733927907841),\n","  (4, 0.013610182088442958),\n","  (5, 0.00794314381270903),\n","  (6, 0.025687476774433296),\n","  (7, 0.006363805276848755),\n","  (8, 0.019277220364176884),\n","  (9, 0.00785024154589372),\n","  (10, 0.09935897435897435),\n","  (11, 0.010730211817168338),\n","  (12, 0.05448717948717949),\n","  (13, 0.07659791898922334),\n","  (14, 0.41141768859160166),\n","  (15, 0.006921218877740617),\n","  (16, 0.007385730211817168),\n","  (17, 0.024665551839464884),\n","  (18, 0.04621887774061687),\n","  (19, 0.10846339650687477)],\n"," [(0, 0.014560931899641574),\n","  (1, 0.050672043010752676),\n","  (2, 0.02737455197132616),\n","  (3, 0.4107974910394264),\n","  (4, 0.007661290322580643),\n","  (5, 0.03660394265232974),\n","  (6, 0.0064068100358422924),\n","  (7, 0.005062724014336916),\n","  (8, 0.04350358422939067),\n","  (9, 0.005241935483870967),\n","  (10, 0.09341397849462364),\n","  (11, 0.038037634408602146),\n","  (12, 0.010170250896057345),\n","  (13, 0.005510752688172042),\n","  (14, 0.007482078853046593),\n","  (15, 0.010170250896057345),\n","  (16, 0.03301971326164874),\n","  (17, 0.014202508960573474),\n","  (18, 0.12710573476702505),\n","  (19, 0.05300179211469533)],\n"," [(0, 0.014303482587064677),\n","  (1, 0.010433941404090657),\n","  (2, 0.012230514096185738),\n","  (3, 0.014994472084024322),\n","  (4, 0.025497512437810944),\n","  (5, 0.00822277501381979),\n","  (6, 0.006978993919292427),\n","  (7, 0.007669983416252073),\n","  (8, 0.18940022111663904),\n","  (9, 0.004906025428413488),\n","  (10, 0.020384190160309564),\n","  (11, 0.007117191818684356),\n","  (12, 0.010295743504698729),\n","  (13, 0.01692924267551133),\n","  (14, 0.015270867882808181),\n","  (15, 0.006564400221116639),\n","  (16, 0.012230514096185738),\n","  (17, 0.004906025428413488),\n","  (18, 0.597636815920398),\n","  (19, 0.014027086788280818)],\n"," [(0, 0.015879996091077885),\n","  (1, 0.006596306068601583),\n","  (2, 0.0161731652496824),\n","  (3, 0.04900811101338806),\n","  (4, 0.015977719143946058),\n","  (5, 0.006791752174337926),\n","  (6, 0.00581452164565621),\n","  (7, 0.008159874914492328),\n","  (8, 0.2622398123717385),\n","  (9, 0.01910485683572755),\n","  (10, 0.04910583406625623),\n","  (11, 0.006791752174337926),\n","  (12, 0.051744356493696866),\n","  (13, 0.06943222906283593),\n","  (14, 0.008159874914492328),\n","  (15, 0.004935014169842666),\n","  (16, 0.023306948109058926),\n","  (17, 0.008746213231701359),\n","  (18, 0.35019055995309295),\n","  (19, 0.021841102316036354)],\n"," [(0, 0.009401709401709403),\n","  (1, 0.009007232084155162),\n","  (2, 0.008349769888231428),\n","  (3, 0.01834319526627219),\n","  (4, 0.009007232084155162),\n","  (5, 0.012294543063773835),\n","  (6, 0.007692307692307694),\n","  (7, 0.030177514792899412),\n","  (8, 0.014792899408284025),\n","  (9, 0.010322156476002632),\n","  (10, 0.033859303090072325),\n","  (11, 0.24556213017751483),\n","  (12, 0.15088757396449706),\n","  (13, 0.004273504273504275),\n","  (14, 0.005851413543721237),\n","  (15, 0.004799474030243262),\n","  (16, 0.029125575279421438),\n","  (17, 0.011111111111111113),\n","  (18, 0.354043392504931),\n","  (19, 0.03109796186719264)],\n"," [(0, 0.03629359286293593),\n","  (1, 0.008042714247093808),\n","  (2, 0.029264666125979995),\n","  (3, 0.012773722627737226),\n","  (4, 0.008988915923222493),\n","  (5, 0.20998918626655852),\n","  (6, 0.013584752635847526),\n","  (7, 0.006826169234928359),\n","  (8, 0.08725331170586645),\n","  (9, 0.009935117599351177),\n","  (10, 0.09198432008650986),\n","  (11, 0.009124087591240875),\n","  (12, 0.04399837793998378),\n","  (13, 0.019532306028656393),\n","  (14, 0.05264936469316031),\n","  (15, 0.008988915923222493),\n","  (16, 0.026155717761557177),\n","  (17, 0.008718572587185726),\n","  (18, 0.30569072722357393),\n","  (19, 0.010205460935387942)],\n"," [(0, 0.02996701921983396),\n","  (1, 0.004492209712271125),\n","  (2, 0.008245195041510294),\n","  (3, 0.008017741385192768),\n","  (4, 0.013135448652337088),\n","  (5, 0.09183441373820085),\n","  (6, 0.0061981121346525656),\n","  (7, 0.009837370635732971),\n","  (8, 0.08125781871943592),\n","  (9, 0.006653019447287616),\n","  (10, 0.030535653360627777),\n","  (11, 0.015864892528147392),\n","  (12, 0.023484590014784493),\n","  (13, 0.004151029227794837),\n","  (14, 0.014386443762083477),\n","  (15, 0.0033549414306834986),\n","  (16, 0.03349255089275561),\n","  (17, 0.007107926759922667),\n","  (18, 0.6003070624360287),\n","  (19, 0.0076765609007164795)],\n"," [(0, 0.015980154609438097),\n","  (1, 0.031556478596977036),\n","  (2, 0.00998038536979347),\n","  (3, 0.017133956386292833),\n","  (4, 0.02371062651436483),\n","  (5, 0.010557286258220838),\n","  (6, 0.009634244836737048),\n","  (7, 0.006749740394600207),\n","  (8, 0.09455405561324565),\n","  (9, 0.004788277373947156),\n","  (10, 0.051748009691934924),\n","  (11, 0.02047998153917157),\n","  (12, 0.009288104303680628),\n","  (13, 0.01448021229952694),\n","  (14, 0.03674858659282335),\n","  (15, 0.013326410522672204),\n","  (16, 0.011711088035075574),\n","  (17, 0.00836506288219684),\n","  (18, 0.6016499365409023),\n","  (19, 0.007557401638398523)],\n"," [(0, 0.02774755168661589),\n","  (1, 0.03729899649377343),\n","  (2, 0.023032281465360904),\n","  (3, 0.12132752992383027),\n","  (4, 0.012876314834965544),\n","  (5, 0.0975093700882602),\n","  (6, 0.012876314834965544),\n","  (7, 0.003929391851045824),\n","  (8, 0.03923346632813445),\n","  (9, 0.008040140249062994),\n","  (10, 0.13607786241083306),\n","  (11, 0.011304558094547215),\n","  (12, 0.011667271188489906),\n","  (13, 0.00731471406117761),\n","  (14, 0.0067101922379397905),\n","  (15, 0.009128279530891067),\n","  (16, 0.03391367428364165),\n","  (17, 0.0059847660500544075),\n","  (18, 0.3882843670656511),\n","  (19, 0.00574295732075928)],\n"," [(0, 0.02355267889248472),\n","  (1, 0.006892005273882297),\n","  (2, 0.012165887570418315),\n","  (3, 0.011206999880139039),\n","  (4, 0.008929641615725759),\n","  (5, 0.09498981181829078),\n","  (6, 0.134424068081026),\n","  (7, 0.01612129929282033),\n","  (8, 0.0048543689320388345),\n","  (9, 0.014323384873546686),\n","  (10, 0.02043629389907707),\n","  (11, 0.11248951216588757),\n","  (12, 0.005333812777178473),\n","  (13, 0.04716528826561189),\n","  (14, 0.04129210116265133),\n","  (15, 0.0637061009229294),\n","  (16, 0.010008390267289944),\n","  (17, 0.1621119501378401),\n","  (18, 0.1634304207119741),\n","  (19, 0.04656598345918734)],\n"," [(0, 0.008851525664976427),\n","  (1, 0.008495685437238682),\n","  (2, 0.006627524241615516),\n","  (3, 0.018370251756961126),\n","  (4, 0.00751712481095988),\n","  (5, 0.014189129081042614),\n","  (6, 0.02780001779201139),\n","  (7, 0.0080508851525665),\n","  (8, 0.011431367316075084),\n","  (9, 0.0064496041277466425),\n","  (10, 0.08873765679210036),\n","  (11, 0.01080864691753403),\n","  (12, 0.02655457699492928),\n","  (13, 0.014544969308780359),\n","  (14, 0.010452806689796283),\n","  (15, 0.011253447202206212),\n","  (16, 0.009830086291255228),\n","  (17, 0.07254692643003292),\n","  (18, 0.5660083622453519),\n","  (19, 0.07147940574681969)],\n"," [(0, 0.025265214963707427),\n","  (1, 0.011027359017308766),\n","  (2, 0.010282895961287921),\n","  (3, 0.03168620882188721),\n","  (4, 0.03661827656802531),\n","  (5, 0.021542899683603203),\n","  (6, 0.004885538805136795),\n","  (7, 0.01242322724734785),\n","  (8, 0.011213474781313978),\n","  (9, 0.007211985855201936),\n","  (10, 0.10538805136795086),\n","  (11, 0.005909175507165457),\n","  (12, 0.030197282709845525),\n","  (13, 0.011771822073329611),\n","  (14, 0.027777777777777776),\n","  (15, 0.005816117625162851),\n","  (16, 0.058021589428624606),\n","  (17, 0.0166108319374651),\n","  (18, 0.5234040573236554),\n","  (19, 0.04294621254420249)],\n"," [(0, 0.020876681925371066),\n","  (1, 0.010750450825357192),\n","  (2, 0.04265501456512692),\n","  (3, 0.01088916631987793),\n","  (4, 0.01643778610070745),\n","  (5, 0.007421278956859481),\n","  (6, 0.029060896102094603),\n","  (7, 0.009779442363712027),\n","  (8, 0.1287973366625052),\n","  (9, 0.021431543903454017),\n","  (10, 0.016021639617145235),\n","  (11, 0.007837425440421696),\n","  (12, 0.009502011374670551),\n","  (13, 0.03433208489388265),\n","  (14, 0.355458454709391),\n","  (15, 0.006172839506172839),\n","  (16, 0.10368983215425163),\n","  (17, 0.019212095991122207),\n","  (18, 0.11756138160632543),\n","  (19, 0.03211263698155084)],\n"," [(0, 0.039725614352480024),\n","  (1, 0.011382481531735263),\n","  (2, 0.0303784109754259),\n","  (3, 0.02389567314940449),\n","  (4, 0.009573345394240916),\n","  (5, 0.018769787426503846),\n","  (6, 0.056007839589929144),\n","  (7, 0.013342379014020805),\n","  (8, 0.012588572290064827),\n","  (9, 0.015000753806723955),\n","  (10, 0.048771295039951756),\n","  (11, 0.012437810945273632),\n","  (12, 0.1286748077792854),\n","  (13, 0.22832805668626563),\n","  (14, 0.011533242876526458),\n","  (15, 0.022538821046283734),\n","  (16, 0.02299110508065732),\n","  (17, 0.07500376903361979),\n","  (18, 0.15445499773857982),\n","  (19, 0.06460123624302729)],\n"," [(0, 0.006952273581360391),\n","  (1, 0.008580734059877242),\n","  (2, 0.0565576850807967),\n","  (3, 0.012338719779531507),\n","  (4, 0.008706000250532383),\n","  (5, 0.01146185644494551),\n","  (6, 0.022986345985218593),\n","  (7, 0.013215583114117501),\n","  (8, 0.17280471000876865),\n","  (9, 0.019103094074909184),\n","  (10, 0.014092446448703496),\n","  (11, 0.010083928347738948),\n","  (12, 0.14274082425153453),\n","  (13, 0.239195791055994),\n","  (14, 0.01935362645621947),\n","  (15, 0.00569961167480897),\n","  (16, 0.10440936991106102),\n","  (17, 0.009833395966428663),\n","  (18, 0.09739446323437306),\n","  (19, 0.0244895402730803)],\n"," [(0, 0.003175646079719667),\n","  (1, 0.006022777047744197),\n","  (2, 0.05756314790480362),\n","  (3, 0.05223390275952694),\n","  (4, 0.028288801284859104),\n","  (5, 0.005730763615126296),\n","  (6, 0.014564169951817784),\n","  (7, 0.003832676303109943),\n","  (8, 0.03953131844064827),\n","  (9, 0.003248649437874142),\n","  (10, 0.04493356694407943),\n","  (11, 0.007336837494524748),\n","  (12, 0.009161921448386626),\n","  (13, 0.17816469557599648),\n","  (14, 0.03544313038399766),\n","  (15, 0.019017374799240764),\n","  (16, 0.10538034749598482),\n","  (17, 0.006898817345597897),\n","  (18, 0.3608190976784932),\n","  (19, 0.01865235800846839)],\n"," [(0, 0.02914011268125981),\n","  (1, 0.009282349681352175),\n","  (2, 0.032188048397524704),\n","  (3, 0.007989286044148888),\n","  (4, 0.029509559434746466),\n","  (5, 0.011406668513900434),\n","  (6, 0.18089036667590283),\n","  (7, 0.0042948185092823495),\n","  (8, 0.014546965918536992),\n","  (9, 0.004202456820910686),\n","  (10, 0.041886025676549365),\n","  (11, 0.013623349034820357),\n","  (12, 0.006511499030202272),\n","  (13, 0.0276623256673132),\n","  (14, 0.012237923709245405),\n","  (15, 0.010944860072042118),\n","  (16, 0.005587882146485638),\n","  (17, 0.013253902281333703),\n","  (18, 0.014916412672023644),\n","  (19, 0.529925187032419)],\n"," [(0, 0.08887073347857663),\n","  (1, 0.00771604938271605),\n","  (2, 0.018064633260711695),\n","  (3, 0.006808278867102398),\n","  (4, 0.014978213507625274),\n","  (5, 0.03186274509803922),\n","  (6, 0.008986928104575164),\n","  (7, 0.02514524328249819),\n","  (8, 0.013162672476397968),\n","  (9, 0.009168482207697896),\n","  (10, 0.02296659404502542),\n","  (11, 0.013525780682643429),\n","  (12, 0.2994734931009441),\n","  (13, 0.1266339869281046),\n","  (14, 0.018609295570079887),\n","  (15, 0.009168482207697896),\n","  (16, 0.053830791575889624),\n","  (17, 0.1736564996368918),\n","  (18, 0.007171387073347859),\n","  (19, 0.05019970951343501)],\n"," [(0, 0.023835270793908268),\n","  (1, 0.0041903217085698845),\n","  (2, 0.1707218167072182),\n","  (3, 0.06384608452734974),\n","  (4, 0.06510768676218799),\n","  (5, 0.005632152834099307),\n","  (6, 0.00617283950617284),\n","  (7, 0.010588447328106697),\n","  (8, 0.3105794358835722),\n","  (9, 0.007885013967739031),\n","  (10, 0.0698837523655042),\n","  (11, 0.004640893935297829),\n","  (12, 0.09601694151572499),\n","  (13, 0.05501486888348203),\n","  (14, 0.008425700639812564),\n","  (15, 0.004370550599261063),\n","  (16, 0.013381995133819952),\n","  (17, 0.00797512841308462),\n","  (18, 0.05456429665675409),\n","  (19, 0.01716680183833469)],\n"," [(0, 0.0075875342364349705),\n","  (1, 0.023650899400399737),\n","  (2, 0.016322451698867424),\n","  (3, 0.01861721815086239),\n","  (4, 0.010030350136945742),\n","  (5, 0.04371159967429122),\n","  (6, 0.0065511880968243405),\n","  (7, 0.004552520541860982),\n","  (8, 0.5964542157080466),\n","  (9, 0.004182396920571472),\n","  (10, 0.12321415352727813),\n","  (11, 0.005292767784440004),\n","  (12, 0.02039381153305204),\n","  (13, 0.008401806203271895),\n","  (14, 0.007439484787919166),\n","  (15, 0.004552520541860982),\n","  (16, 0.012917314383003925),\n","  (17, 0.004848619438892591),\n","  (18, 0.07191501961655195),\n","  (19, 0.009364127618624622)],\n"," [(0, 0.025246772968868642),\n","  (1, 0.013730701088332067),\n","  (2, 0.015882055175904835),\n","  (3, 0.011073146038977475),\n","  (4, 0.01917236142748671),\n","  (5, 0.00803594026828651),\n","  (6, 0.01638825613768666),\n","  (7, 0.08080232852442419),\n","  (8, 0.008542141230068337),\n","  (9, 0.08864844343204252),\n","  (10, 0.02410782080485953),\n","  (11, 0.2658187800556821),\n","  (12, 0.19317894203998986),\n","  (13, 0.009301442672741078),\n","  (14, 0.014110351809668438),\n","  (15, 0.016135155656795748),\n","  (16, 0.12636041508478865),\n","  (17, 0.008542141230068337),\n","  (18, 0.02410782080485953),\n","  (19, 0.030814983548468743)],\n"," [(0, 0.06549787492410444),\n","  (1, 0.02588038858530662),\n","  (2, 0.06519429265330906),\n","  (3, 0.07870370370370372),\n","  (4, 0.0116120218579235),\n","  (5, 0.11255312689738921),\n","  (6, 0.012978142076502735),\n","  (7, 0.009942319368548879),\n","  (8, 0.22715543412264727),\n","  (9, 0.015406800242865818),\n","  (10, 0.13592896174863392),\n","  (11, 0.019960534304796603),\n","  (12, 0.03225561627200972),\n","  (13, 0.00842440801457195),\n","  (14, 0.08872191863995144),\n","  (15, 0.007361870066788101),\n","  (16, 0.042577413479052834),\n","  (17, 0.0094869459623558),\n","  (18, 0.016924711596842747),\n","  (19, 0.013433515482695812)],\n"," [(0, 0.032366437233693875),\n","  (1, 0.007948213700426091),\n","  (2, 0.00843985578498853),\n","  (3, 0.059898393969190444),\n","  (4, 0.009095378564405114),\n","  (5, 0.012209111766633893),\n","  (6, 0.011881350376925602),\n","  (7, 0.010406424123238285),\n","  (8, 0.5400688298918388),\n","  (9, 0.014831202884300234),\n","  (10, 0.07513929859062604),\n","  (11, 0.010898066207800722),\n","  (12, 0.01712553261225828),\n","  (13, 0.006473287446738776),\n","  (14, 0.07431989511635531),\n","  (15, 0.006801048836447068),\n","  (16, 0.06465093411996069),\n","  (17, 0.007784333005571946),\n","  (18, 0.014831202884300234),\n","  (19, 0.014831202884300234)],\n"," [(0, 0.011742892459826943),\n","  (1, 0.006935860458728195),\n","  (2, 0.5052190633154785),\n","  (3, 0.005287735201208624),\n","  (4, 0.010094767202307373),\n","  (5, 0.01064414228814723),\n","  (6, 0.01325367394588655),\n","  (7, 0.006249141601428374),\n","  (8, 0.007347891773108088),\n","  (9, 0.006386485372888338),\n","  (10, 0.008583985716247767),\n","  (11, 0.008171954401867874),\n","  (12, 0.0219063315478643),\n","  (13, 0.11049306413954124),\n","  (14, 0.03632742755116055),\n","  (15, 0.048825710754017294),\n","  (16, 0.014077736574646336),\n","  (17, 0.06695508858673258),\n","  (18, 0.008171954401867874),\n","  (19, 0.09332509270704573)],\n"," [(0, 0.012010630685883676),\n","  (1, 0.009352959214964734),\n","  (2, 0.0068997240110395585),\n","  (3, 0.06179086169886538),\n","  (4, 0.011806194418889911),\n","  (5, 0.07610140038842891),\n","  (6, 0.009557395481958499),\n","  (7, 0.014157211489318205),\n","  (8, 0.48865378718184604),\n","  (9, 0.009659613615455382),\n","  (10, 0.08550546867014208),\n","  (11, 0.005775324542573852),\n","  (12, 0.10676684043749361),\n","  (13, 0.0074108146785239705),\n","  (14, 0.03766738219360115),\n","  (15, 0.00567310640907697),\n","  (16, 0.01436164775631197),\n","  (17, 0.017530409894715323),\n","  (18, 0.005162015741592559),\n","  (19, 0.014157211489318205)],\n"," [(0, 0.012085137085137086),\n","  (1, 0.10624098124098125),\n","  (2, 0.10335497835497835),\n","  (3, 0.026274651274651274),\n","  (4, 0.021945646945646945),\n","  (5, 0.006192881192881193),\n","  (6, 0.21458633958633958),\n","  (7, 0.0193001443001443),\n","  (8, 0.03505291005291005),\n","  (9, 0.017015392015392015),\n","  (10, 0.026635401635401637),\n","  (11, 0.083994708994709),\n","  (12, 0.01485088985088985),\n","  (13, 0.1185064935064935),\n","  (14, 0.008597883597883597),\n","  (15, 0.09794372294372294),\n","  (16, 0.01773689273689274),\n","  (17, 0.008116883116883116),\n","  (18, 0.05453342953342953),\n","  (19, 0.007034632034632035)],\n"," [(0, 0.013012181616832777),\n","  (1, 0.045127353266888146),\n","  (2, 0.0222406792174234),\n","  (3, 0.02002583979328165),\n","  (4, 0.017441860465116275),\n","  (5, 0.01559616094499815),\n","  (6, 0.14479512735326686),\n","  (7, 0.013381321520856401),\n","  (8, 0.009505352528608341),\n","  (9, 0.03423772609819121),\n","  (10, 0.020394979697305277),\n","  (11, 0.01854928017718715),\n","  (12, 0.015227021040974526),\n","  (13, 0.23375784422296045),\n","  (14, 0.007659653008490216),\n","  (15, 0.24464747139165738),\n","  (16, 0.055832410483573264),\n","  (17, 0.04328165374677002),\n","  (18, 0.013012181616832777),\n","  (19, 0.012273901808785527)],\n"," [(0, 0.010958743553680263),\n","  (1, 0.013420065635255508),\n","  (2, 0.0243202062822316),\n","  (3, 0.07342944210032817),\n","  (4, 0.026547116736990155),\n","  (5, 0.03287623066104079),\n","  (6, 0.008849038912330052),\n","  (7, 0.012013595874355368),\n","  (8, 0.01482653539615565),\n","  (9, 0.033579465541490855),\n","  (10, 0.011427566807313643),\n","  (11, 0.568272386310361),\n","  (12, 0.05022269104547586),\n","  (13, 0.014943741209563995),\n","  (14, 0.007090951711204876),\n","  (15, 0.007676980778246601),\n","  (16, 0.016819034224097515),\n","  (17, 0.01506094702297234),\n","  (18, 0.020335208626347866),\n","  (19, 0.0373300515705579)],\n"," [(0, 0.05274735561171955),\n","  (1, 0.00790976317513807),\n","  (2, 0.26139661143873444),\n","  (3, 0.00678648319760367),\n","  (4, 0.017738462978564074),\n","  (5, 0.005382383225685669),\n","  (6, 0.1017036412992605),\n","  (7, 0.02766076944678461),\n","  (8, 0.017551249648975007),\n","  (9, 0.17303191987269492),\n","  (10, 0.03215388935692221),\n","  (11, 0.013338949733221006),\n","  (12, 0.017270429654591406),\n","  (13, 0.06163998876720023),\n","  (14, 0.012028456426097537),\n","  (15, 0.0070673031919872694),\n","  (16, 0.03327716933445661),\n","  (17, 0.026911916128428345),\n","  (18, 0.037676682579799683),\n","  (19, 0.08672657493213516)],\n"," [(0, 0.012177390039985459),\n","  (1, 0.015691263782866838),\n","  (2, 0.009511692717799588),\n","  (3, 0.007088331515812432),\n","  (4, 0.03592632981945959),\n","  (5, 0.006482491215315643),\n","  (6, 0.012298558100084818),\n","  (7, 0.016175936023264267),\n","  (8, 0.00611898703501757),\n","  (9, 0.024173027989821884),\n","  (10, 0.05252635405307161),\n","  (11, 0.020659154246940505),\n","  (12, 0.1646068096449776),\n","  (13, 0.00939052465770023),\n","  (14, 0.022476675148430873),\n","  (15, 0.00830001211680601),\n","  (16, 0.3264873379377196),\n","  (17, 0.018478129165152065),\n","  (18, 0.021870834847934086),\n","  (19, 0.20956015994183932)],\n"," [(0, 0.5318260120585704),\n","  (1, 0.0052971576227390195),\n","  (2, 0.02011197243755384),\n","  (3, 0.00633074935400517),\n","  (4, 0.011154177433247205),\n","  (5, 0.004521963824289407),\n","  (6, 0.06309216192937125),\n","  (7, 0.011154177433247205),\n","  (8, 0.005383290267011199),\n","  (9, 0.012962962962962966),\n","  (10, 0.015374677002583985),\n","  (11, 0.004608096468561587),\n","  (12, 0.007967269595176575),\n","  (13, 0.14509043927648585),\n","  (14, 0.0075366063738156784),\n","  (15, 0.008139534883720934),\n","  (16, 0.012015503875968996),\n","  (17, 0.025452196382428946),\n","  (18, 0.008139534883720934),\n","  (19, 0.09384151593453922)],\n"," [(0, 0.058942295167383106),\n","  (1, 0.007229386479093398),\n","  (2, 0.022469714732317316),\n","  (3, 0.02181841865311971),\n","  (4, 0.03888237592809692),\n","  (5, 0.004233424514784422),\n","  (6, 0.018431679041292174),\n","  (7, 0.19545395336720078),\n","  (8, 0.03523511788459034),\n","  (9, 0.0720984759671747),\n","  (10, 0.025856454344144856),\n","  (11, 0.00957405236420477),\n","  (12, 0.04487429985671487),\n","  (13, 0.024684121401589168),\n","  (14, 0.048912335547740014),\n","  (15, 0.012830532760192788),\n","  (16, 0.015565976292822721),\n","  (17, 0.05060570535365379),\n","  (18, 0.0160870131561808),\n","  (19, 0.2762146671877036)],\n"," [(0, 0.01019283746556474),\n","  (1, 0.010743801652892564),\n","  (2, 0.01974288337924702),\n","  (3, 0.008907254361799817),\n","  (4, 0.006887052341597797),\n","  (5, 0.023966942148760335),\n","  (6, 0.06584022038567494),\n","  (7, 0.009825528007346191),\n","  (8, 0.02745638200183655),\n","  (9, 0.0078053259871441695),\n","  (10, 0.03278236914600552),\n","  (11, 0.08659320477502297),\n","  (12, 0.0669421487603306),\n","  (13, 0.03461891643709826),\n","  (14, 0.17750229568411388),\n","  (15, 0.0184573002754821),\n","  (16, 0.07979797979797981),\n","  (17, 0.25169880624426083),\n","  (18, 0.02415059687786961),\n","  (19, 0.03608815426997246)],\n"," [(0, 0.008797184900831733),\n","  (1, 0.1465664320750693),\n","  (2, 0.01380891448069951),\n","  (3, 0.09602260609938153),\n","  (4, 0.009756877799104287),\n","  (5, 0.039187460012795904),\n","  (6, 0.013915547024952015),\n","  (7, 0.011676263595649392),\n","  (8, 0.07938792919599062),\n","  (9, 0.008157389635316698),\n","  (10, 0.07501599488163788),\n","  (11, 0.044838984858178714),\n","  (12, 0.008690552356579229),\n","  (13, 0.0036788227767114525),\n","  (14, 0.006984431648539134),\n","  (15, 0.004745148219236511),\n","  (16, 0.0074109618255491574),\n","  (17, 0.007730859458306675),\n","  (18, 0.3993921944977607),\n","  (19, 0.014235444657709533)],\n"," [(0, 0.02295409181636727),\n","  (1, 0.040918163672654696),\n","  (2, 0.12075848303393215),\n","  (3, 0.02894211576846308),\n","  (4, 0.02295409181636727),\n","  (5, 0.043579507651363945),\n","  (6, 0.026946107784431142),\n","  (7, 0.02628077178975383),\n","  (8, 0.2797737857618098),\n","  (9, 0.032934131736526956),\n","  (10, 0.03493013972055889),\n","  (11, 0.03692614770459082),\n","  (12, 0.026946107784431142),\n","  (13, 0.0469061876247505),\n","  (14, 0.033599467731204265),\n","  (15, 0.02095808383233533),\n","  (16, 0.0469061876247505),\n","  (17, 0.03825681969394545),\n","  (18, 0.04025282767797739),\n","  (19, 0.028276779773785767)],\n"," [(0, 0.013024986709197236),\n","  (1, 0.01621477937267411),\n","  (2, 0.028442317916002127),\n","  (3, 0.386762360446571),\n","  (4, 0.023125996810207338),\n","  (5, 0.08692185007974482),\n","  (6, 0.014088250930356193),\n","  (7, 0.019936204146730464),\n","  (8, 0.11084529505582137),\n","  (9, 0.013024986709197236),\n","  (10, 0.08851674641148326),\n","  (11, 0.024720893141945772),\n","  (12, 0.014619883040935672),\n","  (13, 0.023657628920786815),\n","  (14, 0.03216374269005848),\n","  (15, 0.029505582137161084),\n","  (16, 0.01674641148325359),\n","  (17, 0.014619883040935672),\n","  (18, 0.019404572036150983),\n","  (19, 0.023657628920786815)],\n"," [(0, 0.03253568429890849),\n","  (1, 0.009445843828715368),\n","  (2, 0.07759585782255808),\n","  (3, 0.04149174363280157),\n","  (4, 0.04792891127903723),\n","  (5, 0.00888609012034705),\n","  (6, 0.038273159809683745),\n","  (7, 0.019241533725160933),\n","  (8, 0.005807444724321299),\n","  (9, 0.054645955779457044),\n","  (10, 0.05352644836272041),\n","  (11, 0.02301987125664708),\n","  (12, 0.00846627483907081),\n","  (13, 0.040931989924433254),\n","  (14, 0.10096557514693537),\n","  (15, 0.3005177721802407),\n","  (16, 0.020920794850265885),\n","  (17, 0.05730478589420655),\n","  (18, 0.029876854184158974),\n","  (19, 0.028617408340330257)],\n"," [(0, 0.007188498402555911),\n","  (1, 0.11403975860844871),\n","  (2, 0.024405395811146615),\n","  (3, 0.09948526801561947),\n","  (4, 0.007720979765708201),\n","  (5, 0.22532836350727728),\n","  (6, 0.020145544905928296),\n","  (7, 0.007543485977990771),\n","  (8, 0.30875044373446936),\n","  (9, 0.0068335108271210514),\n","  (10, 0.06008164714235002),\n","  (11, 0.008075967341143062),\n","  (12, 0.004881079162229323),\n","  (13, 0.02618033368832091),\n","  (14, 0.008963436279730211),\n","  (15, 0.008430954916577922),\n","  (16, 0.01570820021299255),\n","  (17, 0.012868299609513668),\n","  (18, 0.013045793397231099),\n","  (19, 0.020323038693645726)],\n"," [(0, 0.005378132623641605),\n","  (1, 0.08810157462852074),\n","  (2, 0.07789975604346862),\n","  (3, 0.43917720115324904),\n","  (4, 0.004712796628964294),\n","  (5, 0.03454202705699712),\n","  (6, 0.007041472610334886),\n","  (7, 0.016467065868263474),\n","  (8, 0.06847416278554003),\n","  (9, 0.007817697937458417),\n","  (10, 0.028332224440008873),\n","  (11, 0.12901973830117544),\n","  (12, 0.004934575293856731),\n","  (13, 0.007374140607673542),\n","  (14, 0.013694832557108006),\n","  (15, 0.005599911288534043),\n","  (16, 0.017908627190064315),\n","  (17, 0.01768684852517188),\n","  (18, 0.0054890219560878245),\n","  (19, 0.020348192503881125)],\n"," [(0, 0.007085527965889138),\n","  (1, 0.028028592927012784),\n","  (2, 0.01899924755455229),\n","  (3, 0.11042136945071479),\n","  (4, 0.005956859794331576),\n","  (5, 0.27859292701279154),\n","  (6, 0.006333082518184097),\n","  (7, 0.006458490092801603),\n","  (8, 0.36988964133433655),\n","  (9, 0.008088788562829193),\n","  (10, 0.01661650363681966),\n","  (11, 0.010095309756709303),\n","  (12, 0.030411336844745417),\n","  (13, 0.026272886882367688),\n","  (14, 0.006458490092801603),\n","  (15, 0.006834712816654124),\n","  (16, 0.011600200652119386),\n","  (17, 0.008841234010534235),\n","  (18, 0.015487835465262098),\n","  (19, 0.027526962628542758)],\n"," [(0, 0.0065097848940643705),\n","  (1, 0.025351770984958763),\n","  (2, 0.03157852175319425),\n","  (3, 0.005701115963124698),\n","  (4, 0.012655668769205888),\n","  (5, 0.009744460617823065),\n","  (6, 0.007318453825004044),\n","  (7, 0.007318453825004044),\n","  (8, 0.7226669901342392),\n","  (9, 0.010391395762574803),\n","  (10, 0.0931182273977034),\n","  (11, 0.004164644994339319),\n","  (12, 0.0035985767426815465),\n","  (13, 0.004164644994339319),\n","  (14, 0.00925925925925926),\n","  (15, 0.005701115963124698),\n","  (16, 0.0049733139252789915),\n","  (17, 0.004326378780527253),\n","  (18, 0.02478570273330099),\n","  (19, 0.006671518680252305)],\n"," [(0, 0.008162659542891066),\n","  (1, 0.07831206094785792),\n","  (2, 0.0224102107450282),\n","  (3, 0.006183832987038686),\n","  (4, 0.0029187691698822596),\n","  (5, 0.0729692292470565),\n","  (6, 0.00964677945978035),\n","  (7, 0.004699713070149401),\n","  (8, 0.5732165825665381),\n","  (9, 0.004402889086771544),\n","  (10, 0.06030473928960127),\n","  (11, 0.00578806767586821),\n","  (12, 0.012812901949144157),\n","  (13, 0.09641832393390719),\n","  (14, 0.009349955476402494),\n","  (15, 0.0055901850202829725),\n","  (16, 0.005887009003660829),\n","  (17, 0.005689126348075591),\n","  (18, 0.007173246264964876),\n","  (19, 0.008063718215098447)],\n"," [(0, 0.022504091653027823),\n","  (1, 0.05541916712129478),\n","  (2, 0.016139298054191672),\n","  (3, 0.33037825059101655),\n","  (4, 0.009138025095471904),\n","  (5, 0.042053100563738864),\n","  (6, 0.0065011820330969266),\n","  (7, 0.033597017639570834),\n","  (8, 0.20435533733406075),\n","  (9, 0.022685942898708854),\n","  (10, 0.058601563920712856),\n","  (11, 0.014684488088743407),\n","  (12, 0.012229496272049464),\n","  (13, 0.006228405164575377),\n","  (14, 0.0076832151300236405),\n","  (15, 0.005046372067648663),\n","  (16, 0.08551554828150573),\n","  (17, 0.015775595562829606),\n","  (18, 0.012138570649208947),\n","  (19, 0.03932533187852337)],\n"," [(0, 0.010130996136085193),\n","  (1, 0.04255018377155781),\n","  (2, 0.011733107152954479),\n","  (3, 0.006832532277824897),\n","  (4, 0.03284327584582037),\n","  (5, 0.05065498068042596),\n","  (6, 0.005230421260955611),\n","  (7, 0.003911035717651493),\n","  (8, 0.13811139383658466),\n","  (9, 0.004193761191216661),\n","  (10, 0.5631420224295541),\n","  (11, 0.04255018377155781),\n","  (12, 0.014183394590519269),\n","  (13, 0.017199132975214398),\n","  (14, 0.020874564131561583),\n","  (15, 0.006172839506172837),\n","  (16, 0.005984355857129392),\n","  (17, 0.006549806804259729),\n","  (18, 0.008905852417302797),\n","  (19, 0.008246159645650739)],\n"," [(0, 0.008890898131404461),\n","  (1, 0.012105686156319071),\n","  (2, 0.03792445248141452),\n","  (3, 0.003465943339361062),\n","  (4, 0.01079967852119751),\n","  (5, 0.19806108097247344),\n","  (6, 0.009795057263411696),\n","  (7, 0.030490255173799486),\n","  (8, 0.1466244725738397),\n","  (9, 0.008388587502511554),\n","  (10, 0.16420534458509145),\n","  (11, 0.013210769539883467),\n","  (12, 0.03902953586497891),\n","  (13, 0.004972875226039784),\n","  (14, 0.010598754269640347),\n","  (15, 0.003968253968253969),\n","  (16, 0.011703837653204744),\n","  (17, 0.017329716696805306),\n","  (18, 0.1780691179425357),\n","  (19, 0.09036568213783405)],\n"," [(0, 0.027512806924571635),\n","  (1, 0.008788199964670555),\n","  (2, 0.031045751633986932),\n","  (3, 0.0057851969616675505),\n","  (4, 0.00896484720014132),\n","  (5, 0.11504151210033564),\n","  (6, 0.00719837484543367),\n","  (7, 0.011084614025790498),\n","  (8, 0.24478890655361246),\n","  (9, 0.00825825825825826),\n","  (10, 0.02442148030383325),\n","  (11, 0.023449920508744043),\n","  (12, 0.03608019784490373),\n","  (13, 0.03661013955131603),\n","  (14, 0.17854619325207563),\n","  (15, 0.004195371842430667),\n","  (16, 0.04005476064299594),\n","  (17, 0.14542483660130723),\n","  (18, 0.023273273273273276),\n","  (19, 0.01947535771065183)],\n"," [(0, 0.009208937198067632),\n","  (1, 0.016857890499194846),\n","  (2, 0.007095410628019324),\n","  (3, 0.08247785829307569),\n","  (4, 0.006290257648953301),\n","  (5, 0.15554549114331723),\n","  (6, 0.005787037037037037),\n","  (7, 0.0036735104669887277),\n","  (8, 0.46885064412238325),\n","  (9, 0.004780595813204509),\n","  (10, 0.023299114331723027),\n","  (11, 0.01776368760064412),\n","  (12, 0.08579911433172303),\n","  (13, 0.019877214170692433),\n","  (14, 0.01263083735909823),\n","  (15, 0.005585748792270531),\n","  (16, 0.05339170692431562),\n","  (17, 0.004881239935587762),\n","  (18, 0.00981280193236715),\n","  (19, 0.006390901771336554)],\n"," [(0, 0.009021406727828746),\n","  (1, 0.011569826707441387),\n","  (2, 0.015137614678899083),\n","  (3, 0.08047910295616717),\n","  (4, 0.005045871559633028),\n","  (5, 0.22961264016309887),\n","  (6, 0.004638124362895005),\n","  (7, 0.005759429153924567),\n","  (8, 0.495361875637105),\n","  (9, 0.007696228338430173),\n","  (10, 0.06712538226299694),\n","  (11, 0.009938837920489297),\n","  (12, 0.004638124362895005),\n","  (13, 0.0049439347604485215),\n","  (14, 0.012385321100917432),\n","  (15, 0.005045871559633028),\n","  (16, 0.012079510703363914),\n","  (17, 0.005759429153924567),\n","  (18, 0.00545361875637105),\n","  (19, 0.008307849133537206)],\n"," [(0, 0.019264003253024294),\n","  (1, 0.02759987801158889),\n","  (2, 0.01184304157771678),\n","  (3, 0.005438649994917149),\n","  (4, 0.003913794856155331),\n","  (5, 0.5413744027650705),\n","  (6, 0.011334756531462842),\n","  (7, 0.007471790179932904),\n","  (8, 0.19492731523838563),\n","  (9, 0.009606587374199449),\n","  (10, 0.037562264918166104),\n","  (11, 0.009606587374199449),\n","  (12, 0.024550167734065258),\n","  (13, 0.007980075226186843),\n","  (14, 0.007980075226186843),\n","  (15, 0.004523736911660058),\n","  (16, 0.01570600792924672),\n","  (17, 0.02953136118735386),\n","  (18, 0.00818338924468842),\n","  (19, 0.021602114465792414)],\n"," [(0, 0.08287870521913075),\n","  (1, 0.007046735770140026),\n","  (2, 0.03241498454264412),\n","  (3, 0.17416803055100927),\n","  (4, 0.008774322604109838),\n","  (5, 0.08587925077286779),\n","  (6, 0.005773777050372795),\n","  (7, 0.00641025641025641),\n","  (8, 0.11488452445899254),\n","  (9, 0.015593744317148573),\n","  (10, 0.1659847244953628),\n","  (11, 0.008046917621385706),\n","  (12, 0.007410438261502091),\n","  (13, 0.15816512093107837),\n","  (14, 0.015139116202945991),\n","  (15, 0.02159483542462266),\n","  (16, 0.07124022549554465),\n","  (17, 0.0032278596108383344),\n","  (18, 0.003136933987997818),\n","  (19, 0.012229496272049464)],\n"," [(0, 0.014818629133768167),\n","  (1, 0.012530037761757639),\n","  (2, 0.015276347408170271),\n","  (3, 0.012987756036159743),\n","  (4, 0.011614601212953428),\n","  (5, 0.007495136743334478),\n","  (6, 0.007838425449136056),\n","  (7, 0.01218674905595606),\n","  (8, 0.37527177022542624),\n","  (9, 0.03804783155967502),\n","  (10, 0.018137086623183432),\n","  (11, 0.011156882938551322),\n","  (12, 0.10556127703398559),\n","  (13, 0.009554868978143952),\n","  (14, 0.12009383224625243),\n","  (15, 0.009554868978143952),\n","  (16, 0.1846321089369493),\n","  (17, 0.00806728458633711),\n","  (18, 0.011500171644352901),\n","  (19, 0.013674333447762902)],\n"," [(0, 0.007418733718427909),\n","  (1, 0.017838939857288485),\n","  (2, 0.0308641975308642),\n","  (3, 0.009683995922528034),\n","  (4, 0.0049269452939177715),\n","  (5, 0.35944048023558733),\n","  (6, 0.011269679465398122),\n","  (7, 0.008551364820477973),\n","  (8, 0.17334919016876207),\n","  (9, 0.005153471514327784),\n","  (10, 0.24719673802242612),\n","  (11, 0.02939177709819912),\n","  (12, 0.009570732812323028),\n","  (13, 0.0042473666326877345),\n","  (14, 0.0049269452939177715),\n","  (15, 0.00470041907350776),\n","  (16, 0.005946313285762828),\n","  (17, 0.027466304224714015),\n","  (18, 0.02576735757163892),\n","  (19, 0.012289047457243178)],\n"," [(0, 0.012864009047215156),\n","  (1, 0.013335218169823769),\n","  (2, 0.03039298840825559),\n","  (3, 0.04302139289416644),\n","  (4, 0.052162849872773545),\n","  (5, 0.42658561869757805),\n","  (6, 0.005984355857129395),\n","  (7, 0.004947695787390445),\n","  (8, 0.16939967957779664),\n","  (9, 0.008717368768259355),\n","  (10, 0.009000094241824524),\n","  (11, 0.06300065969277166),\n","  (12, 0.01371218546791066),\n","  (13, 0.006267081330694563),\n","  (14, 0.009942512487041751),\n","  (15, 0.0037225520686080486),\n","  (16, 0.08184902459711621),\n","  (17, 0.0033455847705211577),\n","  (18, 0.02709452454999529),\n","  (19, 0.014654603713127887)],\n"," [(0, 0.006775067750677507),\n","  (1, 0.00536986851349995),\n","  (2, 0.03126568302720064),\n","  (3, 0.007176553247013951),\n","  (4, 0.00707618187292984),\n","  (5, 0.38768443239987954),\n","  (6, 0.005470239887584061),\n","  (7, 0.06509083609354611),\n","  (8, 0.2116330422563485),\n","  (9, 0.01520626317374285),\n","  (10, 0.035180166616480976),\n","  (11, 0.012195121951219513),\n","  (12, 0.03297199638663053),\n","  (13, 0.020024089129780185),\n","  (14, 0.04451470440630332),\n","  (15, 0.005169125765331728),\n","  (16, 0.07352203151661146),\n","  (17, 0.013801063936565291),\n","  (18, 0.011392150958546622),\n","  (19, 0.008481381110107397)],\n"," [(0, 0.006700432626358552),\n","  (1, 0.010499103091695685),\n","  (2, 0.009338398227287116),\n","  (3, 0.11148042629524112),\n","  (4, 0.006700432626358552),\n","  (5, 0.047219584256621296),\n","  (6, 0.0053286905138756994),\n","  (7, 0.012187401076289964),\n","  (8, 0.5616228764376913),\n","  (9, 0.008388730610952834),\n","  (10, 0.022528226231929935),\n","  (11, 0.005539727761949984),\n","  (12, 0.010921177587844255),\n","  (13, 0.08140761844465548),\n","  (14, 0.012292919700327108),\n","  (15, 0.004484541521578559),\n","  (16, 0.03941120607787275),\n","  (17, 0.005961802258098554),\n","  (18, 0.032341458267384195),\n","  (19, 0.005645246385987126)],\n"," [(0, 0.007986938487335627),\n","  (1, 0.005692348424675669),\n","  (2, 0.007369164239696408),\n","  (3, 0.010193275086047128),\n","  (4, 0.007633924631541788),\n","  (5, 0.007545671167593328),\n","  (6, 0.5369782013944048),\n","  (7, 0.03402171035213132),\n","  (8, 0.005692348424675669),\n","  (9, 0.028196981731532962),\n","  (10, 0.04214102903538964),\n","  (11, 0.013458653252140147),\n","  (12, 0.020165916512223103),\n","  (13, 0.05899744064954549),\n","  (14, 0.03702232812637896),\n","  (15, 0.05899744064954549),\n","  (16, 0.007192657311799488),\n","  (17, 0.008781219662871768),\n","  (18, 0.0316388668255229),\n","  (19, 0.07029388403494838)],\n"," [(0, 0.01568565732871682),\n","  (1, 0.006137865911237016),\n","  (2, 0.08650718707375932),\n","  (3, 0.08199559332703808),\n","  (4, 0.010439618088343301),\n","  (5, 0.021036617353897807),\n","  (6, 0.0730773266184031),\n","  (7, 0.023344874619662156),\n","  (8, 0.009285489455461126),\n","  (9, 0.0076067568985416015),\n","  (10, 0.01001993494911342),\n","  (11, 0.02775154758157591),\n","  (12, 0.032577903682719546),\n","  (13, 0.20999895079215192),\n","  (14, 0.011593746721225475),\n","  (15, 0.3056867065365649),\n","  (16, 0.013902003986989823),\n","  (17, 0.009810093379498478),\n","  (18, 0.012013429860455355),\n","  (19, 0.03152869583464484)],\n"," [(0, 0.020174050632911392),\n","  (1, 0.005318213783403657),\n","  (2, 0.010240857946554148),\n","  (3, 0.016569971870604782),\n","  (4, 0.007603727144866386),\n","  (5, 0.00874648382559775),\n","  (6, 0.09577180028129395),\n","  (7, 0.046193741209563995),\n","  (8, 0.0033843178621659636),\n","  (9, 0.07564170182841069),\n","  (10, 0.009273909985935302),\n","  (11, 0.005230309423347398),\n","  (12, 0.006285161744022503),\n","  (13, 0.025272503516174404),\n","  (14, 0.008131153305203939),\n","  (15, 0.5977056962025317),\n","  (16, 0.012438466947960618),\n","  (17, 0.021668424753867793),\n","  (18, 0.0037359353023909986),\n","  (19, 0.020613572433192687)],\n"," [(0, 0.0038439750748563567),\n","  (1, 0.004572307194302824),\n","  (2, 0.00764748725418791),\n","  (3, 0.0078093388362871245),\n","  (4, 0.005786194060046937),\n","  (5, 0.005948045642146152),\n","  (6, 0.19652828356397184),\n","  (7, 0.02059561382212511),\n","  (8, 0.0058671198510965444),\n","  (9, 0.07813385125839604),\n","  (10, 0.0053006393137492915),\n","  (11, 0.005948045642146152),\n","  (12, 0.006919155134741442),\n","  (13, 0.009670632030428098),\n","  (14, 0.008213967791535163),\n","  (15, 0.5461277008982763),\n","  (16, 0.011450999433519463),\n","  (17, 0.025693938658250384),\n","  (18, 0.00991340940357692),\n","  (19, 0.03402929513635996)],\n"," [(0, 0.056033967228800385),\n","  (1, 0.006996770721205597),\n","  (2, 0.013335725391699558),\n","  (3, 0.010824064107164215),\n","  (4, 0.016086592512857312),\n","  (5, 0.005441932783159909),\n","  (6, 0.1549455806721684),\n","  (7, 0.02146872383686162),\n","  (8, 0.004245903600047841),\n","  (9, 0.2847147470398278),\n","  (10, 0.011063269943786628),\n","  (11, 0.004843918191603875),\n","  (12, 0.060339672288003826),\n","  (13, 0.07780169836144002),\n","  (14, 0.037256309053940916),\n","  (15, 0.1707331658892477),\n","  (16, 0.007714388231072838),\n","  (17, 0.024578399712952996),\n","  (18, 0.018000239205836623),\n","  (19, 0.01357493122832197)],\n"," [(0, 0.036739109982124374),\n","  (1, 0.01124282623012513),\n","  (2, 0.04492426380656694),\n","  (3, 0.09977420265311883),\n","  (4, 0.01077241509078935),\n","  (5, 0.0062564681531658665),\n","  (6, 0.016135102079217235),\n","  (7, 0.006444632608900179),\n","  (8, 0.006632797064634491),\n","  (9, 0.005409728102361464),\n","  (10, 0.034010725373976855),\n","  (11, 0.012277730736663843),\n","  (12, 0.03504562988051557),\n","  (13, 0.15255433248659328),\n","  (14, 0.07117320538150343),\n","  (15, 0.3997083450936118),\n","  (16, 0.00550381033022862),\n","  (17, 0.02384984476432402),\n","  (18, 0.008044030482641828),\n","  (19, 0.013500799698936871)],\n"," [(0, 0.012004466778336125),\n","  (1, 0.013679508654383027),\n","  (2, 0.05313605062348781),\n","  (3, 0.011632235250325702),\n","  (4, 0.016099013586450772),\n","  (5, 0.025777033314721757),\n","  (6, 0.030616043178857247),\n","  (7, 0.015912897822445562),\n","  (8, 0.015912897822445562),\n","  (9, 0.012004466778336125),\n","  (10, 0.030429927414852037),\n","  (11, 0.012376698306346547),\n","  (12, 0.013307277126372604),\n","  (13, 0.016099013586450772),\n","  (14, 0.011446119486320492),\n","  (15, 0.10078168620882189),\n","  (16, 0.03843290526707612),\n","  (17, 0.12776847198957753),\n","  (18, 0.41643402196166013),\n","  (19, 0.02614926484273218)],\n"," [(0, 0.008849967565563895),\n","  (1, 0.021453062737466402),\n","  (2, 0.0517560930404967),\n","  (3, 0.2737003058103975),\n","  (4, 0.009498656287647112),\n","  (5, 0.04119173385228431),\n","  (6, 0.030998053933833744),\n","  (7, 0.03470484663145213),\n","  (8, 0.004587155963302752),\n","  (9, 0.007459920303956999),\n","  (10, 0.014502826429431931),\n","  (11, 0.10726531368733201),\n","  (12, 0.009591326105087571),\n","  (13, 0.05277546103234175),\n","  (14, 0.01737559077008618),\n","  (15, 0.25034751181540166),\n","  (16, 0.006533222129552403),\n","  (17, 0.024603836530442028),\n","  (18, 0.02034102492818089),\n","  (19, 0.012464090445741819)],\n"," [(0, 0.010093167701863354),\n","  (1, 0.013198757763975156),\n","  (2, 0.047187715665976536),\n","  (3, 0.27061766735679776),\n","  (4, 0.011645962732919254),\n","  (5, 0.009748102139406488),\n","  (6, 0.16968599033816426),\n","  (7, 0.013543823326432022),\n","  (8, 0.017339544513457556),\n","  (9, 0.006642512077294686),\n","  (10, 0.015786749482401656),\n","  (11, 0.1213768115942029),\n","  (12, 0.09118357487922706),\n","  (13, 0.06495859213250517),\n","  (14, 0.03217736369910283),\n","  (15, 0.051328502415458936),\n","  (16, 0.01544168391994479),\n","  (17, 0.010610766045548654),\n","  (18, 0.018029675638371292),\n","  (19, 0.00940303657694962)],\n"," [(0, 0.007294066030492486),\n","  (1, 0.023417790940002195),\n","  (2, 0.07617637380717342),\n","  (3, 0.27437753647033014),\n","  (4, 0.030218273554897444),\n","  (5, 0.006416584402764067),\n","  (6, 0.12224415926291543),\n","  (7, 0.012558955796863003),\n","  (8, 0.0056487879785017),\n","  (9, 0.03712844137325875),\n","  (10, 0.03537347811780191),\n","  (11, 0.04228364593616321),\n","  (12, 0.007842492047822749),\n","  (13, 0.12882527147087858),\n","  (14, 0.006526269606230119),\n","  (15, 0.025721180212789296),\n","  (16, 0.0066359548096961716),\n","  (17, 0.010584622134474059),\n","  (18, 0.03274103323461665),\n","  (19, 0.10798508281232862)],\n"," [(0, 0.020952095209520953),\n","  (1, 0.01165116511651165),\n","  (2, 0.06425642564256426),\n","  (3, 0.006350635063506351),\n","  (4, 0.009150915091509152),\n","  (5, 0.08135813581358135),\n","  (6, 0.00835083508350835),\n","  (7, 0.0038503850385038503),\n","  (8, 0.11316131613161316),\n","  (9, 0.00785078507850785),\n","  (10, 0.04205420542054206),\n","  (11, 0.24667466746674668),\n","  (12, 0.05175517551755175),\n","  (13, 0.01685168516851685),\n","  (14, 0.12116211621162117),\n","  (15, 0.00765076507650765),\n","  (16, 0.00935093509350935),\n","  (17, 0.006450645064506451),\n","  (18, 0.04215421542154216),\n","  (19, 0.12896289628962895)],\n"," [(0, 0.012447966698687161),\n","  (1, 0.00484309958373359),\n","  (2, 0.08841658661543389),\n","  (3, 0.003242074927953891),\n","  (4, 0.005243355747678515),\n","  (5, 0.012768171629843101),\n","  (6, 0.007164585334614154),\n","  (7, 0.013808837656099906),\n","  (8, 0.0026016650656420113),\n","  (9, 0.020212936279218705),\n","  (10, 0.006043868075568365),\n","  (11, 0.5621597822606469),\n","  (12, 0.007324687800192124),\n","  (13, 0.15894172270252965),\n","  (14, 0.004442843419788666),\n","  (15, 0.01356868395773295),\n","  (16, 0.011007044508485433),\n","  (17, 0.03222062119756645),\n","  (18, 0.0053234069804675),\n","  (19, 0.0282180595581172)],\n"," [(0, 0.013346824708489927),\n","  (1, 0.02895827310397995),\n","  (2, 0.15529536474896402),\n","  (3, 0.009877613953936588),\n","  (4, 0.004481063891298062),\n","  (5, 0.12387973402717546),\n","  (6, 0.0079502746458514),\n","  (7, 0.016816035463043267),\n","  (8, 0.0145995952587453),\n","  (9, 0.005251999614532137),\n","  (10, 0.01276862291606437),\n","  (11, 0.32692493013395),\n","  (12, 0.009203045196106773),\n","  (13, 0.13872024669943142),\n","  (14, 0.015852365809000672),\n","  (15, 0.007275705888021585),\n","  (16, 0.026838199865086242),\n","  (17, 0.03560759371687385),\n","  (18, 0.03801676785198033),\n","  (19, 0.008335742507468439)],\n"," [(0, 0.013584945554498361),\n","  (1, 0.43149381541389153),\n","  (2, 0.040120520139549636),\n","  (3, 0.048895232054128344),\n","  (4, 0.03134580822497093),\n","  (5, 0.08716566233217042),\n","  (6, 0.020879585579871022),\n","  (7, 0.03705465694047996),\n","  (8, 0.006396024949783276),\n","  (9, 0.009884765831483244),\n","  (10, 0.09699756845332487),\n","  (11, 0.05196109525319801),\n","  (12, 0.011153398879374141),\n","  (13, 0.0457236494344011),\n","  (14, 0.007136060894386299),\n","  (15, 0.006607463791098425),\n","  (16, 0.005761708425837827),\n","  (17, 0.0062903055291257),\n","  (18, 0.032403002431546674),\n","  (19, 0.00914472988688022)],\n"," [(0, 0.036680287993773104),\n","  (1, 0.05516637478108581),\n","  (2, 0.019167153142634754),\n","  (3, 0.11996497373029773),\n","  (4, 0.00788091068301226),\n","  (5, 0.06314458065771551),\n","  (6, 0.009048453006421483),\n","  (7, 0.015080755010702471),\n","  (8, 0.026172407083090096),\n","  (9, 0.04718816890445612),\n","  (10, 0.026172407083090096),\n","  (11, 0.351138353765324),\n","  (12, 0.08221443860673283),\n","  (13, 0.013524031912823507),\n","  (14, 0.022864370500097295),\n","  (15, 0.011967308814944542),\n","  (16, 0.022669780112862425),\n","  (17, 0.019945514691574236),\n","  (18, 0.02111305701498346),\n","  (19, 0.028896672504378284)],\n"," [(0, 0.013310185185185182),\n","  (1, 0.2659406565656565),\n","  (2, 0.06644570707070706),\n","  (3, 0.014572811447811444),\n","  (4, 0.023516414141414133),\n","  (5, 0.09033038720538719),\n","  (6, 0.01993897306397306),\n","  (7, 0.007207491582491581),\n","  (8, 0.05729166666666665),\n","  (9, 0.005208333333333332),\n","  (10, 0.20680765993265987),\n","  (11, 0.09296085858585858),\n","  (12, 0.007102272727272725),\n","  (13, 0.008154461279461277),\n","  (14, 0.018571127946127943),\n","  (15, 0.005523989898989898),\n","  (16, 0.02888257575757575),\n","  (17, 0.0057344276094276085),\n","  (18, 0.03551136363636363),\n","  (19, 0.026988636363636357)],\n"," [(0, 0.01661111111111111),\n","  (1, 0.07027777777777777),\n","  (2, 0.04683333333333333),\n","  (3, 0.0065),\n","  (4, 0.013833333333333333),\n","  (5, 0.007611111111111111),\n","  (6, 0.0755),\n","  (7, 0.28305555555555556),\n","  (8, 0.006611111111111111),\n","  (9, 0.008611111111111111),\n","  (10, 0.09805555555555556),\n","  (11, 0.018722222222222223),\n","  (12, 0.024055555555555556),\n","  (13, 0.06516666666666666),\n","  (14, 0.0675),\n","  (15, 0.005722222222222222),\n","  (16, 0.025611111111111112),\n","  (17, 0.0565),\n","  (18, 0.08283333333333333),\n","  (19, 0.02038888888888889)],\n"," [(0, 0.015355212864109662),\n","  (1, 0.07822591274548571),\n","  (2, 0.01693686569131409),\n","  (3, 0.02748121787267695),\n","  (4, 0.24219058916567815),\n","  (5, 0.006919731119019376),\n","  (6, 0.009687623566627127),\n","  (7, 0.02194543297746145),\n","  (8, 0.005338078291814948),\n","  (9, 0.008764992750757877),\n","  (10, 0.34829313299064196),\n","  (11, 0.0061289047054171614),\n","  (12, 0.05160142348754449),\n","  (13, 0.07189930143666799),\n","  (14, 0.007051535521286412),\n","  (15, 0.007315144325820483),\n","  (16, 0.036312112824568345),\n","  (17, 0.01865032292078556),\n","  (18, 0.005338078291814948),\n","  (19, 0.01456438645050745)],\n"," [(0, 0.01677969048861068),\n","  (1, 0.014866979655712051),\n","  (2, 0.03208137715179969),\n","  (3, 0.01834463571552774),\n","  (4, 0.010172143974960877),\n","  (5, 0.11206746652756044),\n","  (6, 0.02825595548600244),\n","  (7, 0.015562510867675189),\n","  (8, 0.10424274039297515),\n","  (9, 0.016258042079638328),\n","  (10, 0.2117023126412798),\n","  (11, 0.010693792383933231),\n","  (12, 0.01312815162580421),\n","  (13, 0.05086071987480439),\n","  (14, 0.09311424100156496),\n","  (15, 0.00617283950617284),\n","  (16, 0.023213354199269697),\n","  (17, 0.14962615197356985),\n","  (18, 0.03347243957572597),\n","  (19, 0.03938445487741263)],\n"," [(0, 0.05600044488933378),\n","  (1, 0.04799243688132577),\n","  (2, 0.06734512290067846),\n","  (3, 0.017628739850962073),\n","  (4, 0.012290067845623401),\n","  (5, 0.02819486152819486),\n","  (6, 0.07157157157157157),\n","  (7, 0.02463574685796908),\n","  (8, 0.008397286175063953),\n","  (9, 0.011177844511177844),\n","  (10, 0.4277054832610388),\n","  (11, 0.01440329218106996),\n","  (12, 0.009843176509843177),\n","  (13, 0.02563674785897008),\n","  (14, 0.047325102880658436),\n","  (15, 0.005394283172060949),\n","  (16, 0.02619285952619286),\n","  (17, 0.014180847514180847),\n","  (18, 0.07379601824046268),\n","  (19, 0.0102880658436214)],\n"," [(0, 0.05609167671893848),\n","  (1, 0.013872135102533172),\n","  (2, 0.021243801099048384),\n","  (3, 0.0071706205602466155),\n","  (4, 0.016552740919447795),\n","  (5, 0.006098378233480767),\n","  (6, 0.07364964481972926),\n","  (7, 0.02673904302372336),\n","  (8, 0.016552740919447795),\n","  (9, 0.008242862887012465),\n","  (10, 0.03826564803645624),\n","  (11, 0.005830317651789305),\n","  (12, 0.1788634231336282),\n","  (13, 0.016686771210293525),\n","  (14, 0.14709824420318993),\n","  (15, 0.011861680739847206),\n","  (16, 0.0790108564535585),\n","  (17, 0.2210829647500335),\n","  (18, 0.00877898405039539),\n","  (19, 0.046307465487200106)],\n"," [(0, 0.010380870561282934),\n","  (1, 0.012958190148911801),\n","  (2, 0.011239977090492556),\n","  (3, 0.012099083619702179),\n","  (4, 0.00966494845360825),\n","  (5, 0.005942153493699886),\n","  (6, 0.010237686139747997),\n","  (7, 0.024412943871706762),\n","  (8, 0.028565292096219934),\n","  (9, 0.007946735395189005),\n","  (10, 0.05075887743413517),\n","  (11, 0.011812714776632304),\n","  (12, 0.5534793814432991),\n","  (13, 0.008805841924398627),\n","  (14, 0.061784077892325324),\n","  (15, 0.012815005727376863),\n","  (16, 0.01009450171821306),\n","  (17, 0.027563001145475375),\n","  (18, 0.07195017182130585),\n","  (19, 0.05748854524627721)],\n"," [(0, 0.006767515923566879),\n","  (1, 0.006679051663128096),\n","  (2, 0.39017162066525124),\n","  (3, 0.006944444444444444),\n","  (4, 0.016587048832271762),\n","  (5, 0.015260084925690021),\n","  (6, 0.008182944090587403),\n","  (7, 0.011809978768577496),\n","  (8, 0.006855980184005662),\n","  (9, 0.007740622788393489),\n","  (10, 0.040295470629865535),\n","  (11, 0.004821302193913659),\n","  (12, 0.004909766454352441),\n","  (13, 0.17237261146496816),\n","  (14, 0.07417728237791932),\n","  (15, 0.17759200283085633),\n","  (16, 0.01455237084217976),\n","  (17, 0.004909766454352441),\n","  (18, 0.0040251238499646146),\n","  (19, 0.025345010615711254)],\n"," [(0, 0.023186889818688983),\n","  (1, 0.008193863319386332),\n","  (2, 0.4144002789400279),\n","  (3, 0.005172013017201302),\n","  (4, 0.007264063226406323),\n","  (5, 0.005869363086936309),\n","  (6, 0.004358437935843794),\n","  (7, 0.005753138075313808),\n","  (8, 0.004590887959088796),\n","  (9, 0.005869363086936309),\n","  (10, 0.08897024639702464),\n","  (11, 0.019467689446768945),\n","  (12, 0.029230590423059043),\n","  (13, 0.02097861459786146),\n","  (14, 0.01877033937703394),\n","  (15, 0.16579497907949792),\n","  (16, 0.004823337982333798),\n","  (17, 0.09036494653649466),\n","  (18, 0.020397489539748955),\n","  (19, 0.05654346815434681)],\n"," [(0, 0.02698032961190856),\n","  (1, 0.006423888002835372),\n","  (2, 0.04319510898458267),\n","  (3, 0.00518341307814992),\n","  (4, 0.06259968102073366),\n","  (5, 0.0033227006911217436),\n","  (6, 0.01253765727449938),\n","  (7, 0.007398546872231083),\n","  (8, 0.034954811270600745),\n","  (9, 0.007752968279284069),\n","  (10, 0.009082048555732766),\n","  (11, 0.005006202374623427),\n","  (12, 0.042840687577529686),\n","  (13, 0.09272550062023746),\n","  (14, 0.5584352294878611),\n","  (15, 0.016967924862661705),\n","  (16, 0.017942583732057416),\n","  (17, 0.009082048555732766),\n","  (18, 0.023347510189615454),\n","  (19, 0.014221158958001063)],\n"," [(0, 0.1047602089268756),\n","  (1, 0.005282526115859449),\n","  (2, 0.04457502374169041),\n","  (3, 0.04457502374169041),\n","  (4, 0.00836894586894587),\n","  (5, 0.0068257359924026595),\n","  (6, 0.061906457739791074),\n","  (7, 0.03436609686609687),\n","  (8, 0.0035018993352326686),\n","  (9, 0.278786799620133),\n","  (10, 0.009674738841405508),\n","  (11, 0.025106837606837608),\n","  (12, 0.037927350427350424),\n","  (13, 0.007894112060778728),\n","  (14, 0.14856362773029438),\n","  (15, 0.027955840455840455),\n","  (16, 0.008725071225071225),\n","  (17, 0.08992165242165243),\n","  (18, 0.024869420702754035),\n","  (19, 0.026412630579297245)],\n"," [(0, 0.011050560096881623),\n","  (1, 0.0082248460995055),\n","  (2, 0.027399333938843477),\n","  (3, 0.0082248460995055),\n","  (4, 0.005903723887375113),\n","  (5, 0.018922191946715107),\n","  (6, 0.07674841053587647),\n","  (7, 0.009435866384095267),\n","  (8, 0.008325764456554648),\n","  (9, 0.007013825814915733),\n","  (10, 0.006912907457866586),\n","  (11, 0.21702492683419114),\n","  (12, 0.0082248460995055),\n","  (13, 0.25386012715712986),\n","  (14, 0.009536784741144414),\n","  (15, 0.21016247855484912),\n","  (16, 0.024169946513270764),\n","  (17, 0.02326168129982844),\n","  (18, 0.05303259662932688),\n","  (19, 0.01256433545261883)],\n"," [(0, 0.030106343967730105),\n","  (1, 0.007590759075907591),\n","  (2, 0.046021268793546025),\n","  (3, 0.007224055738907224),\n","  (4, 0.005537220388705537),\n","  (5, 0.0909057572423909),\n","  (6, 0.16417308397506417),\n","  (7, 0.02181884855152182),\n","  (8, 0.004217088375504217),\n","  (9, 0.005977264393105977),\n","  (10, 0.011917858452511918),\n","  (11, 0.06186285295196186),\n","  (12, 0.006637330399706637),\n","  (13, 0.24682801613494681),\n","  (14, 0.0215988265493216),\n","  (15, 0.15705903923725706),\n","  (16, 0.01903190319031903),\n","  (17, 0.00473047304730473),\n","  (18, 0.012797946461312798),\n","  (19, 0.07396406307297397)],\n"," [(0, 0.015309864319765306),\n","  (1, 0.03676200953428675),\n","  (2, 0.039512284561789505),\n","  (3, 0.024110744407774103),\n","  (4, 0.010909424275760908),\n","  (5, 0.009259259259259257),\n","  (6, 0.13137147048038134),\n","  (7, 0.014209754308764207),\n","  (8, 0.01109277594426109),\n","  (9, 0.01164283094976164),\n","  (10, 0.017510084341767505),\n","  (11, 0.07453245324532452),\n","  (12, 0.01164283094976164),\n","  (13, 0.09066740007334065),\n","  (14, 0.019526952695269523),\n","  (15, 0.3163733039970663),\n","  (16, 0.03346167950128345),\n","  (17, 0.06738173817381737),\n","  (18, 0.03419508617528419),\n","  (19, 0.030528052805280523)],\n"," [(0, 0.008997050147492627),\n","  (1, 0.020698131760078665),\n","  (2, 0.37064896755162247),\n","  (3, 0.008210422812192725),\n","  (4, 0.027974434611602757),\n","  (5, 0.00929203539823009),\n","  (6, 0.00624385447394297),\n","  (7, 0.004277286135693216),\n","  (8, 0.03898721730580138),\n","  (9, 0.003785644051130777),\n","  (10, 0.0460668633235005),\n","  (11, 0.0074237954768928225),\n","  (12, 0.006440511307767946),\n","  (13, 0.2522615535889873),\n","  (14, 0.09434611602753197),\n","  (15, 0.019813176007866275),\n","  (16, 0.011356932153392332),\n","  (17, 0.029252704031465095),\n","  (18, 0.00978367748279253),\n","  (19, 0.024139626352015737)],\n"," [(0, 0.018779924054812616),\n","  (1, 0.044039953772494644),\n","  (2, 0.08630510153541358),\n","  (3, 0.004664025094931485),\n","  (4, 0.3658989598811293),\n","  (5, 0.018449727587914813),\n","  (6, 0.008791480931154038),\n","  (7, 0.0067277530130427626),\n","  (8, 0.009864619448571902),\n","  (9, 0.004333828628033681),\n","  (10, 0.034629354465907224),\n","  (11, 0.0058197127290738),\n","  (12, 0.027860326894502237),\n","  (13, 0.12584612844642565),\n","  (14, 0.026209344560013215),\n","  (15, 0.0038385339276869744),\n","  (16, 0.0058197127290738),\n","  (17, 0.022246986957239562),\n","  (18, 0.006397556546144958),\n","  (19, 0.1734769687964339)],\n"," [(0, 0.005897185619001139),\n","  (1, 0.06405563689604685),\n","  (2, 0.047380836180250525),\n","  (3, 0.005083780706035465),\n","  (4, 0.004351716284366358),\n","  (5, 0.008337400357898163),\n","  (6, 0.10074019847079876),\n","  (7, 0.08227590694647796),\n","  (8, 0.004107694810476656),\n","  (9, 0.005490483162518302),\n","  (10, 0.04006019196355946),\n","  (11, 0.025337563038880753),\n","  (12, 0.0027249064584350087),\n","  (13, 0.0904099560761347),\n","  (14, 0.17671221734179274),\n","  (15, 0.03469171953798601),\n","  (16, 0.008337400357898163),\n","  (17, 0.03753863673336587),\n","  (18, 0.03631852936391736),\n","  (19, 0.22014803969415975)],\n"," [(0, 0.02179935841353164),\n","  (1, 0.10010207057451152),\n","  (2, 0.046587926509186355),\n","  (3, 0.02821522309711286),\n","  (4, 0.04410906969962088),\n","  (5, 0.02588218139399242),\n","  (6, 0.009259259259259259),\n","  (7, 0.008238553514144065),\n","  (8, 0.007509477981918927),\n","  (9, 0.005613881598133566),\n","  (10, 0.035068533100029164),\n","  (11, 0.046587926509186355),\n","  (12, 0.010717410323709537),\n","  (13, 0.17125984251968504),\n","  (14, 0.0694808982210557),\n","  (15, 0.0142169728783902),\n","  (16, 0.0999562554680665),\n","  (17, 0.11264216972878391),\n","  (18, 0.035505978419364245),\n","  (19, 0.10724701079031787)],\n"," [(0, 0.04636886102403344),\n","  (1, 0.02564437478230582),\n","  (2, 0.06404562870080112),\n","  (3, 0.005442354580285615),\n","  (4, 0.011537791710205504),\n","  (5, 0.17184778822709856),\n","  (6, 0.0032654127481713687),\n","  (7, 0.04932950191570881),\n","  (8, 0.29053465691396724),\n","  (9, 0.007009752699407872),\n","  (10, 0.04663009404388715),\n","  (11, 0.011015325670498084),\n","  (12, 0.011886102403343783),\n","  (13, 0.08276732845698363),\n","  (14, 0.014063044235458028),\n","  (15, 0.0038749564611633576),\n","  (16, 0.032088122605363985),\n","  (17, 0.086337513061651),\n","  (18, 0.01911354928596308),\n","  (19, 0.01719784047370254)],\n"," [(0, 0.06484173126614988),\n","  (1, 0.007509689922480621),\n","  (2, 0.016069121447028427),\n","  (3, 0.012193152454780363),\n","  (4, 0.011224160206718348),\n","  (5, 0.12152777777777779),\n","  (6, 0.051921834625323),\n","  (7, 0.093265503875969),\n","  (8, 0.014131136950904394),\n","  (9, 0.007832687338501294),\n","  (10, 0.023659560723514217),\n","  (11, 0.007186692506459949),\n","  (12, 0.11377583979328167),\n","  (13, 0.18386627906976746),\n","  (14, 0.011870155038759692),\n","  (15, 0.015261627906976745),\n","  (16, 0.07856912144702843),\n","  (17, 0.1318636950904393),\n","  (18, 0.02139857881136951),\n","  (19, 0.012031653746770028)],\n"," [(0, 0.05914407230196704),\n","  (1, 0.011740209108630162),\n","  (2, 0.015638844586213008),\n","  (3, 0.016967924862661705),\n","  (4, 0.009968102073365232),\n","  (5, 0.010588339535707958),\n","  (6, 0.05285309232677654),\n","  (7, 0.018474215842636896),\n","  (8, 0.14872408293460926),\n","  (9, 0.07482721956406167),\n","  (10, 0.07234626971469077),\n","  (11, 0.028132199184830765),\n","  (12, 0.04487861066808435),\n","  (13, 0.17025518341307816),\n","  (14, 0.015284423179160022),\n","  (15, 0.015904660641502748),\n","  (16, 0.014398369661527556),\n","  (17, 0.05054935318093213),\n","  (18, 0.021486797802587276),\n","  (19, 0.1478380294169768)],\n"," [(0, 0.06431308155446087),\n","  (1, 0.010673234811165848),\n","  (2, 0.015161466885604819),\n","  (3, 0.017679255610290098),\n","  (4, 0.009140667761357418),\n","  (5, 0.010125889436234265),\n","  (6, 0.01789819376026273),\n","  (7, 0.030706075533661743),\n","  (8, 0.04165298303229338),\n","  (9, 0.011767925561029012),\n","  (10, 0.07821565407772306),\n","  (11, 0.006513409961685824),\n","  (12, 0.41855500821018066),\n","  (13, 0.03212917350848386),\n","  (14, 0.08007662835249044),\n","  (15, 0.004980842911877396),\n","  (16, 0.04001094690749864),\n","  (17, 0.038697318007662844),\n","  (18, 0.05862068965517242),\n","  (19, 0.013081554460864807)],\n"," [(0, 0.009765625),\n","  (1, 0.008318865740740741),\n","  (2, 0.028718171296296297),\n","  (3, 0.03855613425925926),\n","  (4, 0.011067708333333334),\n","  (5, 0.007740162037037037),\n","  (6, 0.012803819444444444),\n","  (7, 0.007161458333333333),\n","  (8, 0.007016782407407407),\n","  (9, 0.216796875),\n","  (10, 0.012369791666666666),\n","  (11, 0.05635127314814815),\n","  (12, 0.05027488425925926),\n","  (13, 0.3277633101851852),\n","  (14, 0.09454571759259259),\n","  (15, 0.03175636574074074),\n","  (16, 0.031322337962962965),\n","  (17, 0.028428819444444444),\n","  (18, 0.010633680555555556),\n","  (19, 0.008608217592592593)],\n"," [(0, 0.04640718562874251),\n","  (1, 0.007817697937458417),\n","  (2, 0.033100465735196274),\n","  (3, 0.011033488578398759),\n","  (4, 0.023120425815036594),\n","  (5, 0.011366156575737415),\n","  (6, 0.02267686848525172),\n","  (7, 0.019793745841650032),\n","  (8, 0.01347305389221557),\n","  (9, 0.0074850299401197605),\n","  (10, 0.02400754047460634),\n","  (11, 0.004823685961410512),\n","  (12, 0.05217343091594589),\n","  (13, 0.466012419605234),\n","  (14, 0.046296296296296294),\n","  (15, 0.005156353958749168),\n","  (16, 0.09763805721889554),\n","  (17, 0.023453093812375248),\n","  (18, 0.020459081836327345),\n","  (19, 0.06370592149035263)],\n"," [(0, 0.019582981121442657),\n","  (1, 0.011035972574434111),\n","  (2, 0.028317836010143694),\n","  (3, 0.029726683572837413),\n","  (4, 0.01150558842866535),\n","  (5, 0.011035972574434111),\n","  (6, 0.04851131774208696),\n","  (7, 0.020146520146520144),\n","  (8, 0.005494505494505494),\n","  (9, 0.01019066403681788),\n","  (10, 0.01779844087536395),\n","  (11, 0.009627125011740395),\n","  (12, 0.010942049403587863),\n","  (13, 0.6595754672677748),\n","  (14, 0.005682351836197989),\n","  (15, 0.01892551892551892),\n","  (16, 0.023715600638677555),\n","  (17, 0.01150558842866535),\n","  (18, 0.01864374941298018),\n","  (19, 0.028036066497604952)],\n"," [(0, 0.03045719712386379),\n","  (1, 0.024352191018857685),\n","  (2, 0.024623524623524625),\n","  (3, 0.07414190747524081),\n","  (4, 0.025573192239858905),\n","  (5, 0.008750508750508751),\n","  (6, 0.012684846018179352),\n","  (7, 0.026251526251526252),\n","  (8, 0.008072174738841406),\n","  (9, 0.01689051689051689),\n","  (10, 0.04253154253154253),\n","  (11, 0.0057658390991724325),\n","  (12, 0.16585266585266586),\n","  (13, 0.2644824311490978),\n","  (14, 0.07576990910324244),\n","  (15, 0.01417718084384751),\n","  (16, 0.03751187084520418),\n","  (17, 0.018789852123185458),\n","  (18, 0.014041514041514042),\n","  (19, 0.10927960927960928)],\n"," [(0, 0.26540207522697795),\n","  (1, 0.005782533506268915),\n","  (2, 0.09635754431474276),\n","  (3, 0.01864461738002594),\n","  (4, 0.011619109381755297),\n","  (5, 0.017779939472546476),\n","  (6, 0.13894293125810636),\n","  (7, 0.048800259403372244),\n","  (8, 0.005350194552529183),\n","  (9, 0.037667531344574144),\n","  (10, 0.03085819282317337),\n","  (11, 0.016699092088197146),\n","  (12, 0.010214007782101167),\n","  (13, 0.09527669693039342),\n","  (14, 0.006755296152183312),\n","  (15, 0.007836143536532642),\n","  (16, 0.03701902291396455),\n","  (17, 0.0052421098140942495),\n","  (18, 0.015834414180717682),\n","  (19, 0.12791828793774318)],\n"," [(0, 0.006024096385542169),\n","  (1, 0.009173950704779905),\n","  (2, 0.020198440822111977),\n","  (3, 0.003819198362075754),\n","  (4, 0.005630364595637451),\n","  (5, 0.05531931648161272),\n","  (6, 0.12910465390975667),\n","  (7, 0.18729821245767383),\n","  (8, 0.0035042129301519807),\n","  (9, 0.1480037798251831),\n","  (10, 0.02445074415308292),\n","  (11, 0.010591385148436884),\n","  (12, 0.007047799039294433),\n","  (13, 0.20037010788251045),\n","  (14, 0.011221356012284432),\n","  (15, 0.06791873375856367),\n","  (16, 0.005630364595637451),\n","  (17, 0.06862745098039216),\n","  (18, 0.011930073234112923),\n","  (19, 0.024135758721159147)],\n"," [(0, 0.02485089463220676),\n","  (1, 0.13441572785509168),\n","  (2, 0.018444886238126793),\n","  (3, 0.01027170311464546),\n","  (4, 0.012701568367572344),\n","  (5, 0.017119505191075766),\n","  (6, 0.020212060967528166),\n","  (7, 0.014910536779324055),\n","  (8, 0.04694057874972388),\n","  (9, 0.008725425226419262),\n","  (10, 0.0283852440910095),\n","  (11, 0.022641926220455046),\n","  (12, 0.22056549591340843),\n","  (13, 0.018223989396951624),\n","  (14, 0.1143141153081511),\n","  (15, 0.012259774685222002),\n","  (16, 0.03479125248508946),\n","  (17, 0.14104263309034681),\n","  (18, 0.04517340402032251),\n","  (19, 0.05400927766732936)],\n"," [(0, 0.04112983151635283),\n","  (1, 0.006992621957934148),\n","  (2, 0.026483867415482875),\n","  (3, 0.0101861028521088),\n","  (4, 0.017894505010461404),\n","  (5, 0.005561061557097236),\n","  (6, 0.04410307234886026),\n","  (7, 0.09123444554564475),\n","  (8, 0.006662261865433322),\n","  (9, 0.015361744301288404),\n","  (10, 0.017674264948794184),\n","  (11, 0.008203942297103844),\n","  (12, 0.11457989208236978),\n","  (13, 0.2767866975002753),\n","  (14, 0.006331901772932497),\n","  (15, 0.08594868406563154),\n","  (16, 0.196399074991741),\n","  (17, 0.014480784054619535),\n","  (18, 0.004790221341261975),\n","  (19, 0.009195022574606321)],\n"," [(0, 0.025720164609053502),\n","  (1, 0.004277669482347846),\n","  (2, 0.08365821962313191),\n","  (3, 0.008176304959930693),\n","  (4, 0.007634827810266407),\n","  (5, 0.005577214641542128),\n","  (6, 0.3382607753952784),\n","  (7, 0.018464370803552094),\n","  (8, 0.00308641975308642),\n","  (9, 0.030051981806367773),\n","  (10, 0.03893220706086204),\n","  (11, 0.22065193848819584),\n","  (12, 0.01857266623348495),\n","  (13, 0.09730344379467187),\n","  (14, 0.0044942603422135595),\n","  (15, 0.013699371886506391),\n","  (16, 0.007743123240199264),\n","  (17, 0.026261641758717785),\n","  (18, 0.035033571583279195),\n","  (19, 0.012399826727312109)],\n"," [(0, 0.01597325408618128),\n","  (1, 0.013579329701172201),\n","  (2, 0.02827307247812449),\n","  (3, 0.040160145286445445),\n","  (4, 0.006975400363216116),\n","  (5, 0.0038385339276869744),\n","  (6, 0.25503549612019155),\n","  (7, 0.13137691926696388),\n","  (8, 0.005489516262175995),\n","  (9, 0.004086181277860328),\n","  (10, 0.06442958560343405),\n","  (11, 0.010442463265643059),\n","  (12, 0.028603268945022294),\n","  (13, 0.06855704143965662),\n","  (14, 0.007800891530460626),\n","  (15, 0.20517582961862313),\n","  (16, 0.013826977051345554),\n","  (17, 0.012506191183754338),\n","  (18, 0.010772659732540863),\n","  (19, 0.07309724285950142)],\n"," [(0, 0.040335648148148155),\n","  (1, 0.006886574074074075),\n","  (2, 0.04091435185185186),\n","  (3, 0.07609953703703705),\n","  (4, 0.009201388888888891),\n","  (5, 0.007002314814814815),\n","  (6, 0.17748842592592595),\n","  (7, 0.03547453703703704),\n","  (8, 0.006423611111111112),\n","  (9, 0.03813657407407408),\n","  (10, 0.0416087962962963),\n","  (11, 0.007233796296296297),\n","  (12, 0.014872685185185187),\n","  (13, 0.24392361111111113),\n","  (14, 0.005497685185185186),\n","  (15, 0.1637152777777778),\n","  (16, 0.035011574074074084),\n","  (17, 0.03350694444444445),\n","  (18, 0.005844907407407408),\n","  (19, 0.01082175925925926)],\n"," [(0, 0.0203646013614861),\n","  (1, 0.011249567324333682),\n","  (2, 0.04147917387792778),\n","  (3, 0.0374408676589362),\n","  (4, 0.006980500749971156),\n","  (5, 0.007672781816083997),\n","  (6, 0.03605630552671052),\n","  (7, 0.10274604822891428),\n","  (8, 0.020133841006115153),\n","  (9, 0.009172724125995155),\n","  (10, 0.03397946232837199),\n","  (11, 0.01090342679127726),\n","  (12, 0.06824737510095767),\n","  (13, 0.3780431521864544),\n","  (14, 0.02671051113418715),\n","  (15, 0.03340256143994463),\n","  (16, 0.11682242990654207),\n","  (17, 0.013211030344986732),\n","  (18, 0.006865120572285682),\n","  (19, 0.01851851851851852)],\n"," [(0, 0.03413563427187406),\n","  (1, 0.08651226158038149),\n","  (2, 0.0053739025128670915),\n","  (3, 0.006433545261883138),\n","  (4, 0.007947320617620346),\n","  (5, 0.012185891613684533),\n","  (6, 0.01188313654253709),\n","  (7, 0.013851044504995462),\n","  (8, 0.027777777777777783),\n","  (9, 0.14343021495610053),\n","  (10, 0.00673630033303058),\n","  (11, 0.2584771419921284),\n","  (12, 0.14494399031183774),\n","  (13, 0.007341810475325464),\n","  (14, 0.023993339388434762),\n","  (15, 0.005525280048440813),\n","  (16, 0.12072358462004242),\n","  (17, 0.006282167726309417),\n","  (18, 0.01566757493188011),\n","  (19, 0.06077808053284894)],\n"," [(0, 0.4315350032113038),\n","  (1, 0.005844572896596018),\n","  (2, 0.015093127809890815),\n","  (3, 0.005716120745022479),\n","  (4, 0.009184328837508028),\n","  (5, 0.007385998715478484),\n","  (6, 0.16987797045600514),\n","  (7, 0.005202312138728324),\n","  (8, 0.005973025048169557),\n","  (9, 0.02048811817597945),\n","  (10, 0.011753371868978806),\n","  (11, 0.007000642260757868),\n","  (12, 0.03859987154784843),\n","  (13, 0.027681438664097623),\n","  (14, 0.08253050738599872),\n","  (15, 0.008285163776493257),\n","  (16, 0.009184328837508028),\n","  (17, 0.07957610789980732),\n","  (18, 0.051445086705202314),\n","  (19, 0.007642903018625562)],\n"," [(0, 0.02759942928482255),\n","  (1, 0.006286784376672019),\n","  (2, 0.06888710540395936),\n","  (3, 0.011548064918851438),\n","  (4, 0.6507490636704121),\n","  (5, 0.0045032994471196726),\n","  (6, 0.01252898163010523),\n","  (7, 0.038389513108614236),\n","  (8, 0.0037007312288211175),\n","  (9, 0.01047797396112003),\n","  (10, 0.04828785446762976),\n","  (11, 0.003165685749955414),\n","  (12, 0.04427501337613698),\n","  (13, 0.0075352238273586605),\n","  (14, 0.018860353130016053),\n","  (15, 0.010032102728731944),\n","  (16, 0.007356875334403426),\n","  (17, 0.008159443552701982),\n","  (18, 0.011280542179418585),\n","  (19, 0.006375958623149636)],\n"," [(0, 0.024270196877121522),\n","  (1, 0.04429735234215886),\n","  (2, 0.008655804480651732),\n","  (3, 0.008090065625707174),\n","  (4, 0.003903598099117447),\n","  (5, 0.23596967639737498),\n","  (6, 0.008655804480651732),\n","  (7, 0.005940257976917855),\n","  (8, 0.04407105680018104),\n","  (9, 0.03626386060194614),\n","  (10, 0.010579316587463227),\n","  (11, 0.011145055442407785),\n","  (12, 0.1332315003394433),\n","  (13, 0.009108395564607377),\n","  (14, 0.010918759900429962),\n","  (15, 0.004356189183073093),\n","  (16, 0.19319981896356642),\n","  (17, 0.16898619597193934),\n","  (18, 0.00707173568680697),\n","  (19, 0.03128535867843404)],\n"," [(0, 0.6338410061151494),\n","  (1, 0.01124956732433368),\n","  (2, 0.008134302526825891),\n","  (3, 0.00778816199376947),\n","  (4, 0.011826468212761048),\n","  (5, 0.006865120572285681),\n","  (6, 0.008249682704511365),\n","  (7, 0.004095996307834314),\n","  (8, 0.006403599861543787),\n","  (9, 0.005480558440059998),\n","  (10, 0.02359524633667936),\n","  (11, 0.00778816199376947),\n","  (12, 0.007326641283027576),\n","  (13, 0.03663320641513788),\n","  (14, 0.027056651667243567),\n","  (15, 0.004442136840890735),\n","  (16, 0.016441675320179993),\n","  (17, 0.00998038536979347),\n","  (18, 0.12951424945194415),\n","  (19, 0.033287181262259144)],\n"," [(0, 0.12186209088020337),\n","  (1, 0.012445715496239806),\n","  (2, 0.021660841012604598),\n","  (3, 0.006302298485329944),\n","  (4, 0.00916216502489143),\n","  (5, 0.10438512869399429),\n","  (6, 0.023461497722698867),\n","  (7, 0.10184302510327296),\n","  (8, 0.12609893019807225),\n","  (9, 0.02483847050100625),\n","  (10, 0.011068742717932423),\n","  (11, 0.014140451223387354),\n","  (12, 0.11095222963669103),\n","  (13, 0.010856900752038979),\n","  (14, 0.0073615083147971615),\n","  (15, 0.013081241393920135),\n","  (16, 0.035112805846838256),\n","  (17, 0.18869823111958478),\n","  (18, 0.011386505666772588),\n","  (19, 0.04528122020972355)],\n"," [(0, 0.005901794145420208),\n","  (1, 0.018649669499527857),\n","  (2, 0.013298709474346868),\n","  (3, 0.01251180358829084),\n","  (4, 0.011567516525023608),\n","  (5, 0.006688700031476235),\n","  (6, 0.024630154233553667),\n","  (7, 0.0074756059175322635),\n","  (8, 0.009836323575700346),\n","  (9, 0.010151085930122757),\n","  (10, 0.01786276361347183),\n","  (11, 0.008892036512433112),\n","  (12, 0.14124960654705698),\n","  (13, 0.4405886056027699),\n","  (14, 0.011882278879446019),\n","  (15, 0.007790368271954674),\n","  (16, 0.19082467736858671),\n","  (17, 0.0074756059175322635),\n","  (18, 0.014715140069247718),\n","  (19, 0.03800755429650614)],\n"," [(0, 0.007351027842831123),\n","  (1, 0.2133099141295863),\n","  (2, 0.014246682279469168),\n","  (3, 0.009692948217538383),\n","  (4, 0.020491803278688527),\n","  (5, 0.06602914389799637),\n","  (6, 0.00826177465521728),\n","  (7, 0.00631017434296123),\n","  (8, 0.021662763466042158),\n","  (9, 0.012295081967213118),\n","  (10, 0.20342180588082234),\n","  (11, 0.02517564402810305),\n","  (12, 0.009172521467603436),\n","  (13, 0.013856362217017958),\n","  (14, 0.005399427530575073),\n","  (15, 0.006830601092896176),\n","  (16, 0.005789747593026282),\n","  (17, 0.004879000780640126),\n","  (18, 0.33626073380171745),\n","  (19, 0.009562841530054647)],\n"," [(0, 0.005856283634061412),\n","  (1, 0.0648411944708241),\n","  (2, 0.011870845204178538),\n","  (3, 0.01577503429355281),\n","  (4, 0.04700854700854701),\n","  (5, 0.02263374485596708),\n","  (6, 0.004590060145615701),\n","  (7, 0.041521578558615595),\n","  (8, 0.01155428933206711),\n","  (9, 0.018307481270444233),\n","  (10, 0.030020048538567056),\n","  (11, 0.24401181808589217),\n","  (12, 0.03941120607787275),\n","  (13, 0.006067320882135697),\n","  (14, 0.006067320882135697),\n","  (15, 0.0031127994090957055),\n","  (16, 0.02843726917800992),\n","  (17, 0.005539727761949984),\n","  (18, 0.37274453941120606),\n","  (19, 0.02062889099926137)],\n"," [(0, 0.013005496662740479),\n","  (1, 0.008490380840204162),\n","  (2, 0.019680015704750687),\n","  (3, 0.005447585394581861),\n","  (4, 0.006036513545347468),\n","  (5, 0.005545740086376129),\n","  (6, 0.008294071456615626),\n","  (7, 0.00770514330585002),\n","  (8, 0.048733804475853945),\n","  (9, 0.006527287004318806),\n","  (10, 0.03214566156262269),\n","  (11, 0.008686690223792697),\n","  (12, 0.023802512760109935),\n","  (13, 0.0537396937573616),\n","  (14, 0.007999607381232824),\n","  (15, 0.01869846878680801),\n","  (16, 0.010453474676089517),\n","  (17, 0.012612877895563408),\n","  (18, 0.674862583431488),\n","  (19, 0.02753239104829211)],\n"," [(0, 0.006651676972081696),\n","  (1, 0.019580288551620764),\n","  (2, 0.02314034101555181),\n","  (3, 0.015645493723065396),\n","  (4, 0.011523327712197867),\n","  (5, 0.008338017612891139),\n","  (6, 0.009274873524451941),\n","  (7, 0.01264755480607083),\n","  (8, 0.0255761663856099),\n","  (9, 0.009649615889076263),\n","  (10, 0.04468802698145026),\n","  (11, 0.015270751358441075),\n","  (12, 0.41474611204796713),\n","  (13, 0.009462244706764102),\n","  (14, 0.015645493723065396),\n","  (15, 0.007963275248266819),\n","  (16, 0.0184560614577478),\n","  (17, 0.013584410717631631),\n","  (18, 0.2903316469926926),\n","  (19, 0.027824620573355826)],\n"," [(0, 0.02381850175967823),\n","  (1, 0.009112619406737054),\n","  (2, 0.009741075917546506),\n","  (3, 0.03789592760180995),\n","  (4, 0.022561588738059326),\n","  (5, 0.009364002011060834),\n","  (6, 0.011877828054298642),\n","  (7, 0.009866767219708396),\n","  (8, 0.0830191050779286),\n","  (9, 0.009238310708898944),\n","  (10, 0.03663901458019105),\n","  (11, 0.00773001508295626),\n","  (12, 0.01715686274509804),\n","  (13, 0.004587732528909),\n","  (14, 0.025703871292106585),\n","  (15, 0.01124937154348919),\n","  (16, 0.013386123680241327),\n","  (17, 0.01778531925590749),\n","  (18, 0.614693313222725),\n","  (19, 0.024572649572649572)],\n"," [(0, 0.04266855256954267),\n","  (1, 0.01595159515951595),\n","  (2, 0.008093666509508093),\n","  (3, 0.019566242338519568),\n","  (4, 0.009508093666509508),\n","  (5, 0.020823510922520824),\n","  (6, 0.017208863743517207),\n","  (7, 0.006836397925506836),\n","  (8, 0.02695269526952695),\n","  (9, 0.010451045104510451),\n","  (10, 0.03701084394153701),\n","  (11, 0.019094766619519095),\n","  (12, 0.025695426685525696),\n","  (13, 0.019094766619519095),\n","  (14, 0.1852113782806852),\n","  (15, 0.00887945937450888),\n","  (16, 0.06105610561056106),\n","  (17, 0.025538268112525537),\n","  (18, 0.41937765205091937),\n","  (19, 0.020980669495520982)],\n"," [(0, 0.024674258746280906),\n","  (1, 0.007232994767620807),\n","  (2, 0.008053760131322458),\n","  (3, 0.004668103006053145),\n","  (4, 0.01790294449574228),\n","  (5, 0.006001846722068329),\n","  (6, 0.012875756643069662),\n","  (7, 0.007438186108546219),\n","  (8, 0.005283677028829383),\n","  (9, 0.007232994767620807),\n","  (10, 0.008464142813173285),\n","  (11, 0.04550117985021032),\n","  (12, 0.029085872576177285),\n","  (13, 0.282599774289525),\n","  (14, 0.009797886529188468),\n","  (15, 0.025392428439519853),\n","  (16, 0.010516056222427414),\n","  (17, 0.007745973119934339),\n","  (18, 0.3085564789165897),\n","  (19, 0.17097568482610034)],\n"," [(0, 0.022538344456152676),\n","  (1, 0.01293759512937595),\n","  (2, 0.01422550052687039),\n","  (3, 0.006029738906451235),\n","  (4, 0.013991335909144129),\n","  (5, 0.010010537407797681),\n","  (6, 0.009425125863482028),\n","  (7, 0.007317644303945674),\n","  (8, 0.006615150450766889),\n","  (9, 0.010478866643250205),\n","  (10, 0.01422550052687039),\n","  (11, 0.011766772040744644),\n","  (12, 0.045018147757873786),\n","  (13, 0.16046130429692074),\n","  (14, 0.02324083830933146),\n","  (15, 0.008722632010303244),\n","  (16, 0.06386839948483784),\n","  (17, 0.07803535885727667),\n","  (18, 0.398840885142255),\n","  (19, 0.08225032197634938)],\n"," [(0, 0.005659640905542545),\n","  (1, 0.007741347905282332),\n","  (2, 0.07084309133489461),\n","  (3, 0.007351027842831121),\n","  (4, 0.07448607858443924),\n","  (5, 0.011644548529794432),\n","  (6, 0.011514441842310695),\n","  (7, 0.009823054905022117),\n","  (8, 0.018540202966432474),\n","  (9, 0.008391881342701015),\n","  (10, 0.03011969815248504),\n","  (11, 0.013726255529534218),\n","  (12, 0.006960707780379912),\n","  (13, 0.04599271402550091),\n","  (14, 0.020882123341139733),\n","  (15, 0.07591725214676034),\n","  (16, 0.015287535779339058),\n","  (17, 0.008261774655217278),\n","  (18, 0.4601223002862347),\n","  (19, 0.09673432214415821)],\n"," [(0, 0.08033161806746714),\n","  (1, 0.015151515151515154),\n","  (2, 0.07817165364335177),\n","  (3, 0.0425322406454482),\n","  (4, 0.006702242551299156),\n","  (5, 0.04793215170573662),\n","  (6, 0.010831586303284417),\n","  (7, 0.004034051203862525),\n","  (8, 0.16641255320500606),\n","  (9, 0.016104440632742523),\n","  (10, 0.20040022870211552),\n","  (11, 0.00657518582046884),\n","  (12, 0.042468712280033044),\n","  (13, 0.04799568007115178),\n","  (14, 0.08141160027952482),\n","  (15, 0.004224636300107999),\n","  (16, 0.04748745314783051),\n","  (17, 0.00809986659043263),\n","  (18, 0.0666730195032082),\n","  (19, 0.026459564195413256)],\n"," [(0, 0.6154226174436361),\n","  (1, 0.00956615467528968),\n","  (2, 0.023937842450372768),\n","  (3, 0.01621306027126561),\n","  (4, 0.02124315099254469),\n","  (5, 0.010823677355609452),\n","  (6, 0.07432857271175786),\n","  (7, 0.0074104015090272176),\n","  (8, 0.005075002245576216),\n","  (9, 0.014506422347974493),\n","  (10, 0.009386508578101143),\n","  (11, 0.00705110931465014),\n","  (12, 0.0706458277193928),\n","  (13, 0.04100422168328394),\n","  (14, 0.006601994071678794),\n","  (15, 0.005164825294170485),\n","  (16, 0.009296685529506873),\n","  (17, 0.01217102308452349),\n","  (18, 0.010105092966855297),\n","  (19, 0.03004580975478308)],\n"," [(0, 0.0471996386630533),\n","  (1, 0.013324299909665766),\n","  (2, 0.020551038843721774),\n","  (3, 0.03139114724480579),\n","  (4, 0.021906052393857275),\n","  (5, 0.019196025293586272),\n","  (6, 0.02506775067750678),\n","  (7, 0.03274616079494129),\n","  (8, 0.019647696476964772),\n","  (9, 0.01603432700993677),\n","  (10, 0.021454381210478775),\n","  (11, 0.01874435411020777),\n","  (12, 0.05442637759710931),\n","  (13, 0.3593044263775972),\n","  (14, 0.17411924119241196),\n","  (15, 0.015130984643179766),\n","  (16, 0.020099367660343273),\n","  (17, 0.02461607949412828),\n","  (18, 0.029584462511291782),\n","  (19, 0.03545618789521229)],\n"," [(0, 0.06424836601307188),\n","  (1, 0.0058169934640522865),\n","  (2, 0.06882352941176469),\n","  (3, 0.05248366013071894),\n","  (4, 0.03274509803921568),\n","  (5, 0.02555555555555555),\n","  (6, 0.04464052287581698),\n","  (7, 0.06058823529411763),\n","  (8, 0.045032679738562086),\n","  (9, 0.05405228758169933),\n","  (10, 0.055359477124182994),\n","  (11, 0.008039215686274508),\n","  (12, 0.059281045751633975),\n","  (13, 0.20790849673202608),\n","  (14, 0.01248366013071895),\n","  (15, 0.006862745098039214),\n","  (16, 0.0737908496732026),\n","  (17, 0.016013071895424832),\n","  (18, 0.016666666666666663),\n","  (19, 0.0896078431372549)],\n"," [(0, 0.021757354061880816),\n","  (1, 0.008116140832190217),\n","  (2, 0.2517527815881726),\n","  (3, 0.03539856729157141),\n","  (4, 0.005677488187776254),\n","  (5, 0.01322206980643195),\n","  (6, 0.10611949397957632),\n","  (7, 0.0038484987044657836),\n","  (8, 0.22210791037951538),\n","  (9, 0.007887517146776407),\n","  (10, 0.007963725041914345),\n","  (11, 0.07731290961743638),\n","  (12, 0.006363359244017682),\n","  (13, 0.022443225118122242),\n","  (14, 0.118465172991922),\n","  (15, 0.04324798049077885),\n","  (16, 0.005829903978052127),\n","  (17, 0.004077122389879592),\n","  (18, 0.00743026977594879),\n","  (19, 0.03097850937357111)],\n"," [(0, 0.0038024691358024693),\n","  (1, 0.007753086419753087),\n","  (2, 0.005679012345679012),\n","  (3, 0.006962962962962963),\n","  (4, 0.009728395061728396),\n","  (5, 0.02345679012345679),\n","  (6, 0.006962962962962963),\n","  (7, 0.010419753086419754),\n","  (8, 0.045580246913580244),\n","  (9, 0.009333333333333334),\n","  (10, 0.014765432098765432),\n","  (11, 0.007851851851851851),\n","  (12, 0.07046913580246913),\n","  (13, 0.03619753086419753),\n","  (14, 0.020493827160493826),\n","  (15, 0.006172839506172839),\n","  (16, 0.020493827160493826),\n","  (17, 0.013777777777777778),\n","  (18, 0.5883950617283951),\n","  (19, 0.0917037037037037)],\n"," [(0, 0.009973078805677925),\n","  (1, 0.005445423396965248),\n","  (2, 0.04839696524718551),\n","  (3, 0.0063020068526676454),\n","  (4, 0.008015173764072443),\n","  (5, 0.042156142927068034),\n","  (6, 0.01719285364659814),\n","  (7, 0.007280959373470387),\n","  (8, 0.007770435633871757),\n","  (9, 0.013766519823788546),\n","  (10, 0.02233235438081253),\n","  (11, 0.005323054331864904),\n","  (12, 0.007403328438570729),\n","  (13, 0.12867107195301028),\n","  (14, 0.008259911894273128),\n","  (15, 0.5968551150269212),\n","  (16, 0.011074400391581009),\n","  (17, 0.014500734214390602),\n","  (18, 0.013277043563387176),\n","  (19, 0.02600342633382281)],\n"," [(0, 0.008641975308641976),\n","  (1, 0.05091358024691359),\n","  (2, 0.014765432098765434),\n","  (3, 0.010222222222222225),\n","  (4, 0.019506172839506175),\n","  (5, 0.015160493827160495),\n","  (6, 0.015555555555555557),\n","  (7, 0.012296296296296298),\n","  (8, 0.024740740740740744),\n","  (9, 0.014567901234567903),\n","  (10, 0.051012345679012354),\n","  (11, 0.008641975308641976),\n","  (12, 0.3721975308641976),\n","  (13, 0.06217283950617285),\n","  (14, 0.12301234567901236),\n","  (15, 0.008938271604938274),\n","  (16, 0.11135802469135804),\n","  (17, 0.01288888888888889),\n","  (18, 0.05407407407407408),\n","  (19, 0.009333333333333336)],\n"," [(0, 0.023216308040770097),\n","  (1, 0.011639612432364412),\n","  (2, 0.036932175663772486),\n","  (3, 0.009626274065685163),\n","  (4, 0.012520447967786582),\n","  (5, 0.00585126462816157),\n","  (6, 0.02472631181577953),\n","  (7, 0.006606266515666288),\n","  (8, 0.007235434755253553),\n","  (9, 0.007235434755253553),\n","  (10, 0.16263998993330814),\n","  (11, 0.00786460299484082),\n","  (12, 0.016672958349062537),\n","  (13, 0.32899207248018114),\n","  (14, 0.05555555555555554),\n","  (15, 0.006354599219831381),\n","  (16, 0.08135145337863343),\n","  (17, 0.14628161570403922),\n","  (18, 0.0069837674594186465),\n","  (19, 0.041713854284635705)],\n"," [(0, 0.16028430519292136),\n","  (1, 0.007977951842181606),\n","  (2, 0.02103278212938787),\n","  (3, 0.01261966927763272),\n","  (4, 0.020162460110240785),\n","  (5, 0.022773426167682038),\n","  (6, 0.02364374818682912),\n","  (7, 0.035538149115172606),\n","  (8, 0.016391064693936752),\n","  (9, 0.055845662895271236),\n","  (10, 0.009718595880475774),\n","  (11, 0.023933855526544815),\n","  (12, 0.194807078619089),\n","  (13, 0.011749347258485636),\n","  (14, 0.10110240789091961),\n","  (15, 0.009138381201044385),\n","  (16, 0.07586306933565418),\n","  (17, 0.011459239918769943),\n","  (18, 0.019582245430809397),\n","  (19, 0.16637655932695095)],\n"," [(0, 0.05230880230880231),\n","  (1, 0.010894660894660894),\n","  (2, 0.01681096681096681),\n","  (3, 0.013203463203463203),\n","  (4, 0.05274170274170274),\n","  (5, 0.008152958152958153),\n","  (6, 0.029076479076479075),\n","  (7, 0.012914862914862916),\n","  (8, 0.005411255411255411),\n","  (9, 0.00873015873015873),\n","  (10, 0.0398989898989899),\n","  (11, 0.006132756132756133),\n","  (12, 0.29054834054834056),\n","  (13, 0.21262626262626264),\n","  (14, 0.01940836940836941),\n","  (15, 0.006998556998556999),\n","  (16, 0.19112554112554112),\n","  (17, 0.005411255411255411),\n","  (18, 0.008008658008658008),\n","  (19, 0.009595959595959595)],\n"," [(0, 0.0903316106804479),\n","  (1, 0.01087424633936262),\n","  (2, 0.009582256675279933),\n","  (3, 0.02444013781223084),\n","  (4, 0.0072136089577950055),\n","  (5, 0.009582256675279933),\n","  (6, 0.01388888888888889),\n","  (7, 0.010228251507321276),\n","  (8, 0.027885443583118005),\n","  (9, 0.00785960378983635),\n","  (10, 0.04834194659776056),\n","  (11, 0.00785960378983635),\n","  (12, 0.2559216192937124),\n","  (13, 0.12090869939707151),\n","  (14, 0.1734496124031008),\n","  (15, 0.00785960378983635),\n","  (16, 0.011304909560723516),\n","  (17, 0.0862403100775194),\n","  (18, 0.06879844961240311),\n","  (19, 0.007428940568475453)],\n"," [(0, 0.051915708812260535),\n","  (1, 0.01768837803320562),\n","  (2, 0.09010217113665389),\n","  (3, 0.008492975734355045),\n","  (4, 0.1863984674329502),\n","  (5, 0.0059386973180076625),\n","  (6, 0.03748403575989783),\n","  (7, 0.007854406130268199),\n","  (8, 0.0072158365261813535),\n","  (9, 0.009642401021711367),\n","  (10, 0.06698595146871009),\n","  (11, 0.006960408684546616),\n","  (12, 0.01641123882503193),\n","  (13, 0.21436781609195402),\n","  (14, 0.025606641123882504),\n","  (15, 0.007982120051085569),\n","  (16, 0.059450830140485314),\n","  (17, 0.06264367816091954),\n","  (18, 0.03876117496807152),\n","  (19, 0.0780970625798212)],\n"," [(0, 0.03684097957437707),\n","  (1, 0.007218479307025986),\n","  (2, 0.06903005026200407),\n","  (3, 0.13372901293979253),\n","  (4, 0.01705699925141696),\n","  (5, 0.009571168858945567),\n","  (6, 0.04197412041492889),\n","  (7, 0.0726660250240616),\n","  (8, 0.01010587102983638),\n","  (9, 0.024863650946422844),\n","  (10, 0.0333119452464977),\n","  (11, 0.004865789755106406),\n","  (12, 0.04090471607314726),\n","  (13, 0.09341246925462518),\n","  (14, 0.016736177948882474),\n","  (15, 0.07876162977221687),\n","  (16, 0.018233344027376752),\n","  (17, 0.05491391295048658),\n","  (18, 0.035022992193348305),\n","  (19, 0.20078066516950058)],\n"," [(0, 0.012468104848063094),\n","  (1, 0.008640686615634424),\n","  (2, 0.0759104616098353),\n","  (3, 0.008292739503595454),\n","  (4, 0.02070285316631872),\n","  (5, 0.05132219902574808),\n","  (6, 0.11754813268383206),\n","  (7, 0.02661795407098121),\n","  (8, 0.00771282765019717),\n","  (9, 0.016527487821851077),\n","  (10, 0.07892600324750638),\n","  (11, 0.011772210623985153),\n","  (12, 0.036824402690791),\n","  (13, 0.12694270470888425),\n","  (14, 0.011308281141266528),\n","  (15, 0.03473672001855718),\n","  (16, 0.0333449315704013),\n","  (17, 0.027777777777777776),\n","  (18, 0.0938877290651821),\n","  (19, 0.19873579215959175)],\n"," [(0, 0.014818629133768167),\n","  (1, 0.15133310447419612),\n","  (2, 0.0279780295228287),\n","  (3, 0.017679368348781324),\n","  (4, 0.0061219819201281615),\n","  (5, 0.07970019453026662),\n","  (6, 0.020196818857992904),\n","  (7, 0.009554868978143952),\n","  (8, 0.009211580272342373),\n","  (9, 0.006922988900331846),\n","  (10, 0.008868291566540794),\n","  (11, 0.3529580043483236),\n","  (12, 0.0093260098409429),\n","  (13, 0.025689438150818172),\n","  (14, 0.027177022542625013),\n","  (15, 0.0526948163405424),\n","  (16, 0.011729030781553953),\n","  (17, 0.11585993820803296),\n","  (18, 0.016763931799977115),\n","  (19, 0.03541595148186291)],\n"," [(0, 0.01620795107033639),\n","  (1, 0.018246687054026503),\n","  (2, 0.016615698267074413),\n","  (3, 0.046788990825688076),\n","  (4, 0.007033639143730887),\n","  (5, 0.20071355759429155),\n","  (6, 0.011926605504587157),\n","  (7, 0.00927624872579001),\n","  (8, 0.03965341488277268),\n","  (9, 0.044954128440366975),\n","  (10, 0.01620795107033639),\n","  (11, 0.028440366972477066),\n","  (12, 0.17115188583078492),\n","  (13, 0.07757390417940876),\n","  (14, 0.012334352701325178),\n","  (15, 0.00764525993883792),\n","  (16, 0.19072375127420998),\n","  (17, 0.01416921508664628),\n","  (18, 0.045973496432212026),\n","  (19, 0.02436289500509684)],\n"," [(0, 0.010185185185185186),\n","  (1, 0.008816425120772946),\n","  (2, 0.016304347826086956),\n","  (3, 0.02210144927536232),\n","  (4, 0.010909822866344605),\n","  (5, 0.003985507246376811),\n","  (6, 0.010507246376811594),\n","  (7, 0.005595813204508857),\n","  (8, 0.7539049919484702),\n","  (9, 0.008977455716586152),\n","  (10, 0.026851851851851852),\n","  (11, 0.0044685990338164255),\n","  (12, 0.010507246376811594),\n","  (13, 0.023711755233494364),\n","  (14, 0.01179549114331723),\n","  (15, 0.004629629629629629),\n","  (16, 0.02741545893719807),\n","  (17, 0.004951690821256039),\n","  (18, 0.026610305958132047),\n","  (19, 0.007769726247987117)],\n"," [(0, 0.01611351611351611),\n","  (1, 0.03371813371813371),\n","  (2, 0.034199134199134194),\n","  (3, 0.011495911495911493),\n","  (4, 0.006878306878306877),\n","  (5, 0.49114959114959106),\n","  (6, 0.013419913419913417),\n","  (7, 0.013227513227513223),\n","  (8, 0.17763347763347762),\n","  (9, 0.0063011063011062995),\n","  (10, 0.04872534872534871),\n","  (11, 0.01534391534391534),\n","  (12, 0.025541125541125535),\n","  (13, 0.017267917267917265),\n","  (14, 0.00803270803270803),\n","  (15, 0.014670514670514667),\n","  (16, 0.014189514189514186),\n","  (17, 0.022751322751322748),\n","  (18, 0.008225108225108224),\n","  (19, 0.021115921115921112)],\n"," [(0, 0.021862676873115286),\n","  (1, 0.02383437717466945),\n","  (2, 0.03230109023428439),\n","  (3, 0.00806077476223614),\n","  (4, 0.005393180236604036),\n","  (5, 0.4835884945488286),\n","  (6, 0.00701693342611923),\n","  (7, 0.007480862908837857),\n","  (8, 0.20383901646949665),\n","  (9, 0.006089074460681976),\n","  (10, 0.07799814428206912),\n","  (11, 0.00771282765019717),\n","  (12, 0.008640686615634424),\n","  (13, 0.007828810020876827),\n","  (14, 0.00771282765019717),\n","  (15, 0.004929250753885409),\n","  (16, 0.009336580839712364),\n","  (17, 0.051090234284388775),\n","  (18, 0.008872651356993737),\n","  (19, 0.016411505451171423)],\n"," [(0, 0.02153355946140405),\n","  (1, 0.010535512385651147),\n","  (2, 0.016702641587007915),\n","  (3, 0.014133004419775927),\n","  (4, 0.012180080172679618),\n","  (5, 0.4751259122211944),\n","  (6, 0.008171446191797719),\n","  (7, 0.011768938225922499),\n","  (8, 0.02112241751464693),\n","  (9, 0.009302086545379793),\n","  (10, 0.005190667077808613),\n","  (11, 0.08741905642923219),\n","  (12, 0.03140096618357488),\n","  (13, 0.010741083359029705),\n","  (14, 0.017936067427279268),\n","  (15, 0.003648884777469421),\n","  (16, 0.07467365607976154),\n","  (17, 0.13459759481961148),\n","  (18, 0.006835234864837085),\n","  (19, 0.02698119025593586)],\n"," [(0, 0.009397715472481827),\n","  (1, 0.013655244029075805),\n","  (2, 0.004828660436137072),\n","  (3, 0.038058151609553476),\n","  (4, 0.013032191069574248),\n","  (5, 0.6416926272066459),\n","  (6, 0.005451713395638629),\n","  (7, 0.003790238836967809),\n","  (8, 0.10441329179646937),\n","  (9, 0.01822429906542056),\n","  (10, 0.013966770508826583),\n","  (11, 0.008670820353063343),\n","  (12, 0.007528556593977154),\n","  (13, 0.01074766355140187),\n","  (14, 0.010228452751817238),\n","  (15, 0.005659397715472482),\n","  (16, 0.06568016614745587),\n","  (17, 0.005555555555555556),\n","  (18, 0.006490134994807892),\n","  (19, 0.012928348909657321)],\n"," [(0, 0.018434913468773514),\n","  (1, 0.01291698018560321),\n","  (2, 0.013418610484073237),\n","  (3, 0.014421871081013293),\n","  (4, 0.0109104589917231),\n","  (5, 0.4056935038876348),\n","  (6, 0.014421871081013293),\n","  (7, 0.018936543767243543),\n","  (8, 0.0417607223476298),\n","  (9, 0.009907198394783046),\n","  (10, 0.016930022573363433),\n","  (11, 0.019187358916478554),\n","  (12, 0.12678705793829947),\n","  (13, 0.012666165036368197),\n","  (14, 0.036995234512164535),\n","  (15, 0.009154752947078003),\n","  (16, 0.1340606972661149),\n","  (17, 0.035490343616754454),\n","  (18, 0.010659643842488086),\n","  (19, 0.03724604966139955)],\n"," [(0, 0.005357142857142857),\n","  (1, 0.17903439153439155),\n","  (2, 0.006944444444444444),\n","  (3, 0.05773809523809524),\n","  (4, 0.005621693121693121),\n","  (5, 0.1359126984126984),\n","  (6, 0.005357142857142857),\n","  (7, 0.005753968253968254),\n","  (8, 0.25125661375661373),\n","  (9, 0.00496031746031746),\n","  (10, 0.11408730158730158),\n","  (11, 0.0083994708994709),\n","  (12, 0.007473544973544973),\n","  (13, 0.1472883597883598),\n","  (14, 0.02705026455026455),\n","  (15, 0.005621693121693121),\n","  (16, 0.010515873015873017),\n","  (17, 0.005092592592592593),\n","  (18, 0.006283068783068783),\n","  (19, 0.01025132275132275)],\n"," [(0, 0.01855522519546305),\n","  (1, 0.00908490254377271),\n","  (2, 0.020537385750468005),\n","  (3, 0.011948023345446533),\n","  (4, 0.01073670300627684),\n","  (5, 0.08407664354146017),\n","  (6, 0.007873582204603016),\n","  (7, 0.028245787908820607),\n","  (8, 0.6083581103402708),\n","  (9, 0.010075982821275187),\n","  (10, 0.01701354476379253),\n","  (11, 0.007322982050434973),\n","  (12, 0.014921264177953966),\n","  (13, 0.0345226296663363),\n","  (14, 0.02031714568880079),\n","  (15, 0.004680101310428366),\n","  (16, 0.014370664023785923),\n","  (17, 0.044543552472194686),\n","  (18, 0.016903424732958923),\n","  (19, 0.015912344455456445)],\n"," [(0, 0.012596475899228193),\n","  (1, 0.005169651958642784),\n","  (2, 0.021916411824668705),\n","  (3, 0.009101499927188),\n","  (4, 0.019149555846803553),\n","  (5, 0.11234891510120867),\n","  (6, 0.048128731614970145),\n","  (7, 0.013178971894568224),\n","  (8, 0.3577253531381972),\n","  (9, 0.0181301878549585),\n","  (10, 0.09239842726081259),\n","  (11, 0.00764525993883792),\n","  (12, 0.03050822775593418),\n","  (13, 0.036624435707004514),\n","  (14, 0.1018639871850881),\n","  (15, 0.0064802679481578565),\n","  (16, 0.027886995776904035),\n","  (17, 0.022498907820008737),\n","  (18, 0.047400611620795105),\n","  (19, 0.009247123926023008)],\n"," [(0, 0.055876222542889216),\n","  (1, 0.03375020041686709),\n","  (2, 0.027817861151194487),\n","  (3, 0.028940195606862278),\n","  (4, 0.013868847202180538),\n","  (5, 0.008417508417508419),\n","  (6, 0.0420875420875421),\n","  (7, 0.028298861632194968),\n","  (8, 0.3018278018278019),\n","  (9, 0.028138528138528143),\n","  (10, 0.07271123937790606),\n","  (11, 0.007134840468173803),\n","  (12, 0.10990860990860993),\n","  (13, 0.0088985088985089),\n","  (14, 0.02461119127785795),\n","  (15, 0.011944845278178613),\n","  (16, 0.10173160173160174),\n","  (17, 0.04481321147987816),\n","  (18, 0.010662177328843997),\n","  (19, 0.0385602052268719)],\n"," [(0, 0.030472271213011957),\n","  (1, 0.02968841857730747),\n","  (2, 0.015579071134626692),\n","  (3, 0.007544581618655693),\n","  (4, 0.02674897119341564),\n","  (5, 0.030472271213011957),\n","  (6, 0.0065647658240250845),\n","  (7, 0.014991181657848325),\n","  (8, 0.16196355085243977),\n","  (9, 0.02576915539878503),\n","  (10, 0.04497354497354498),\n","  (11, 0.0065647658240250845),\n","  (12, 0.14001567705271412),\n","  (13, 0.014011365863217717),\n","  (14, 0.2087987458357829),\n","  (15, 0.007936507936507938),\n","  (16, 0.14569860866157167),\n","  (17, 0.03772290809327847),\n","  (18, 0.01910640799529689),\n","  (19, 0.02537722908093279)],\n"," [(0, 0.0154419595314164),\n","  (1, 0.022423381848301976),\n","  (2, 0.014022009229676962),\n","  (3, 0.023843332150041415),\n","  (4, 0.005028990651993847),\n","  (5, 0.08206129452135842),\n","  (6, 0.008223878830907584),\n","  (7, 0.0051473198438054665),\n","  (8, 0.027748195479824874),\n","  (9, 0.01698023902496746),\n","  (10, 0.06017039403620873),\n","  (11, 0.011300437818009703),\n","  (12, 0.19743225653768784),\n","  (13, 0.01579694710685126),\n","  (14, 0.10194059874571057),\n","  (15, 0.009525499940835404),\n","  (16, 0.09365755531889718),\n","  (17, 0.19400070997515087),\n","  (18, 0.007513903680037865),\n","  (19, 0.08774109572831618)],\n"," [(0, 0.0514558232931727),\n","  (1, 0.026188085676037486),\n","  (2, 0.006777108433734941),\n","  (3, 0.018657965194109775),\n","  (4, 0.028530789825970553),\n","  (5, 0.010793172690763055),\n","  (6, 0.016315261044176712),\n","  (7, 0.01029116465863454),\n","  (8, 0.043925702811244985),\n","  (9, 0.007279116465863455),\n","  (10, 0.04978246318607765),\n","  (11, 0.010458500669344045),\n","  (12, 0.3995147255689425),\n","  (13, 0.02233935742971888),\n","  (14, 0.052459839357429726),\n","  (15, 0.015143908969210175),\n","  (16, 0.014139892904953148),\n","  (17, 0.03154283801874164),\n","  (18, 0.05915327978580991),\n","  (19, 0.1252510040160643)],\n"," [(0, 0.01986076986076986),\n","  (1, 0.009623259623259621),\n","  (2, 0.01576576576576576),\n","  (3, 0.006620256620256618),\n","  (4, 0.010169260169260168),\n","  (5, 0.007439257439257437),\n","  (6, 0.008258258258258256),\n","  (7, 0.011124761124761124),\n","  (8, 0.47495222495222483),\n","  (9, 0.019587769587769584),\n","  (10, 0.01986076986076986),\n","  (11, 0.01085176085176085),\n","  (12, 0.017949767949767947),\n","  (13, 0.013445263445263441),\n","  (14, 0.12428337428337426),\n","  (15, 0.010578760578760577),\n","  (16, 0.14707889707889704),\n","  (17, 0.011124761124761124),\n","  (18, 0.01972426972426972),\n","  (19, 0.041700791700791696)],\n"," [(0, 0.017119984567901234),\n","  (1, 0.028211805555555556),\n","  (2, 0.020302854938271605),\n","  (3, 0.01750578703703704),\n","  (4, 0.008150077160493827),\n","  (5, 0.026282793209876542),\n","  (6, 0.01750578703703704),\n","  (7, 0.009114583333333334),\n","  (8, 0.018566743827160493),\n","  (9, 0.004195601851851852),\n","  (10, 0.042486496913580245),\n","  (11, 0.00882523148148148),\n","  (12, 0.03583140432098766),\n","  (13, 0.04374035493827161),\n","  (14, 0.3806423611111111),\n","  (15, 0.006510416666666667),\n","  (16, 0.008150077160493827),\n","  (17, 0.22641782407407407),\n","  (18, 0.045572916666666664),\n","  (19, 0.03486689814814815)],\n"," [(0, 0.014833615341229555),\n","  (1, 0.005132543711223915),\n","  (2, 0.014833615341229555),\n","  (3, 0.008291032148900168),\n","  (4, 0.004568527918781726),\n","  (5, 0.13824027072758038),\n","  (6, 0.013254371122391428),\n","  (7, 0.007050197405527355),\n","  (8, 0.11252115059221658),\n","  (9, 0.004681331077270164),\n","  (10, 0.029272419627749575),\n","  (11, 0.01742808798646362),\n","  (12, 0.25848843767625496),\n","  (13, 0.007952622673434856),\n","  (14, 0.15346869712351946),\n","  (15, 0.00535815002820079),\n","  (16, 0.06356457980823463),\n","  (17, 0.011449520586576424),\n","  (18, 0.07732656514382402),\n","  (19, 0.05228426395939086)],\n"," [(0, 0.019426523297491043),\n","  (1, 0.05268817204301076),\n","  (2, 0.006953405017921148),\n","  (3, 0.006810035842293908),\n","  (4, 0.011971326164874554),\n","  (5, 0.007526881720430109),\n","  (6, 0.01799283154121864),\n","  (7, 0.010107526881720431),\n","  (8, 0.005376344086021507),\n","  (9, 0.00953405017921147),\n","  (10, 0.017562724014336922),\n","  (11, 0.00838709677419355),\n","  (12, 0.3663799283154122),\n","  (13, 0.15620071684587816),\n","  (14, 0.23362007168458784),\n","  (15, 0.00896057347670251),\n","  (16, 0.01584229390681004),\n","  (17, 0.019856630824372765),\n","  (18, 0.010250896057347672),\n","  (19, 0.014551971326164877)],\n"," [(0, 0.03763816172193136),\n","  (1, 0.010529377545084352),\n","  (2, 0.028097731239092497),\n","  (3, 0.02472367655613729),\n","  (4, 0.017277486910994764),\n","  (5, 0.01878999418266434),\n","  (6, 0.013205351948807446),\n","  (7, 0.019720767888307154),\n","  (8, 0.026352530541012217),\n","  (9, 0.012623618382780687),\n","  (10, 0.07091332169866202),\n","  (11, 0.007620709714950553),\n","  (12, 0.441826643397324),\n","  (13, 0.051832460732984295),\n","  (14, 0.07184409540430482),\n","  (15, 0.00924956369982548),\n","  (16, 0.04717859220477021),\n","  (17, 0.02123327515997673),\n","  (18, 0.058347876672484),\n","  (19, 0.01099476439790576)],\n"," [(0, 0.011277155112771551),\n","  (1, 0.023453715234537154),\n","  (2, 0.007679535076795351),\n","  (3, 0.044762695447626955),\n","  (4, 0.006572575065725751),\n","  (5, 0.15794935657949358),\n","  (6, 0.012522485125224851),\n","  (7, 0.009201605092016051),\n","  (8, 0.3379687283796873),\n","  (9, 0.04725335547253356),\n","  (10, 0.01169226511692265),\n","  (11, 0.025529265255292654),\n","  (12, 0.013906185139061852),\n","  (13, 0.03120243531202435),\n","  (14, 0.049328905493289055),\n","  (15, 0.005880725058807251),\n","  (16, 0.10246298602462986),\n","  (17, 0.05569392555693926),\n","  (18, 0.01764217517642175),\n","  (19, 0.028019925280199254)],\n"," [(0, 0.008470615726136232),\n","  (1, 0.025762355415352264),\n","  (2, 0.01723332164972544),\n","  (3, 0.08920434630213811),\n","  (4, 0.006133894146512444),\n","  (5, 0.024243486388596802),\n","  (6, 0.011858862016590725),\n","  (7, 0.02669704404720178),\n","  (8, 0.1499591073723566),\n","  (9, 0.011508353779647157),\n","  (10, 0.17391050356350043),\n","  (11, 0.007068582778361959),\n","  (12, 0.07191260661292208),\n","  (13, 0.004731861198738171),\n","  (14, 0.17017174903610238),\n","  (15, 0.009872648673910505),\n","  (16, 0.16269423998130625),\n","  (17, 0.009638976515948127),\n","  (18, 0.009405304357985748),\n","  (19, 0.009522140436966937)],\n"," [(0, 0.022882181110029213),\n","  (1, 0.013686032673374445),\n","  (2, 0.01346965271015904),\n","  (3, 0.061397814562371526),\n","  (4, 0.00622092394244293),\n","  (5, 0.11148977604673807),\n","  (6, 0.005679974034404414),\n","  (7, 0.010656713188358757),\n","  (8, 0.21686681813264092),\n","  (9, 0.006004543979227524),\n","  (10, 0.01823001190089798),\n","  (11, 0.06604998377150276),\n","  (12, 0.01346965271015904),\n","  (13, 0.006112733960835227),\n","  (14, 0.020177431569836632),\n","  (15, 0.007843773666558477),\n","  (16, 0.2876230661040788),\n","  (17, 0.024505030834144757),\n","  (18, 0.06550903386346424),\n","  (19, 0.02212485123877529)],\n"," [(0, 0.02537437603993345),\n","  (1, 0.009197633573673508),\n","  (2, 0.005500092438528379),\n","  (3, 0.07353484932519876),\n","  (4, 0.017054908485856907),\n","  (5, 0.003558883342577187),\n","  (6, 0.004668145683120725),\n","  (7, 0.12677944167128863),\n","  (8, 0.004483268626363469),\n","  (9, 0.1044093178036606),\n","  (10, 0.12650212608615274),\n","  (11, 0.0049454612682566105),\n","  (12, 0.029256794231835834),\n","  (13, 0.0864762432982067),\n","  (14, 0.12215751525235721),\n","  (15, 0.1912090959511925),\n","  (16, 0.022878535773710486),\n","  (17, 0.023987798114254028),\n","  (18, 0.005500092438528379),\n","  (19, 0.012525420595304126)],\n"," [(0, 0.00541860188162439),\n","  (1, 0.00815767536024771),\n","  (2, 0.06091461236155771),\n","  (3, 0.1604739788019531),\n","  (4, 0.005537692032868882),\n","  (5, 0.008276765511492201),\n","  (6, 0.016970346552340126),\n","  (7, 0.01839942836727403),\n","  (8, 0.08282720019054425),\n","  (9, 0.014112182922472313),\n","  (10, 0.027807550315588903),\n","  (11, 0.008991306418959153),\n","  (12, 0.02006669048469692),\n","  (13, 0.31529117541979285),\n","  (14, 0.01720852685482911),\n","  (15, 0.19167559842801002),\n","  (16, 0.014588543527450282),\n","  (17, 0.006609503394069311),\n","  (18, 0.006966773847802788),\n","  (19, 0.009705847326426107)],\n"," [(0, 0.047125273984151075),\n","  (1, 0.005648288652841006),\n","  (2, 0.026049570055639862),\n","  (3, 0.15351542741527568),\n","  (4, 0.007334344967121903),\n","  (5, 0.018125105378519645),\n","  (6, 0.00952621817568707),\n","  (7, 0.084555724161187),\n","  (8, 0.03953802056988704),\n","  (9, 0.01745068285280729),\n","  (10, 0.015427415275670209),\n","  (11, 0.013235542067105043),\n","  (12, 0.1912830888551678),\n","  (13, 0.03397403473276008),\n","  (14, 0.20999831394368576),\n","  (15, 0.03903220367560277),\n","  (16, 0.03549148541561289),\n","  (17, 0.02031697858708481),\n","  (18, 0.018125105378519645),\n","  (19, 0.014247175855673581)],\n"," [(0, 0.026416482707873442),\n","  (1, 0.009345106696100076),\n","  (2, 0.036865342163355415),\n","  (3, 0.010669610007358353),\n","  (4, 0.01846946284032377),\n","  (5, 0.014495952906548936),\n","  (6, 0.09793966151582048),\n","  (7, 0.12796173657100812),\n","  (8, 0.014937454010301696),\n","  (9, 0.025239146431199417),\n","  (10, 0.07733627667402503),\n","  (11, 0.011699779249448126),\n","  (12, 0.02626931567328919),\n","  (13, 0.23009565857247982),\n","  (14, 0.05570272259013982),\n","  (15, 0.010816777041942607),\n","  (16, 0.009639440765268582),\n","  (17, 0.07203826342899192),\n","  (18, 0.02317880794701987),\n","  (19, 0.10088300220750554)],\n"," [(0, 0.054022988505747126),\n","  (1, 0.020634920634920634),\n","  (2, 0.03760262725779967),\n","  (3, 0.022058018609742748),\n","  (4, 0.0064039408866995075),\n","  (5, 0.015489874110563765),\n","  (6, 0.008264915161466886),\n","  (7, 0.006075533661740558),\n","  (8, 0.14444444444444443),\n","  (9, 0.007936507936507936),\n","  (10, 0.027640941434044882),\n","  (11, 0.004105090311986864),\n","  (12, 0.0518336070060208),\n","  (13, 0.009906951286261631),\n","  (14, 0.2584017515051998),\n","  (15, 0.013300492610837438),\n","  (16, 0.25161466885604816),\n","  (17, 0.017569786535303777),\n","  (18, 0.03169129720853859),\n","  (19, 0.011001642036124795)],\n"," [(0, 0.5411355212006149),\n","  (1, 0.07987523732031464),\n","  (2, 0.017403489738721637),\n","  (3, 0.027438748756893592),\n","  (4, 0.08367236235421753),\n","  (5, 0.003932736642256578),\n","  (6, 0.037112376819455756),\n","  (7, 0.00745863845945213),\n","  (8, 0.014239218877135884),\n","  (9, 0.01586655817737999),\n","  (10, 0.01939245999457554),\n","  (11, 0.016680227827502038),\n","  (12, 0.01713226652201429),\n","  (13, 0.008543531326281531),\n","  (14, 0.05410903173311636),\n","  (15, 0.024636108850917642),\n","  (16, 0.017313081999819187),\n","  (17, 0.004294367597866378),\n","  (18, 0.003932736642256578),\n","  (19, 0.005831299159208029)],\n"," [(0, 0.011232289336316183),\n","  (1, 0.004893736017897092),\n","  (2, 0.03714578672632364),\n","  (3, 0.021206189410887397),\n","  (4, 0.03127330350484713),\n","  (5, 0.003961595824011932),\n","  (6, 0.01738441461595824),\n","  (7, 0.011418717375093214),\n","  (8, 0.0038683818046234154),\n","  (9, 0.004334451901565996),\n","  (10, 0.011418717375093214),\n","  (11, 0.0036819537658463832),\n","  (12, 0.006664802386278896),\n","  (13, 0.47944630872483224),\n","  (14, 0.2119220730797912),\n","  (15, 0.05075503355704698),\n","  (16, 0.023536539895600297),\n","  (17, 0.004893736017897092),\n","  (18, 0.004986950037285608),\n","  (19, 0.05597501864280388)],\n"," [(0, 0.010483364720652859),\n","  (1, 0.005210295040803516),\n","  (2, 0.03182674199623353),\n","  (3, 0.016007532956685503),\n","  (4, 0.01086001255492781),\n","  (5, 0.006214689265536724),\n","  (6, 0.013622096672944133),\n","  (7, 0.11619585687382299),\n","  (8, 0.006089139987445073),\n","  (9, 0.011111111111111113),\n","  (10, 0.05216572504708099),\n","  (11, 0.006214689265536724),\n","  (12, 0.011613308223477717),\n","  (13, 0.040364092906465796),\n","  (14, 0.015379786566227245),\n","  (15, 0.6015693659761457),\n","  (16, 0.01324544883866918),\n","  (17, 0.012241054613935971),\n","  (18, 0.00822347771500314),\n","  (19, 0.011362209667294414)],\n"," [(0, 0.004929985914325959),\n","  (1, 0.006172839506172839),\n","  (2, 0.09383544618443947),\n","  (3, 0.03467561521252797),\n","  (4, 0.011061396967437236),\n","  (5, 0.004432844477587207),\n","  (6, 0.058952688706603695),\n","  (7, 0.0036042754163559533),\n","  (8, 0.0036871323224790787),\n","  (9, 0.004598558289833458),\n","  (10, 0.0070842654735272185),\n","  (11, 0.009321401938851604),\n","  (12, 0.0038528461347253295),\n","  (13, 0.11015825669069516),\n","  (14, 0.06790123456790123),\n","  (15, 0.5554312701963708),\n","  (16, 0.0050128428204490845),\n","  (17, 0.004101416853094705),\n","  (18, 0.004267130665340956),\n","  (19, 0.006918551661280968)],\n"," [(0, 0.017573179378101895),\n","  (1, 0.007039400384888078),\n","  (2, 0.013420439582700294),\n","  (3, 0.009470272460245113),\n","  (4, 0.01493973462979844),\n","  (5, 0.008862554441405854),\n","  (6, 0.01564873898511091),\n","  (7, 0.09282892737769675),\n","  (8, 0.0063303960295756105),\n","  (9, 0.06608933454876938),\n","  (10, 0.01544616631216449),\n","  (11, 0.005114959991897093),\n","  (12, 0.02253620986528917),\n","  (13, 0.0383368783551099),\n","  (14, 0.014534589283905601),\n","  (15, 0.5919679935176745),\n","  (16, 0.021016914818191026),\n","  (17, 0.016560316013369797),\n","  (18, 0.005621391674263142),\n","  (19, 0.016661602349843006)],\n"," [(0, 0.005536626916524702),\n","  (1, 0.015852735188339958),\n","  (2, 0.05550823395797842),\n","  (3, 0.16576755631270113),\n","  (4, 0.003738406208593602),\n","  (5, 0.006199129282604581),\n","  (6, 0.059388604959303426),\n","  (7, 0.007808063600227144),\n","  (8, 0.004022335794056408),\n","  (9, 0.0033598334279765284),\n","  (10, 0.00742949081961007),\n","  (11, 0.007618777209918607),\n","  (12, 0.007145561234147265),\n","  (13, 0.010742002650009465),\n","  (14, 0.004306265379519212),\n","  (15, 0.5934601552148401),\n","  (16, 0.006861631648684459),\n","  (17, 0.007145561234147265),\n","  (18, 0.0041169789892106755),\n","  (19, 0.023992049971607043)],\n"," [(0, 0.016255385820603215),\n","  (1, 0.010510510510510513),\n","  (2, 0.012338425381903644),\n","  (3, 0.009988249118683902),\n","  (4, 0.014427470949210082),\n","  (5, 0.00868259563911738),\n","  (6, 0.01155503329416373),\n","  (7, 0.14864864864864868),\n","  (8, 0.006985246115680899),\n","  (9, 0.0046350698524611576),\n","  (10, 0.013905209557383473),\n","  (11, 0.005026765896331114),\n","  (12, 0.04680767724245986),\n","  (13, 0.06848152500326415),\n","  (14, 0.016385951168559867),\n","  (15, 0.5460895678286983),\n","  (16, 0.016385951168559867),\n","  (17, 0.012599556077816949),\n","  (18, 0.007507507507507508),\n","  (19, 0.02278365321843583)],\n"," [(0, 0.01856255780155899),\n","  (1, 0.01895891134892324),\n","  (2, 0.013806315233188003),\n","  (3, 0.025168450257629805),\n","  (4, 0.008785836966574184),\n","  (5, 0.009446426212181266),\n","  (6, 0.05542343770643414),\n","  (7, 0.01499537587528075),\n","  (8, 0.005482890738538777),\n","  (9, 0.159796538512353),\n","  (10, 0.06506804069229753),\n","  (11, 0.010635486854274012),\n","  (12, 0.04181529924692826),\n","  (13, 0.05317743427137006),\n","  (14, 0.01433478662967367),\n","  (15, 0.3388162240718721),\n","  (16, 0.007332540626238605),\n","  (17, 0.1110450521865504),\n","  (18, 0.01671290791385916),\n","  (19, 0.010635486854274012)],\n"," [(0, 0.08091016548463358),\n","  (1, 0.012588652482269504),\n","  (2, 0.06164302600472813),\n","  (3, 0.020035460992907803),\n","  (4, 0.011879432624113475),\n","  (5, 0.05797872340425532),\n","  (6, 0.07570921985815603),\n","  (7, 0.05856973995271868),\n","  (8, 0.0874113475177305),\n","  (9, 0.030555555555555555),\n","  (10, 0.01826241134751773),\n","  (11, 0.041075650118203307),\n","  (12, 0.021217494089834515),\n","  (13, 0.051122931442080376),\n","  (14, 0.007387706855791962),\n","  (15, 0.1225177304964539),\n","  (16, 0.03835697399527187),\n","  (17, 0.1803191489361702),\n","  (18, 0.006323877068557919),\n","  (19, 0.016134751773049644)],\n"," [(0, 0.029530600118835414),\n","  (1, 0.008021390374331552),\n","  (2, 0.013606654783125371),\n","  (3, 0.01824123588829471),\n","  (4, 0.006595365418894831),\n","  (5, 0.006595365418894831),\n","  (6, 0.10320855614973262),\n","  (7, 0.01253713606654783),\n","  (8, 0.004931669637551991),\n","  (9, 0.11271538918597743),\n","  (10, 0.03202614379084967),\n","  (11, 0.011705288175876412),\n","  (12, 0.017290552584670233),\n","  (13, 0.02751039809863339),\n","  (14, 0.00516934046345811),\n","  (15, 0.5261437908496732),\n","  (16, 0.018835412953060012),\n","  (17, 0.00825906120023767),\n","  (18, 0.010398098633392751),\n","  (19, 0.026678550207961973)],\n"," [(0, 0.010363429869392392),\n","  (1, 0.027588491387469244),\n","  (2, 0.25520537573348484),\n","  (3, 0.0307117168275601),\n","  (4, 0.05607609312890404),\n","  (5, 0.010647359454855197),\n","  (6, 0.00989021389362105),\n","  (7, 0.004874124550444824),\n","  (8, 0.0044955517698277506),\n","  (9, 0.007997349990535682),\n","  (10, 0.08995835699413214),\n","  (11, 0.04008139314783268),\n","  (12, 0.0032651902328222604),\n","  (13, 0.005252697331061897),\n","  (14, 0.017461669505962524),\n","  (15, 0.3286484951731971),\n","  (16, 0.005441983721370434),\n","  (17, 0.010836645845163735),\n","  (18, 0.015095589627105813),\n","  (19, 0.0661082718152565)],\n"," [(0, 0.019864341085271322),\n","  (1, 0.005221791559000862),\n","  (2, 0.043658484065460815),\n","  (3, 0.045488802756244624),\n","  (4, 0.011035745047372956),\n","  (5, 0.009097760551248925),\n","  (6, 0.11105727820844101),\n","  (7, 0.00812876830318691),\n","  (8, 0.007590439276485789),\n","  (9, 0.006729112833763998),\n","  (10, 0.07229758828596039),\n","  (11, 0.006944444444444445),\n","  (12, 0.015880706287683035),\n","  (13, 0.05657838070628769),\n","  (14, 0.020187338501291993),\n","  (15, 0.4650624461670974),\n","  (16, 0.015880706287683035),\n","  (17, 0.04635012919896642),\n","  (18, 0.009636089577950044),\n","  (19, 0.023309646856158488)],\n"," [(0, 0.015864950316169833),\n","  (1, 0.02930216802168022),\n","  (2, 0.08734191508581754),\n","  (3, 0.09513324299909667),\n","  (4, 0.016881210478771457),\n","  (5, 0.04669150858175249),\n","  (6, 0.1401874435411021),\n","  (7, 0.00965447154471545),\n","  (8, 0.0116869918699187),\n","  (9, 0.012929087624209577),\n","  (10, 0.2008242999096658),\n","  (11, 0.06125790424570913),\n","  (12, 0.007170280036133695),\n","  (13, 0.18851626016260165),\n","  (14, 0.008864046973803073),\n","  (15, 0.017558717253839208),\n","  (16, 0.009541553748870824),\n","  (17, 0.007170280036133695),\n","  (18, 0.01801038843721771),\n","  (19, 0.01541327913279133)],\n"," [(0, 0.009747637842363356),\n","  (1, 0.007953594067695254),\n","  (2, 0.015368975002990075),\n","  (3, 0.23304628632938645),\n","  (4, 0.004365506518359049),\n","  (5, 0.02972132520033489),\n","  (6, 0.005681138619782323),\n","  (7, 0.004604712354981462),\n","  (8, 0.5153091735438345),\n","  (9, 0.00544193278315991),\n","  (10, 0.010824064107164216),\n","  (11, 0.0047243152732926695),\n","  (12, 0.010943667025475423),\n","  (13, 0.06930989116134435),\n","  (14, 0.006996770721205598),\n","  (15, 0.011900490371965078),\n","  (16, 0.0049635211099150825),\n","  (17, 0.015608180839612488),\n","  (18, 0.016923812941035763),\n","  (19, 0.016565004186102145)],\n"," [(0, 0.011311712916527357),\n","  (1, 0.009640031204725286),\n","  (2, 0.017106876184107875),\n","  (3, 0.36894015379471745),\n","  (4, 0.007522567703109326),\n","  (5, 0.01666109439429399),\n","  (6, 0.08954641702886436),\n","  (7, 0.03493814777666332),\n","  (8, 0.008748467625097513),\n","  (9, 0.005850885991307254),\n","  (10, 0.018221330658642593),\n","  (11, 0.10358854340800175),\n","  (12, 0.00551654964894684),\n","  (13, 0.09077231695085254),\n","  (14, 0.00785690404546974),\n","  (15, 0.020004457817898134),\n","  (16, 0.008079794940376684),\n","  (17, 0.00551654964894684),\n","  (18, 0.11473308815334891),\n","  (19, 0.05544411010810207)],\n"," [(0, 0.007462686567164177),\n","  (1, 0.010194127402204661),\n","  (2, 0.043654277631450585),\n","  (3, 0.015949663447468537),\n","  (4, 0.00951126719344454),\n","  (5, 0.008145546775924299),\n","  (6, 0.03321627158326016),\n","  (7, 0.044044483465027796),\n","  (8, 0.004633694273729391),\n","  (9, 0.04014242512925568),\n","  (10, 0.006584723441615451),\n","  (11, 0.45317529997073447),\n","  (12, 0.006487171983221148),\n","  (13, 0.17622670958930833),\n","  (14, 0.015364354697102718),\n","  (15, 0.03692322700224367),\n","  (16, 0.021217442200760897),\n","  (17, 0.050873085552629),\n","  (18, 0.004828797190517997),\n","  (19, 0.011364744902936296)],\n"," [(0, 0.07222222222222222),\n","  (1, 0.009420289855072464),\n","  (2, 0.016022544283413848),\n","  (3, 0.06239935587761675),\n","  (4, 0.01038647342995169),\n","  (5, 0.029871175523349437),\n","  (6, 0.009742351046698872),\n","  (7, 0.008454106280193236),\n","  (8, 0.009098228663446055),\n","  (9, 0.008293075684380032),\n","  (10, 0.011191626409017713),\n","  (11, 0.3250402576489533),\n","  (12, 0.0572463768115942),\n","  (13, 0.11006441223832528),\n","  (14, 0.02890499194847021),\n","  (15, 0.007809983896940419),\n","  (16, 0.013123993558776168),\n","  (17, 0.013607085346215781),\n","  (18, 0.1888083735909823),\n","  (19, 0.008293075684380032)],\n"," [(0, 0.005850456621004566),\n","  (1, 0.1773687214611872),\n","  (2, 0.017265981735159818),\n","  (3, 0.027064307458143075),\n","  (4, 0.004708904109589041),\n","  (5, 0.3977834855403349),\n","  (6, 0.011748477929984779),\n","  (7, 0.008704337899543379),\n","  (8, 0.04818302891933029),\n","  (9, 0.006326103500761035),\n","  (10, 0.00670662100456621),\n","  (11, 0.0707286910197869),\n","  (12, 0.022307838660578387),\n","  (13, 0.12628424657534246),\n","  (14, 0.012509512937595129),\n","  (15, 0.005089421613394216),\n","  (16, 0.023734779299847793),\n","  (17, 0.015839041095890412),\n","  (18, 0.005374809741248097),\n","  (19, 0.006421232876712328)],\n"," [(0, 0.010668633235004919),\n","  (1, 0.015093411996066865),\n","  (2, 0.06740412979351033),\n","  (3, 0.007030481809242872),\n","  (4, 0.0084070796460177),\n","  (5, 0.013421828908554574),\n","  (6, 0.03652900688298919),\n","  (7, 0.011061946902654869),\n","  (8, 0.022468043264503443),\n","  (9, 0.00968534906588004),\n","  (10, 0.02138643067846608),\n","  (11, 0.2934611602753196),\n","  (12, 0.04183874139626353),\n","  (13, 0.12817109144542776),\n","  (14, 0.03761061946902656),\n","  (15, 0.017158308751229107),\n","  (16, 0.009095378564405114),\n","  (17, 0.04931170108161259),\n","  (18, 0.011061946902654869),\n","  (19, 0.18913470993117013)],\n"," [(0, 0.15246701090074588),\n","  (1, 0.007028112449799198),\n","  (2, 0.012574105947599928),\n","  (3, 0.0037770128131573923),\n","  (4, 0.009131765155861542),\n","  (5, 0.005880665519219738),\n","  (6, 0.006071906674316315),\n","  (7, 0.014390896921017407),\n","  (8, 0.008653662268120102),\n","  (9, 0.007601835915088929),\n","  (10, 0.009609868043602985),\n","  (11, 0.2652036718301779),\n","  (12, 0.05550774526678143),\n","  (13, 0.016781411359724618),\n","  (14, 0.006263147829412891),\n","  (15, 0.005785044941671449),\n","  (16, 0.04613692866704915),\n","  (17, 0.0177376171352075),\n","  (18, 0.004446356855995411),\n","  (19, 0.3449512335054504)],\n"," [(0, 0.018273564569860866),\n","  (1, 0.21051342347638644),\n","  (2, 0.010631001371742112),\n","  (3, 0.10841661767587693),\n","  (4, 0.005829903978052126),\n","  (5, 0.35679992161473645),\n","  (6, 0.004066235547717029),\n","  (7, 0.004654125024495395),\n","  (8, 0.008965314520870077),\n","  (9, 0.010435038212815991),\n","  (10, 0.03159905937683716),\n","  (11, 0.1340877914951989),\n","  (12, 0.00622183029590437),\n","  (13, 0.006711738193219675),\n","  (14, 0.01856750930825005),\n","  (15, 0.008671369782480894),\n","  (16, 0.007593572408387223),\n","  (17, 0.009651185577111502),\n","  (18, 0.007887517146776405),\n","  (19, 0.03042328042328042)],\n"," [(0, 0.011172161172161174),\n","  (1, 0.23302808302808306),\n","  (2, 0.009340659340659342),\n","  (3, 0.1145909645909646),\n","  (4, 0.010073260073260076),\n","  (5, 0.24768009768009772),\n","  (6, 0.01043956043956044),\n","  (7, 0.02655677655677656),\n","  (8, 0.05305250305250306),\n","  (9, 0.013492063492063494),\n","  (10, 0.011782661782661784),\n","  (11, 0.1217948717948718),\n","  (12, 0.055128205128205134),\n","  (13, 0.011416361416361417),\n","  (14, 0.006043956043956045),\n","  (15, 0.005555555555555557),\n","  (16, 0.008852258852258854),\n","  (17, 0.0053113553113553124),\n","  (18, 0.02374847374847375),\n","  (19, 0.020940170940170942)],\n"," [(0, 0.04865252926883146),\n","  (1, 0.19157278550916723),\n","  (2, 0.13645902363596202),\n","  (3, 0.004804506295559973),\n","  (4, 0.009001546277888225),\n","  (5, 0.044455489286503204),\n","  (6, 0.049094322951181796),\n","  (7, 0.02700463883366468),\n","  (8, 0.00590899050143583),\n","  (9, 0.014303070466092334),\n","  (10, 0.0191628009719461),\n","  (11, 0.303567483984979),\n","  (12, 0.014303070466092334),\n","  (13, 0.01894190413077093),\n","  (14, 0.008449304174950299),\n","  (15, 0.01772697150430749),\n","  (16, 0.028550916721890876),\n","  (17, 0.01518665783079302),\n","  (18, 0.013750828363154407),\n","  (19, 0.029103158824828806)],\n"," [(0, 0.07242063492063494),\n","  (1, 0.031804388422035486),\n","  (2, 0.015114379084967322),\n","  (3, 0.14559990662931843),\n","  (4, 0.014997665732959853),\n","  (5, 0.19426937441643327),\n","  (6, 0.00554388422035481),\n","  (7, 0.02573529411764706),\n","  (8, 0.022350606909430443),\n","  (9, 0.02398459383753502),\n","  (10, 0.014997665732959853),\n","  (11, 0.07989028944911299),\n","  (12, 0.11058590102707751),\n","  (13, 0.007994864612511672),\n","  (14, 0.04219187675070029),\n","  (15, 0.004610177404295052),\n","  (16, 0.12669234360410833),\n","  (17, 0.017448646125116717),\n","  (18, 0.014530812324929974),\n","  (19, 0.029236694677871152)],\n"," [(0, 0.01666666666666667),\n","  (1, 0.04477124183006537),\n","  (2, 0.021521942110177408),\n","  (3, 0.04878618113912232),\n","  (4, 0.015732959850606913),\n","  (5, 0.11741363211951449),\n","  (6, 0.02217553688141924),\n","  (7, 0.008636788048552757),\n","  (8, 0.00919701213818861),\n","  (9, 0.007609710550887022),\n","  (10, 0.056816059757236234),\n","  (11, 0.38669467787114853),\n","  (12, 0.006022408963585435),\n","  (13, 0.0037815126050420172),\n","  (14, 0.006395891690009338),\n","  (15, 0.004901960784313726),\n","  (16, 0.026844070961718022),\n","  (17, 0.010037348272642393),\n","  (18, 0.16643323996265175),\n","  (19, 0.019561157796451917)],\n"," [(0, 0.02098081460962347),\n","  (1, 0.029393659587565398),\n","  (2, 0.03390786908792448),\n","  (3, 0.32527957320201084),\n","  (4, 0.012670565302144245),\n","  (5, 0.11803631886734377),\n","  (6, 0.005591464040217502),\n","  (7, 0.054837385862316596),\n","  (8, 0.05165692007797269),\n","  (9, 0.017389966143428744),\n","  (10, 0.012362778290756127),\n","  (11, 0.011849799938442594),\n","  (12, 0.011029034574740944),\n","  (13, 0.00589925105160562),\n","  (14, 0.005488868369754795),\n","  (15, 0.004360315994665024),\n","  (16, 0.12562839848158405),\n","  (17, 0.006309633733456446),\n","  (18, 0.13537498717554117),\n","  (19, 0.0119523956089053)],\n"," [(0, 0.004942241276646422),\n","  (1, 0.10509705847326427),\n","  (2, 0.005656782184113375),\n","  (3, 0.02173395260211981),\n","  (4, 0.006133142789091343),\n","  (5, 0.2858759080624033),\n","  (6, 0.005656782184113375),\n","  (7, 0.009229486721448138),\n","  (8, 0.350660950339407),\n","  (9, 0.003751339764201501),\n","  (10, 0.03316660712159105),\n","  (11, 0.006609503394069311),\n","  (12, 0.0048231511254019305),\n","  (13, 0.0048231511254019305),\n","  (14, 0.004942241276646422),\n","  (15, 0.003751339764201501),\n","  (16, 0.004346790520423961),\n","  (17, 0.007324044301536264),\n","  (18, 0.12689055615100633),\n","  (19, 0.004584970822912946)],\n"," [(0, 0.013292433537832313),\n","  (1, 0.21915473755964557),\n","  (2, 0.007644366540072062),\n","  (3, 0.07688187749537444),\n","  (4, 0.006573181419807187),\n","  (5, 0.08272470542409194),\n","  (6, 0.0034570065244911873),\n","  (7, 0.005988898626935438),\n","  (8, 0.30504430811179284),\n","  (9, 0.007741747005550688),\n","  (10, 0.1417372675041387),\n","  (11, 0.030626156393027563),\n","  (12, 0.0048203330411919374),\n","  (13, 0.009591975849644564),\n","  (14, 0.004333430713798813),\n","  (15, 0.0048203330411919374),\n","  (16, 0.025464991722660438),\n","  (17, 0.0039439088518843125),\n","  (18, 0.04065634433732594),\n","  (19, 0.005501996299542312)],\n"," [(0, 0.011700336700336702),\n","  (1, 0.06776094276094277),\n","  (2, 0.007323232323232324),\n","  (3, 0.09789562289562291),\n","  (4, 0.010353535353535356),\n","  (5, 0.034090909090909095),\n","  (6, 0.007491582491582493),\n","  (7, 0.01085858585858586),\n","  (8, 0.009680134680134681),\n","  (9, 0.018771043771043775),\n","  (10, 0.01388888888888889),\n","  (11, 0.1591750841750842),\n","  (12, 0.008501683501683503),\n","  (13, 0.005639730639730641),\n","  (14, 0.00883838383838384),\n","  (15, 0.006313131313131314),\n","  (16, 0.009006734006734008),\n","  (17, 0.009175084175084176),\n","  (18, 0.4790404040404041),\n","  (19, 0.0244949494949495)],\n"," [(0, 0.010379169666426685),\n","  (1, 0.14872810175185985),\n","  (2, 0.01001919846412287),\n","  (3, 0.12568994480441564),\n","  (4, 0.008579313654907607),\n","  (5, 0.04385649148068155),\n","  (6, 0.00833933285337173),\n","  (7, 0.007619390448764099),\n","  (8, 0.08249340052795777),\n","  (9, 0.0073794096472282215),\n","  (10, 0.028257739380849533),\n","  (11, 0.09761219102471802),\n","  (12, 0.010139188864890809),\n","  (13, 0.018058555315574754),\n","  (14, 0.005339572834173266),\n","  (15, 0.007019438444924406),\n","  (16, 0.017218622510199184),\n","  (17, 0.005939524838012959),\n","  (18, 0.3469522438204944),\n","  (19, 0.010379169666426685)],\n"," [(0, 0.007723447320762757),\n","  (1, 0.04618088846276766),\n","  (2, 0.03712581229359753),\n","  (3, 0.027218493661446682),\n","  (4, 0.056940449557899224),\n","  (5, 0.009427932246724193),\n","  (6, 0.012943432406519654),\n","  (7, 0.04309150953446256),\n","  (8, 0.008256098860125705),\n","  (9, 0.016139341642697348),\n","  (10, 0.37599872163630554),\n","  (11, 0.00676467454990945),\n","  (12, 0.14482795355278577),\n","  (13, 0.098806860551827),\n","  (14, 0.04234579737935443),\n","  (15, 0.006871204857782039),\n","  (16, 0.013689144561627783),\n","  (17, 0.009747523170341962),\n","  (18, 0.018163417492276554),\n","  (19, 0.017737296260786194)],\n"," [(0, 0.010643533899347853),\n","  (1, 0.2228989787129322),\n","  (2, 0.012120093515442352),\n","  (3, 0.035375907468930726),\n","  (4, 0.05075673680324843),\n","  (5, 0.01113572043804602),\n","  (6, 0.046696197858988556),\n","  (7, 0.005106435338993479),\n","  (8, 0.013596653131536853),\n","  (9, 0.016672818998400393),\n","  (10, 0.042143472376030514),\n","  (11, 0.052233296419342934),\n","  (12, 0.05395594930478651),\n","  (13, 0.005844715147040728),\n","  (14, 0.030330995447274516),\n","  (15, 0.00904392764857881),\n","  (16, 0.008797834379229728),\n","  (17, 0.013965793035560477),\n","  (18, 0.29365079365079366),\n","  (19, 0.06503014642549526)],\n"," [(0, 0.003591380686352753),\n","  (1, 0.21845348940321008),\n","  (2, 0.10796311075640684),\n","  (3, 0.004921521681298217),\n","  (4, 0.006162986609913984),\n","  (5, 0.009798705329431586),\n","  (6, 0.049348230912476726),\n","  (7, 0.00483284561496852),\n","  (8, 0.0040347610180012415),\n","  (9, 0.0031480003547042653),\n","  (10, 0.07462090981644054),\n","  (11, 0.21871951760219915),\n","  (12, 0.0028819721557151725),\n","  (13, 0.21934025006650704),\n","  (14, 0.004212113150660637),\n","  (15, 0.030282876651591736),\n","  (16, 0.003680056752682451),\n","  (17, 0.00820253613549703),\n","  (18, 0.0168041145694777),\n","  (19, 0.009000620732464307)],\n"," [(0, 0.0053780284043441935),\n","  (1, 0.5715852130325815),\n","  (2, 0.005691311612364244),\n","  (3, 0.006526733500417711),\n","  (4, 0.005064745196324143),\n","  (5, 0.00464703425229741),\n","  (6, 0.0062134502923976605),\n","  (7, 0.007466583124477861),\n","  (8, 0.012896825396825396),\n","  (9, 0.011121553884711779),\n","  (10, 0.18144319131161235),\n","  (11, 0.005064745196324143),\n","  (12, 0.11011904761904762),\n","  (13, 0.005795739348370927),\n","  (14, 0.004751461988304093),\n","  (15, 0.004124895572263993),\n","  (16, 0.008928571428571428),\n","  (17, 0.013523391812865496),\n","  (18, 0.007571010860484544),\n","  (19, 0.022086466165413533)],\n"," [(0, 0.0070934059698104625),\n","  (1, 0.009363295880149811),\n","  (2, 0.01594597662013392),\n","  (3, 0.011065713312904322),\n","  (4, 0.014697537169447277),\n","  (5, 0.014357053682896376),\n","  (6, 0.08381568493928043),\n","  (7, 0.011406196799455225),\n","  (8, 0.006525933492225625),\n","  (9, 0.012541141754624898),\n","  (10, 0.053285665645216196),\n","  (11, 0.13352627397571215),\n","  (12, 0.02638747020769492),\n","  (13, 0.0337646124162978),\n","  (14, 0.010611735330836453),\n","  (15, 0.0076608784473953),\n","  (16, 0.08483713539893314),\n","  (17, 0.08858245375099306),\n","  (18, 0.343037112700034),\n","  (19, 0.03149472250595845)],\n"," [(0, 0.016785350966429302),\n","  (1, 0.09251723748163221),\n","  (2, 0.022323951622018768),\n","  (3, 0.006160280321012773),\n","  (4, 0.446422516107155),\n","  (5, 0.004577822990844355),\n","  (6, 0.009099129648468408),\n","  (7, 0.0751102068497796),\n","  (8, 0.007403639651859389),\n","  (9, 0.01249010964168645),\n","  (10, 0.03125353227082628),\n","  (11, 0.0035605289928789426),\n","  (12, 0.08223126483553748),\n","  (13, 0.10981123544704421),\n","  (14, 0.03147959760370748),\n","  (15, 0.013055272973889457),\n","  (16, 0.012942240307448855),\n","  (17, 0.007290606985418787),\n","  (18, 0.004690855657284956),\n","  (19, 0.010794619645077429)],\n"," [(0, 0.005603759942154737),\n","  (1, 0.18444203422511452),\n","  (2, 0.008134490238611715),\n","  (3, 0.008857556037599423),\n","  (4, 0.27301759460110875),\n","  (5, 0.02584960231381056),\n","  (6, 0.0066883586406362985),\n","  (7, 0.00946011087008918),\n","  (8, 0.004760183176669078),\n","  (9, 0.008857556037599423),\n","  (10, 0.13238129669799956),\n","  (11, 0.07284887924801159),\n","  (12, 0.011629308267052304),\n","  (13, 0.185647143890094),\n","  (14, 0.007893468305615813),\n","  (15, 0.00584478187515064),\n","  (16, 0.007772957339117861),\n","  (17, 0.02307785008435768),\n","  (18, 0.0066883586406362985),\n","  (19, 0.010544709568570742)],\n"," [(0, 0.0082767655114922),\n","  (1, 0.06377277599142551),\n","  (2, 0.14308681672025725),\n","  (3, 0.00482315112540193),\n","  (4, 0.39007979040133384),\n","  (5, 0.07865904489698702),\n","  (6, 0.01232583065380493),\n","  (7, 0.04376563058235084),\n","  (8, 0.0368584018101703),\n","  (9, 0.010896748838871026),\n","  (10, 0.029117541979278315),\n","  (11, 0.00839585566273669),\n","  (12, 0.00887221626771466),\n","  (13, 0.05305466237942122),\n","  (14, 0.011254019292604502),\n","  (15, 0.005894962486602358),\n","  (16, 0.028283910920566868),\n","  (17, 0.027212099559366442),\n","  (18, 0.010777658687626533),\n","  (19, 0.024592116231987614)],\n"," [(0, 0.009242957746478873),\n","  (1, 0.015796165884194052),\n","  (2, 0.04787754303599374),\n","  (3, 0.005330594679186228),\n","  (4, 0.6734643974960877),\n","  (5, 0.004645931142410016),\n","  (6, 0.007873630672926447),\n","  (7, 0.0048415492957746475),\n","  (8, 0.0038634585289514865),\n","  (9, 0.009047339593114241),\n","  (10, 0.10323748043818466),\n","  (11, 0.0056240219092331765),\n","  (12, 0.010905712050078247),\n","  (13, 0.03076095461658842),\n","  (14, 0.012666275430359938),\n","  (15, 0.0044503129890453835),\n","  (16, 0.004939358372456964),\n","  (17, 0.005428403755868545),\n","  (18, 0.006113067292644757),\n","  (19, 0.03389084507042254)],\n"," [(0, 0.015037120926135645),\n","  (1, 0.007235434755253553),\n","  (2, 0.01956713225116396),\n","  (3, 0.021706304265760663),\n","  (4, 0.35038379262614816),\n","  (5, 0.008745438530262991),\n","  (6, 0.009626274065685163),\n","  (7, 0.006732100163583741),\n","  (8, 0.008619604882345538),\n","  (9, 0.0071096011073361),\n","  (10, 0.25437271926513144),\n","  (11, 0.006102931923996475),\n","  (12, 0.11369070089341887),\n","  (13, 0.00950044041776771),\n","  (14, 0.030388825972064923),\n","  (15, 0.016547124701145083),\n","  (16, 0.05102554423052723),\n","  (17, 0.013652950799043662),\n","  (18, 0.02007046684283377),\n","  (19, 0.02988549138039511)],\n"," [(0, 0.006216556499842619),\n","  (1, 0.026990871891721755),\n","  (2, 0.016603714195782188),\n","  (3, 0.02683349071451055),\n","  (4, 0.2540919106074914),\n","  (5, 0.027620396600566574),\n","  (6, 0.03061063896757948),\n","  (7, 0.009993704752911553),\n","  (8, 0.04162732137236387),\n","  (9, 0.009678942398489142),\n","  (10, 0.2740793201133145),\n","  (11, 0.03485993075228203),\n","  (12, 0.008577274158010703),\n","  (13, 0.015502045955303747),\n","  (14, 0.019121813031161478),\n","  (15, 0.008105130626377088),\n","  (16, 0.022741580107019203),\n","  (17, 0.09041548630783759),\n","  (18, 0.01188227887944602),\n","  (19, 0.06444759206798868)],\n"," [(0, 0.06730192976147668),\n","  (1, 0.008090614886731391),\n","  (2, 0.014922689679971233),\n","  (3, 0.014203523912261776),\n","  (4, 0.26579168164928685),\n","  (5, 0.004494786048184107),\n","  (6, 0.012046026609133405),\n","  (7, 0.041172240201366415),\n","  (8, 0.009768668344720125),\n","  (9, 0.03302169483399257),\n","  (10, 0.23846338247632745),\n","  (11, 0.005573534699748292),\n","  (12, 0.07784969435454872),\n","  (13, 0.0498022294138799),\n","  (14, 0.024751288505333813),\n","  (15, 0.05807263574253865),\n","  (16, 0.04956250749131008),\n","  (17, 0.005333812777178473),\n","  (18, 0.009648807383435215),\n","  (19, 0.010128251228574853)],\n"," [(0, 0.023707117255504357),\n","  (1, 0.01080389144905274),\n","  (2, 0.013364055299539173),\n","  (3, 0.004966717869943678),\n","  (4, 0.2895545314900154),\n","  (5, 0.006605222734254994),\n","  (6, 0.005478750640040964),\n","  (7, 0.03599590373783923),\n","  (8, 0.005376344086021507),\n","  (9, 0.022990271377368154),\n","  (10, 0.13102918586789558),\n","  (11, 0.006195596518177165),\n","  (12, 0.07194060419866873),\n","  (13, 0.22882744495647725),\n","  (14, 0.07460317460317462),\n","  (15, 0.006400409626216079),\n","  (16, 0.01490015360983103),\n","  (17, 0.022273425499231954),\n","  (18, 0.012032770097286228),\n","  (19, 0.012954429083461344)],\n"," [(0, 0.00628209330274227),\n","  (1, 0.1252594777668524),\n","  (2, 0.06910302633016498),\n","  (3, 0.009996722386102917),\n","  (4, 0.00475254015077024),\n","  (5, 0.12536873156342182),\n","  (6, 0.010215229979241779),\n","  (7, 0.00486179394733967),\n","  (8, 0.17966786845842894),\n","  (9, 0.0067191084890199934),\n","  (10, 0.3360100513492844),\n","  (11, 0.0067191084890199934),\n","  (12, 0.005845078116464547),\n","  (13, 0.03119195892057249),\n","  (14, 0.010652245165519502),\n","  (15, 0.008139407844422594),\n","  (16, 0.007046869878728286),\n","  (17, 0.007920900251283731),\n","  (18, 0.03392330383480826),\n","  (19, 0.01032448377581121)],\n"," [(0, 0.0075),\n","  (1, 0.01338888888888889),\n","  (2, 0.05294444444444445),\n","  (3, 0.03038888888888889),\n","  (4, 0.012388888888888889),\n","  (5, 0.06616666666666667),\n","  (6, 0.007722222222222222),\n","  (7, 0.007722222222222222),\n","  (8, 0.4290555555555556),\n","  (9, 0.01361111111111111),\n","  (10, 0.13916666666666666),\n","  (11, 0.005833333333333334),\n","  (12, 0.009833333333333333),\n","  (13, 0.13772222222222222),\n","  (14, 0.014388888888888889),\n","  (15, 0.005722222222222222),\n","  (16, 0.01661111111111111),\n","  (17, 0.016944444444444446),\n","  (18, 0.008722222222222221),\n","  (19, 0.004166666666666667)],\n"," [(0, 0.017078681552365766),\n","  (1, 0.02664805954279639),\n","  (2, 0.14028442317916004),\n","  (3, 0.028375863902179697),\n","  (4, 0.006047315257841575),\n","  (5, 0.01614832535885168),\n","  (6, 0.0358187134502924),\n","  (7, 0.058678894205210004),\n","  (8, 0.03076820839978735),\n","  (9, 0.006578947368421053),\n","  (10, 0.012161084529505584),\n","  (11, 0.06465975544922915),\n","  (12, 0.022926634768740035),\n","  (13, 0.033825093035619355),\n","  (14, 0.2261430090377459),\n","  (15, 0.01455342902711324),\n","  (16, 0.024920255183413082),\n","  (17, 0.16660021265284425),\n","  (18, 0.01402179691653376),\n","  (19, 0.05376129718234982)],\n"," [(0, 0.009947447447447447),\n","  (1, 0.02170920920920921),\n","  (2, 0.24155405405405406),\n","  (3, 0.0074449449449449446),\n","  (4, 0.009071571571571572),\n","  (5, 0.01732982982982983),\n","  (6, 0.2202827827827828),\n","  (7, 0.010948448448448448),\n","  (8, 0.027464964964964964),\n","  (9, 0.00857107107107107),\n","  (10, 0.012575075075075074),\n","  (11, 0.013075575575575576),\n","  (12, 0.005943443443443443),\n","  (13, 0.08314564564564565),\n","  (14, 0.11292542542542543),\n","  (15, 0.10078828828828829),\n","  (16, 0.026714214214214216),\n","  (17, 0.010573073073073072),\n","  (18, 0.00832082082082082),\n","  (19, 0.05161411411411412)],\n"," [(0, 0.011848778238819736),\n","  (1, 0.023467035500230528),\n","  (2, 0.07491931765790688),\n","  (3, 0.057860765329645014),\n","  (4, 0.01526048870447211),\n","  (5, 0.00603964960811434),\n","  (6, 0.21203319502074694),\n","  (7, 0.016274781005071466),\n","  (8, 0.004379898570769941),\n","  (9, 0.005486399262332873),\n","  (10, 0.318349469801752),\n","  (11, 0.006316274781005073),\n","  (12, 0.028630705394190877),\n","  (13, 0.04937759336099586),\n","  (14, 0.009174734900875982),\n","  (15, 0.011940986629783313),\n","  (16, 0.014153988012909178),\n","  (17, 0.012955278930382669),\n","  (18, 0.07574919317657908),\n","  (19, 0.045781466113416325)],\n"," [(0, 0.020584894946053374),\n","  (1, 0.017556312701116787),\n","  (2, 0.01254022335794056),\n","  (3, 0.3351788756388415),\n","  (4, 0.004306265379519211),\n","  (5, 0.009322354722695436),\n","  (6, 0.17996403558584134),\n","  (7, 0.011025932235472267),\n","  (8, 0.06071360969146317),\n","  (9, 0.0075241340147643365),\n","  (10, 0.026736702631080817),\n","  (11, 0.011025932235472267),\n","  (12, 0.012634866553094828),\n","  (13, 0.11021200075714553),\n","  (14, 0.008943781942078363),\n","  (15, 0.06762256293772477),\n","  (16, 0.005820556501987506),\n","  (17, 0.028724209729320455),\n","  (18, 0.006199129282604579),\n","  (19, 0.06336361915578269)],\n"," [(0, 0.010495382031905962),\n","  (1, 0.0320458998040862),\n","  (2, 0.0382031905961377),\n","  (3, 0.018052057094878254),\n","  (4, 0.009935628323537643),\n","  (5, 0.009935628323537643),\n","  (6, 0.05135740274279317),\n","  (7, 0.38525048978449483),\n","  (8, 0.011894766302826756),\n","  (9, 0.009655751469353484),\n","  (10, 0.0491183879093199),\n","  (11, 0.009935628323537643),\n","  (12, 0.015253288553036664),\n","  (13, 0.1910159529806885),\n","  (14, 0.01609291911558914),\n","  (15, 0.04631961936747831),\n","  (16, 0.03540442205429611),\n","  (17, 0.0382031905961377),\n","  (18, 0.009655751469353484),\n","  (19, 0.012174643157010915)],\n"," [(0, 0.02895310170851967),\n","  (1, 0.050280931085884654),\n","  (2, 0.058766196537094376),\n","  (3, 0.007969269579176702),\n","  (4, 0.005217291595000575),\n","  (5, 0.08961128310973514),\n","  (6, 0.007625272331154685),\n","  (7, 0.01886251576654054),\n","  (8, 0.010721247563352828),\n","  (9, 0.025857126476321525),\n","  (10, 0.026201123724343543),\n","  (11, 0.06977410847379889),\n","  (12, 0.012097236555440892),\n","  (13, 0.2578259373925009),\n","  (14, 0.025054466230936823),\n","  (15, 0.22078890035546386),\n","  (16, 0.03927301914918015),\n","  (17, 0.021614493750716664),\n","  (18, 0.010147918816649469),\n","  (19, 0.013358559798188284)],\n"," [(0, 0.011456534254461716),\n","  (1, 0.007656879677605066),\n","  (2, 0.2130685089234312),\n","  (3, 0.03644214162348877),\n","  (4, 0.014565342544617155),\n","  (5, 0.05970063327576281),\n","  (6, 0.051180195739781234),\n","  (7, 0.008808290155440414),\n","  (8, 0.006966033390903857),\n","  (9, 0.003742084052964882),\n","  (10, 0.022855497985031663),\n","  (11, 0.1648244099021301),\n","  (12, 0.005699481865284974),\n","  (13, 0.17253886010362696),\n","  (14, 0.01686816350028785),\n","  (15, 0.05313759355210133),\n","  (16, 0.01871042026482441),\n","  (17, 0.11381692573402417),\n","  (18, 0.007311456534254462),\n","  (19, 0.010650546919976972)],\n"," [(0, 0.007299741602067184),\n","  (1, 0.014405684754521964),\n","  (2, 0.18236434108527133),\n","  (3, 0.020219638242894057),\n","  (4, 0.007170542635658915),\n","  (5, 0.08675710594315246),\n","  (6, 0.05109819121447028),\n","  (7, 0.009366925064599484),\n","  (8, 0.024483204134366925),\n","  (9, 0.008333333333333333),\n","  (10, 0.03766149870801033),\n","  (11, 0.27461240310077517),\n","  (12, 0.005749354005167959),\n","  (13, 0.14127906976744187),\n","  (14, 0.01479328165374677),\n","  (15, 0.07280361757105944),\n","  (16, 0.007299741602067184),\n","  (17, 0.016989664082687338),\n","  (18, 0.007428940568475452),\n","  (19, 0.009883720930232558)],\n"," [(0, 0.07500824266402902),\n","  (1, 0.007088691064952192),\n","  (2, 0.3671282558522915),\n","  (3, 0.006539180129684581),\n","  (4, 0.016869985712715682),\n","  (5, 0.1737004066380921),\n","  (6, 0.10545114847785471),\n","  (7, 0.03016815034619189),\n","  (8, 0.02269480162655237),\n","  (9, 0.016430376964501593),\n","  (10, 0.011704582921200132),\n","  (11, 0.0052203538850423125),\n","  (12, 0.03874052093636663),\n","  (13, 0.04951093526761183),\n","  (14, 0.028629519727442578),\n","  (15, 0.00697878887789867),\n","  (16, 0.007088691064952192),\n","  (17, 0.006429277942631058),\n","  (18, 0.011704582921200132),\n","  (19, 0.012913506978788878)],\n"," [(0, 0.11519916142557655),\n","  (1, 0.01687631027253669),\n","  (2, 0.014779874213836482),\n","  (3, 0.03846960167714886),\n","  (4, 0.014150943396226419),\n","  (5, 0.007861635220125788),\n","  (6, 0.044758909853249484),\n","  (7, 0.02064989517819707),\n","  (8, 0.013731656184486376),\n","  (9, 0.011006289308176102),\n","  (10, 0.009538784067085956),\n","  (11, 0.023794549266247387),\n","  (12, 0.059224318658280935),\n","  (13, 0.060272536687631044),\n","  (14, 0.05104821802935012),\n","  (15, 0.008700209643605872),\n","  (16, 0.026100628930817618),\n","  (17, 0.11121593291404615),\n","  (18, 0.17348008385744237),\n","  (19, 0.17914046121593294)],\n"," [(0, 0.039719147191471925),\n","  (1, 0.008148831488314886),\n","  (2, 0.3991902419024191),\n","  (3, 0.006508815088150883),\n","  (4, 0.011838868388683888),\n","  (5, 0.006713817138171383),\n","  (6, 0.0035362853628536288),\n","  (7, 0.005176301763017631),\n","  (8, 0.004458794587945881),\n","  (9, 0.007123821238212383),\n","  (10, 0.006406314063140632),\n","  (11, 0.011531365313653138),\n","  (12, 0.03930914309143092),\n","  (13, 0.01388888888888889),\n","  (14, 0.03859163591635917),\n","  (15, 0.004868798687986881),\n","  (16, 0.008456334563345635),\n","  (17, 0.13361008610086103),\n","  (18, 0.008251332513325134),\n","  (19, 0.24267117671176713)],\n"," [(0, 0.006473182529520558),\n","  (1, 0.009887608479157775),\n","  (2, 0.493028880352824),\n","  (3, 0.0245411865130175),\n","  (4, 0.008038127756437615),\n","  (5, 0.04033290653008963),\n","  (6, 0.03663394508464931),\n","  (7, 0.007611324512732964),\n","  (8, 0.015862853891022904),\n","  (9, 0.010456679470763978),\n","  (10, 0.015436050647318253),\n","  (11, 0.025252525252525252),\n","  (12, 0.011310285958173282),\n","  (13, 0.1524398918765116),\n","  (14, 0.02667520273154076),\n","  (15, 0.06622563664817185),\n","  (16, 0.011452553706074833),\n","  (17, 0.009034001991748471),\n","  (18, 0.013444302176696543),\n","  (19, 0.015862853891022904)],\n"," [(0, 0.010375494071146248),\n","  (1, 0.007081686429512517),\n","  (2, 0.4077184892402284),\n","  (3, 0.013230127360562146),\n","  (4, 0.02102547211242864),\n","  (5, 0.011802810715854196),\n","  (6, 0.012241985068072026),\n","  (7, 0.004776021080368907),\n","  (8, 0.005544576196750111),\n","  (9, 0.005873956960913484),\n","  (10, 0.015206411945542382),\n","  (11, 0.011363636363636366),\n","  (12, 0.004336846728151077),\n","  (13, 0.10348045674132632),\n","  (14, 0.1584870443566096),\n","  (15, 0.18176328502415462),\n","  (16, 0.007630654369784806),\n","  (17, 0.005654369784804568),\n","  (18, 0.004995608256477823),\n","  (19, 0.00741106719367589)],\n"," [(0, 0.01117589893100097),\n","  (1, 0.0059928733398121135),\n","  (2, 0.47710830363891576),\n","  (3, 0.022405787711910156),\n","  (4, 0.004805096641831335),\n","  (5, 0.016142965122556956),\n","  (6, 0.005129035741280638),\n","  (7, 0.006532771838894286),\n","  (8, 0.007828528236691501),\n","  (9, 0.009772162833387322),\n","  (10, 0.04367778857574775),\n","  (11, 0.013983371126228265),\n","  (12, 0.009340244034121583),\n","  (13, 0.012363675628981748),\n","  (14, 0.03460749379116725),\n","  (15, 0.1365403304178814),\n","  (16, 0.00804448763632437),\n","  (17, 0.03093618399740848),\n","  (18, 0.023593564409890935),\n","  (19, 0.12001943634596693)],\n"," [(0, 0.04064642507345739),\n","  (1, 0.007454565241049081),\n","  (2, 0.08809446076830993),\n","  (3, 0.008869300250299271),\n","  (4, 0.014201762977473066),\n","  (5, 0.008978126020241593),\n","  (6, 0.016051801066492545),\n","  (7, 0.004298617912721733),\n","  (8, 0.008433997170529981),\n","  (9, 0.008542822940472304),\n","  (10, 0.060126237893133094),\n","  (11, 0.013766459897703776),\n","  (12, 0.009631080639895527),\n","  (13, 0.1838611383175536),\n","  (14, 0.3967243443247361),\n","  (15, 0.021819566873435628),\n","  (16, 0.03694634889541844),\n","  (17, 0.03389922733703341),\n","  (18, 0.006257481771683534),\n","  (19, 0.03139623462836)],\n"," [(0, 0.009293113954905547),\n","  (1, 0.006043063172862077),\n","  (2, 0.1213183018484664),\n","  (3, 0.03468413568962016),\n","  (4, 0.010715011172049566),\n","  (5, 0.06545805403209426),\n","  (6, 0.012543164736949016),\n","  (7, 0.005433678651228927),\n","  (8, 0.006957139955311803),\n","  (9, 0.021176112126751984),\n","  (10, 0.008886857607150113),\n","  (11, 0.07043469429209832),\n","  (12, 0.029910623603493808),\n","  (13, 0.27061750964858833),\n","  (14, 0.012035344302254725),\n","  (15, 0.006043063172862077),\n","  (16, 0.007668088563883811),\n","  (17, 0.20226487913873656),\n","  (18, 0.014066626041031894),\n","  (19, 0.08445053828966079)],\n"," [(0, 0.030118203309692664),\n","  (1, 0.008085106382978722),\n","  (2, 0.4947044917257682),\n","  (3, 0.021607565011820328),\n","  (4, 0.005721040189125295),\n","  (5, 0.019432624113475173),\n","  (6, 0.005153664302600472),\n","  (7, 0.004869976359338061),\n","  (8, 0.007517730496453899),\n","  (9, 0.009976359338061463),\n","  (10, 0.012529550827423165),\n","  (11, 0.0031678486997635926),\n","  (12, 0.0043026004728132375),\n","  (13, 0.026808510638297867),\n","  (14, 0.090354609929078),\n","  (15, 0.003640661938534278),\n","  (16, 0.011678486997635932),\n","  (17, 0.1507801418439716),\n","  (18, 0.01394799054373522),\n","  (19, 0.07560283687943262)],\n"," [(0, 0.010445899594636732),\n","  (1, 0.00670408481446835),\n","  (2, 0.13881093441430203),\n","  (3, 0.019696497245608565),\n","  (4, 0.008159235006756055),\n","  (5, 0.01886498285001559),\n","  (6, 0.008990749402349028),\n","  (7, 0.008159235006756055),\n","  (8, 0.06636524269826422),\n","  (9, 0.012420746284170045),\n","  (10, 0.07000311817898347),\n","  (11, 0.004001663028791186),\n","  (12, 0.0438104147178048),\n","  (13, 0.008574992204552542),\n","  (14, 0.3267331878183141),\n","  (15, 0.11074732356303918),\n","  (16, 0.0930776426566885),\n","  (17, 0.017201954058829644),\n","  (18, 0.00971832449849288),\n","  (19, 0.01751377195717701)],\n"," [(0, 0.007218757218757221),\n","  (1, 0.008604758604758607),\n","  (2, 0.45316470316470325),\n","  (3, 0.015419265419265422),\n","  (4, 0.009990759990759993),\n","  (5, 0.07097482097482098),\n","  (6, 0.0412912912912913),\n","  (7, 0.022811272811272813),\n","  (8, 0.01264726264726265),\n","  (9, 0.024197274197274205),\n","  (10, 0.025698775698775704),\n","  (11, 0.006410256410256412),\n","  (12, 0.03228228228228229),\n","  (13, 0.102968352968353),\n","  (14, 0.013224763224763228),\n","  (15, 0.012993762993762998),\n","  (16, 0.012993762993762998),\n","  (17, 0.05249480249480251),\n","  (18, 0.007334257334257336),\n","  (19, 0.0672788172788173)],\n"," [(0, 0.10613823588507133),\n","  (1, 0.016124171187462327),\n","  (2, 0.2348302190074342),\n","  (3, 0.015220012055455093),\n","  (4, 0.011402451275868997),\n","  (5, 0.1345690174804099),\n","  (6, 0.041239702632107696),\n","  (7, 0.044454490657022304),\n","  (8, 0.010799678521197509),\n","  (9, 0.07127787823990356),\n","  (10, 0.05450070323488045),\n","  (11, 0.02255374723729154),\n","  (12, 0.027576853526220614),\n","  (13, 0.08534257584890496),\n","  (14, 0.021348201727948563),\n","  (15, 0.00889089813140446),\n","  (16, 0.039833232871207554),\n","  (17, 0.031695800683142455),\n","  (18, 0.00818766325095439),\n","  (19, 0.014014466546112115)],\n"," [(0, 0.02341731266149871),\n","  (1, 0.021048664944013782),\n","  (2, 0.23002799310938846),\n","  (3, 0.009097760551248923),\n","  (4, 0.02406330749354005),\n","  (5, 0.19988156761412576),\n","  (6, 0.04914944013781223),\n","  (7, 0.012866063738156762),\n","  (8, 0.007159776055124892),\n","  (9, 0.02944659776055125),\n","  (10, 0.0422588285960379),\n","  (11, 0.03181524547803618),\n","  (12, 0.014911714039621016),\n","  (13, 0.1928832902670112),\n","  (14, 0.006836778639104221),\n","  (15, 0.003714470284237726),\n","  (16, 0.06529931093884582),\n","  (17, 0.007482773471145564),\n","  (18, 0.013296726959517656),\n","  (19, 0.015342377260981912)],\n"," [(0, 0.06370302474793767),\n","  (1, 0.009624197983501375),\n","  (2, 0.014614522863835421),\n","  (3, 0.005957836846929423),\n","  (4, 0.01349424584988288),\n","  (5, 0.01379977594459721),\n","  (6, 0.01553111314797841),\n","  (7, 0.4420511253691822),\n","  (8, 0.004837559832976882),\n","  (9, 0.012170282106120787),\n","  (10, 0.1276606579081373),\n","  (11, 0.008707607699358386),\n","  (12, 0.04557490579488746),\n","  (13, 0.12786434463794683),\n","  (14, 0.00921682452388227),\n","  (15, 0.006772583766167634),\n","  (16, 0.011151848457073022),\n","  (17, 0.013086872390263774),\n","  (18, 0.00575415011711987),\n","  (19, 0.048426520012221204)],\n"," [(0, 0.03177177177177177),\n","  (1, 0.016276276276276275),\n","  (2, 0.015555555555555555),\n","  (3, 0.0046246246246246245),\n","  (4, 0.02156156156156156),\n","  (5, 0.08978978978978978),\n","  (6, 0.0045045045045045045),\n","  (7, 0.16066066066066065),\n","  (8, 0.031171171171171172),\n","  (9, 0.006426426426426426),\n","  (10, 0.03981981981981982),\n","  (11, 0.011831831831831832),\n","  (12, 0.04066066066066066),\n","  (13, 0.2526726726726727),\n","  (14, 0.030690690690690692),\n","  (15, 0.0360960960960961),\n","  (16, 0.03753753753753754),\n","  (17, 0.011831831831831832),\n","  (18, 0.011711711711711712),\n","  (19, 0.14480480480480482)],\n"," [(0, 0.048369264787175244),\n","  (1, 0.05444997236042013),\n","  (2, 0.06406854615809841),\n","  (3, 0.004919845218352682),\n","  (4, 0.01575456053067994),\n","  (5, 0.009784411276948592),\n","  (6, 0.039635157545605315),\n","  (7, 0.16909894969596465),\n","  (8, 0.0060254284134881155),\n","  (9, 0.08076285240464347),\n","  (10, 0.017855168601437262),\n","  (11, 0.043394140409065786),\n","  (12, 0.011111111111111113),\n","  (13, 0.0065782200110558325),\n","  (14, 0.01741293532338309),\n","  (15, 0.004698728579325595),\n","  (16, 0.027363184079601994),\n","  (17, 0.06263128800442234),\n","  (18, 0.01166390270867883),\n","  (19, 0.3044223327805418)],\n"," [(0, 0.08228511530398323),\n","  (1, 0.007046354530631261),\n","  (2, 0.030689494525972513),\n","  (3, 0.0047169811320754715),\n","  (4, 0.029990682506405776),\n","  (5, 0.00506638714185884),\n","  (6, 0.06365012811553691),\n","  (7, 0.24918471931050548),\n","  (8, 0.006114605171208945),\n","  (9, 0.0132191940368041),\n","  (10, 0.05328441649196366),\n","  (11, 0.004949918471931051),\n","  (12, 0.04594689028651293),\n","  (13, 0.05805963195900303),\n","  (14, 0.008211041229909155),\n","  (15, 0.005648730491497787),\n","  (16, 0.006580479850920103),\n","  (17, 0.07995574190542744),\n","  (18, 0.010191008618681574),\n","  (19, 0.23520847891917074)],\n"," [(0, 0.011768573307034841),\n","  (1, 0.09132149901380669),\n","  (2, 0.013083497698882311),\n","  (3, 0.009664694280078894),\n","  (4, 0.020184089414858643),\n","  (5, 0.006377383300460222),\n","  (6, 0.04411571334648257),\n","  (7, 0.3850756081525311),\n","  (8, 0.0051939513477975005),\n","  (9, 0.048717948717948704),\n","  (10, 0.013872452333990792),\n","  (11, 0.012557527942143324),\n","  (12, 0.12182774490466795),\n","  (13, 0.020841551610782376),\n","  (14, 0.005325443786982248),\n","  (15, 0.009007232084155159),\n","  (16, 0.09342537804076265),\n","  (17, 0.06896778435239972),\n","  (18, 0.0094017094017094),\n","  (19, 0.009270216962524653)],\n"," [(0, 0.013346883468834688),\n","  (1, 0.007113821138211382),\n","  (2, 0.011720867208672086),\n","  (3, 0.012533875338753388),\n","  (4, 0.01605691056910569),\n","  (5, 0.016327913279132793),\n","  (6, 0.059823848238482386),\n","  (7, 0.20887533875338754),\n","  (8, 0.013211382113821139),\n","  (9, 0.007113821138211382),\n","  (10, 0.029471544715447155),\n","  (11, 0.010365853658536586),\n","  (12, 0.06429539295392954),\n","  (13, 0.32947154471544715),\n","  (14, 0.0263550135501355),\n","  (15, 0.009146341463414634),\n","  (16, 0.0244579945799458),\n","  (17, 0.07052845528455284),\n","  (18, 0.03475609756097561),\n","  (19, 0.03502710027100271)],\n"," [(0, 0.02878346511305312),\n","  (1, 0.00672770148425579),\n","  (2, 0.0060341240116520995),\n","  (3, 0.005340546539048411),\n","  (4, 0.011860174781523092),\n","  (5, 0.012969898737688997),\n","  (6, 0.025731724233596886),\n","  (7, 0.395131086142322),\n","  (8, 0.005340546539048411),\n","  (9, 0.057497572478845876),\n","  (10, 0.10563184907754193),\n","  (11, 0.0060341240116520995),\n","  (12, 0.09730891940629766),\n","  (13, 0.01754751005687335),\n","  (14, 0.045568039950062415),\n","  (15, 0.007976140934942432),\n","  (16, 0.01477320016645859),\n","  (17, 0.08302122347066165),\n","  (18, 0.006172839506172837),\n","  (19, 0.06054931335830211)],\n"," [(0, 0.06174786774155859),\n","  (1, 0.01244304241149667),\n","  (2, 0.04153522607781283),\n","  (3, 0.01828484636055614),\n","  (4, 0.010339992989835262),\n","  (5, 0.06782334384858044),\n","  (6, 0.010924173384741208),\n","  (7, 0.1574366164271527),\n","  (8, 0.00473186119873817),\n","  (9, 0.010106320831872882),\n","  (10, 0.018518518518518517),\n","  (11, 0.18314055380301436),\n","  (12, 0.07109475406005375),\n","  (13, 0.12273630096973946),\n","  (14, 0.007535927094286716),\n","  (15, 0.02015422362425517),\n","  (16, 0.021906764808973012),\n","  (17, 0.12191844841687113),\n","  (18, 0.007886435331230283),\n","  (19, 0.0297347821007127)],\n"," [(0, 0.29888167388167386),\n","  (1, 0.024711399711399712),\n","  (2, 0.016293891293891295),\n","  (3, 0.02386964886964887),\n","  (4, 0.01424963924963925),\n","  (5, 0.035173160173160176),\n","  (6, 0.005832130832130832),\n","  (7, 0.0477994227994228),\n","  (8, 0.015933140933140934),\n","  (9, 0.04455266955266955),\n","  (10, 0.008116883116883116),\n","  (11, 0.01424963924963925),\n","  (12, 0.17273929773929775),\n","  (13, 0.09505772005772006),\n","  (14, 0.0180976430976431),\n","  (15, 0.008237133237133237),\n","  (16, 0.11044973544973545),\n","  (17, 0.014730639730639731),\n","  (18, 0.016414141414141416),\n","  (19, 0.01461038961038961)],\n"," [(0, 0.24412611793239353),\n","  (1, 0.010838259815067457),\n","  (2, 0.0149310292557223),\n","  (3, 0.17485220554797637),\n","  (4, 0.006745490374412613),\n","  (5, 0.018569046536304385),\n","  (6, 0.009170835228134002),\n","  (7, 0.052069122328331066),\n","  (8, 0.014173108988934365),\n","  (9, 0.017053206002728517),\n","  (10, 0.0332726997119903),\n","  (11, 0.06146733363650145),\n","  (12, 0.042670911020160684),\n","  (13, 0.06677277550401699),\n","  (14, 0.023116568137031987),\n","  (15, 0.03630438077914204),\n","  (16, 0.012657268455358498),\n","  (17, 0.10603304532363196),\n","  (18, 0.005835986054267092),\n","  (19, 0.04934060936789451)],\n"," [(0, 0.11215780998389695),\n","  (1, 0.0319645732689211),\n","  (2, 0.014412238325281804),\n","  (3, 0.006360708534621578),\n","  (4, 0.007971014492753623),\n","  (5, 0.010225442834138487),\n","  (6, 0.014734299516908212),\n","  (7, 0.18381642512077295),\n","  (8, 0.008454106280193236),\n","  (9, 0.1424315619967794),\n","  (10, 0.05740740740740741),\n","  (11, 0.06884057971014493),\n","  (12, 0.16771336553945249),\n","  (13, 0.04001610305958132),\n","  (14, 0.00748792270531401),\n","  (15, 0.008293075684380032),\n","  (16, 0.07592592592592592),\n","  (17, 0.008615136876006442),\n","  (18, 0.006521739130434782),\n","  (19, 0.026650563607085346)],\n"," [(0, 0.02531229454306378),\n","  (1, 0.011374095989480606),\n","  (2, 0.03438527284681132),\n","  (3, 0.056213017751479306),\n","  (4, 0.008481262327416176),\n","  (5, 0.20072320841551616),\n","  (6, 0.0069033530571992125),\n","  (7, 0.20440499671268908),\n","  (8, 0.028468113083497705),\n","  (9, 0.0775147928994083),\n","  (10, 0.02741617357001973),\n","  (11, 0.023734385272846818),\n","  (12, 0.0200525969756739),\n","  (13, 0.10880999342537807),\n","  (14, 0.008218277449046681),\n","  (15, 0.014924391847468774),\n","  (16, 0.053977646285338604),\n","  (17, 0.02360289283366207),\n","  (18, 0.012820512820512824),\n","  (19, 0.05266272189349114)],\n"," [(0, 0.00786681302597878),\n","  (1, 0.03347969264544457),\n","  (2, 0.13849249908525432),\n","  (3, 0.0048176606903280895),\n","  (4, 0.027747286254421275),\n","  (5, 0.019575557994877427),\n","  (6, 0.014940846444688379),\n","  (7, 0.1688620563483352),\n","  (8, 0.0048176606903280895),\n","  (9, 0.08129040126844739),\n","  (10, 0.10287839980485426),\n","  (11, 0.025917794853030862),\n","  (12, 0.031406269057202105),\n","  (13, 0.2593608976704477),\n","  (14, 0.006281253811440421),\n","  (15, 0.017989998780339068),\n","  (16, 0.00786681302597878),\n","  (17, 0.03128430296377608),\n","  (18, 0.003963898036345896),\n","  (19, 0.011159897548481524)],\n"," [(0, 0.005561409670998713),\n","  (1, 0.006849315068493152),\n","  (2, 0.07604495960660346),\n","  (3, 0.015747570542091093),\n","  (4, 0.009073878936892637),\n","  (5, 0.007668891230535068),\n","  (6, 0.04478398314014753),\n","  (7, 0.15694883503102683),\n","  (8, 0.007434726612808806),\n","  (9, 0.094426882098115),\n","  (10, 0.04642313546423136),\n","  (11, 0.010010537407797683),\n","  (12, 0.0054443273621355825),\n","  (13, 0.24206767357452294),\n","  (14, 0.006615150450766891),\n","  (15, 0.19781056082425952),\n","  (16, 0.01961128673457441),\n","  (17, 0.029329118370214267),\n","  (18, 0.005327245053272451),\n","  (19, 0.012820512820512824)],\n"," [(0, 0.03757346767422334),\n","  (1, 0.007066890568150014),\n","  (2, 0.06248250769661349),\n","  (3, 0.01322418136020151),\n","  (4, 0.033655191715645114),\n","  (5, 0.0069269521410579345),\n","  (6, 0.04009235936188077),\n","  (7, 0.08333333333333333),\n","  (8, 0.005667506297229219),\n","  (9, 0.03267562272600056),\n","  (10, 0.023439686537923315),\n","  (11, 0.017282395745871815),\n","  (12, 0.24328295549958018),\n","  (13, 0.0767562272600056),\n","  (14, 0.10642317380352645),\n","  (15, 0.11747830954380073),\n","  (16, 0.012524489224741114),\n","  (17, 0.05380632521690456),\n","  (18, 0.009865659109991603),\n","  (19, 0.01644276518331934)],\n"," [(0, 0.010474978795589483),\n","  (1, 0.01768447837150127),\n","  (2, 0.4765479219677693),\n","  (3, 0.009711620016963528),\n","  (4, 0.008100084817642069),\n","  (5, 0.013019508057675997),\n","  (6, 0.021077184054283292),\n","  (7, 0.011407972858354537),\n","  (8, 0.0037743850720949957),\n","  (9, 0.02633587786259542),\n","  (10, 0.0052162849872773535),\n","  (11, 0.011916878710771841),\n","  (12, 0.010474978795589483),\n","  (13, 0.1876590330788804),\n","  (14, 0.08155216284987277),\n","  (15, 0.027862595419847327),\n","  (16, 0.007845631891433419),\n","  (17, 0.053307888040712466),\n","  (18, 0.008354537743850721),\n","  (19, 0.007675996607294317)],\n"," [(0, 0.010076775431861805),\n","  (1, 0.009436980166346769),\n","  (2, 0.007517594369801663),\n","  (3, 0.005491576029004052),\n","  (4, 0.0074109618255491574),\n","  (5, 0.0036788227767114525),\n","  (6, 0.49770740029857113),\n","  (7, 0.010929835785881852),\n","  (8, 0.006238003838771593),\n","  (9, 0.006131371294519087),\n","  (10, 0.012529323949669439),\n","  (11, 0.00837065472382171),\n","  (12, 0.004958413307741522),\n","  (13, 0.00762422691405417),\n","  (14, 0.02799104286628279),\n","  (15, 0.08845169545745361),\n","  (16, 0.010609938153124334),\n","  (17, 0.006451268927276605),\n","  (18, 0.009863510343356793),\n","  (19, 0.25853060354020047)],\n"," [(0, 0.006886412875270814),\n","  (1, 0.023135252243887343),\n","  (2, 0.04015784586815228),\n","  (3, 0.02158774373259053),\n","  (4, 0.0127669452181987),\n","  (5, 0.06692974311358713),\n","  (6, 0.06429897864438254),\n","  (7, 0.007505416279789539),\n","  (8, 0.02669452181987001),\n","  (9, 0.005803156917363045),\n","  (10, 0.04464562055091303),\n","  (11, 0.040003095017022594),\n","  (12, 0.016790467347570413),\n","  (13, 0.11196224079232436),\n","  (14, 0.14832869080779945),\n","  (15, 0.024992262457443515),\n","  (16, 0.10577220674713711),\n","  (17, 0.00889817393995667),\n","  (18, 0.008588672237697307),\n","  (19, 0.21425255338904364)],\n"," [(0, 0.279291553133515),\n","  (1, 0.009234029669996972),\n","  (2, 0.023362599656877585),\n","  (3, 0.008023009385407205),\n","  (4, 0.008426682813603795),\n","  (5, 0.03476637400343122),\n","  (6, 0.1767585023715814),\n","  (7, 0.025179130083762238),\n","  (8, 0.016096477949338984),\n","  (9, 0.026289232011302857),\n","  (10, 0.038702189928347966),\n","  (11, 0.014885457664749219),\n","  (12, 0.013775355737208598),\n","  (13, 0.11913412049651832),\n","  (14, 0.0055000504591785244),\n","  (15, 0.022353416086386114),\n","  (16, 0.025885558583106268),\n","  (17, 0.07069330911292764),\n","  (18, 0.020133212231304876),\n","  (19, 0.06150973862145524)],\n"," [(0, 0.22856341189674517),\n","  (1, 0.004545454545454544),\n","  (2, 0.026318742985409645),\n","  (3, 0.006341189674523006),\n","  (4, 0.010830527497194162),\n","  (5, 0.00802469135802469),\n","  (6, 0.15011223344556676),\n","  (7, 0.06290684624017956),\n","  (8, 0.006790123456790122),\n","  (9, 0.22755331088664416),\n","  (10, 0.01891133557800224),\n","  (11, 0.008361391694725027),\n","  (12, 0.015881032547699212),\n","  (13, 0.05785634118967451),\n","  (14, 0.010493827160493826),\n","  (15, 0.016105499438832768),\n","  (16, 0.024635241301907963),\n","  (17, 0.012401795735129065),\n","  (18, 0.007239057239057238),\n","  (19, 0.0961279461279461)],\n"," [(0, 0.42665104863900033),\n","  (1, 0.005410531012940651),\n","  (2, 0.011769299419901826),\n","  (3, 0.005522088353413654),\n","  (4, 0.032295850066934396),\n","  (5, 0.006079875055778668),\n","  (6, 0.011099955377063809),\n","  (7, 0.0036256135653726005),\n","  (8, 0.004964301651048638),\n","  (9, 0.007641677822400712),\n","  (10, 0.010542168674698793),\n","  (11, 0.0036256135653726005),\n","  (12, 0.022367246764837125),\n","  (13, 0.16984605087014723),\n","  (14, 0.07412985274431057),\n","  (15, 0.004294957608210619),\n","  (16, 0.005298973672467648),\n","  (17, 0.00396028558679161),\n","  (18, 0.03519634091923248),\n","  (19, 0.15567826863007583)],\n"," [(0, 0.011394196744515214),\n","  (1, 0.023283793347487612),\n","  (2, 0.016772823779193202),\n","  (3, 0.012809624911535736),\n","  (4, 0.17190375088464258),\n","  (5, 0.005449398443029016),\n","  (6, 0.1138711960368011),\n","  (7, 0.039561217268223633),\n","  (8, 0.005449398443029016),\n","  (9, 0.011535739561217267),\n","  (10, 0.010686482661004953),\n","  (11, 0.006864826610049538),\n","  (12, 0.3124557678697806),\n","  (13, 0.013234253361641893),\n","  (14, 0.005449398443029016),\n","  (15, 0.0105449398443029),\n","  (16, 0.11882519462137293),\n","  (17, 0.06801132342533615),\n","  (18, 0.007289455060155696),\n","  (19, 0.0346072186836518)],\n"," [(0, 0.10638845955692942),\n","  (1, 0.01743087755452516),\n","  (2, 0.009359436716469174),\n","  (3, 0.008672505581315474),\n","  (4, 0.01485488579769878),\n","  (5, 0.014339687446333505),\n","  (6, 0.16065601923407177),\n","  (7, 0.023269792203331616),\n","  (8, 0.012107161257083977),\n","  (9, 0.07187017001545595),\n","  (10, 0.015541816932852481),\n","  (11, 0.008672505581315474),\n","  (12, 0.26335222393955005),\n","  (13, 0.020178602095139962),\n","  (14, 0.03391722479821398),\n","  (15, 0.005753048256912244),\n","  (16, 0.05607075390692083),\n","  (17, 0.1397046196118839),\n","  (18, 0.007813841662373347),\n","  (19, 0.010046367851622875)],\n"," [(0, 0.029724468403713688),\n","  (1, 0.013851452530697814),\n","  (2, 0.031221922731356693),\n","  (3, 0.016696615753219526),\n","  (4, 0.010706798442647499),\n","  (5, 0.005465708295896975),\n","  (6, 0.05578017370470201),\n","  (7, 0.07943995208146151),\n","  (8, 0.01969152440850554),\n","  (9, 0.2055256064690027),\n","  (10, 0.025232105420784665),\n","  (11, 0.007112908056304283),\n","  (12, 0.018942797244684038),\n","  (13, 0.09561245882000599),\n","  (14, 0.010107816711590296),\n","  (15, 0.015348906858340821),\n","  (16, 0.320230607966457),\n","  (17, 0.013701707097933512),\n","  (18, 0.011156034740940401),\n","  (19, 0.014450434261755017)],\n"," [(0, 0.057968516603470056),\n","  (1, 0.0332643915891072),\n","  (2, 0.04452487647937492),\n","  (3, 0.00787084913248305),\n","  (4, 0.00970929564517982),\n","  (5, 0.007181431690221761),\n","  (6, 0.098414339882799),\n","  (7, 0.023842353211536244),\n","  (8, 0.004308859014133057),\n","  (9, 0.19700103412616335),\n","  (10, 0.018901528208663677),\n","  (11, 0.028898081121452365),\n","  (12, 0.048316672411812006),\n","  (13, 0.14874181316787313),\n","  (14, 0.005572790991612087),\n","  (15, 0.09117545673905549),\n","  (16, 0.11714351373089736),\n","  (17, 0.038779731127197514),\n","  (18, 0.008100654946570146),\n","  (19, 0.010283810180397562)],\n"," [(0, 0.02913105413105413),\n","  (1, 0.031125356125356125),\n","  (2, 0.11773504273504273),\n","  (3, 0.011182336182336182),\n","  (4, 0.04935897435897436),\n","  (5, 0.024002849002849002),\n","  (6, 0.03482905982905983),\n","  (7, 0.014458689458689458),\n","  (8, 0.006908831908831909),\n","  (9, 0.008475783475783476),\n","  (10, 0.060612535612535615),\n","  (11, 0.026282051282051282),\n","  (12, 0.15847578347578348),\n","  (13, 0.16801994301994302),\n","  (14, 0.11588319088319088),\n","  (15, 0.01858974358974359),\n","  (16, 0.05349002849002849),\n","  (17, 0.017735042735042734),\n","  (18, 0.013603988603988605),\n","  (19, 0.0400997150997151)],\n"," [(0, 0.026424963924963924),\n","  (1, 0.029130591630591632),\n","  (2, 0.030032467532467532),\n","  (3, 0.00983044733044733),\n","  (4, 0.006944444444444444),\n","  (5, 0.00965007215007215),\n","  (6, 0.00928932178932179),\n","  (7, 0.006944444444444444),\n","  (8, 0.015602453102453102),\n","  (9, 0.007124819624819625),\n","  (10, 0.056367243867243864),\n","  (11, 0.06881313131313131),\n","  (12, 0.2773268398268398),\n","  (13, 0.08757215007215008),\n","  (14, 0.03201659451659452),\n","  (15, 0.005140692640692641),\n","  (16, 0.011994949494949494),\n","  (17, 0.2773268398268398),\n","  (18, 0.020472582972582972),\n","  (19, 0.011994949494949494)],\n"," [(0, 0.032857941834451905),\n","  (1, 0.007876584638329605),\n","  (2, 0.0793717375093214),\n","  (3, 0.012816927665920954),\n","  (4, 0.029688665175242355),\n","  (5, 0.004520879940343028),\n","  (6, 0.011791573452647279),\n","  (7, 0.2652404921700224),\n","  (8, 0.005919090231170768),\n","  (9, 0.06091536167039523),\n","  (10, 0.016172632363907532),\n","  (11, 0.006012304250559284),\n","  (12, 0.018969052945563013),\n","  (13, 0.22543810589112603),\n","  (14, 0.015333706189410887),\n","  (15, 0.004800521998508576),\n","  (16, 0.16102721849366144),\n","  (17, 0.003215883668903803),\n","  (18, 0.013562639821029083),\n","  (19, 0.02446868008948546)],\n"," [(0, 0.3896649851248222),\n","  (1, 0.0062734445737938176),\n","  (2, 0.025675850472125214),\n","  (3, 0.006144095201138276),\n","  (4, 0.009507178890182384),\n","  (5, 0.007566938300349244),\n","  (6, 0.00536799896520502),\n","  (7, 0.006144095201138276),\n","  (8, 0.0514163756305782),\n","  (9, 0.0062734445737938176),\n","  (10, 0.019725779329970255),\n","  (11, 0.0038158064933385078),\n","  (12, 0.0083430345362825),\n","  (13, 0.004850601474582849),\n","  (14, 0.20663562281722936),\n","  (15, 0.005626697710516105),\n","  (16, 0.012094166343293238),\n","  (17, 0.05646100116414436),\n","  (18, 0.01610399689561506),\n","  (19, 0.15230888630190145)],\n"," [(0, 0.046600877192982455),\n","  (1, 0.005543372319688109),\n","  (2, 0.01516812865497076),\n","  (3, 0.03782894736842105),\n","  (4, 0.006639863547758284),\n","  (5, 0.012365984405458089),\n","  (6, 0.09228801169590643),\n","  (7, 0.10629873294346978),\n","  (8, 0.00651803118908382),\n","  (9, 0.018944931773879143),\n","  (10, 0.02004142300194932),\n","  (11, 0.006761695906432748),\n","  (12, 0.1583211500974659),\n","  (13, 0.1356603313840156),\n","  (14, 0.025158382066276803),\n","  (15, 0.0417275828460039),\n","  (16, 0.22435428849902533),\n","  (17, 0.02905701754385965),\n","  (18, 0.005421539961013645),\n","  (19, 0.005299707602339181)],\n"," [(0, 0.04401041666666668),\n","  (1, 0.012413194444444445),\n","  (2, 0.011892361111111112),\n","  (3, 0.007031250000000001),\n","  (4, 0.009635416666666669),\n","  (5, 0.007204861111111112),\n","  (6, 0.014149305555555557),\n","  (7, 0.011024305555555558),\n","  (8, 0.00998263888888889),\n","  (9, 0.012239583333333335),\n","  (10, 0.016406250000000004),\n","  (11, 0.008940972222222223),\n","  (12, 0.047656250000000004),\n","  (13, 0.06276041666666668),\n","  (14, 0.04539930555555556),\n","  (15, 0.01762152777777778),\n","  (16, 0.5894965277777778),\n","  (17, 0.05737847222222223),\n","  (18, 0.005989583333333335),\n","  (19, 0.008767361111111113)],\n"," [(0, 0.17000786163522016),\n","  (1, 0.023257337526205454),\n","  (2, 0.06387578616352203),\n","  (3, 0.006747903563941301),\n","  (4, 0.02548480083857443),\n","  (5, 0.00714098532494759),\n","  (6, 0.07553721174004194),\n","  (7, 0.012644129979035643),\n","  (8, 0.0037342767295597493),\n","  (9, 0.015526729559748431),\n","  (10, 0.015133647798742142),\n","  (11, 0.0351808176100629),\n","  (12, 0.011071802935010484),\n","  (13, 0.4180424528301888),\n","  (14, 0.007009958071278828),\n","  (15, 0.00714098532494759),\n","  (16, 0.019457547169811323),\n","  (17, 0.019064465408805034),\n","  (18, 0.004389412997903565),\n","  (19, 0.059551886792452845)],\n"," [(0, 0.28478912438450016),\n","  (1, 0.006797259687433099),\n","  (2, 0.019535431385142372),\n","  (3, 0.021997430956968534),\n","  (4, 0.011614215371440806),\n","  (5, 0.00701134660672233),\n","  (6, 0.10848854634981804),\n","  (7, 0.07680368229501179),\n","  (8, 0.004870477413830016),\n","  (9, 0.009901520017126955),\n","  (10, 0.03056090772853779),\n","  (11, 0.004121173196317706),\n","  (12, 0.021141083279811606),\n","  (13, 0.2149967886962107),\n","  (14, 0.005833868550631558),\n","  (15, 0.0057268250909869416),\n","  (16, 0.14606080068507818),\n","  (17, 0.008509955041746951),\n","  (18, 0.005833868550631558),\n","  (19, 0.005405694712053094)],\n"," [(0, 0.028462879920279023),\n","  (1, 0.0072869955156950675),\n","  (2, 0.00903089187842551),\n","  (3, 0.02136273044344793),\n","  (4, 0.024476831091180866),\n","  (5, 0.011023916292974589),\n","  (6, 0.0074115595416043845),\n","  (7, 0.014138016940707524),\n","  (8, 0.033071748878923765),\n","  (9, 0.031826108619830595),\n","  (10, 0.021860986547085202),\n","  (11, 0.02485052316890882),\n","  (12, 0.2798330842052815),\n","  (13, 0.09373442949676133),\n","  (14, 0.033071748878923765),\n","  (15, 0.011023916292974589),\n","  (16, 0.24159192825112108),\n","  (17, 0.0762954658694569),\n","  (18, 0.007660687593423019),\n","  (19, 0.02198555057299452)],\n"," [(0, 0.11725589225589225),\n","  (1, 0.00765993265993266),\n","  (2, 0.05951178451178451),\n","  (3, 0.012878787878787878),\n","  (4, 0.012373737373737374),\n","  (5, 0.006986531986531986),\n","  (6, 0.1819023569023569),\n","  (7, 0.05614478114478114),\n","  (8, 0.005303030303030303),\n","  (9, 0.010858585858585859),\n","  (10, 0.00968013468013468),\n","  (11, 0.00968013468013468),\n","  (12, 0.00664983164983165),\n","  (13, 0.23526936026936027),\n","  (14, 0.012205387205387205),\n","  (15, 0.027525252525252526),\n","  (16, 0.16607744107744107),\n","  (17, 0.016582491582491584),\n","  (18, 0.025841750841750843),\n","  (19, 0.019612794612794613)],\n"," [(0, 0.21510122410546145),\n","  (1, 0.00417843691148776),\n","  (2, 0.031485404896421855),\n","  (3, 0.007591807909604522),\n","  (4, 0.013123822975517894),\n","  (5, 0.009357344632768364),\n","  (6, 0.3135004708097929),\n","  (7, 0.018302730696798497),\n","  (8, 0.007238700564971753),\n","  (9, 0.08433380414312619),\n","  (10, 0.010416666666666668),\n","  (11, 0.030308380414312625),\n","  (12, 0.08268596986817327),\n","  (13, 0.009710451977401132),\n","  (14, 0.008298022598870058),\n","  (15, 0.016890301318267423),\n","  (16, 0.06832627118644069),\n","  (17, 0.04972928436911489),\n","  (18, 0.010181261770244823),\n","  (19, 0.00923964218455744)],\n"," [(0, 0.5749851455733807),\n","  (1, 0.010398098633392746),\n","  (2, 0.010160427807486626),\n","  (3, 0.004337492572786689),\n","  (4, 0.007664884135472367),\n","  (5, 0.008972073677956027),\n","  (6, 0.015864527629233505),\n","  (7, 0.013487819370172306),\n","  (8, 0.0206179441473559),\n","  (9, 0.004337492572786689),\n","  (10, 0.018954248366013064),\n","  (11, 0.03380867498514556),\n","  (12, 0.034997029114676156),\n","  (13, 0.17023172905525838),\n","  (14, 0.005525846702317288),\n","  (15, 0.02382650029708852),\n","  (16, 0.013368983957219246),\n","  (17, 0.011705288175876407),\n","  (18, 0.009922756981580506),\n","  (19, 0.006833036244800948)],\n"," [(0, 0.5774853801169592),\n","  (1, 0.0061657767607424365),\n","  (2, 0.009852529875413173),\n","  (3, 0.04633867276887873),\n","  (4, 0.007564200355962371),\n","  (5, 0.005911517925247903),\n","  (6, 0.009725400457665906),\n","  (7, 0.01188660055936944),\n","  (8, 0.004131706076786169),\n","  (9, 0.012013729977116706),\n","  (10, 0.017988812611238244),\n","  (11, 0.007818459191456906),\n","  (12, 0.009725400457665906),\n","  (13, 0.009471141622171372),\n","  (14, 0.011123824052885839),\n","  (15, 0.007691329773709637),\n","  (16, 0.025235189422832448),\n","  (17, 0.11053902873124842),\n","  (18, 0.06858632087465041),\n","  (19, 0.04074497838799899)],\n"," [(0, 0.05964590964590965),\n","  (1, 0.010073260073260074),\n","  (2, 0.0952991452991453),\n","  (3, 0.008241758241758242),\n","  (4, 0.01166056166056166),\n","  (5, 0.005921855921855922),\n","  (6, 0.3553724053724054),\n","  (7, 0.0996947496947497),\n","  (8, 0.026923076923076925),\n","  (9, 0.0257020757020757),\n","  (10, 0.021184371184371185),\n","  (11, 0.004700854700854701),\n","  (12, 0.041086691086691086),\n","  (13, 0.060744810744810744),\n","  (14, 0.011538461538461539),\n","  (15, 0.07979242979242979),\n","  (16, 0.01166056166056166),\n","  (17, 0.02728937728937729),\n","  (18, 0.005189255189255189),\n","  (19, 0.03827838827838828)],\n"," [(0, 0.008349086326402019),\n","  (1, 0.0030980886368410002),\n","  (2, 0.00614366729678639),\n","  (3, 0.006563747111951272),\n","  (4, 0.004463348036126865),\n","  (5, 0.008349086326402019),\n","  (6, 0.5751417769376183),\n","  (7, 0.09604074774207101),\n","  (8, 0.00582860743541273),\n","  (9, 0.007193866834698593),\n","  (10, 0.015910522999369885),\n","  (11, 0.007823986557445916),\n","  (12, 0.0075089266960722545),\n","  (13, 0.020321361058601137),\n","  (14, 0.06085906322201219),\n","  (15, 0.060544003360638525),\n","  (16, 0.007193866834698593),\n","  (17, 0.013390044108380595),\n","  (18, 0.008349086326402019),\n","  (19, 0.0769271161520689)],\n"," [(0, 0.028580024067388687),\n","  (1, 0.04482551143200963),\n","  (2, 0.03680304853590052),\n","  (3, 0.018351383874849577),\n","  (4, 0.04502607300441235),\n","  (5, 0.008724428399518652),\n","  (6, 0.03720417168070598),\n","  (7, 0.04482551143200963),\n","  (8, 0.01313678299237866),\n","  (9, 0.01734857601283594),\n","  (10, 0.025571600481347774),\n","  (11, 0.016746891295627758),\n","  (12, 0.18722422783794626),\n","  (13, 0.17198154833533896),\n","  (14, 0.03700361010830325),\n","  (15, 0.03359406337745688),\n","  (16, 0.07069795427196149),\n","  (17, 0.04221821099077417),\n","  (18, 0.024769354191736863),\n","  (19, 0.095367027677497)],\n"," [(0, 0.013681592039800992),\n","  (1, 0.016169154228855717),\n","  (2, 0.03192371475953565),\n","  (3, 0.025408670931058985),\n","  (4, 0.03156834873252783),\n","  (5, 0.02979151859748874),\n","  (6, 0.09636342099028664),\n","  (7, 0.016642975598199477),\n","  (8, 0.0063373608149727536),\n","  (9, 0.010127931769722813),\n","  (10, 0.02126273394930111),\n","  (11, 0.17146410803127218),\n","  (12, 0.03441127694859038),\n","  (13, 0.06366974650556739),\n","  (14, 0.012615493958777537),\n","  (15, 0.008114190950011844),\n","  (16, 0.017116796967543233),\n","  (17, 0.27108505093579716),\n","  (18, 0.032397536128879405),\n","  (19, 0.08984837716180999)],\n"," [(0, 0.015925575528224538),\n","  (1, 0.013507831388626093),\n","  (2, 0.07111321349731947),\n","  (3, 0.006675076211500054),\n","  (4, 0.02023546725533481),\n","  (5, 0.009828655524019764),\n","  (6, 0.041574687270051515),\n","  (7, 0.008987701040681174),\n","  (8, 0.03358561967833492),\n","  (9, 0.0061494796594134355),\n","  (10, 0.050509828655524024),\n","  (11, 0.01403342794071271),\n","  (12, 0.011510564490696942),\n","  (13, 0.03600336381793336),\n","  (14, 0.027068222432460848),\n","  (15, 0.03116787553873647),\n","  (16, 0.021917376222011987),\n","  (17, 0.012666876905287503),\n","  (18, 0.5419426048565122),\n","  (19, 0.025596552086618315)],\n"," [(0, 0.019276956137024397),\n","  (1, 0.005498672734167614),\n","  (2, 0.01459992415623815),\n","  (3, 0.010175704714953862),\n","  (4, 0.005877891543420554),\n","  (5, 0.006004297813171533),\n","  (6, 0.011566173682214638),\n","  (7, 0.03735305271141449),\n","  (8, 0.010428517254455821),\n","  (9, 0.00714195424093035),\n","  (10, 0.0924661863228416),\n","  (11, 0.007647579319934269),\n","  (12, 0.058210087220326126),\n","  (13, 0.008279610668689166),\n","  (14, 0.018139299709265578),\n","  (15, 0.005372266464416635),\n","  (16, 0.030653520414612563),\n","  (17, 0.009038048287195045),\n","  (18, 0.6205915813424346),\n","  (19, 0.021678675262293008)],\n"," [(0, 0.006915866483767718),\n","  (1, 0.023148148148148147),\n","  (2, 0.011945587562871514),\n","  (3, 0.029778235025148606),\n","  (4, 0.025091449474165525),\n","  (5, 0.0578989483310471),\n","  (6, 0.015603566529492456),\n","  (7, 0.14077503429355281),\n","  (8, 0.013545953360768175),\n","  (9, 0.013203017832647462),\n","  (10, 0.055498399634202104),\n","  (11, 0.010230909922267948),\n","  (12, 0.02040466392318244),\n","  (13, 0.12717192501143118),\n","  (14, 0.006115683584819388),\n","  (15, 0.008173296753543667),\n","  (16, 0.012288523090992227),\n","  (17, 0.024291266575217192),\n","  (18, 0.38105852766346593),\n","  (19, 0.016860996799268405)],\n"," [(0, 0.012426900584795324),\n","  (1, 0.006446039340776185),\n","  (2, 0.011363636363636366),\n","  (3, 0.017344497607655506),\n","  (4, 0.011762360446570976),\n","  (5, 0.0060473152578415755),\n","  (6, 0.010433280170122277),\n","  (7, 0.006844763423710794),\n","  (8, 0.05801435406698566),\n","  (9, 0.016547049441786286),\n","  (10, 0.049508240297713994),\n","  (11, 0.006844763423710794),\n","  (12, 0.013490164805954283),\n","  (13, 0.24488304093567256),\n","  (14, 0.049242424242424254),\n","  (15, 0.017743221690590116),\n","  (16, 0.15211323763955345),\n","  (17, 0.006313131313131315),\n","  (18, 0.2653508771929825),\n","  (19, 0.03728070175438597)],\n"," [(0, 0.017053917187965444),\n","  (1, 0.08229073577599046),\n","  (2, 0.0070747691391123005),\n","  (3, 0.020479594876377714),\n","  (4, 0.011394101876675602),\n","  (5, 0.052353291629431024),\n","  (6, 0.009606791778373546),\n","  (7, 0.009904676794757221),\n","  (8, 0.008266309204647004),\n","  (9, 0.0037980339588918667),\n","  (10, 0.2623622281799225),\n","  (11, 0.030309800417039016),\n","  (12, 0.010649389335716412),\n","  (13, 0.029118260351504312),\n","  (14, 0.0373100983020554),\n","  (15, 0.009011021745606194),\n","  (16, 0.009011021745606194),\n","  (17, 0.04058683348227583),\n","  (18, 0.33191837950551084),\n","  (19, 0.017500744712540955)],\n"," [(0, 0.012723387723387724),\n","  (1, 0.010975135975135976),\n","  (2, 0.0135003885003885),\n","  (3, 0.009615384615384616),\n","  (4, 0.011363636363636364),\n","  (5, 0.006118881118881119),\n","  (6, 0.014471639471639472),\n","  (7, 0.00844988344988345),\n","  (8, 0.007478632478632479),\n","  (9, 0.007867132867132868),\n","  (10, 0.038947163947163944),\n","  (11, 0.008255633255633256),\n","  (12, 0.02904040404040404),\n","  (13, 0.2096930846930847),\n","  (14, 0.01486013986013986),\n","  (15, 0.011752136752136752),\n","  (16, 0.010586635586635586),\n","  (17, 0.038947163947163944),\n","  (18, 0.45522533022533024),\n","  (19, 0.08012820512820513)],\n"," [(0, 0.005548532097204664),\n","  (1, 0.006110408765276022),\n","  (2, 0.015662312122489114),\n","  (3, 0.01327433628318584),\n","  (4, 0.008919792105632814),\n","  (5, 0.014959966287399916),\n","  (6, 0.022404832139345413),\n","  (7, 0.009622137940722012),\n","  (8, 0.007515100435454418),\n","  (9, 0.005829470431240343),\n","  (10, 0.01004354544177553),\n","  (11, 0.11342885236690546),\n","  (12, 0.005127124596151145),\n","  (13, 0.07620452310717797),\n","  (14, 0.011729175445989606),\n","  (15, 0.007655569602472257),\n","  (16, 0.005969939598258182),\n","  (17, 0.012993397949150162),\n","  (18, 0.6362550920073043),\n","  (19, 0.010745891276864728)],\n"," [(0, 0.02150747238466537),\n","  (1, 0.010591293047433399),\n","  (2, 0.013190383365821963),\n","  (3, 0.010851202079272254),\n","  (4, 0.010201429499675113),\n","  (5, 0.07309941520467836),\n","  (6, 0.03372319688109162),\n","  (7, 0.008771929824561403),\n","  (8, 0.009941520467836258),\n","  (9, 0.005133203378817414),\n","  (10, 0.007472384665367121),\n","  (11, 0.14912280701754385),\n","  (12, 0.008901884340480832),\n","  (13, 0.13014944769330733),\n","  (14, 0.008512020792722548),\n","  (15, 0.11403508771929824),\n","  (16, 0.01397011046133853),\n","  (17, 0.010331384015594542),\n","  (18, 0.2810266406757635),\n","  (19, 0.07946718648473035)],\n"," [(0, 0.008710801393728224),\n","  (1, 0.014001806684733515),\n","  (2, 0.013227513227513229),\n","  (3, 0.010130339398632083),\n","  (4, 0.022648083623693385),\n","  (5, 0.011291779584462513),\n","  (6, 0.033488192024777395),\n","  (7, 0.006387921022067365),\n","  (8, 0.07155762033810815),\n","  (9, 0.007678410117434508),\n","  (10, 0.05065169699316042),\n","  (11, 0.005613627564847078),\n","  (12, 0.012324170860756229),\n","  (13, 0.0505226480836237),\n","  (14, 0.017098980513614664),\n","  (15, 0.021357594528326238),\n","  (16, 0.02226093689508324),\n","  (17, 0.14369596076913152),\n","  (18, 0.4260549748354627),\n","  (19, 0.051296941540843986)],\n"," [(0, 0.4386574074074074),\n","  (1, 0.014155982905982906),\n","  (2, 0.09410612535612535),\n","  (3, 0.01006054131054131),\n","  (4, 0.013621794871794872),\n","  (5, 0.01023860398860399),\n","  (6, 0.020032051282051284),\n","  (7, 0.05600071225071225),\n","  (8, 0.007923789173789175),\n","  (9, 0.03819444444444445),\n","  (10, 0.021100427350427352),\n","  (11, 0.018963675213675212),\n","  (12, 0.04941239316239316),\n","  (13, 0.05083689458689459),\n","  (14, 0.007211538461538462),\n","  (15, 0.014155982905982906),\n","  (16, 0.025551994301994303),\n","  (17, 0.03356481481481482),\n","  (18, 0.019675925925925927),\n","  (19, 0.05653490028490028)],\n"," [(0, 0.010239651416122004),\n","  (1, 0.0140159767610748),\n","  (2, 0.01822803195352215),\n","  (3, 0.009513435003631082),\n","  (4, 0.010820624546114743),\n","  (5, 0.009513435003631082),\n","  (6, 0.008932461873638345),\n","  (7, 0.021423384168482208),\n","  (8, 0.02868554829339143),\n","  (9, 0.03943355119825708),\n","  (10, 0.016775599128540306),\n","  (11, 0.42665214233841686),\n","  (12, 0.11975308641975309),\n","  (13, 0.027814088598402325),\n","  (14, 0.05816993464052288),\n","  (15, 0.006753812636165577),\n","  (16, 0.04306463326071169),\n","  (17, 0.036673928830791576),\n","  (18, 0.030137981118373274),\n","  (19, 0.06339869281045751)],\n"," [(0, 0.01588212782242633),\n","  (1, 0.020729684908789386),\n","  (2, 0.09982140579155505),\n","  (3, 0.025959943870391633),\n","  (4, 0.05440744992983799),\n","  (5, 0.057469064931751496),\n","  (6, 0.007972955734149764),\n","  (7, 0.007590253858910575),\n","  (8, 0.30303610154356425),\n","  (9, 0.01077943615257048),\n","  (10, 0.23172598545732875),\n","  (11, 0.022898328868478122),\n","  (12, 0.007207551983671387),\n","  (13, 0.012055109070034443),\n","  (14, 0.06269932389335374),\n","  (15, 0.011289705319556066),\n","  (16, 0.009631330526852914),\n","  (17, 0.013968618446230386),\n","  (18, 0.00873835948462814),\n","  (19, 0.01613726240591912)],\n"," [(0, 0.012950125168496054),\n","  (1, 0.007365684575389949),\n","  (2, 0.025370691315232046),\n","  (3, 0.12194300019256693),\n","  (4, 0.005440015405353361),\n","  (5, 0.032880801078374745),\n","  (6, 0.010928172539957638),\n","  (7, 0.07023878297708455),\n","  (8, 0.5454939341421144),\n","  (9, 0.03856152512998268),\n","  (10, 0.010928172539957638),\n","  (11, 0.007654534950895437),\n","  (12, 0.0167051800500674),\n","  (13, 0.02960716348931254),\n","  (14, 0.007076834199884461),\n","  (15, 0.007365684575389949),\n","  (16, 0.01978625072212594),\n","  (17, 0.01824571538609667),\n","  (18, 0.005247448488349703),\n","  (19, 0.006210283073367996)],\n"," [(0, 0.01563465579038438),\n","  (1, 0.0064886911383018175),\n","  (2, 0.04467927326659252),\n","  (3, 0.00920776171054258),\n","  (4, 0.019960449882585593),\n","  (5, 0.2672722778395749),\n","  (6, 0.011432455815103203),\n","  (7, 0.013904338153503894),\n","  (8, 0.017364973427264865),\n","  (9, 0.03009516747002843),\n","  (10, 0.018353726362625142),\n","  (11, 0.00438759115066123),\n","  (12, 0.3636756890372019),\n","  (13, 0.009331355827462615),\n","  (14, 0.00945494994438265),\n","  (15, 0.004634779384501298),\n","  (16, 0.08299344951180325),\n","  (17, 0.056173526140155736),\n","  (18, 0.006365097021381783),\n","  (19, 0.008589791125942407)],\n"," [(0, 0.023518942283244342),\n","  (1, 0.07106534452072209),\n","  (2, 0.04377489617764217),\n","  (3, 0.27218408339689804),\n","  (4, 0.015636918382913808),\n","  (5, 0.11157725230951776),\n","  (6, 0.008348165098737181),\n","  (7, 0.007500635647088736),\n","  (8, 0.15742859564369863),\n","  (9, 0.03877447241291635),\n","  (10, 0.019789812695991185),\n","  (11, 0.008771929824561403),\n","  (12, 0.007839647427748115),\n","  (13, 0.005551317908297313),\n","  (14, 0.01173828290533096),\n","  (15, 0.005720823798627002),\n","  (16, 0.09827103991863717),\n","  (17, 0.006907365030934825),\n","  (18, 0.029960166115772523),\n","  (19, 0.0556403085007204)],\n"," [(0, 0.014461626575028638),\n","  (1, 0.013507063764795723),\n","  (2, 0.024961817487590684),\n","  (3, 0.005488736158839252),\n","  (4, 0.005870561282932417),\n","  (5, 0.08462199312714777),\n","  (6, 0.052739595265368464),\n","  (7, 0.0082569683085147),\n","  (8, 0.4281691485299733),\n","  (9, 0.005679648720885834),\n","  (10, 0.054553264604811),\n","  (11, 0.015034364261168385),\n","  (12, 0.013029782359679267),\n","  (13, 0.12394998090874379),\n","  (14, 0.010929744177166857),\n","  (15, 0.006825124093165331),\n","  (16, 0.018184421534937),\n","  (17, 0.06810805651011836),\n","  (18, 0.015034364261168385),\n","  (19, 0.030593738067964872)],\n"," [(0, 0.01797385620915033),\n","  (1, 0.0044237207077953145),\n","  (2, 0.011836441893830705),\n","  (3, 0.01781444285031086),\n","  (4, 0.004742547425474255),\n","  (5, 0.05647218236888252),\n","  (6, 0.00569902757851108),\n","  (7, 0.03694404591104735),\n","  (8, 0.569623784473139),\n","  (9, 0.0260242308305436),\n","  (10, 0.06516021042563368),\n","  (11, 0.00673521441096764),\n","  (12, 0.010322014984855733),\n","  (13, 0.005938147616770286),\n","  (14, 0.008409054678782084),\n","  (15, 0.002989000478240077),\n","  (16, 0.01311174876454647),\n","  (17, 0.03614697911685),\n","  (18, 0.08006535947712419),\n","  (19, 0.019567989797545038)],\n"," [(0, 0.011512027491408937),\n","  (1, 0.03258877434135167),\n","  (2, 0.030068728522336774),\n","  (3, 0.11620847651775489),\n","  (4, 0.013917525773195879),\n","  (5, 0.035681557846506305),\n","  (6, 0.00807560137457045),\n","  (7, 0.008419243986254297),\n","  (8, 0.36454753722794964),\n","  (9, 0.041408934707903786),\n","  (10, 0.03877434135166095),\n","  (11, 0.009679266895761743),\n","  (12, 0.05744558991981673),\n","  (13, 0.03281786941580757),\n","  (14, 0.043699885452462776),\n","  (15, 0.005211912943871707),\n","  (16, 0.02812142038946163),\n","  (17, 0.03270332187857962),\n","  (18, 0.0663802978235968),\n","  (19, 0.022737686139747998)],\n"," [(0, 0.018616131191432395),\n","  (1, 0.009328982597054886),\n","  (2, 0.01443273092369478),\n","  (3, 0.13349230254350736),\n","  (4, 0.00589859437751004),\n","  (5, 0.44021921017402943),\n","  (6, 0.0038068942436412317),\n","  (7, 0.015353078982597055),\n","  (8, 0.20410809906291835),\n","  (9, 0.005229250334672021),\n","  (10, 0.0600317938420348),\n","  (11, 0.0043089022757697455),\n","  (12, 0.005563922356091031),\n","  (13, 0.00481091030789826),\n","  (14, 0.012257362784471218),\n","  (15, 0.004225234270414993),\n","  (16, 0.023552543507362786),\n","  (17, 0.004894578313253012),\n","  (18, 0.0115880187416332),\n","  (19, 0.018281459170013385)],\n"," [(0, 0.013142692085499714),\n","  (1, 0.024407856730213756),\n","  (2, 0.008328519160408243),\n","  (3, 0.038946658963989994),\n","  (4, 0.006691700365877144),\n","  (5, 0.5710090506450993),\n","  (6, 0.014490660504525327),\n","  (7, 0.006980550741382632),\n","  (8, 0.009291353745426537),\n","  (9, 0.011409589832466784),\n","  (10, 0.008328519160408243),\n","  (11, 0.07524552281917968),\n","  (12, 0.01545349508954362),\n","  (13, 0.006306566531869826),\n","  (14, 0.006499133448873485),\n","  (15, 0.0037069131523204324),\n","  (16, 0.03971692663200463),\n","  (17, 0.1172251107259773),\n","  (18, 0.011794723666474103),\n","  (19, 0.011024455998459467)],\n"," [(0, 0.004161262429744921),\n","  (1, 0.016374837872892352),\n","  (2, 0.017888024210981412),\n","  (3, 0.010105923043666237),\n","  (4, 0.008376567228707308),\n","  (5, 0.6051124081279724),\n","  (6, 0.006755296152183314),\n","  (7, 0.007944228274967576),\n","  (8, 0.21471033290099442),\n","  (9, 0.0056744487678339825),\n","  (10, 0.006755296152183314),\n","  (11, 0.00524210981409425),\n","  (12, 0.010105923043666237),\n","  (13, 0.013672719412019026),\n","  (14, 0.008268482490272375),\n","  (15, 0.003945092952875055),\n","  (16, 0.01626675313445742),\n","  (17, 0.007619974059662777),\n","  (18, 0.010430177258971034),\n","  (19, 0.020590142671854737)],\n"," [(0, 0.011731759006963367),\n","  (1, 0.11799878897971541),\n","  (2, 0.009915228580078716),\n","  (3, 0.043672419013018465),\n","  (4, 0.018543748107780804),\n","  (5, 0.009915228580078716),\n","  (6, 0.007644565546472903),\n","  (7, 0.007795943082046624),\n","  (8, 0.2746745382985165),\n","  (9, 0.016575840145322433),\n","  (10, 0.13207689978807144),\n","  (11, 0.005676657584014532),\n","  (12, 0.16204965183166817),\n","  (13, 0.05820466242809567),\n","  (14, 0.058961550105964274),\n","  (15, 0.006130790190735695),\n","  (16, 0.01430517711171662),\n","  (17, 0.018543748107780804),\n","  (18, 0.016273085074174992),\n","  (19, 0.009309718437783833)],\n"," [(0, 0.022793517546927832),\n","  (1, 0.021161245190626097),\n","  (2, 0.018013291360615604),\n","  (3, 0.04529555788737322),\n","  (4, 0.019295791069138396),\n","  (5, 0.05730441879445028),\n","  (6, 0.004488748979829778),\n","  (7, 0.004721930745015741),\n","  (8, 0.44788387548093744),\n","  (9, 0.010784656639850766),\n","  (10, 0.010434883992071821),\n","  (11, 0.009385566048734991),\n","  (12, 0.04494578523959427),\n","  (13, 0.005887839570945553),\n","  (14, 0.04751078465663986),\n","  (15, 0.008103066340212198),\n","  (16, 0.18951847965489102),\n","  (17, 0.006237612218724497),\n","  (18, 0.01684738253468579),\n","  (19, 0.009385566048734991)],\n"," [(0, 0.028270509977827044),\n","  (1, 0.011148066026114805),\n","  (2, 0.06966001478196598),\n","  (3, 0.012256713476225667),\n","  (4, 0.012256713476225667),\n","  (5, 0.039356984478935694),\n","  (6, 0.006713476225671346),\n","  (7, 0.0042498152254249804),\n","  (8, 0.43280364621828027),\n","  (9, 0.004742547425474254),\n","  (10, 0.029009608277900955),\n","  (11, 0.0387410692288741),\n","  (12, 0.01496674057649667),\n","  (13, 0.01151761517615176),\n","  (14, 0.18249568859324955),\n","  (15, 0.03393693027839369),\n","  (16, 0.039233801428923375),\n","  (17, 0.011148066026114805),\n","  (18, 0.006467110125646709),\n","  (19, 0.011024882976102487)],\n"," [(0, 0.007868722815342032),\n","  (1, 0.0075523922499011464),\n","  (2, 0.01435349940688019),\n","  (3, 0.006207987346777383),\n","  (4, 0.028509292210359825),\n","  (5, 0.003202846975088968),\n","  (6, 0.012455516014234875),\n","  (7, 0.012613681296955318),\n","  (8, 0.004547251878212732),\n","  (9, 0.031198102016607356),\n","  (10, 0.009450375642546461),\n","  (11, 0.0035191775405298536),\n","  (12, 0.05397390272835113),\n","  (13, 0.08086200079082641),\n","  (14, 0.027876631079478055),\n","  (15, 0.6566627125345986),\n","  (16, 0.012850929221035983),\n","  (17, 0.005417160933175168),\n","  (18, 0.005100830367734282),\n","  (19, 0.015776986951364175)],\n"," [(0, 0.051944143493439274),\n","  (1, 0.0060792103045624185),\n","  (2, 0.009088720356325991),\n","  (3, 0.16173107018177443),\n","  (4, 0.008366437943902735),\n","  (5, 0.010533285181172507),\n","  (6, 0.008246057541832191),\n","  (7, 0.10659684603346577),\n","  (8, 0.009209100758396534),\n","  (9, 0.008005296737691106),\n","  (10, 0.10045744552786808),\n","  (11, 0.011857469603948479),\n","  (12, 0.03376670278078729),\n","  (13, 0.12513542795232938),\n","  (14, 0.2092813289996389),\n","  (15, 0.06410256410256411),\n","  (16, 0.028469965089683403),\n","  (17, 0.011737089201877935),\n","  (18, 0.018478391717828342),\n","  (19, 0.016913446490911282)],\n"," [(0, 0.039073600454674624),\n","  (1, 0.0038363171355498722),\n","  (2, 0.013971772283792745),\n","  (3, 0.014729563322913707),\n","  (4, 0.0052571753339016765),\n","  (5, 0.004215212655110353),\n","  (6, 0.029411764705882353),\n","  (7, 0.014540115563133466),\n","  (8, 0.05280856303874207),\n","  (9, 0.05366107795775315),\n","  (10, 0.006014966373022639),\n","  (11, 0.0036468693757696316),\n","  (12, 0.06910107037984276),\n","  (13, 0.14080704745666384),\n","  (14, 0.035189921379179694),\n","  (15, 0.4416500899876859),\n","  (16, 0.00933030216917685),\n","  (17, 0.03954721985412522),\n","  (18, 0.008572511130055888),\n","  (19, 0.014634839443023587)],\n"," [(0, 0.01623646960865945),\n","  (1, 0.0043944860764178),\n","  (2, 0.004579517069109076),\n","  (3, 0.0422333240817837),\n","  (4, 0.010037931353501712),\n","  (5, 0.004949579054491627),\n","  (6, 0.04611897492830049),\n","  (7, 0.08636321583865297),\n","  (8, 0.003561846609307059),\n","  (9, 0.06887778702932741),\n","  (10, 0.012905911740216486),\n","  (11, 0.004672032565454714),\n","  (12, 0.019011934499028587),\n","  (13, 0.04500878897215284),\n","  (14, 0.017624202053844018),\n","  (15, 0.5206309556850772),\n","  (16, 0.010407993338884263),\n","  (17, 0.013090942732907762),\n","  (18, 0.00661485798871311),\n","  (19, 0.06267924877416968)],\n"," [(0, 0.014608859566446749),\n","  (1, 0.004031835794324013),\n","  (2, 0.025918944392082942),\n","  (3, 0.007906587077180857),\n","  (4, 0.010105770237721228),\n","  (5, 0.004660173840192691),\n","  (6, 0.024452822285056026),\n","  (7, 0.08602995078018641),\n","  (8, 0.004450727824903131),\n","  (9, 0.24971201172897686),\n","  (10, 0.006126295947219604),\n","  (11, 0.004346004817258352),\n","  (12, 0.07807100219918316),\n","  (13, 0.05513666352497644),\n","  (14, 0.03974238140119384),\n","  (15, 0.329929835584878),\n","  (16, 0.006440464970153943),\n","  (17, 0.01188606136768248),\n","  (18, 0.00486961985548225),\n","  (19, 0.03157398680490104)],\n"," [(0, 0.01612903225806452),\n","  (1, 0.006400409626216079),\n","  (2, 0.015207373271889403),\n","  (3, 0.007219662058371737),\n","  (4, 0.01049667178699437),\n","  (5, 0.01080389144905274),\n","  (6, 0.02657450076804916),\n","  (7, 0.018074756784434206),\n","  (8, 0.006400409626216079),\n","  (9, 0.21561699948796725),\n","  (10, 0.02657450076804916),\n","  (11, 0.005171530977982592),\n","  (12, 0.06282642089093704),\n","  (13, 0.02831541218637993),\n","  (14, 0.01940604198668715),\n","  (15, 0.3882744495647722),\n","  (16, 0.012237583205325142),\n","  (17, 0.10501792114695342),\n","  (18, 0.008755760368663596),\n","  (19, 0.01049667178699437)],\n"," [(0, 0.010913500404203717),\n","  (1, 0.013338722716248986),\n","  (2, 0.19298481990478752),\n","  (3, 0.011991376987334946),\n","  (4, 0.02277014281864726),\n","  (5, 0.005254648342764753),\n","  (6, 0.01657235246564268),\n","  (7, 0.01055420820982664),\n","  (8, 0.004895356148387675),\n","  (9, 0.008488278092158446),\n","  (10, 0.03651306925357046),\n","  (11, 0.028518817928680492),\n","  (12, 0.0042665948082277905),\n","  (13, 0.015853768076888528),\n","  (14, 0.023578550255995683),\n","  (15, 0.5121261115602262),\n","  (16, 0.011632084792957872),\n","  (17, 0.011991376987334946),\n","  (18, 0.04684271984191143),\n","  (19, 0.010913500404203717)],\n"," [(0, 0.008814814814814814),\n","  (1, 0.42333333333333323),\n","  (2, 0.03385185185185185),\n","  (3, 0.08659259259259258),\n","  (4, 0.007925925925925925),\n","  (5, 0.02777777777777777),\n","  (6, 0.0073333333333333315),\n","  (7, 0.017259259259259255),\n","  (8, 0.02555555555555555),\n","  (9, 0.016666666666666663),\n","  (10, 0.11725925925925923),\n","  (11, 0.0073333333333333315),\n","  (12, 0.019777777777777773),\n","  (13, 0.02777777777777777),\n","  (14, 0.04392592592592592),\n","  (15, 0.08807407407407405),\n","  (16, 0.00985185185185185),\n","  (17, 0.006592592592592591),\n","  (18, 0.015185185185185182),\n","  (19, 0.00911111111111111)],\n"," [(0, 0.0057344276094276085),\n","  (1, 0.008049242424242422),\n","  (2, 0.04887415824915824),\n","  (3, 0.05339856902356901),\n","  (4, 0.0054187710437710425),\n","  (5, 0.0047874579124579115),\n","  (6, 0.07286405723905723),\n","  (7, 0.009101430976430975),\n","  (8, 0.005629208754208754),\n","  (9, 0.004892676767676767),\n","  (10, 0.004577020202020201),\n","  (11, 0.013625841750841748),\n","  (12, 0.004682239057239057),\n","  (13, 0.012152777777777775),\n","  (14, 0.006681397306397305),\n","  (15, 0.6718749999999999),\n","  (16, 0.009522306397306396),\n","  (17, 0.004892676767676767),\n","  (18, 0.015835437710437706),\n","  (19, 0.037405303030303025)],\n"," [(0, 0.008894851918107734),\n","  (1, 0.016249254621347648),\n","  (2, 0.3390479030013914),\n","  (3, 0.020323991254223815),\n","  (4, 0.01088252832438879),\n","  (5, 0.005317034386801829),\n","  (6, 0.013168356191612007),\n","  (7, 0.01595110316040549),\n","  (8, 0.004422580003975354),\n","  (9, 0.005714569668058041),\n","  (10, 0.010087457761876369),\n","  (11, 0.021317829457364344),\n","  (12, 0.0040250447227191425),\n","  (13, 0.08154442456768039),\n","  (14, 0.08452593917710198),\n","  (15, 0.3060524746571259),\n","  (16, 0.024498111707414037),\n","  (17, 0.007205326972768834),\n","  (18, 0.008298548996223417),\n","  (19, 0.012472669449413637)],\n"," [(0, 0.19896296296296295),\n","  (1, 0.005185185185185185),\n","  (2, 0.14879012345679013),\n","  (3, 0.0057777777777777775),\n","  (4, 0.013975308641975309),\n","  (5, 0.007160493827160494),\n","  (6, 0.0036049382716049384),\n","  (7, 0.00637037037037037),\n","  (8, 0.0094320987654321),\n","  (9, 0.011506172839506173),\n","  (10, 0.005382716049382716),\n","  (11, 0.007061728395061729),\n","  (12, 0.148),\n","  (13, 0.09012345679012346),\n","  (14, 0.017333333333333333),\n","  (15, 0.0045925925925925926),\n","  (16, 0.00874074074074074),\n","  (17, 0.011209876543209877),\n","  (18, 0.04706172839506173),\n","  (19, 0.2497283950617284)],\n"," [(0, 0.07583687711263766),\n","  (1, 0.005833605931741359),\n","  (2, 0.6390251880929015),\n","  (3, 0.004961291026060408),\n","  (4, 0.006160724021371715),\n","  (5, 0.007142078290262785),\n","  (6, 0.011721731545087776),\n","  (7, 0.014556754988550866),\n","  (8, 0.005070330389270527),\n","  (9, 0.0055064878421110025),\n","  (10, 0.027096281757714533),\n","  (11, 0.006596881474212191),\n","  (12, 0.008341511285574092),\n","  (13, 0.07103914513139244),\n","  (14, 0.0286228328426562),\n","  (15, 0.0361465489041544),\n","  (16, 0.019245447606585978),\n","  (17, 0.005833605931741359),\n","  (18, 0.008123432559153855),\n","  (19, 0.013139243266819322)],\n"," [(0, 0.018115942028985508),\n","  (1, 0.005233494363929146),\n","  (2, 0.4242708892467347),\n","  (3, 0.10176239040973341),\n","  (4, 0.009706566469851494),\n","  (5, 0.0062175702272320625),\n","  (6, 0.010511719448917517),\n","  (7, 0.002460189658257291),\n","  (8, 0.005591340132402934),\n","  (9, 0.0035337269636786543),\n","  (10, 0.02276793701914475),\n","  (11, 0.0344873859366613),\n","  (12, 0.0036231884057971015),\n","  (13, 0.270218285918769),\n","  (14, 0.034576847378779745),\n","  (15, 0.004786187153336912),\n","  (16, 0.011137949543746644),\n","  (17, 0.005054571479692253),\n","  (18, 0.006128108785113616),\n","  (19, 0.019815709429235998)],\n"," [(0, 0.005845078116464547),\n","  (1, 0.008467169234130887),\n","  (2, 0.028023598820058997),\n","  (3, 0.09532393750682837),\n","  (4, 0.004534032557631378),\n","  (5, 0.00759313886157544),\n","  (6, 0.10045886594559161),\n","  (7, 0.011854036927783241),\n","  (8, 0.00901343821697804),\n","  (9, 0.0092319458101169),\n","  (10, 0.009559707199825193),\n","  (11, 0.005954331913033978),\n","  (12, 0.005735824319895116),\n","  (13, 0.02562001529553152),\n","  (14, 0.009887468589533486),\n","  (15, 0.633398885611275),\n","  (16, 0.0071561236752977165),\n","  (17, 0.00628209330274227),\n","  (18, 0.0036600021850759315),\n","  (19, 0.012400305910630394)],\n"," [(0, 0.020077007700770078),\n","  (1, 0.019306930693069307),\n","  (2, 0.12414741474147414),\n","  (3, 0.004675467546754675),\n","  (4, 0.4465896589658966),\n","  (5, 0.004455445544554455),\n","  (6, 0.012266226622662265),\n","  (7, 0.026347634763476346),\n","  (8, 0.004015401540154015),\n","  (9, 0.012706270627062706),\n","  (10, 0.021947194719471947),\n","  (11, 0.009955995599559956),\n","  (12, 0.026237623762376237),\n","  (13, 0.14251925192519252),\n","  (14, 0.05165016501650165),\n","  (15, 0.006545654565456546),\n","  (16, 0.010836083608360835),\n","  (17, 0.021837183718371837),\n","  (18, 0.006545654565456546),\n","  (19, 0.027337733773377337)],\n"," [(0, 0.012441488051244148),\n","  (1, 0.0076784101174345075),\n","  (2, 0.1642030056664203),\n","  (3, 0.012359366017902603),\n","  (4, 0.3631025704196436),\n","  (5, 0.009074484684240781),\n","  (6, 0.09357805699269114),\n","  (7, 0.03371109468670444),\n","  (8, 0.008663874517533055),\n","  (9, 0.0105526812843886),\n","  (10, 0.02180339985218034),\n","  (11, 0.013426952451342695),\n","  (12, 0.008171142317483782),\n","  (13, 0.13053297199638664),\n","  (14, 0.008171142317483782),\n","  (15, 0.009813582984314692),\n","  (16, 0.05613040978894637),\n","  (17, 0.006118091483945143),\n","  (18, 0.0076784101174345075),\n","  (19, 0.022788864252278888)],\n"," [(0, 0.03595261928595263),\n","  (1, 0.013763763763763768),\n","  (2, 0.4113279946613281),\n","  (3, 0.023106439773106443),\n","  (4, 0.07849516182849517),\n","  (5, 0.009092425759092427),\n","  (6, 0.05213546880213548),\n","  (7, 0.018935602268935606),\n","  (8, 0.009926593259926595),\n","  (9, 0.009759759759759762),\n","  (10, 0.02010343677010344),\n","  (11, 0.0159325992659326),\n","  (12, 0.03261594928261596),\n","  (13, 0.10452118785452122),\n","  (14, 0.019436102769436105),\n","  (15, 0.06097764431097766),\n","  (16, 0.015098431765098435),\n","  (17, 0.017934601267934606),\n","  (18, 0.018268268268268273),\n","  (19, 0.03261594928261596)],\n"," [(0, 0.011300236406619386),\n","  (1, 0.006477541371158392),\n","  (2, 0.22539007092198582),\n","  (3, 0.054515366430260045),\n","  (4, 0.013096926713947991),\n","  (5, 0.01706855791962175),\n","  (6, 0.010732860520094563),\n","  (7, 0.02387706855791962),\n","  (8, 0.015177304964539007),\n","  (9, 0.005626477541371158),\n","  (10, 0.026052009456264777),\n","  (11, 0.008747044917257684),\n","  (12, 0.01971631205673759),\n","  (13, 0.2721985815602837),\n","  (14, 0.18444444444444444),\n","  (15, 0.019527186761229314),\n","  (16, 0.01612293144208038),\n","  (17, 0.006666666666666667),\n","  (18, 0.004964539007092199),\n","  (19, 0.05829787234042553)],\n"," [(0, 0.01145427564459284),\n","  (1, 0.0061676868855499905),\n","  (2, 0.11746429233908366),\n","  (3, 0.022120200333889815),\n","  (4, 0.10160452606195511),\n","  (5, 0.010248562418846226),\n","  (6, 0.005703951029493601),\n","  (7, 0.07322389167130403),\n","  (8, 0.007558894453719161),\n","  (9, 0.005796698200704878),\n","  (10, 0.05439621591541458),\n","  (11, 0.005703951029493601),\n","  (12, 0.017111853088480802),\n","  (13, 0.192960489705064),\n","  (14, 0.22727694305323687),\n","  (15, 0.01340196624002968),\n","  (16, 0.0065386755703951025),\n","  (17, 0.03482656278983491),\n","  (18, 0.008486366165831942),\n","  (19, 0.0779539974030792)],\n"," [(0, 0.021928277483833043),\n","  (1, 0.022398589065255735),\n","  (2, 0.022281011169900063),\n","  (3, 0.027572016460905353),\n","  (4, 0.010523221634332747),\n","  (5, 0.0071134626690182255),\n","  (6, 0.050734861845972964),\n","  (7, 0.32786596119929456),\n","  (8, 0.006995884773662552),\n","  (9, 0.015579071134626692),\n","  (10, 0.17983539094650208),\n","  (11, 0.005820105820105821),\n","  (12, 0.020987654320987658),\n","  (13, 0.06213991769547326),\n","  (14, 0.11857730746619637),\n","  (15, 0.005467372134038802),\n","  (16, 0.056731334509112295),\n","  (17, 0.015579071134626692),\n","  (18, 0.012169312169312172),\n","  (19, 0.009700176366843035)],\n"," [(0, 0.009452563210392022),\n","  (1, 0.006205056831361634),\n","  (2, 0.024762236140106707),\n","  (3, 0.022790535838552544),\n","  (4, 0.014671769890976573),\n","  (5, 0.006205056831361634),\n","  (6, 0.12972628160519603),\n","  (7, 0.037172349802829976),\n","  (8, 0.007828810020876828),\n","  (9, 0.012236140106703782),\n","  (10, 0.007248898167478544),\n","  (11, 0.016759452563210396),\n","  (12, 0.028125724889816753),\n","  (13, 0.1316979819067502),\n","  (14, 0.4456622593365809),\n","  (15, 0.03589654372535375),\n","  (16, 0.006089074460681977),\n","  (17, 0.010728369287868246),\n","  (18, 0.008176757132915798),\n","  (19, 0.038564138250985855)],\n"," [(0, 0.0671583850931677),\n","  (1, 0.005909247757073844),\n","  (2, 0.0764751552795031),\n","  (3, 0.0032349896480331265),\n","  (4, 0.009446169772256728),\n","  (5, 0.004615251897860594),\n","  (6, 0.04559178743961353),\n","  (7, 0.044815389924085576),\n","  (8, 0.047489648033126296),\n","  (9, 0.012120427881297446),\n","  (10, 0.011344030365769496),\n","  (11, 0.005822981366459627),\n","  (12, 0.04533298826777088),\n","  (13, 0.07690648723257419),\n","  (14, 0.40842822636300896),\n","  (15, 0.019539337474120084),\n","  (16, 0.0180728088336784),\n","  (17, 0.030753968253968252),\n","  (18, 0.0196256038647343),\n","  (19, 0.047317115251897864)],\n"," [(0, 0.05516477790803698),\n","  (1, 0.00814120098997004),\n","  (2, 0.06428292301680343),\n","  (3, 0.019473752768008334),\n","  (4, 0.018561938257131688),\n","  (5, 0.013612088055229904),\n","  (6, 0.028591897876774776),\n","  (7, 0.019213234336329293),\n","  (8, 0.006838608831574832),\n","  (9, 0.007620164126611956),\n","  (10, 0.010746385306760451),\n","  (11, 0.017519864530415524),\n","  (12, 0.04760974338934478),\n","  (13, 0.24391038165950235),\n","  (14, 0.32206591116321476),\n","  (15, 0.013091051191871822),\n","  (16, 0.015565976292822714),\n","  (17, 0.07118666145629803),\n","  (18, 0.007229386479093394),\n","  (19, 0.009574052364204767)],\n"," [(0, 0.021459471766848814),\n","  (1, 0.02646857923497268),\n","  (2, 0.007912112932604735),\n","  (3, 0.011555100182149362),\n","  (4, 0.010188979963570127),\n","  (5, 0.004952185792349727),\n","  (6, 0.04673269581056466),\n","  (7, 0.012352003642987249),\n","  (8, 0.014401183970856102),\n","  (9, 0.010530510018214936),\n","  (10, 0.015198087431693989),\n","  (11, 0.00973360655737705),\n","  (12, 0.40681921675774135),\n","  (13, 0.09272540983606557),\n","  (14, 0.1655851548269581),\n","  (15, 0.013035063752276867),\n","  (16, 0.028403916211293262),\n","  (17, 0.03956056466302368),\n","  (18, 0.0327299635701275),\n","  (19, 0.029656193078324226)],\n"," [(0, 0.006583959081510502),\n","  (1, 0.005931004461856568),\n","  (2, 0.03542278811622592),\n","  (3, 0.004080966372837088),\n","  (4, 0.009957557949722494),\n","  (5, 0.04238763739253455),\n","  (6, 0.005278049842202634),\n","  (7, 0.011807596038741973),\n","  (8, 0.10050059854173468),\n","  (9, 0.006366307541625857),\n","  (10, 0.1287952987267385),\n","  (11, 0.006910436391337468),\n","  (12, 0.040211121993688106),\n","  (13, 0.3787680922842529),\n","  (14, 0.05784089672434432),\n","  (15, 0.004407443682664055),\n","  (16, 0.008542822940472304),\n","  (17, 0.006257481771683534),\n","  (18, 0.12226575253019915),\n","  (19, 0.01768418761562738)],\n"," [(0, 0.010984668879405723),\n","  (1, 0.26813655761024185),\n","  (2, 0.02125810020546863),\n","  (3, 0.08084400189663349),\n","  (4, 0.007349454717875771),\n","  (5, 0.03927611822348665),\n","  (6, 0.011300774458669198),\n","  (7, 0.01272324956535483),\n","  (8, 0.1159317211948791),\n","  (9, 0.012565196775723094),\n","  (10, 0.21345029239766083),\n","  (11, 0.009404140983088354),\n","  (12, 0.04259522680575313),\n","  (13, 0.050023707918444765),\n","  (14, 0.007823613086770983),\n","  (15, 0.009246088193456617),\n","  (16, 0.012881302354986567),\n","  (17, 0.030899320373004587),\n","  (18, 0.032954006638217175),\n","  (19, 0.010352457720878776)],\n"," [(0, 0.013108614232209739),\n","  (1, 0.01227632126508531),\n","  (2, 0.04931335830212235),\n","  (3, 0.16833125260091553),\n","  (4, 0.014218338188375642),\n","  (5, 0.19177417117492024),\n","  (6, 0.07136912193091968),\n","  (7, 0.04265501456512692),\n","  (8, 0.17984463864613678),\n","  (9, 0.01574420862810376),\n","  (10, 0.02850603412401165),\n","  (11, 0.024205853793868776),\n","  (12, 0.006727701484255792),\n","  (13, 0.06665279511721459),\n","  (14, 0.018795949507559993),\n","  (15, 0.01435705368289638),\n","  (16, 0.032390067970592316),\n","  (17, 0.02323484533222361),\n","  (18, 0.008669718407546123),\n","  (19, 0.017824941045914828)],\n"," [(0, 0.049280879864636216),\n","  (1, 0.0085307388606881),\n","  (2, 0.024323181049069376),\n","  (3, 0.0051466441060349696),\n","  (4, 0.01938804286520023),\n","  (5, 0.006274675690919346),\n","  (6, 0.013183869148336155),\n","  (7, 0.1058234630569656),\n","  (8, 0.014593908629441626),\n","  (9, 0.04871686407219403),\n","  (10, 0.006979695431472082),\n","  (11, 0.31309926677946986),\n","  (12, 0.1916948674562888),\n","  (13, 0.03644952058657643),\n","  (14, 0.027425267907501414),\n","  (15, 0.005287648054145517),\n","  (16, 0.051536943034404974),\n","  (17, 0.031091370558375638),\n","  (18, 0.010504794134235761),\n","  (19, 0.030668358714043997)],\n"," [(0, 0.007521527129370268),\n","  (1, 0.008351488743645608),\n","  (2, 0.01737732129888993),\n","  (3, 0.03366531797904348),\n","  (4, 0.012190061209669057),\n","  (5, 0.040823736902168284),\n","  (6, 0.008870214752567695),\n","  (7, 0.12911090362070757),\n","  (8, 0.004616661479406578),\n","  (9, 0.09342255420686794),\n","  (10, 0.04414358335926964),\n","  (11, 0.4289345367776741),\n","  (12, 0.007210291524017016),\n","  (13, 0.08740533250337174),\n","  (14, 0.010945118788256046),\n","  (15, 0.007417781927585851),\n","  (16, 0.02536570183629008),\n","  (17, 0.021008403361344543),\n","  (18, 0.004927897084759831),\n","  (19, 0.0066915655150949285)],\n"," [(0, 0.004469987228607918),\n","  (1, 0.3082326358188427),\n","  (2, 0.013016995775616465),\n","  (3, 0.07490912663326456),\n","  (4, 0.007122507122507123),\n","  (5, 0.19378131447096963),\n","  (6, 0.010167992926613616),\n","  (7, 0.007417231555162589),\n","  (8, 0.014392376461341979),\n","  (9, 0.006041850869437077),\n","  (10, 0.06970232832301798),\n","  (11, 0.10241674034777483),\n","  (12, 0.014195893506238334),\n","  (13, 0.10163080852736026),\n","  (14, 0.031584635032910896),\n","  (15, 0.003978779840848806),\n","  (16, 0.0034875724530896947),\n","  (17, 0.004469987228607918),\n","  (18, 0.016750171922585717),\n","  (19, 0.012231063955201887)],\n"," [(0, 0.0563185503099666),\n","  (1, 0.048783977110157355),\n","  (2, 0.017119694802098233),\n","  (3, 0.006151645207439197),\n","  (4, 0.011015736766809727),\n","  (5, 0.046208869814020025),\n","  (6, 0.01149260848831664),\n","  (7, 0.12651406771578444),\n","  (8, 0.00920362422508345),\n","  (9, 0.034668574153552685),\n","  (10, 0.020267048164043868),\n","  (11, 0.41263710061993314),\n","  (12, 0.05288507391511682),\n","  (13, 0.01902718168812589),\n","  (14, 0.0212207916070577),\n","  (15, 0.007773009060562707),\n","  (16, 0.018836432999523124),\n","  (17, 0.011015736766809727),\n","  (18, 0.026752503576537905),\n","  (19, 0.04210777300906056)],\n"," [(0, 0.08166388425153033),\n","  (1, 0.011083286959747727),\n","  (2, 0.04539974030792061),\n","  (3, 0.005611203858282322),\n","  (4, 0.011083286959747727),\n","  (5, 0.010897792617325172),\n","  (6, 0.014422185123353738),\n","  (7, 0.0063531812279725465),\n","  (8, 0.00431274346132443),\n","  (9, 0.15613986273418662),\n","  (10, 0.04781116675941384),\n","  (11, 0.21466332776850305),\n","  (12, 0.03250788350955296),\n","  (13, 0.03890743832313114),\n","  (14, 0.007837135967352997),\n","  (15, 0.005425709515859766),\n","  (16, 0.008764607679465776),\n","  (17, 0.012752736041550732),\n","  (18, 0.010712298274902615),\n","  (19, 0.2736505286588759)],\n"," [(0, 0.06819286478079128),\n","  (1, 0.09482842422474969),\n","  (2, 0.008311461067366578),\n","  (3, 0.015505006318654613),\n","  (4, 0.004131428015942452),\n","  (5, 0.00840867113832993),\n","  (6, 0.009477981918926801),\n","  (7, 0.008311461067366578),\n","  (8, 0.004617478370759211),\n","  (9, 0.023379022066686107),\n","  (10, 0.04311266647224653),\n","  (11, 0.5110819480898221),\n","  (12, 0.05302809371050841),\n","  (13, 0.004520268299795859),\n","  (14, 0.0051035287255759695),\n","  (15, 0.011227763196267133),\n","  (16, 0.005978419364246136),\n","  (17, 0.0818994847866239),\n","  (18, 0.023379022066686107),\n","  (19, 0.015505006318654613)],\n"," [(0, 0.005145103480171119),\n","  (1, 0.1740663660538791),\n","  (2, 0.012544802867383515),\n","  (3, 0.009307434385478092),\n","  (4, 0.006532547115273443),\n","  (5, 0.4264654873395769),\n","  (6, 0.010001156203029254),\n","  (7, 0.004798242571395538),\n","  (8, 0.09856630824372761),\n","  (9, 0.007919990750375768),\n","  (10, 0.00768875014452538),\n","  (11, 0.05983350676378773),\n","  (12, 0.007919990750375768),\n","  (13, 0.11290322580645162),\n","  (14, 0.0061856862064978615),\n","  (15, 0.006995028326974217),\n","  (16, 0.003526419239218407),\n","  (17, 0.022372528616024977),\n","  (18, 0.00768875014452538),\n","  (19, 0.00953867499132848)],\n"," [(0, 0.014205787219162514),\n","  (1, 0.06989247311827956),\n","  (2, 0.10800769298015561),\n","  (3, 0.01009703645423551),\n","  (4, 0.026444619284902528),\n","  (5, 0.33267768161552586),\n","  (6, 0.00939767462190751),\n","  (7, 0.013681265844916513),\n","  (8, 0.04375382463502055),\n","  (9, 0.007561849812046508),\n","  (10, 0.03448728035667453),\n","  (11, 0.14402482734504765),\n","  (12, 0.005900865460267506),\n","  (13, 0.02102456508436052),\n","  (14, 0.01035929714135851),\n","  (15, 0.004502141795611505),\n","  (16, 0.004152460879447505),\n","  (17, 0.02207360783285252),\n","  (18, 0.023822012413672523),\n","  (19, 0.0939330361045546)],\n"," [(0, 0.02954911433172303),\n","  (1, 0.06175523349436393),\n","  (2, 0.0393719806763285),\n","  (3, 0.009581320450885668),\n","  (4, 0.01392914653784219),\n","  (5, 0.014573268921095008),\n","  (6, 0.008615136876006442),\n","  (7, 0.02954911433172303),\n","  (8, 0.006038647342995169),\n","  (9, 0.08494363929146538),\n","  (10, 0.0107085346215781),\n","  (11, 0.33647342995169083),\n","  (12, 0.06884057971014493),\n","  (13, 0.02117552334943639),\n","  (14, 0.08735909822866345),\n","  (15, 0.019404186795491142),\n","  (16, 0.05789049919484702),\n","  (17, 0.02713365539452496),\n","  (18, 0.01618357487922705),\n","  (19, 0.056924315619967795)],\n"," [(0, 0.00990770901194354),\n","  (1, 0.010722041259500544),\n","  (2, 0.06790626131017012),\n","  (3, 0.024022801302931603),\n","  (4, 0.01117444806369888),\n","  (5, 0.0641060441549041),\n","  (6, 0.00846000723850887),\n","  (7, 0.005836047774158524),\n","  (8, 0.07405899384726748),\n","  (9, 0.0063789359391965275),\n","  (10, 0.014431777053926894),\n","  (11, 0.3707473760405357),\n","  (12, 0.010360115816141876),\n","  (13, 0.2061617806731814),\n","  (14, 0.04293340571842201),\n","  (15, 0.008369525877669202),\n","  (16, 0.015427072023163232),\n","  (17, 0.020403546869344917),\n","  (18, 0.010903003981179878),\n","  (19, 0.01768910604415491)],\n"," [(0, 0.013087867926577604),\n","  (1, 0.0046160530031497775),\n","  (2, 0.14526990333441947),\n","  (3, 0.05110242207016401),\n","  (4, 0.005919409145215597),\n","  (5, 0.04447702834799609),\n","  (6, 0.18437058759639405),\n","  (7, 0.008960573476702509),\n","  (8, 0.0072227652872814165),\n","  (9, 0.07879874008906267),\n","  (10, 0.030683175844466167),\n","  (11, 0.21119800152058216),\n","  (12, 0.00700553926360378),\n","  (13, 0.0196046486369067),\n","  (14, 0.012436189855544694),\n","  (15, 0.009069186488541327),\n","  (16, 0.004724666014988596),\n","  (17, 0.11963723254045834),\n","  (18, 0.017541001411969154),\n","  (19, 0.02427500814597589)],\n"," [(0, 0.015686842359301095),\n","  (1, 0.013756154068925575),\n","  (2, 0.1598127232358336),\n","  (3, 0.0065160729800173775),\n","  (4, 0.003716574958972875),\n","  (5, 0.10768413939569459),\n","  (6, 0.08007529684332466),\n","  (7, 0.018100202722270493),\n","  (8, 0.027367506516072986),\n","  (9, 0.007095279467130034),\n","  (10, 0.01964475335457091),\n","  (11, 0.3634038034559321),\n","  (12, 0.01723139299160151),\n","  (13, 0.014914567043150887),\n","  (14, 0.004971522347716962),\n","  (15, 0.03364224345979342),\n","  (16, 0.013563085239888023),\n","  (17, 0.07206294043826626),\n","  (18, 0.007384882710686361),\n","  (19, 0.013370016410850472)],\n"," [(0, 0.09037103352233165),\n","  (1, 0.0075101959614045566),\n","  (2, 0.008305978314930868),\n","  (3, 0.011290162140654533),\n","  (4, 0.006117576842733514),\n","  (5, 0.0072117775788321905),\n","  (6, 0.18128916741271264),\n","  (7, 0.1551278225405352),\n","  (8, 0.006117576842733514),\n","  (9, 0.007311250373022979),\n","  (10, 0.017059584203720286),\n","  (11, 0.02342584303193077),\n","  (12, 0.07505222321695018),\n","  (13, 0.04968666069829902),\n","  (14, 0.030587884213667565),\n","  (15, 0.03018999303690441),\n","  (16, 0.15264100268576547),\n","  (17, 0.1176265791306078),\n","  (18, 0.0078086143439769235),\n","  (19, 0.015269073908286086)],\n"," [(0, 0.018915698540363422),\n","  (1, 0.0117664581471552),\n","  (2, 0.013752358256379707),\n","  (3, 0.02338397378611856),\n","  (4, 0.013950948267302156),\n","  (5, 0.029341674113792078),\n","  (6, 0.08653559725945786),\n","  (7, 0.010574918081620496),\n","  (8, 0.006205937841326582),\n","  (9, 0.006205937841326582),\n","  (10, 0.009581968027008243),\n","  (11, 0.3607884023433622),\n","  (12, 0.006404527852249033),\n","  (13, 0.08137225697547415),\n","  (14, 0.006106642835865357),\n","  (15, 0.0741237215768047),\n","  (16, 0.013057293218151128),\n","  (17, 0.004517922748485752),\n","  (18, 0.1906960579882832),\n","  (19, 0.032717704299473745)],\n"," [(0, 0.010992627698788837),\n","  (1, 0.022182727751448136),\n","  (2, 0.007833070036861508),\n","  (3, 0.005989994734070565),\n","  (4, 0.01639020537124803),\n","  (5, 0.01639020537124803),\n","  (6, 0.028765139547130076),\n","  (7, 0.013362295945234337),\n","  (8, 0.0063849394418114816),\n","  (9, 0.011255924170616115),\n","  (10, 0.008096366508688784),\n","  (11, 0.12539494470774096),\n","  (12, 0.027711953659820966),\n","  (13, 0.134478672985782),\n","  (14, 0.007306477093206953),\n","  (15, 0.45701685097419703),\n","  (16, 0.008886255924170618),\n","  (17, 0.02823854660347552),\n","  (18, 0.013362295945234337),\n","  (19, 0.04996050552922592)],\n"," [(0, 0.012216139497161396),\n","  (1, 0.07527372262773724),\n","  (2, 0.023671938361719387),\n","  (3, 0.018603000811030012),\n","  (4, 0.00430859691808597),\n","  (5, 0.06493309002433091),\n","  (6, 0.07131995133819953),\n","  (7, 0.008465125709651258),\n","  (8, 0.006336171938361721),\n","  (9, 0.014243714517437146),\n","  (10, 0.004916869424168695),\n","  (11, 0.055606244931062455),\n","  (12, 0.006336171938361721),\n","  (13, 0.21831914030819144),\n","  (14, 0.018907137064071374),\n","  (15, 0.29871248986212495),\n","  (16, 0.02792984590429846),\n","  (17, 0.010898215733982159),\n","  (18, 0.014649229521492297),\n","  (19, 0.04435320356853204)],\n"," [(0, 0.007863640227833037),\n","  (1, 0.12450055258012413),\n","  (2, 0.020275439938791127),\n","  (3, 0.007353566267108732),\n","  (4, 0.006418430672447506),\n","  (5, 0.008883788149281648),\n","  (6, 0.07476834140950439),\n","  (7, 0.004973221117061975),\n","  (8, 0.0033579869081016753),\n","  (9, 0.008288701861769958),\n","  (10, 0.010839071665391484),\n","  (11, 0.11973986228003061),\n","  (12, 0.0035280115616764434),\n","  (13, 0.11047351866020574),\n","  (14, 0.004378134829550286),\n","  (15, 0.38990903681033756),\n","  (16, 0.007353566267108732),\n","  (17, 0.010924083992178868),\n","  (18, 0.0209555385530902),\n","  (19, 0.05521550624840602)],\n"," [(0, 0.005452402004126142),\n","  (1, 0.042784163473818644),\n","  (2, 0.00948030258375086),\n","  (3, 0.009676785538854505),\n","  (4, 0.022939385008350525),\n","  (5, 0.008694370763336281),\n","  (6, 0.018518518518518517),\n","  (7, 0.11607230572747815),\n","  (8, 0.0037822968857451618),\n","  (9, 0.10605167501719226),\n","  (10, 0.08728755280479418),\n","  (11, 0.33220355634148735),\n","  (12, 0.02274290205324688),\n","  (13, 0.06321839080459771),\n","  (14, 0.00535416052657432),\n","  (15, 0.08237547892720307),\n","  (16, 0.02382355830631693),\n","  (17, 0.011346890657235485),\n","  (18, 0.005059436093918852),\n","  (19, 0.02313586796345417)],\n"," [(0, 0.011865704286964129),\n","  (1, 0.17448600174978127),\n","  (2, 0.02553587051618548),\n","  (3, 0.014927821522309712),\n","  (4, 0.28833114610673666),\n","  (5, 0.05386045494313211),\n","  (6, 0.01634951881014873),\n","  (7, 0.01613079615048119),\n","  (8, 0.009569116360454943),\n","  (9, 0.022364391951006125),\n","  (10, 0.14539588801399825),\n","  (11, 0.017115048118985128),\n","  (12, 0.01755249343832021),\n","  (13, 0.020177165354330708),\n","  (14, 0.04653324584426947),\n","  (15, 0.005304024496937882),\n","  (16, 0.0501421697287839),\n","  (17, 0.01744313210848644),\n","  (18, 0.010772090988626422),\n","  (19, 0.03614391951006124)],\n"," [(0, 0.008669334154098695),\n","  (1, 0.06222427413563146),\n","  (2, 0.02159638863239971),\n","  (3, 0.004873294346978556),\n","  (4, 0.44408535959782486),\n","  (5, 0.017287370472966038),\n","  (6, 0.020775623268698057),\n","  (7, 0.08274340822817276),\n","  (8, 0.0057966553811429145),\n","  (9, 0.00969529085872576),\n","  (10, 0.10634041243459523),\n","  (11, 0.014619883040935668),\n","  (12, 0.041397353031702055),\n","  (13, 0.07412537190930542),\n","  (14, 0.013286139324920485),\n","  (15, 0.01113163024520365),\n","  (16, 0.016466605109264384),\n","  (17, 0.008874525495024108),\n","  (18, 0.005181081358366676),\n","  (19, 0.030829998974043288)],\n"," [(0, 0.018335027100271007),\n","  (1, 0.13562838753387538),\n","  (2, 0.006648035230352304),\n","  (3, 0.008680555555555558),\n","  (4, 0.15163448509485097),\n","  (5, 0.009612127371273715),\n","  (6, 0.005038956639566396),\n","  (7, 0.015286246612466126),\n","  (8, 0.14316565040650409),\n","  (9, 0.010713075880758809),\n","  (10, 0.0596629403794038),\n","  (11, 0.004615514905149052),\n","  (12, 0.21946985094850952),\n","  (13, 0.007494918699186993),\n","  (14, 0.027820121951219516),\n","  (15, 0.011475271002710029),\n","  (16, 0.07253556910569108),\n","  (17, 0.01283028455284553),\n","  (18, 0.030699525745257455),\n","  (19, 0.04865345528455285)],\n"," [(0, 0.0042134831460674165),\n","  (1, 0.2756450270495215),\n","  (2, 0.03178318768206409),\n","  (3, 0.008895131086142323),\n","  (4, 0.023252184769038706),\n","  (5, 0.007126508531002914),\n","  (6, 0.012536412817311696),\n","  (7, 0.1387328339575531),\n","  (8, 0.0066063254265501465),\n","  (9, 0.011912193091968375),\n","  (10, 0.16661464835622142),\n","  (11, 0.16089263420724098),\n","  (12, 0.016905950894714944),\n","  (13, 0.07058884727424054),\n","  (14, 0.005982105701206826),\n","  (15, 0.018778610070744904),\n","  (16, 0.014721181856013319),\n","  (17, 0.01357677902621723),\n","  (18, 0.006190178942987933),\n","  (19, 0.005045776113191844)],\n"," [(0, 0.0147136447611507),\n","  (1, 0.01062285563473212),\n","  (2, 0.07211665347057271),\n","  (3, 0.021575613618368964),\n","  (4, 0.05139878595935603),\n","  (5, 0.010095011876484561),\n","  (6, 0.15498812351543942),\n","  (7, 0.025138558986539983),\n","  (8, 0.0067959883874373185),\n","  (9, 0.008511480601741884),\n","  (10, 0.302652414885194),\n","  (11, 0.004420691475323304),\n","  (12, 0.024742676167854315),\n","  (13, 0.16752441277381894),\n","  (14, 0.01022697281604645),\n","  (15, 0.017748746371074164),\n","  (16, 0.03213248878332014),\n","  (17, 0.016297176035893375),\n","  (18, 0.017484824491950383),\n","  (19, 0.03081287938770124)],\n"," [(0, 0.011076316399907214),\n","  (1, 0.024414289028067734),\n","  (2, 0.011540245882625842),\n","  (3, 0.010612386917188587),\n","  (4, 0.2973208072372999),\n","  (5, 0.006437021572720946),\n","  (6, 0.020238923683600092),\n","  (7, 0.06744374855022037),\n","  (8, 0.00771282765019717),\n","  (9, 0.009336580839712364),\n","  (10, 0.22459986082115518),\n","  (11, 0.00527719786592438),\n","  (12, 0.07544653212711668),\n","  (13, 0.09122013453954998),\n","  (14, 0.005857109719322663),\n","  (15, 0.005509162607283693),\n","  (16, 0.03427279053583855),\n","  (17, 0.06408025980051033),\n","  (18, 0.013163999072141035),\n","  (19, 0.014439805149617258)],\n"," [(0, 0.022007722007722007),\n","  (1, 0.07872157872157873),\n","  (2, 0.05023595023595023),\n","  (3, 0.009652509652509652),\n","  (4, 0.27743457743457745),\n","  (5, 0.007764907764907765),\n","  (6, 0.034706134706134706),\n","  (7, 0.15765765765765766),\n","  (8, 0.004761904761904762),\n","  (9, 0.013256113256113256),\n","  (10, 0.11913341913341913),\n","  (11, 0.017117117117117116),\n","  (12, 0.05941655941655942),\n","  (13, 0.009395109395109395),\n","  (14, 0.04903474903474903),\n","  (15, 0.003818103818103818),\n","  (16, 0.025954525954525954),\n","  (17, 0.008365508365508366),\n","  (18, 0.017975117975117975),\n","  (19, 0.03359073359073359)],\n"," [(0, 0.006955663256147908),\n","  (1, 0.10989947944713696),\n","  (2, 0.006686411775264764),\n","  (3, 0.010814934482139651),\n","  (4, 0.01978998384491115),\n","  (5, 0.00426314844731646),\n","  (6, 0.006686411775264764),\n","  (7, 0.005968407826243045),\n","  (8, 0.007763417698797344),\n","  (9, 0.009289176090468497),\n","  (10, 0.023021001615508886),\n","  (11, 0.01584096212529169),\n","  (12, 0.1340423622329923),\n","  (13, 0.31892837910608507),\n","  (14, 0.05631843475139113),\n","  (15, 0.1989319691258302),\n","  (16, 0.010725183988511936),\n","  (17, 0.007224914737031054),\n","  (18, 0.009917429545862503),\n","  (19, 0.036932328127804705)],\n"," [(0, 0.014211886304909561),\n","  (1, 0.24812353882121324),\n","  (2, 0.013227513227513227),\n","  (3, 0.007321274763135229),\n","  (4, 0.014211886304909561),\n","  (5, 0.010643533899347853),\n","  (6, 0.10329764980927772),\n","  (7, 0.040420819490586936),\n","  (8, 0.005475575243017104),\n","  (9, 0.014581026208933185),\n","  (10, 0.02159468438538206),\n","  (11, 0.0438661252614741),\n","  (12, 0.07499692383413313),\n","  (13, 0.054078995939461055),\n","  (14, 0.007321274763135229),\n","  (15, 0.0068290882244370615),\n","  (16, 0.024793896886920144),\n","  (17, 0.11178786760182109),\n","  (18, 0.006090808416389812),\n","  (19, 0.17712563061400272)],\n"," [(0, 0.013171964615534174),\n","  (1, 0.36701662292213477),\n","  (2, 0.04427918732380675),\n","  (3, 0.017546417808885002),\n","  (4, 0.009089141635073395),\n","  (5, 0.038252162924078945),\n","  (6, 0.041557305336832905),\n","  (7, 0.006756099931952952),\n","  (8, 0.04389034703995335),\n","  (9, 0.010158452415670266),\n","  (10, 0.3008165645960922),\n","  (11, 0.016379896957324783),\n","  (12, 0.008894721493146691),\n","  (13, 0.026781374550403428),\n","  (14, 0.008603091280256636),\n","  (15, 0.00510352872557597),\n","  (16, 0.012880334402644118),\n","  (17, 0.007922620783513174),\n","  (18, 0.007242150286769711),\n","  (19, 0.013658014970350931)],\n"," [(0, 0.013822179528231402),\n","  (1, 0.21395026150069377),\n","  (2, 0.005710321272280926),\n","  (3, 0.022574447646493755),\n","  (4, 0.005710321272280926),\n","  (5, 0.09995730600917921),\n","  (6, 0.007845020813320525),\n","  (7, 0.005176646387021026),\n","  (8, 0.2133098516383819),\n","  (9, 0.006243996157540826),\n","  (10, 0.20818657273988686),\n","  (11, 0.0067776710428007255),\n","  (12, 0.005817056249332906),\n","  (13, 0.12013021667200341),\n","  (14, 0.024922617141637314),\n","  (15, 0.0046429715017611275),\n","  (16, 0.0164905539545309),\n","  (17, 0.0038958266623972675),\n","  (18, 0.004963176432917067),\n","  (19, 0.009872985377308144)],\n"," [(0, 0.006495098039215688),\n","  (1, 0.3913807189542485),\n","  (2, 0.02961601307189543),\n","  (3, 0.010743464052287583),\n","  (4, 0.046936274509803935),\n","  (5, 0.047916666666666684),\n","  (6, 0.009191176470588237),\n","  (7, 0.005514705882352942),\n","  (8, 0.11883169934640525),\n","  (9, 0.00829248366013072),\n","  (10, 0.18917483660130724),\n","  (11, 0.004697712418300654),\n","  (12, 0.007148692810457518),\n","  (13, 0.043096405228758176),\n","  (14, 0.020710784313725493),\n","  (15, 0.004616013071895426),\n","  (16, 0.007393790849673204),\n","  (17, 0.028880718954248374),\n","  (18, 0.012867647058823532),\n","  (19, 0.006495098039215688)],\n"," [(0, 0.009962005374849413),\n","  (1, 0.6699564451858031),\n","  (2, 0.045269205819664544),\n","  (3, 0.012649430080622743),\n","  (4, 0.013854137707348719),\n","  (5, 0.04378648874061719),\n","  (6, 0.009869335557408954),\n","  (7, 0.023769808173477902),\n","  (8, 0.018394958761931243),\n","  (9, 0.015522194421276992),\n","  (10, 0.020619034380502273),\n","  (11, 0.005977203224909648),\n","  (12, 0.019970345658419056),\n","  (13, 0.026364563061810772),\n","  (14, 0.006996571216754704),\n","  (15, 0.006347882494671486),\n","  (16, 0.015892873691038832),\n","  (17, 0.021175053285145032),\n","  (18, 0.0056991937725882685),\n","  (19, 0.007923269391159301)],\n"," [(0, 0.007517594369801661),\n","  (1, 0.5863190445724034),\n","  (2, 0.010716570697376838),\n","  (3, 0.008370654723821709),\n","  (4, 0.008903817445084238),\n","  (5, 0.013062486670931964),\n","  (6, 0.006877799104286627),\n","  (7, 0.013062486670931964),\n","  (8, 0.02169972275538494),\n","  (9, 0.004851780763489016),\n","  (10, 0.11852207293666024),\n","  (11, 0.0058114736617615684),\n","  (12, 0.13142461079121345),\n","  (13, 0.005704841117509063),\n","  (14, 0.007091064192791638),\n","  (15, 0.00474514821923651),\n","  (16, 0.013702281936447),\n","  (17, 0.006451268927276603),\n","  (18, 0.011462998507144379),\n","  (19, 0.013702281936447)],\n"," [(0, 0.01625226625226625),\n","  (1, 0.008223258223258223),\n","  (2, 0.01961926961926962),\n","  (3, 0.05121730121730122),\n","  (4, 0.030756280756280757),\n","  (5, 0.013403263403263404),\n","  (6, 0.3407795907795908),\n","  (7, 0.00938875938875939),\n","  (8, 0.016640766640766642),\n","  (9, 0.01159026159026159),\n","  (10, 0.031662781662781664),\n","  (11, 0.007316757316757317),\n","  (12, 0.03917378917378917),\n","  (13, 0.04448329448329448),\n","  (14, 0.024151774151774153),\n","  (15, 0.05082880082880083),\n","  (16, 0.010036260036260037),\n","  (17, 0.006928256928256929),\n","  (18, 0.23925148925148926),\n","  (19, 0.028295778295778295)],\n"," [(0, 0.011369938990571269),\n","  (1, 0.05426141615825475),\n","  (2, 0.05555555555555554),\n","  (3, 0.06553891662044739),\n","  (4, 0.016176742466259934),\n","  (5, 0.01636161952301719),\n","  (6, 0.28276945831022365),\n","  (7, 0.020613791828434088),\n","  (8, 0.017655758920317986),\n","  (9, 0.011554816047328526),\n","  (10, 0.060547236088001465),\n","  (11, 0.15261601035311514),\n","  (12, 0.016546496579774448),\n","  (13, 0.08569051580698833),\n","  (14, 0.007117766685154371),\n","  (15, 0.018210390090589754),\n","  (16, 0.014882603068959138),\n","  (17, 0.012109447217600293),\n","  (18, 0.010630430763542243),\n","  (19, 0.06979108892586429)],\n"," [(0, 0.007488479262672809),\n","  (1, 0.01260880696364567),\n","  (2, 0.12295186891961082),\n","  (3, 0.36335125448028666),\n","  (4, 0.010432667690732205),\n","  (5, 0.06022785458269328),\n","  (6, 0.16762672811059906),\n","  (7, 0.015809011776753708),\n","  (8, 0.02361751152073732),\n","  (9, 0.007488479262672809),\n","  (10, 0.03936251920122887),\n","  (11, 0.005312339989759344),\n","  (12, 0.009792626728110597),\n","  (13, 0.08429339477726573),\n","  (14, 0.012096774193548383),\n","  (15, 0.013120839733742956),\n","  (16, 0.018113159242191496),\n","  (17, 0.005184331797235022),\n","  (18, 0.012352790578597027),\n","  (19, 0.008768561187916024)],\n"," [(0, 0.022598093142647598),\n","  (1, 0.011505317198386506),\n","  (2, 0.11143197653098644),\n","  (3, 0.010863586358635864),\n","  (4, 0.029657132379904656),\n","  (5, 0.01563072973964063),\n","  (6, 0.05037587092042538),\n","  (7, 0.22501833516685002),\n","  (8, 0.005454712137880455),\n","  (9, 0.09107994132746608),\n","  (10, 0.07888705537220389),\n","  (11, 0.01123028969563623),\n","  (12, 0.06926109277594426),\n","  (13, 0.1139072240557389),\n","  (14, 0.012147048038137147),\n","  (15, 0.013705537220388706),\n","  (16, 0.06614411441144115),\n","  (17, 0.031582324899156584),\n","  (18, 0.010313531353135313),\n","  (19, 0.019206087275394208)],\n"," [(0, 0.017925676308153143),\n","  (1, 0.04497800243377329),\n","  (2, 0.009594683141439672),\n","  (3, 0.07296639520733877),\n","  (4, 0.015959936347467944),\n","  (5, 0.006692876532809137),\n","  (6, 0.20495179256763085),\n","  (7, 0.10750725451652159),\n","  (8, 0.005569596555274736),\n","  (9, 0.16760273331461203),\n","  (10, 0.14036319385940282),\n","  (11, 0.011092389778152207),\n","  (12, 0.013900589721988207),\n","  (13, 0.021576336235139944),\n","  (14, 0.020078629598427412),\n","  (15, 0.05995506880089863),\n","  (16, 0.008003369839932604),\n","  (17, 0.042731442478704494),\n","  (18, 0.021389122905550877),\n","  (19, 0.0071609098567818035)],\n"," [(0, 0.049533799533799536),\n","  (1, 0.09913234913234913),\n","  (2, 0.04344729344729345),\n","  (3, 0.05665630665630666),\n","  (4, 0.01418026418026418),\n","  (5, 0.028554778554778556),\n","  (6, 0.08954933954933955),\n","  (7, 0.0696063196063196),\n","  (8, 0.006669256669256669),\n","  (9, 0.04046879046879047),\n","  (10, 0.06054131054131054),\n","  (11, 0.01948976948976949),\n","  (12, 0.04020979020979021),\n","  (13, 0.0696063196063196),\n","  (14, 0.050699300699300696),\n","  (15, 0.10508935508935509),\n","  (16, 0.05380730380730381),\n","  (17, 0.028425278425278425),\n","  (18, 0.03891478891478892),\n","  (19, 0.03541828541828542)],\n"," [(0, 0.03071500503524672),\n","  (1, 0.006433926373503411),\n","  (2, 0.035190779903770834),\n","  (3, 0.010126440640035804),\n","  (4, 0.007776658834060645),\n","  (5, 0.004755510797806869),\n","  (6, 0.059136175450374834),\n","  (7, 0.5290925366454067),\n","  (8, 0.004531722054380664),\n","  (9, 0.005874454514937897),\n","  (10, 0.013259483048002682),\n","  (11, 0.005762560143224795),\n","  (12, 0.010462123755175114),\n","  (13, 0.12090186863600758),\n","  (14, 0.006545820745216514),\n","  (15, 0.004867405169519973),\n","  (16, 0.09829920554996081),\n","  (17, 0.012476222446010963),\n","  (18, 0.005203088284659281),\n","  (19, 0.028589011972697767)],\n"," [(0, 0.014633219308247814),\n","  (1, 0.015773470163435957),\n","  (2, 0.06189028252882301),\n","  (3, 0.026669200557455974),\n","  (4, 0.009692132269099202),\n","  (5, 0.01780058279488154),\n","  (6, 0.08558216140884328),\n","  (7, 0.007158241479792221),\n","  (8, 0.02299505891296085),\n","  (9, 0.01628024832129735),\n","  (10, 0.014126441150386419),\n","  (11, 0.010578994045356645),\n","  (12, 0.015900164702901304),\n","  (13, 0.09191688838211073),\n","  (14, 0.08102115798809072),\n","  (15, 0.04554668693779298),\n","  (16, 0.017167110097554797),\n","  (17, 0.1347396427213987),\n","  (18, 0.01906752818953503),\n","  (19, 0.2914607880400355)],\n"," [(0, 0.013227513227513227),\n","  (1, 0.010889627168696936),\n","  (2, 0.10575858250276855),\n","  (3, 0.01556539928632952),\n","  (4, 0.010520487264673311),\n","  (5, 0.010274393995324227),\n","  (6, 0.13935031376891843),\n","  (7, 0.34606866002214837),\n","  (8, 0.010766580534022395),\n","  (9, 0.02147163775070752),\n","  (10, 0.04755752430171035),\n","  (11, 0.02097945121200935),\n","  (12, 0.029469669004552727),\n","  (13, 0.008305647840531562),\n","  (14, 0.006336901685738895),\n","  (15, 0.008674787744555186),\n","  (16, 0.12175464501045896),\n","  (17, 0.00670604158976252),\n","  (18, 0.029592715639227267),\n","  (19, 0.036729420450350686)],\n"," [(0, 0.008502024291497974),\n","  (1, 0.030634278002699047),\n","  (2, 0.09757085020242912),\n","  (3, 0.24071075123706698),\n","  (4, 0.0074224021592442626),\n","  (5, 0.09037336932073772),\n","  (6, 0.1344579397210976),\n","  (7, 0.0191183085919928),\n","  (8, 0.03891138101664417),\n","  (9, 0.006342780026990551),\n","  (10, 0.012280701754385961),\n","  (11, 0.009221772379667115),\n","  (12, 0.008861898335582544),\n","  (13, 0.13319838056680158),\n","  (14, 0.00832208726945569),\n","  (15, 0.05789473684210525),\n","  (16, 0.009041835357624829),\n","  (17, 0.006612685560053979),\n","  (18, 0.006702654071075122),\n","  (19, 0.07381916329284749)],\n"," [(0, 0.016142787524366476),\n","  (1, 0.22167397660818716),\n","  (2, 0.017726608187134507),\n","  (3, 0.12615740740740744),\n","  (4, 0.0060307017543859654),\n","  (5, 0.18354044834307995),\n","  (6, 0.008711013645224173),\n","  (7, 0.008223684210526317),\n","  (8, 0.021747076023391817),\n","  (9, 0.0089546783625731),\n","  (10, 0.014315302144249514),\n","  (11, 0.20169346978557506),\n","  (12, 0.057443957115009756),\n","  (13, 0.036976120857699815),\n","  (14, 0.015046296296296297),\n","  (15, 0.015046296296296297),\n","  (16, 0.010051169590643276),\n","  (17, 0.011269493177387915),\n","  (18, 0.007249025341130605),\n","  (19, 0.012000487329434699)],\n"," [(0, 0.028939108600125548),\n","  (1, 0.007344632768361582),\n","  (2, 0.009353421217827998),\n","  (3, 0.005586942875078468),\n","  (4, 0.1819836785938481),\n","  (5, 0.016635279347143754),\n","  (6, 0.020527306967984934),\n","  (7, 0.006089139987445072),\n","  (8, 0.015756434400502197),\n","  (9, 0.00709353421217828),\n","  (10, 0.024795982423101066),\n","  (11, 0.007721280602636535),\n","  (12, 0.09560577526679222),\n","  (13, 0.008223477715003138),\n","  (14, 0.07225360954174513),\n","  (15, 0.011487758945386064),\n","  (16, 0.01374764595103578),\n","  (17, 0.008976773383553044),\n","  (18, 0.2137476459510358),\n","  (19, 0.2441305712492153)],\n"," [(0, 0.016040203772545784),\n","  (1, 0.007503786314195237),\n","  (2, 0.03641745835054386),\n","  (3, 0.32114828583230076),\n","  (4, 0.02870714580751756),\n","  (5, 0.005438524025884622),\n","  (6, 0.008192207076965443),\n","  (7, 0.010119785212722018),\n","  (8, 0.007641470466749278),\n","  (9, 0.02870714580751756),\n","  (10, 0.0697370232686218),\n","  (11, 0.008192207076965443),\n","  (12, 0.16749277158199094),\n","  (13, 0.032149249621368586),\n","  (14, 0.017554729450640235),\n","  (15, 0.03132314470604434),\n","  (16, 0.04316398182569187),\n","  (17, 0.10677406030565884),\n","  (18, 0.03669282665565194),\n","  (19, 0.01700399284042407)],\n"," [(0, 0.02105792726750036),\n","  (1, 0.010277418427483111),\n","  (2, 0.012002299841885871),\n","  (3, 0.04376886589047003),\n","  (4, 0.013583441138421734),\n","  (5, 0.007546356188012074),\n","  (6, 0.020914187149633463),\n","  (7, 0.013583441138421734),\n","  (8, 0.010421158545350008),\n","  (9, 0.008696277130947247),\n","  (10, 0.014445881845623114),\n","  (11, 0.053399453787552106),\n","  (12, 0.02105792726750036),\n","  (13, 0.0407503234152652),\n","  (14, 0.01760816443869484),\n","  (15, 0.08904700301854247),\n","  (16, 0.00855253701308035),\n","  (17, 0.013439701020554837),\n","  (18, 0.5370849504096593),\n","  (19, 0.042762685065401755)],\n"," [(0, 0.0051882942753876645),\n","  (1, 0.011950565465780575),\n","  (2, 0.006587384866503439),\n","  (3, 0.017430336947650695),\n","  (4, 0.010668065757257782),\n","  (5, 0.011600792818001632),\n","  (6, 0.0053048851579806456),\n","  (7, 0.006237612218724496),\n","  (8, 0.011950565465780575),\n","  (9, 0.007170339279468346),\n","  (10, 0.020578290777661188),\n","  (11, 0.014632155765419144),\n","  (12, 0.01603124635653492),\n","  (13, 0.010901247522443746),\n","  (14, 0.009968520461699895),\n","  (15, 0.006937157514282383),\n","  (16, 0.010668065757257782),\n","  (17, 0.16165325871516847),\n","  (18, 0.6332633788037776),\n","  (19, 0.021277836073219075)],\n"," [(0, 0.007074695623560382),\n","  (1, 0.0981134145003839),\n","  (2, 0.008610288472085116),\n","  (3, 0.08231874520127234),\n","  (4, 0.016178567511242734),\n","  (5, 0.24487221673796206),\n","  (6, 0.0057584731819677526),\n","  (7, 0.008061862454754853),\n","  (8, 0.10820445321926073),\n","  (9, 0.004442250740375123),\n","  (10, 0.03789623779752111),\n","  (11, 0.10612043435340572),\n","  (12, 0.007403751233958539),\n","  (13, 0.014423604255785894),\n","  (14, 0.01146210376220248),\n","  (15, 0.003893824723044861),\n","  (16, 0.01288801140726116),\n","  (17, 0.00608752879236591),\n","  (18, 0.20450806186245477),\n","  (19, 0.011681474169134584)],\n"," [(0, 0.014932824981844589),\n","  (1, 0.10035403050108932),\n","  (2, 0.0054920116194626),\n","  (3, 0.04007806826434277),\n","  (4, 0.0054920116194626),\n","  (5, 0.14610566448801743),\n","  (6, 0.004311909949164851),\n","  (7, 0.0185639070442992),\n","  (8, 0.15046296296296297),\n","  (9, 0.00721677559912854),\n","  (10, 0.0381717501815541),\n","  (11, 0.06921750181554102),\n","  (12, 0.018927015250544663),\n","  (13, 0.00875998547567175),\n","  (14, 0.003676470588235294),\n","  (15, 0.004856572258533043),\n","  (16, 0.03227124183006536),\n","  (17, 0.007761437908496732),\n","  (18, 0.30741648511256353),\n","  (19, 0.015931372549019607)],\n"," [(0, 0.005492424242424242),\n","  (1, 0.006755050505050505),\n","  (2, 0.014835858585858586),\n","  (3, 0.007891414141414142),\n","  (4, 0.043244949494949496),\n","  (5, 0.007891414141414142),\n","  (6, 0.027714646464646466),\n","  (7, 0.005744949494949495),\n","  (8, 0.008522727272727272),\n","  (9, 0.004987373737373737),\n","  (10, 0.01029040404040404),\n","  (11, 0.02821969696969697),\n","  (12, 0.008396464646464647),\n","  (13, 0.044255050505050506),\n","  (14, 0.03756313131313131),\n","  (15, 0.12152777777777778),\n","  (16, 0.00827020202020202),\n","  (17, 0.022032828282828282),\n","  (18, 0.5730429292929293),\n","  (19, 0.013320707070707071)],\n"," [(0, 0.01845209628566874),\n","  (1, 0.003945480631276901),\n","  (2, 0.028813964610234336),\n","  (3, 0.004344014028375578),\n","  (4, 0.012792922046867527),\n","  (5, 0.005539614219671608),\n","  (6, 0.12334608640204049),\n","  (7, 0.004184600669536107),\n","  (8, 0.0031484138370795473),\n","  (9, 0.004981667463733461),\n","  (10, 0.029690738083851427),\n","  (11, 0.060696636378128485),\n","  (12, 0.00809022796110314),\n","  (13, 0.17499601466602902),\n","  (14, 0.01980710983580424),\n","  (15, 0.022676550294914714),\n","  (16, 0.005300494181412403),\n","  (17, 0.0339151920930974),\n","  (18, 0.3248445719751315),\n","  (19, 0.11043360433604336)],\n"," [(0, 0.01065989847715736),\n","  (1, 0.04179357021996616),\n","  (2, 0.0419063733784546),\n","  (3, 0.10462492949802595),\n","  (4, 0.047659334461364916),\n","  (5, 0.006598984771573604),\n","  (6, 0.03942470389170897),\n","  (7, 0.03908629441624366),\n","  (8, 0.004455724760293288),\n","  (9, 0.004794134235758601),\n","  (10, 0.1506486181613085),\n","  (11, 0.008403835307388608),\n","  (12, 0.11477721376198534),\n","  (13, 0.005132543711223915),\n","  (14, 0.004230118443316413),\n","  (15, 0.006147772137619853),\n","  (16, 0.012013536379018613),\n","  (17, 0.0194585448392555),\n","  (18, 0.32831359278059785),\n","  (19, 0.009870276367738297)],\n"," [(0, 0.01489309108886417),\n","  (1, 0.00710666172290199),\n","  (2, 0.010938079347423062),\n","  (3, 0.009949326412062786),\n","  (4, 0.012297614633543444),\n","  (5, 0.0146459028550241),\n","  (6, 0.005376344086021506),\n","  (7, 0.08558892596712396),\n","  (8, 0.008713385242862439),\n","  (9, 0.01192683228278334),\n","  (10, 0.02589296749474725),\n","  (11, 0.18458781362007168),\n","  (12, 0.19200346063527376),\n","  (13, 0.0069830676059819555),\n","  (14, 0.012421208750463477),\n","  (15, 0.009454949944382647),\n","  (16, 0.05938697318007663),\n","  (17, 0.009084167593622544),\n","  (18, 0.2992831541218638),\n","  (19, 0.01946607341490545)],\n"," [(0, 0.011111111111111112),\n","  (1, 0.011111111111111112),\n","  (2, 0.04225721784776903),\n","  (3, 0.3972878390201225),\n","  (4, 0.014085739282589676),\n","  (5, 0.007611548556430446),\n","  (6, 0.01916010498687664),\n","  (7, 0.017760279965004375),\n","  (8, 0.02020997375328084),\n","  (9, 0.01268591426071741),\n","  (10, 0.029658792650918635),\n","  (11, 0.013560804899387576),\n","  (12, 0.04663167104111986),\n","  (13, 0.03753280839895013),\n","  (14, 0.013560804899387576),\n","  (15, 0.032983377077865264),\n","  (16, 0.009361329833770778),\n","  (17, 0.024409448818897637),\n","  (18, 0.23053368328958881),\n","  (19, 0.008486439195100613)],\n"," [(0, 0.01088131927547986),\n","  (1, 0.006555825898891593),\n","  (2, 0.013179237631792377),\n","  (3, 0.34313327926466614),\n","  (4, 0.012638550959718843),\n","  (5, 0.012368207623682076),\n","  (6, 0.007502027575020276),\n","  (7, 0.006015139226818059),\n","  (8, 0.12875101378751014),\n","  (9, 0.004528250878615842),\n","  (10, 0.11090835360908353),\n","  (11, 0.007772370911057042),\n","  (12, 0.004933765882670992),\n","  (13, 0.00885374425520411),\n","  (14, 0.010205460935387942),\n","  (15, 0.01007028926736956),\n","  (16, 0.009529602595296026),\n","  (17, 0.004663422546634225),\n","  (18, 0.28068396864017303),\n","  (19, 0.006826169234928359)],\n"," [(0, 0.008322731128074641),\n","  (1, 0.014153944020356236),\n","  (2, 0.03355597964376591),\n","  (3, 0.09928965224766753),\n","  (4, 0.00779262086513995),\n","  (5, 0.06801314673452079),\n","  (6, 0.011185326547921969),\n","  (7, 0.004611959287531807),\n","  (8, 0.05772900763358779),\n","  (9, 0.005990245971162002),\n","  (10, 0.07140585241730281),\n","  (11, 0.10734732824427483),\n","  (12, 0.016592451229855813),\n","  (13, 0.009913061916878713),\n","  (14, 0.08858142493638678),\n","  (15, 0.008004664970313827),\n","  (16, 0.042037743850720954),\n","  (17, 0.11243638676844785),\n","  (18, 0.22672815945716712),\n","  (19, 0.006308312128922817)],\n"," [(0, 0.21381620528378206),\n","  (1, 0.006383516622424472),\n","  (2, 0.017380862090759702),\n","  (3, 0.008026798129187208),\n","  (4, 0.0105549235242068),\n","  (5, 0.016496018202502846),\n","  (6, 0.06667930729364177),\n","  (7, 0.25616230565036024),\n","  (8, 0.0067627354316774115),\n","  (9, 0.021299456453040072),\n","  (10, 0.031664770572620404),\n","  (11, 0.005372266464416635),\n","  (12, 0.13064087978763747),\n","  (13, 0.032423208191126277),\n","  (14, 0.012703830109973455),\n","  (15, 0.05049930476551637),\n","  (16, 0.02509164454556946),\n","  (17, 0.02193148780179497),\n","  (18, 0.006130704082922513),\n","  (19, 0.059979774996839846)],\n"," [(0, 0.04752342704149932),\n","  (1, 0.0052002883328184524),\n","  (2, 0.05617341159509833),\n","  (3, 0.008289568530532384),\n","  (4, 0.010658016682113067),\n","  (5, 0.006538976418494489),\n","  (6, 0.06029245185871691),\n","  (7, 0.1387601688806508),\n","  (8, 0.006127072392132631),\n","  (9, 0.3756049840387189),\n","  (10, 0.01590979301822675),\n","  (11, 0.005715168365770774),\n","  (12, 0.029605601894758516),\n","  (13, 0.035887138296776845),\n","  (14, 0.006538976418494489),\n","  (15, 0.010143136649160743),\n","  (16, 0.15152919369786838),\n","  (17, 0.007774688497580062),\n","  (18, 0.009525280609617957),\n","  (19, 0.012202656780970031)],\n"," [(0, 0.06286084599976435),\n","  (1, 0.0051254860374690715),\n","  (2, 0.01148815835984447),\n","  (3, 0.021032166843407567),\n","  (4, 0.006657240485448334),\n","  (5, 0.009249440320490163),\n","  (6, 0.03646753858842937),\n","  (7, 0.4429716036290799),\n","  (8, 0.004065040650406505),\n","  (9, 0.0817132084364322),\n","  (10, 0.015612112642865562),\n","  (11, 0.005596795098385767),\n","  (12, 0.08642629904559916),\n","  (13, 0.03917756568870037),\n","  (14, 0.004065040650406505),\n","  (15, 0.008660303994344294),\n","  (16, 0.12507364204076826),\n","  (17, 0.012548603746907038),\n","  (18, 0.005596795098385767),\n","  (19, 0.015612112642865562)],\n"," [(0, 0.5164808294209704),\n","  (1, 0.008656103286384978),\n","  (2, 0.02049100156494523),\n","  (3, 0.00581964006259781),\n","  (4, 0.01921948356807512),\n","  (5, 0.004254694835680752),\n","  (6, 0.026261737089201882),\n","  (7, 0.012568466353677622),\n","  (8, 0.004059076682316119),\n","  (9, 0.12690727699530518),\n","  (10, 0.016774256651017217),\n","  (11, 0.004841549295774648),\n","  (12, 0.006797730829420971),\n","  (13, 0.011590375586854463),\n","  (14, 0.011101330203442882),\n","  (15, 0.012177230046948359),\n","  (16, 0.16837832550860724),\n","  (17, 0.006406494522691706),\n","  (18, 0.007775821596244133),\n","  (19, 0.009438575899843506)],\n"," [(0, 0.21494497692580766),\n","  (1, 0.05105904626671401),\n","  (2, 0.06514022009229678),\n","  (3, 0.017808543367648803),\n","  (4, 0.010945450242574845),\n","  (5, 0.0379245059756242),\n","  (6, 0.012128742160691045),\n","  (7, 0.010117145899893506),\n","  (8, 0.013667021654242105),\n","  (9, 0.005502307419240327),\n","  (10, 0.014022009229676965),\n","  (11, 0.010945450242574845),\n","  (12, 0.007040586912791387),\n","  (13, 0.05188735060939535),\n","  (14, 0.007277245296414627),\n","  (15, 0.005857294994675188),\n","  (16, 0.02478996568453438),\n","  (17, 0.02691989113714354),\n","  (18, 0.08040468583599575),\n","  (19, 0.3316175600520649)],\n"," [(0, 0.013615994484660462),\n","  (1, 0.007181431690221763),\n","  (2, 0.01625876134666207),\n","  (3, 0.026370217166494313),\n","  (4, 0.012926577042399173),\n","  (5, 0.006377111340916925),\n","  (6, 0.013386188670573366),\n","  (7, 0.007066528783178214),\n","  (8, 0.05118924508790072),\n","  (9, 0.01373089739170401),\n","  (10, 0.05038492473859588),\n","  (11, 0.01246696541422498),\n","  (12, 0.008215557853613697),\n","  (13, 0.16953923934275536),\n","  (14, 0.05521084683442491),\n","  (15, 0.005687693898655636),\n","  (16, 0.15046535677352638),\n","  (17, 0.004883373549350798),\n","  (18, 0.3430426289785132),\n","  (19, 0.032000459611628174)],\n"," [(0, 0.6319607843137255),\n","  (1, 0.011830065359477124),\n","  (2, 0.015490196078431372),\n","  (3, 0.00607843137254902),\n","  (4, 0.017712418300653596),\n","  (5, 0.009215686274509804),\n","  (6, 0.027908496732026142),\n","  (7, 0.006862745098039216),\n","  (8, 0.005947712418300654),\n","  (9, 0.012745098039215686),\n","  (10, 0.005032679738562092),\n","  (11, 0.006732026143790849),\n","  (12, 0.007908496732026144),\n","  (13, 0.05235294117647059),\n","  (14, 0.014052287581699347),\n","  (15, 0.024248366013071895),\n","  (16, 0.024640522875816993),\n","  (17, 0.00869281045751634),\n","  (18, 0.00869281045751634),\n","  (19, 0.1018954248366013)],\n"," [(0, 0.03881639646764202),\n","  (1, 0.01970475813892184),\n","  (2, 0.01535521286410966),\n","  (3, 0.013905364439172268),\n","  (4, 0.01232371161196784),\n","  (5, 0.020100171345722947),\n","  (6, 0.010214841175695268),\n","  (7, 0.033807829181494664),\n","  (8, 0.01416897324370634),\n","  (9, 0.006656122314485304),\n","  (10, 0.06794516936865691),\n","  (11, 0.023922499011466983),\n","  (12, 0.39534730459997364),\n","  (13, 0.08995650454725188),\n","  (14, 0.09905100830367734),\n","  (15, 0.006919731119019375),\n","  (16, 0.07928034796362199),\n","  (17, 0.009028601555291947),\n","  (18, 0.02642678265454066),\n","  (19, 0.017068670093581127)],\n"," [(0, 0.0072016460905349805),\n","  (1, 0.032256112321471804),\n","  (2, 0.015068990559186639),\n","  (3, 0.005628177196804649),\n","  (4, 0.009622367465504721),\n","  (5, 0.0072016460905349805),\n","  (6, 0.05948922778988139),\n","  (7, 0.06663035584604214),\n","  (8, 0.0051440329218107005),\n","  (9, 0.007685790365528929),\n","  (10, 0.01712660372791092),\n","  (11, 0.23765432098765435),\n","  (12, 0.13755749213265556),\n","  (13, 0.1436092955700799),\n","  (14, 0.020999757927862506),\n","  (15, 0.04024449285887195),\n","  (16, 0.052711207939966115),\n","  (17, 0.10378842895182766),\n","  (18, 0.022331154684095866),\n","  (19, 0.00804889857177439)],\n"," [(0, 0.8123442949676132),\n","  (1, 0.003674638764324862),\n","  (2, 0.009653712007972095),\n","  (3, 0.05287742899850522),\n","  (4, 0.007536123567513701),\n","  (5, 0.004172894867962132),\n","  (6, 0.009404583956153461),\n","  (7, 0.004795714997508718),\n","  (8, 0.005293971101145988),\n","  (9, 0.014387144992526154),\n","  (10, 0.0064150473343298434),\n","  (11, 0.0031763826606875925),\n","  (12, 0.03269805680119581),\n","  (13, 0.003923766816143497),\n","  (14, 0.007286995515695066),\n","  (15, 0.004671150971599401),\n","  (16, 0.005293971101145988),\n","  (17, 0.005792227204783258),\n","  (18, 0.003051818634778275),\n","  (19, 0.0035500747384155447)],\n"," [(0, 0.10114071752002787),\n","  (1, 0.013279345175896903),\n","  (2, 0.021987112504353888),\n","  (3, 0.007445141065830722),\n","  (4, 0.01493382096830373),\n","  (5, 0.013714733542319751),\n","  (6, 0.046020550330895166),\n","  (7, 0.049851967955416236),\n","  (8, 0.0036137234413096487),\n","  (9, 0.31613549285963083),\n","  (10, 0.005529432253570186),\n","  (11, 0.02529606408916754),\n","  (12, 0.014759665621734589),\n","  (13, 0.158002438174852),\n","  (14, 0.02651515151515152),\n","  (15, 0.005529432253570186),\n","  (16, 0.015543364681295718),\n","  (17, 0.10575583420411008),\n","  (18, 0.006487286659700454),\n","  (19, 0.04845872518286312)],\n"," [(0, 0.4634759413152765),\n","  (1, 0.10572483841181902),\n","  (2, 0.013080947983995075),\n","  (3, 0.010516056222427414),\n","  (4, 0.10028726787729558),\n","  (5, 0.004360315994665025),\n","  (6, 0.031035190314968708),\n","  (7, 0.012157586949830717),\n","  (8, 0.013593926336308607),\n","  (9, 0.0050784856879039705),\n","  (10, 0.020570431927772647),\n","  (11, 0.010310864881502001),\n","  (12, 0.056991894942033446),\n","  (13, 0.04067918333846312),\n","  (14, 0.022417153996101363),\n","  (15, 0.025289832769057146),\n","  (16, 0.02518723709859444),\n","  (17, 0.020980814609623474),\n","  (18, 0.0073355904380835125),\n","  (19, 0.01092643890427824)],\n"," [(0, 0.5313031850230976),\n","  (1, 0.0046802820325796265),\n","  (2, 0.006625334305859471),\n","  (3, 0.00613907123753951),\n","  (4, 0.0072331631412594225),\n","  (5, 0.006990031607099442),\n","  (6, 0.0976780938487722),\n","  (7, 0.009178215414539267),\n","  (8, 0.013189885728178946),\n","  (9, 0.03081692195477754),\n","  (10, 0.005531242402139558),\n","  (11, 0.004072453197179675),\n","  (12, 0.013068319961098956),\n","  (13, 0.07859226841721373),\n","  (14, 0.031667882324337475),\n","  (15, 0.009056649647459277),\n","  (16, 0.06680038901045467),\n","  (17, 0.012460491125699006),\n","  (18, 0.011001701920739121),\n","  (19, 0.053914417699975695)],\n"," [(0, 0.2831132452981193),\n","  (1, 0.0803654795251434),\n","  (2, 0.019140989729225028),\n","  (3, 0.013405362144857947),\n","  (4, 0.006602641056422571),\n","  (5, 0.009136988128584768),\n","  (6, 0.006602641056422571),\n","  (7, 0.3088568760837669),\n","  (8, 0.007803121248499401),\n","  (9, 0.03474723222622383),\n","  (10, 0.007936507936507938),\n","  (11, 0.012605042016806726),\n","  (12, 0.059023609443777526),\n","  (13, 0.023275977057489666),\n","  (14, 0.007269574496465254),\n","  (15, 0.005802320928371349),\n","  (16, 0.008603441376550622),\n","  (17, 0.07729758570094707),\n","  (18, 0.007536347872482328),\n","  (19, 0.020875016673336005)],\n"," [(0, 0.686524493895843),\n","  (1, 0.009658476278782259),\n","  (2, 0.016148972338123936),\n","  (3, 0.039174779786740846),\n","  (4, 0.0076495132127955496),\n","  (5, 0.008731262556019163),\n","  (6, 0.007958584453716581),\n","  (7, 0.00641322824911142),\n","  (8, 0.005176943285427291),\n","  (9, 0.006567763869571936),\n","  (10, 0.012594653067532066),\n","  (11, 0.010585690001545356),\n","  (12, 0.006567763869571936),\n","  (13, 0.019703291608715808),\n","  (14, 0.008267655694637614),\n","  (15, 0.01043115438108484),\n","  (16, 0.013367331169834648),\n","  (17, 0.0076495132127955496),\n","  (18, 0.10346159789831556),\n","  (19, 0.013367331169834648)],\n"," [(0, 0.06927381927381927),\n","  (1, 0.007712257712257712),\n","  (2, 0.02231777231777232),\n","  (3, 0.007712257712257712),\n","  (4, 0.005118755118755119),\n","  (5, 0.010442260442260442),\n","  (6, 0.017676767676767676),\n","  (7, 0.053985803985803986),\n","  (8, 0.004709254709254709),\n","  (9, 0.013854763854763854),\n","  (10, 0.005664755664755665),\n","  (11, 0.00662025662025662),\n","  (12, 0.06627081627081627),\n","  (13, 0.11732186732186732),\n","  (14, 0.010988260988260988),\n","  (15, 0.02723177723177723),\n","  (16, 0.41557466557466555),\n","  (17, 0.11855036855036855),\n","  (18, 0.00935025935025935),\n","  (19, 0.009623259623259623)],\n"," [(0, 0.029680365296803644),\n","  (1, 0.007117915659414449),\n","  (2, 0.08116214522338615),\n","  (3, 0.015175933387053447),\n","  (4, 0.02171188109947175),\n","  (5, 0.012489927477840447),\n","  (6, 0.034425642403079944),\n","  (7, 0.13801593696839465),\n","  (8, 0.0038051750380517493),\n","  (9, 0.09718864714835704),\n","  (10, 0.05474975378279164),\n","  (11, 0.09307010475423044),\n","  (12, 0.00461097681081565),\n","  (13, 0.023502551705613745),\n","  (14, 0.015802668099203148),\n","  (15, 0.025114155251141544),\n","  (16, 0.024666487599606046),\n","  (17, 0.03218730414540245),\n","  (18, 0.006222580356343449),\n","  (19, 0.27929984779299843)],\n"," [(0, 0.017785630153121318),\n","  (1, 0.004593639575971731),\n","  (2, 0.46552807224185316),\n","  (3, 0.0034157832744405184),\n","  (4, 0.026423243031016883),\n","  (5, 0.007027875932469572),\n","  (6, 0.02807224185316058),\n","  (7, 0.013780918727915195),\n","  (8, 0.004986258343148802),\n","  (9, 0.005692972124067531),\n","  (10, 0.00812720848056537),\n","  (11, 0.006478209658421673),\n","  (12, 0.022575579112681585),\n","  (13, 0.1591283863368669),\n","  (14, 0.08351001177856301),\n","  (15, 0.011739301138594425),\n","  (16, 0.007106399685904987),\n","  (17, 0.053278366705928545),\n","  (18, 0.011817824892029838),\n","  (19, 0.058932076953278366)],\n"," [(0, 0.009338263177853622),\n","  (1, 0.006588926810769814),\n","  (2, 0.42998672734167614),\n","  (3, 0.005925293894577171),\n","  (4, 0.007821387940841865),\n","  (5, 0.00431361395525218),\n","  (6, 0.20776450511945393),\n","  (7, 0.014836935912021237),\n","  (8, 0.008105802047781569),\n","  (9, 0.012940841865756541),\n","  (10, 0.01369927948426242),\n","  (11, 0.008390216154721275),\n","  (12, 0.010381114903299203),\n","  (13, 0.10651308304891922),\n","  (14, 0.011234357224118316),\n","  (15, 0.01787068638604475),\n","  (16, 0.010855138414865377),\n","  (17, 0.026213500189609405),\n","  (18, 0.0038395904436860067),\n","  (19, 0.08338073568448995)],\n"," [(0, 0.019115269115269118),\n","  (1, 0.02904827904827905),\n","  (2, 0.027200277200277203),\n","  (3, 0.011723261723261725),\n","  (4, 0.00802725802725803),\n","  (5, 0.021540771540771546),\n","  (6, 0.0315892815892816),\n","  (7, 0.26409101409101415),\n","  (8, 0.0077962577962577976),\n","  (9, 0.0985793485793486),\n","  (10, 0.030780780780780784),\n","  (11, 0.038057288057288065),\n","  (12, 0.021194271194271197),\n","  (13, 0.15344190344190348),\n","  (14, 0.0077962577962577976),\n","  (15, 0.007565257565257566),\n","  (16, 0.007103257103257104),\n","  (17, 0.08957033957033958),\n","  (18, 0.00386925386925387),\n","  (19, 0.12191037191037192)],\n"," [(0, 0.041694278771813566),\n","  (1, 0.008559752595537883),\n","  (2, 0.02998674618952949),\n","  (3, 0.02015683675723437),\n","  (4, 0.008559752595537883),\n","  (5, 0.008117958913187541),\n","  (6, 0.03815992931301082),\n","  (7, 0.23829246741771593),\n","  (8, 0.005798542080848244),\n","  (9, 0.1839518444886238),\n","  (10, 0.02391208305721228),\n","  (11, 0.0052462999779103155),\n","  (12, 0.02700463883366468),\n","  (13, 0.2223878948531036),\n","  (14, 0.015407554671968192),\n","  (15, 0.019715043074884028),\n","  (16, 0.04876297768941904),\n","  (17, 0.02844046830130329),\n","  (18, 0.00789706207201237),\n","  (19, 0.01794786834548266)],\n"," [(0, 0.023809523809523808),\n","  (1, 0.021275176737361613),\n","  (2, 0.1039749233026544),\n","  (3, 0.02287581699346405),\n","  (4, 0.10264105642256903),\n","  (5, 0.05902360944377751),\n","  (6, 0.06876083766840069),\n","  (7, 0.08156595971722022),\n","  (8, 0.01233826864078965),\n","  (9, 0.007136187808456716),\n","  (10, 0.04235027344271042),\n","  (11, 0.006602641056422569),\n","  (12, 0.03714819261037748),\n","  (13, 0.30392156862745096),\n","  (14, 0.015006002400960384),\n","  (15, 0.009403761504601841),\n","  (16, 0.043950913698812856),\n","  (17, 0.011004401760704281),\n","  (18, 0.011004401760704281),\n","  (19, 0.016206482593037214)],\n"," [(0, 0.037249893571732653),\n","  (1, 0.01397757911167873),\n","  (2, 0.016248048815098625),\n","  (3, 0.014261387824606215),\n","  (4, 0.061373634170569034),\n","  (5, 0.01071377891301263),\n","  (6, 0.05229175535688946),\n","  (7, 0.04789272030651341),\n","  (8, 0.005463317723854122),\n","  (9, 0.01525471831985242),\n","  (10, 0.0416489286221087),\n","  (11, 0.009294735348375195),\n","  (12, 0.3321271463033915),\n","  (13, 0.01397757911167873),\n","  (14, 0.015964240102171137),\n","  (15, 0.009010926635447709),\n","  (16, 0.23790265361146587),\n","  (17, 0.02362707535121328),\n","  (18, 0.012558535547041293),\n","  (19, 0.029161345253299276)],\n"," [(0, 0.02823491448854469),\n","  (1, 0.00984188447886415),\n","  (2, 0.05071528450037647),\n","  (3, 0.03651715607185114),\n","  (4, 0.0062923523717328175),\n","  (5, 0.1792513714101323),\n","  (6, 0.034258362912767557),\n","  (7, 0.010702377110895988),\n","  (8, 0.03307518554372378),\n","  (9, 0.02016779606324621),\n","  (10, 0.008120899214800473),\n","  (11, 0.11708077874583199),\n","  (12, 0.04727331397224911),\n","  (13, 0.14666021297192644),\n","  (14, 0.0148972786920512),\n","  (15, 0.009088953425836292),\n","  (16, 0.011885554479939766),\n","  (17, 0.191405829837582),\n","  (18, 0.006937721845756696),\n","  (19, 0.03759277186189093)],\n"," [(0, 0.011502481863306605),\n","  (1, 0.008447880870561282),\n","  (2, 0.016561664757541047),\n","  (3, 0.005679648720885834),\n","  (4, 0.01255250095456281),\n","  (5, 0.011979763268423062),\n","  (6, 0.032789232531500574),\n","  (7, 0.02381634211531119),\n","  (8, 0.008829705994654448),\n","  (9, 0.03918480336006109),\n","  (10, 0.04195303550973654),\n","  (11, 0.014079801450935472),\n","  (12, 0.32440817105765557),\n","  (13, 0.020093547155402826),\n","  (14, 0.022193585337915236),\n","  (15, 0.007779686903398243),\n","  (16, 0.13702749140893472),\n","  (17, 0.01761168384879725),\n","  (18, 0.01684803360061092),\n","  (19, 0.22666093928980527)],\n"," [(0, 0.07937526743688489),\n","  (1, 0.005776636713735558),\n","  (2, 0.011196690914277564),\n","  (3, 0.010055626872058195),\n","  (4, 0.00820139780345172),\n","  (5, 0.007773498787619455),\n","  (6, 0.11203822564541435),\n","  (7, 0.37519612038225647),\n","  (8, 0.009057195835116246),\n","  (9, 0.08465268863214948),\n","  (10, 0.011624589930109827),\n","  (11, 0.005634003708458137),\n","  (12, 0.018185708172871202),\n","  (13, 0.12815575524176295),\n","  (14, 0.01205248894594209),\n","  (15, 0.008771929824561403),\n","  (16, 0.038582227927542435),\n","  (17, 0.06040507773498788),\n","  (18, 0.004207673655683925),\n","  (19, 0.009057195835116246)],\n"," [(0, 0.5730244414454939),\n","  (1, 0.010721247563352824),\n","  (2, 0.04970760233918127),\n","  (3, 0.010571300044984253),\n","  (4, 0.021817363922627078),\n","  (5, 0.008921877342929973),\n","  (6, 0.01177088019193282),\n","  (7, 0.15721997300944668),\n","  (8, 0.008771929824561401),\n","  (9, 0.015369620632778525),\n","  (10, 0.011021142600089967),\n","  (11, 0.0074224021592442626),\n","  (12, 0.025116209326735636),\n","  (13, 0.026615684510421345),\n","  (14, 0.004873294346978556),\n","  (15, 0.006222822012295695),\n","  (16, 0.009221772379667115),\n","  (17, 0.012970460338881388),\n","  (18, 0.015519568151147096),\n","  (19, 0.013120407857249959)],\n"," [(0, 0.1201772324471711),\n","  (1, 0.03619631901840491),\n","  (2, 0.023244717109747785),\n","  (3, 0.030879345603271983),\n","  (4, 0.0074301295160190866),\n","  (5, 0.03047034764826176),\n","  (6, 0.04451261077027948),\n","  (7, 0.285412406271302),\n","  (8, 0.006203135650988412),\n","  (9, 0.006475800954328562),\n","  (10, 0.020245398773006136),\n","  (11, 0.008520790729379687),\n","  (12, 0.04369461486025903),\n","  (13, 0.23605998636673484),\n","  (14, 0.010838445807770962),\n","  (15, 0.010974778459441037),\n","  (16, 0.03047034764826176),\n","  (17, 0.017518745739604637),\n","  (18, 0.005248807089297887),\n","  (19, 0.025426039536468985)],\n"," [(0, 0.13547453703703705),\n","  (1, 0.012789351851851852),\n","  (2, 0.014872685185185185),\n","  (3, 0.006539351851851852),\n","  (4, 0.055844907407407406),\n","  (5, 0.008854166666666666),\n","  (6, 0.04415509259259259),\n","  (7, 0.02505787037037037),\n","  (8, 0.004803240740740741),\n","  (9, 0.011284722222222222),\n","  (10, 0.05561342592592593),\n","  (11, 0.0078125),\n","  (12, 0.4257523148148148),\n","  (13, 0.05943287037037037),\n","  (14, 0.021238425925925924),\n","  (15, 0.009085648148148148),\n","  (16, 0.030381944444444444),\n","  (17, 0.008391203703703705),\n","  (18, 0.009895833333333333),\n","  (19, 0.05271990740740741)],\n"," [(0, 0.2125140924464487),\n","  (1, 0.015846173117875485),\n","  (2, 0.014593511211324064),\n","  (3, 0.01033446072904923),\n","  (4, 0.036014029813353374),\n","  (5, 0.006827007390705249),\n","  (6, 0.0348866340974571),\n","  (7, 0.17130151572090693),\n","  (8, 0.004321683577602405),\n","  (9, 0.02987598647125141),\n","  (10, 0.029625454089941126),\n","  (11, 0.07847926844544657),\n","  (12, 0.01096079168232494),\n","  (13, 0.11067267944381812),\n","  (14, 0.00632594262808468),\n","  (15, 0.004822748340222974),\n","  (16, 0.01496930978328949),\n","  (17, 0.1109232118251284),\n","  (18, 0.013340849304772641),\n","  (19, 0.08336464988099712)],\n"," [(0, 0.007701249636733509),\n","  (1, 0.044318512060447554),\n","  (2, 0.00818560495979851),\n","  (3, 0.009735541993606512),\n","  (4, 0.012641673931996516),\n","  (5, 0.03540637411605154),\n","  (6, 0.09885692143756662),\n","  (7, 0.0777390293519326),\n","  (8, 0.013029158190448517),\n","  (9, 0.049743291678775566),\n","  (10, 0.025428654460912532),\n","  (11, 0.007507507507507509),\n","  (12, 0.04480286738351255),\n","  (13, 0.1711227356388647),\n","  (14, 0.010413639445897512),\n","  (15, 0.022038167199457524),\n","  (16, 0.03608447156834254),\n","  (17, 0.0868449094255546),\n","  (18, 0.00876683134747651),\n","  (19, 0.2296328586651168)],\n"," [(0, 0.020511831275720163),\n","  (1, 0.005079732510288066),\n","  (2, 0.02797067901234568),\n","  (3, 0.010352366255144033),\n","  (4, 0.013953189300411523),\n","  (5, 0.005079732510288066),\n","  (6, 0.27642746913580246),\n","  (7, 0.02179783950617284),\n","  (8, 0.006880144032921811),\n","  (9, 0.007008744855967078),\n","  (10, 0.03054269547325103),\n","  (11, 0.005722736625514403),\n","  (12, 0.008809156378600823),\n","  (13, 0.3838091563786008),\n","  (14, 0.009709362139917695),\n","  (15, 0.08789866255144033),\n","  (16, 0.021540637860082305),\n","  (17, 0.009580761316872428),\n","  (18, 0.008680555555555556),\n","  (19, 0.03864454732510288)],\n"," [(0, 0.011326860841423947),\n","  (1, 0.007131727196452114),\n","  (2, 0.04968236845259497),\n","  (3, 0.016121299292820326),\n","  (4, 0.01036797315114467),\n","  (5, 0.011566582763993765),\n","  (6, 0.10733549083063644),\n","  (7, 0.3908066642694473),\n","  (8, 0.010967277957569218),\n","  (9, 0.010248112189859762),\n","  (10, 0.04536737384633824),\n","  (11, 0.004374925086899196),\n","  (12, 0.02175476447321107),\n","  (13, 0.010727556034999398),\n","  (14, 0.01048783411242958),\n","  (15, 0.01228574853170322),\n","  (16, 0.0195972671700827),\n","  (17, 0.011566582763993765),\n","  (18, 0.011087138918854128),\n","  (19, 0.2271964521155459)],\n"," [(0, 0.03519932145886345),\n","  (1, 0.027735368956743008),\n","  (2, 0.027565733672603905),\n","  (3, 0.0068702290076335885),\n","  (4, 0.020441051738761665),\n","  (5, 0.01060220525869381),\n","  (6, 0.20975402883799832),\n","  (7, 0.02994062765055132),\n","  (8, 0.0068702290076335885),\n","  (9, 0.23486005089058526),\n","  (10, 0.03333333333333334),\n","  (11, 0.012977099236641223),\n","  (12, 0.05487701441899916),\n","  (13, 0.0989821882951654),\n","  (14, 0.007718405428329093),\n","  (15, 0.021458863443596272),\n","  (16, 0.021967769296013573),\n","  (17, 0.10695504664970315),\n","  (18, 0.013146734520780324),\n","  (19, 0.018744698897370658)],\n"," [(0, 0.01443355119825708),\n","  (1, 0.017265795206971676),\n","  (2, 0.011492374727668845),\n","  (3, 0.023801742919389978),\n","  (4, 0.006699346405228758),\n","  (5, 0.061492374727668844),\n","  (6, 0.2153050108932462),\n","  (7, 0.03480392156862745),\n","  (8, 0.005065359477124183),\n","  (9, 0.011928104575163398),\n","  (10, 0.02826797385620915),\n","  (11, 0.060076252723311546),\n","  (12, 0.07086056644880175),\n","  (13, 0.1258714596949891),\n","  (14, 0.005065359477124183),\n","  (15, 0.10702614379084967),\n","  (16, 0.04177559912854031),\n","  (17, 0.07772331154684096),\n","  (18, 0.010620915032679739),\n","  (19, 0.07042483660130719)],\n"," [(0, 0.055286391042204994),\n","  (1, 0.009313092161929371),\n","  (2, 0.008667097329888027),\n","  (3, 0.005975452196382429),\n","  (4, 0.006729112833763997),\n","  (5, 0.005652454780361757),\n","  (6, 0.3408161068044789),\n","  (7, 0.06583763996554694),\n","  (8, 0.004145133505598622),\n","  (9, 0.14400301464254953),\n","  (10, 0.008451765719207579),\n","  (11, 0.024386304909560723),\n","  (12, 0.08435615848406545),\n","  (13, 0.019003014642549526),\n","  (14, 0.009097760551248923),\n","  (15, 0.007805770887166236),\n","  (16, 0.029015934539190352),\n","  (17, 0.06863695090439277),\n","  (18, 0.008667097329888027),\n","  (19, 0.09415374677002585)],\n"," [(0, 0.012442542399746395),\n","  (1, 0.009589475352670788),\n","  (2, 0.19392930733872246),\n","  (3, 0.015612616896497067),\n","  (4, 0.00626089713108258),\n","  (5, 0.026073862735774292),\n","  (6, 0.024805832937074022),\n","  (7, 0.006894912030432715),\n","  (8, 0.016880646695197336),\n","  (9, 0.00752892692978285),\n","  (10, 0.051592962434617216),\n","  (11, 0.04984942146140434),\n","  (12, 0.007370423204945316),\n","  (13, 0.22309399270882865),\n","  (14, 0.03653510857505152),\n","  (15, 0.2373593279442067),\n","  (16, 0.015295609446822),\n","  (17, 0.026866381359961958),\n","  (18, 0.012759549849421462),\n","  (19, 0.01925820256776034)],\n"," [(0, 0.20442134005461038),\n","  (1, 0.0037282083595883216),\n","  (2, 0.058338584331022895),\n","  (3, 0.004253308128544423),\n","  (4, 0.06358958202058392),\n","  (5, 0.004778407897500525),\n","  (6, 0.1502310438983407),\n","  (7, 0.06568998109640832),\n","  (8, 0.005408527620247847),\n","  (9, 0.0981411468178954),\n","  (10, 0.004988447805082966),\n","  (11, 0.08889939088426801),\n","  (12, 0.07514177693761814),\n","  (13, 0.010344465448435203),\n","  (14, 0.009084226002940558),\n","  (15, 0.024942239025414828),\n","  (16, 0.008034026465028356),\n","  (17, 0.010449485402226424),\n","  (18, 0.10097668557025835),\n","  (19, 0.008559126233984457)],\n"," [(0, 0.007990543735224587),\n","  (1, 0.04260047281323877),\n","  (2, 0.05328605200945626),\n","  (3, 0.0043971631205673755),\n","  (4, 0.0057210401891252954),\n","  (5, 0.00978723404255319),\n","  (6, 0.21952718676122931),\n","  (7, 0.17914893617021277),\n","  (8, 0.0032624113475177305),\n","  (9, 0.013002364066193853),\n","  (10, 0.004680851063829788),\n","  (11, 0.007044917257683215),\n","  (12, 0.015933806146572103),\n","  (13, 0.15229314420803783),\n","  (14, 0.004491725768321513),\n","  (15, 0.15134751773049646),\n","  (16, 0.006193853427895981),\n","  (17, 0.05555555555555555),\n","  (18, 0.00723404255319149),\n","  (19, 0.056501182033096925)],\n"," [(0, 0.041836273179556766),\n","  (1, 0.024951002562942866),\n","  (2, 0.018468264736921456),\n","  (3, 0.009271822704658527),\n","  (4, 0.007914970601537768),\n","  (5, 0.09083371023669533),\n","  (6, 0.030981456354590688),\n","  (7, 0.012588572290064829),\n","  (8, 0.012136288255691243),\n","  (9, 0.019674355495251022),\n","  (10, 0.01741293532338309),\n","  (11, 0.01289009497964722),\n","  (12, 0.05103271521181969),\n","  (13, 0.010779436152570482),\n","  (14, 0.0237449118046133),\n","  (15, 0.0750037690336198),\n","  (16, 0.038519523594150465),\n","  (17, 0.4032112166440525),\n","  (18, 0.006859641187999398),\n","  (19, 0.09188903965023369)],\n"," [(0, 0.02294389978213508),\n","  (1, 0.016680283224400873),\n","  (2, 0.22256263616557737),\n","  (3, 0.00497004357298475),\n","  (4, 0.007148692810457517),\n","  (5, 0.014637799564270155),\n","  (6, 0.3127042483660131),\n","  (7, 0.037785947712418305),\n","  (8, 0.0056508714596949896),\n","  (9, 0.006740196078431374),\n","  (10, 0.04391339869281046),\n","  (11, 0.06869553376906319),\n","  (12, 0.014093137254901963),\n","  (13, 0.05126633986928105),\n","  (14, 0.04377723311546842),\n","  (15, 0.026211873638344232),\n","  (16, 0.02171840958605665),\n","  (17, 0.008510348583877997),\n","  (18, 0.00633169934640523),\n","  (19, 0.06365740740740743)],\n"," [(0, 0.034169755726641944),\n","  (1, 0.013449291892405662),\n","  (2, 0.008696891930424862),\n","  (3, 0.017726451858188382),\n","  (4, 0.006225643950194847),\n","  (5, 0.0071761239425910065),\n","  (6, 0.031223267750213852),\n","  (7, 0.364841745081266),\n","  (8, 0.06106833951145327),\n","  (9, 0.02742134778062921),\n","  (10, 0.014589867883281054),\n","  (11, 0.006225643950194847),\n","  (12, 0.12275449101796404),\n","  (13, 0.1693280106453759),\n","  (14, 0.005845451953236384),\n","  (15, 0.014589867883281054),\n","  (16, 0.03340937173272502),\n","  (17, 0.0362608117099135),\n","  (18, 0.009647371922821023),\n","  (19, 0.015350251877197981)],\n"," [(0, 0.010734463276836158),\n","  (1, 0.005963590709353421),\n","  (2, 0.01098556183301946),\n","  (3, 0.04588826114249843),\n","  (4, 0.045386064030131824),\n","  (5, 0.008097928436911487),\n","  (6, 0.06660389202762085),\n","  (7, 0.09372253609541745),\n","  (8, 0.008097928436911487),\n","  (9, 0.012617702448210923),\n","  (10, 0.04362837413684871),\n","  (11, 0.006089139987445072),\n","  (12, 0.014249843063402386),\n","  (13, 0.31192718141870684),\n","  (14, 0.01977401129943503),\n","  (15, 0.004080351537978657),\n","  (16, 0.19717514124293786),\n","  (17, 0.008097928436911487),\n","  (18, 0.012994350282485875),\n","  (19, 0.0738857501569366)],\n"," [(0, 0.011443462497167461),\n","  (1, 0.015220182793262333),\n","  (2, 0.005778382053025154),\n","  (3, 0.20647329858750663),\n","  (4, 0.014842510763652846),\n","  (5, 0.018166024624216334),\n","  (6, 0.003965556310899615),\n","  (7, 0.010461515220182795),\n","  (8, 0.1638718936475565),\n","  (9, 0.01741068056499736),\n","  (10, 0.09226527683359771),\n","  (11, 0.007289070171463102),\n","  (12, 0.06258025530629203),\n","  (13, 0.0036634186872120254),\n","  (14, 0.051325628823929305),\n","  (15, 0.005627313241181359),\n","  (16, 0.26274643099932027),\n","  (17, 0.009328499131354334),\n","  (18, 0.030175995165798025),\n","  (19, 0.0073646045773849995)],\n"," [(0, 0.09006340297266396),\n","  (1, 0.024581644319717284),\n","  (2, 0.04557738280843987),\n","  (3, 0.004521359526036795),\n","  (4, 0.007119842012264837),\n","  (5, 0.02614073381145411),\n","  (6, 0.006911963413366594),\n","  (7, 0.027388005404843572),\n","  (8, 0.005352873921629768),\n","  (9, 0.008471052905103419),\n","  (10, 0.024061947822471678),\n","  (11, 0.4961542459203825),\n","  (12, 0.04516162561064339),\n","  (13, 0.021151647437896267),\n","  (14, 0.01626650036378755),\n","  (15, 0.010134081696289368),\n","  (16, 0.005664691819977133),\n","  (17, 0.11532065273880054),\n","  (18, 0.013979835775906871),\n","  (19, 0.0059765097183244985)],\n"," [(0, 0.006501866872666409),\n","  (1, 0.026200592249259688),\n","  (2, 0.015771855285180894),\n","  (3, 0.013068108664864169),\n","  (4, 0.007145616067979915),\n","  (5, 0.014613106733616583),\n","  (6, 0.014098107377365778),\n","  (7, 0.005343118321102099),\n","  (8, 0.6511523110596111),\n","  (9, 0.005343118321102099),\n","  (10, 0.13229045963692546),\n","  (11, 0.004699369125788592),\n","  (12, 0.007403115746105317),\n","  (13, 0.01963435045706193),\n","  (14, 0.016930603836745203),\n","  (15, 0.01087936140079825),\n","  (16, 0.00663061671172911),\n","  (17, 0.007403115746105317),\n","  (18, 0.009334363332045835),\n","  (19, 0.025556843053946184)],\n"," [(0, 0.021142336716107213),\n","  (1, 0.020621909966172264),\n","  (2, 0.018540202966432478),\n","  (3, 0.01880041634139995),\n","  (4, 0.024915430653135576),\n","  (5, 0.06641946396044758),\n","  (6, 0.006180067655477493),\n","  (7, 0.005789747593026282),\n","  (8, 0.2634009888108249),\n","  (9, 0.007090814467863649),\n","  (10, 0.18676814988290402),\n","  (11, 0.00982305490502212),\n","  (12, 0.014246682279469167),\n","  (13, 0.012034868592245644),\n","  (14, 0.24167317200104088),\n","  (15, 0.01255529534218059),\n","  (16, 0.03727556596409056),\n","  (17, 0.013986468904501692),\n","  (18, 0.0068306010928961755),\n","  (19, 0.011904761904761906)],\n"," [(0, 0.011506488240064882),\n","  (1, 0.021542984590429845),\n","  (2, 0.006032035685320357),\n","  (3, 0.033505677210056775),\n","  (4, 0.0054237631792376315),\n","  (5, 0.054288321167883215),\n","  (6, 0.006234793187347932),\n","  (7, 0.004207218167072182),\n","  (8, 0.7140612327656123),\n","  (9, 0.01120235198702352),\n","  (10, 0.03563463098134631),\n","  (11, 0.0054237631792376315),\n","  (12, 0.007045823195458232),\n","  (13, 0.008465125709651257),\n","  (14, 0.007654095701540957),\n","  (15, 0.007045823195458232),\n","  (16, 0.01444647201946472),\n","  (17, 0.007045823195458232),\n","  (18, 0.032998783454987836),\n","  (19, 0.006234793187347932)],\n"," [(0, 0.031226961558715113),\n","  (1, 0.007319641916798315),\n","  (2, 0.03691416535018431),\n","  (3, 0.015218536071616641),\n","  (4, 0.008267509215376515),\n","  (5, 0.02890995260663507),\n","  (6, 0.05892575039494471),\n","  (7, 0.2519747235387046),\n","  (8, 0.02564507635597683),\n","  (9, 0.026066350710900472),\n","  (10, 0.013322801474460242),\n","  (11, 0.08146392838335967),\n","  (12, 0.056608741442864666),\n","  (13, 0.0731437598736177),\n","  (14, 0.004054765666140074),\n","  (15, 0.07588204318062138),\n","  (16, 0.02374934175882043),\n","  (17, 0.1572933122696156),\n","  (18, 0.007424960505529226),\n","  (19, 0.016587677725118485)],\n"," [(0, 0.041166453810131974),\n","  (1, 0.010600255427841635),\n","  (2, 0.0371647509578544),\n","  (3, 0.004044274159216688),\n","  (4, 0.029842486164325246),\n","  (5, 0.004044274159216688),\n","  (6, 0.01656023839931886),\n","  (7, 0.03699446573009792),\n","  (8, 0.004299702000851426),\n","  (9, 0.2547892720306513),\n","  (10, 0.012473392933163048),\n","  (11, 0.0026819923371647508),\n","  (12, 0.006939123031077054),\n","  (13, 0.011621966794380588),\n","  (14, 0.15074499787143467),\n","  (15, 0.08450404427415922),\n","  (16, 0.019369944657300978),\n","  (17, 0.0051511281396338865),\n","  (18, 0.004044274159216688),\n","  (19, 0.26296296296296295)],\n"," [(0, 0.727777777777778),\n","  (1, 0.0036881419234360423),\n","  (2, 0.0072362278244631215),\n","  (3, 0.009477124183006539),\n","  (4, 0.006302521008403363),\n","  (5, 0.005648926237161533),\n","  (6, 0.006489262371615315),\n","  (7, 0.021055088702147534),\n","  (8, 0.004061624649859946),\n","  (9, 0.007329598506069097),\n","  (10, 0.025910364145658272),\n","  (11, 0.006209150326797387),\n","  (12, 0.008636788048552758),\n","  (13, 0.04635854341736696),\n","  (14, 0.0058356676003734845),\n","  (15, 0.007329598506069097),\n","  (16, 0.006489262371615315),\n","  (17, 0.004154995331465921),\n","  (18, 0.06465919701213822),\n","  (19, 0.025350140056022416)],\n"," [(0, 0.01454060943112038),\n","  (1, 0.006314447920287337),\n","  (2, 0.17419765959911945),\n","  (3, 0.007357200787857722),\n","  (4, 0.005851002201367165),\n","  (5, 0.015004055150040552),\n","  (6, 0.013497856563549994),\n","  (7, 0.005503417912177036),\n","  (8, 0.004576526474336693),\n","  (9, 0.003881357895956436),\n","  (10, 0.004808249333796779),\n","  (11, 0.3653690186536902),\n","  (12, 0.004228942185146565),\n","  (13, 0.11615108330436798),\n","  (14, 0.12206001622060016),\n","  (15, 0.007125477928397636),\n","  (16, 0.005851002201367165),\n","  (17, 0.08301471440157572),\n","  (18, 0.005387556482446994),\n","  (19, 0.03527980535279805)],\n"," [(0, 0.009873509873509875),\n","  (1, 0.030530530530530536),\n","  (2, 0.08749658749658751),\n","  (3, 0.037810537810537816),\n","  (4, 0.008417508417508419),\n","  (5, 0.0030485030485030494),\n","  (6, 0.015060515060515063),\n","  (7, 0.02206752206752207),\n","  (8, 0.0036855036855036865),\n","  (9, 0.017335517335517338),\n","  (10, 0.013604513604513608),\n","  (11, 0.005960505960505963),\n","  (12, 0.04727454727454729),\n","  (13, 0.20870870870870875),\n","  (14, 0.2836017836017837),\n","  (15, 0.010237510237510239),\n","  (16, 0.008690508690508691),\n","  (17, 0.104968604968605),\n","  (18, 0.006597506597506599),\n","  (19, 0.07502957502957504)],\n"," [(0, 0.020923520923520927),\n","  (1, 0.01014911014911015),\n","  (2, 0.010341510341510343),\n","  (3, 0.005627705627705629),\n","  (4, 0.006685906685906687),\n","  (5, 0.005916305916305918),\n","  (6, 0.5568542568542569),\n","  (7, 0.015536315536315537),\n","  (8, 0.004954304954304955),\n","  (9, 0.016113516113516118),\n","  (10, 0.012650312650312651),\n","  (11, 0.11471861471861473),\n","  (12, 0.035545935545935554),\n","  (13, 0.05911495911495912),\n","  (14, 0.006589706589706591),\n","  (15, 0.06344396344396346),\n","  (16, 0.02438672438672439),\n","  (17, 0.007359307359307361),\n","  (18, 0.014670514670514672),\n","  (19, 0.008417508417508419)],\n"," [(0, 0.008241758241758242),\n","  (1, 0.006497470783185069),\n","  (2, 0.007282400139542997),\n","  (3, 0.004404325832897261),\n","  (4, 0.0055381126809698235),\n","  (5, 0.004578754578754579),\n","  (6, 0.249738356881214),\n","  (7, 0.028388278388278388),\n","  (8, 0.003532182103610675),\n","  (9, 0.007456828885400314),\n","  (10, 0.007544043258328972),\n","  (11, 0.21485260770975056),\n","  (12, 0.06135531135531135),\n","  (13, 0.021149485435199723),\n","  (14, 0.015044479330193617),\n","  (15, 0.023417059131344845),\n","  (16, 0.037545787545787544),\n","  (17, 0.27485609628466773),\n","  (18, 0.004665968951683238),\n","  (19, 0.013910692482121054)],\n"," [(0, 0.5758317639673571),\n","  (1, 0.004708097928436911),\n","  (2, 0.025423728813559324),\n","  (3, 0.007846829880728186),\n","  (4, 0.008976773383553044),\n","  (5, 0.006089139987445072),\n","  (6, 0.018016321406151916),\n","  (7, 0.005461393596986817),\n","  (8, 0.017011927181418706),\n","  (9, 0.005712492153170119),\n","  (10, 0.006465787821720025),\n","  (11, 0.005210295040803515),\n","  (12, 0.006089139987445072),\n","  (13, 0.037225360954174516),\n","  (14, 0.015630885122410548),\n","  (15, 0.008725674827369743),\n","  (16, 0.007721280602636535),\n","  (17, 0.00847457627118644),\n","  (18, 0.2142498430634024),\n","  (19, 0.015128688010043943)],\n"," [(0, 0.021614493750716664),\n","  (1, 0.006249283339066622),\n","  (2, 0.13834422657952072),\n","  (3, 0.006707946336429309),\n","  (4, 0.05005159958720331),\n","  (5, 0.00556128884302259),\n","  (6, 0.025283797729618168),\n","  (7, 0.016339869281045756),\n","  (8, 0.04454764361885106),\n","  (9, 0.007739938080495357),\n","  (10, 0.006707946336429309),\n","  (11, 0.005790620341703934),\n","  (12, 0.01851851851851852),\n","  (13, 0.11460841646600162),\n","  (14, 0.4463364293085656),\n","  (15, 0.01725719527577113),\n","  (16, 0.022990482742804727),\n","  (17, 0.009115927072583421),\n","  (18, 0.01828918701983718),\n","  (19, 0.01794518977181516)],\n"," [(0, 0.12032327342602012),\n","  (1, 0.009551260314230812),\n","  (2, 0.09692551147281564),\n","  (3, 0.015768056968463885),\n","  (4, 0.09884706680230587),\n","  (5, 0.004012659658641347),\n","  (6, 0.007177574318978185),\n","  (7, 0.30807053238385895),\n","  (8, 0.005369051655928564),\n","  (9, 0.13343506273312988),\n","  (10, 0.008307900983384199),\n","  (11, 0.02695829094608342),\n","  (12, 0.07499717418333898),\n","  (13, 0.008646998982706003),\n","  (14, 0.009664292980671414),\n","  (15, 0.007855770317621793),\n","  (16, 0.018254775630157115),\n","  (17, 0.012037978975924042),\n","  (18, 0.00932519498134961),\n","  (19, 0.02447157228439019)],\n"," [(0, 0.02158047061784321),\n","  (1, 0.03152132880332201),\n","  (2, 0.022964640744935195),\n","  (3, 0.018057128476154524),\n","  (4, 0.32194538819680385),\n","  (5, 0.007612935699005914),\n","  (6, 0.01063294324902479),\n","  (7, 0.03403800176167107),\n","  (8, 0.0052220963885743045),\n","  (9, 0.015414621869888009),\n","  (10, 0.09557065559330565),\n","  (11, 0.007109601107336102),\n","  (12, 0.03630300742418523),\n","  (13, 0.27362526739650184),\n","  (14, 0.00421542720523468),\n","  (15, 0.006983767459418648),\n","  (16, 0.019441298603246508),\n","  (17, 0.03391216811375362),\n","  (18, 0.011136277840694602),\n","  (19, 0.02271297344910029)],\n"," [(0, 0.04745817245817246),\n","  (1, 0.004772629772629772),\n","  (2, 0.09185971685971686),\n","  (3, 0.07051694551694551),\n","  (4, 0.005523380523380523),\n","  (5, 0.015604890604890604),\n","  (6, 0.01903689403689404),\n","  (7, 0.01463963963963964),\n","  (8, 0.019680394680394682),\n","  (9, 0.02836765336765337),\n","  (10, 0.08403045903045903),\n","  (11, 0.008311883311883312),\n","  (12, 0.01270913770913771),\n","  (13, 0.2592771342771343),\n","  (14, 0.04059416559416559),\n","  (15, 0.010135135135135136),\n","  (16, 0.04702917202917203),\n","  (17, 0.06987344487344488),\n","  (18, 0.020323895323895325),\n","  (19, 0.13025525525525525)],\n"," [(0, 0.058134319991030386),\n","  (1, 0.01552864670927234),\n","  (2, 0.07607355084650745),\n","  (3, 0.009698396681242291),\n","  (4, 0.03077699293642785),\n","  (5, 0.1236125126135217),\n","  (6, 0.02404978136562395),\n","  (7, 0.05051014687745263),\n","  (8, 0.015192286130732145),\n","  (9, 0.004877228388832829),\n","  (10, 0.08324924318869828),\n","  (11, 0.008128713981388048),\n","  (12, 0.028310348693799753),\n","  (13, 0.04277385357102814),\n","  (14, 0.16150913779571702),\n","  (15, 0.00566206973875995),\n","  (16, 0.03974660836416639),\n","  (17, 0.01160443995963673),\n","  (18, 0.013062002466644242),\n","  (19, 0.19749971969951788)],\n"," [(0, 0.023423131018067728),\n","  (1, 0.0067618738504814455),\n","  (2, 0.3443146164665152),\n","  (3, 0.004598074218327383),\n","  (4, 0.0155252623607054),\n","  (5, 0.003948934328681164),\n","  (6, 0.15974250784377367),\n","  (7, 0.011197663096397274),\n","  (8, 0.006004543979227524),\n","  (9, 0.05598831548198637),\n","  (10, 0.04938872660391648),\n","  (11, 0.003732554365465758),\n","  (12, 0.027317970355945038),\n","  (13, 0.12003678459374661),\n","  (14, 0.012928702802120523),\n","  (15, 0.014767932489451477),\n","  (16, 0.01693173212160554),\n","  (17, 0.005571784052796711),\n","  (18, 0.009466623390674023),\n","  (19, 0.10835226658011468)],\n"," [(0, 0.007257257257257259),\n","  (1, 0.012162162162162166),\n","  (2, 0.0377877877877878),\n","  (3, 0.014264264264264267),\n","  (4, 0.020070070070070074),\n","  (5, 0.007657657657657659),\n","  (6, 0.00845845845845846),\n","  (7, 0.015265265265265268),\n","  (8, 0.004454454454454455),\n","  (9, 0.006356356356356358),\n","  (10, 0.029179179179179185),\n","  (11, 0.004054054054054055),\n","  (12, 0.035385385385385396),\n","  (13, 0.4429929929929931),\n","  (14, 0.20965965965965971),\n","  (15, 0.021271271271271274),\n","  (16, 0.01906906906906907),\n","  (17, 0.007257257257257259),\n","  (18, 0.006856856856856859),\n","  (19, 0.09054054054054056)],\n"," [(0, 0.01391776391776392),\n","  (1, 0.01160776160776161),\n","  (2, 0.017729267729267733),\n","  (3, 0.041637791637791646),\n","  (4, 0.008373758373758375),\n","  (5, 0.009875259875259878),\n","  (6, 0.017151767151767156),\n","  (7, 0.046835296835296844),\n","  (8, 0.07998382998383),\n","  (9, 0.06323631323631325),\n","  (10, 0.054227304227304235),\n","  (11, 0.010106260106260107),\n","  (12, 0.00802725802725803),\n","  (13, 0.022118272118272123),\n","  (14, 0.004562254562254563),\n","  (15, 0.00721875721875722),\n","  (16, 0.017960267960267964),\n","  (17, 0.3522176022176023),\n","  (18, 0.01957726957726958),\n","  (19, 0.19363594363594366)],\n"," [(0, 0.014959298023046835),\n","  (1, 0.008298974521619623),\n","  (2, 0.03589174331324665),\n","  (3, 0.004704514219262079),\n","  (4, 0.014007823237128663),\n","  (5, 0.03705465694047997),\n","  (6, 0.047838037847552604),\n","  (7, 0.2959615181308807),\n","  (8, 0.007241780315043874),\n","  (9, 0.011364837720689292),\n","  (10, 0.011153398879374143),\n","  (11, 0.01675652817422561),\n","  (12, 0.03303731895549213),\n","  (13, 0.2588540014800719),\n","  (14, 0.005761708425837828),\n","  (15, 0.007770377418331749),\n","  (16, 0.01918807484934983),\n","  (17, 0.06179300137435247),\n","  (18, 0.004175917115974205),\n","  (19, 0.10418648905803998)],\n"," [(0, 0.00340466926070039),\n","  (1, 0.005458279290964116),\n","  (2, 0.003296584522265457),\n","  (3, 0.011511024643320366),\n","  (4, 0.007403804582792911),\n","  (5, 0.01961738002594034),\n","  (6, 0.46946606139213154),\n","  (7, 0.1288910505836576),\n","  (8, 0.006214872460008648),\n","  (9, 0.03853220925205362),\n","  (10, 0.011619109381755298),\n","  (11, 0.0058906182447038494),\n","  (12, 0.009349329874621705),\n","  (13, 0.03972114137483788),\n","  (14, 0.006431041936878514),\n","  (15, 0.1382944228274968),\n","  (16, 0.008592736705577175),\n","  (17, 0.04036964980544748),\n","  (18, 0.006322957198443582),\n","  (19, 0.039613056636402945)],\n"," [(0, 0.1668083900226757),\n","  (1, 0.0035430839002267567),\n","  (2, 0.014880952380952377),\n","  (3, 0.004582388510959939),\n","  (4, 0.004676870748299319),\n","  (5, 0.007700302343159484),\n","  (6, 0.43438208616780033),\n","  (7, 0.08640400604686317),\n","  (8, 0.006660997732426302),\n","  (9, 0.010251322751322749),\n","  (10, 0.008078231292517005),\n","  (11, 0.02092781557067271),\n","  (12, 0.06901927437641722),\n","  (13, 0.038218065003779285),\n","  (14, 0.006660997732426302),\n","  (15, 0.015353363567649278),\n","  (16, 0.039635298563869985),\n","  (17, 0.04700491307634164),\n","  (18, 0.0034486016628873764),\n","  (19, 0.011763038548752831)],\n"," [(0, 0.009316164276655936),\n","  (1, 0.012730465320456541),\n","  (2, 0.0034630767729977563),\n","  (3, 0.006487171983221149),\n","  (4, 0.026485220954053264),\n","  (5, 0.0041459369817578775),\n","  (6, 0.05487269534679543),\n","  (7, 0.09857574870744318),\n","  (8, 0.0042434884401521804),\n","  (9, 0.009023509901473027),\n","  (10, 0.008828406984684421),\n","  (11, 0.019363964491269144),\n","  (12, 0.336113549897571),\n","  (13, 0.20783338210906252),\n","  (14, 0.0081455467759243),\n","  (15, 0.0309725880401912),\n","  (16, 0.05243390888693786),\n","  (17, 0.0896985659935616),\n","  (18, 0.009511267193444542),\n","  (19, 0.007755340942347088)],\n"," [(0, 0.015557359307359308),\n","  (1, 0.005456349206349206),\n","  (2, 0.012761544011544012),\n","  (3, 0.008432539682539682),\n","  (4, 0.011228354978354978),\n","  (5, 0.006538600288600289),\n","  (6, 0.032873376623376624),\n","  (7, 0.025748556998557),\n","  (8, 0.007981601731601732),\n","  (9, 0.020337301587301588),\n","  (10, 0.010957792207792208),\n","  (11, 0.005817099567099567),\n","  (12, 0.21270743145743146),\n","  (13, 0.22073412698412698),\n","  (14, 0.004644660894660895),\n","  (15, 0.00744047619047619),\n","  (16, 0.34248737373737376),\n","  (17, 0.025748556998557),\n","  (18, 0.006628787878787879),\n","  (19, 0.015918109668109668)],\n"," [(0, 0.008883477633477634),\n","  (1, 0.0864448051948052),\n","  (2, 0.042523448773448776),\n","  (3, 0.007711038961038961),\n","  (4, 0.013483044733044734),\n","  (5, 0.008252164502164502),\n","  (6, 0.006268037518037518),\n","  (7, 0.01709054834054834),\n","  (8, 0.006628787878787879),\n","  (9, 0.1544462481962482),\n","  (10, 0.005726911976911977),\n","  (11, 0.12829184704184704),\n","  (12, 0.09483225108225109),\n","  (13, 0.2214556277056277),\n","  (14, 0.10926226551226552),\n","  (15, 0.004644660894660895),\n","  (16, 0.04297438672438673),\n","  (17, 0.023042929292929292),\n","  (18, 0.00744047619047619),\n","  (19, 0.010597041847041848)],\n"," [(0, 0.007776980138789185),\n","  (1, 0.013679508654383029),\n","  (2, 0.014556911541836167),\n","  (3, 0.03106803860572705),\n","  (4, 0.025245273988992584),\n","  (5, 0.003070910106085986),\n","  (6, 0.06369147323921194),\n","  (7, 0.015035494934992424),\n","  (8, 0.015035494934992424),\n","  (9, 0.011127063890882988),\n","  (10, 0.010568716598867354),\n","  (11, 0.010409188801148602),\n","  (12, 0.14225891361569756),\n","  (13, 0.04414931801866476),\n","  (14, 0.2943686687405281),\n","  (15, 0.1901970168301827),\n","  (16, 0.07071069633883706),\n","  (17, 0.01128659168860174),\n","  (18, 0.014556911541836167),\n","  (19, 0.011206827789742364)],\n"," [(0, 0.033897047915739505),\n","  (1, 0.009419967363892598),\n","  (2, 0.012980269989615784),\n","  (3, 0.013721999703308115),\n","  (4, 0.008678237650200267),\n","  (5, 0.015057113187954309),\n","  (6, 0.029298323690847054),\n","  (7, 0.13729417000445038),\n","  (8, 0.007936507936507936),\n","  (9, 0.01431538347426198),\n","  (10, 0.022474410324877615),\n","  (11, 0.011200118676754191),\n","  (12, 0.02544132917964694),\n","  (13, 0.32821539830885627),\n","  (14, 0.03997923156801662),\n","  (15, 0.012831924046877318),\n","  (16, 0.05985758789497107),\n","  (17, 0.06742323097463285),\n","  (18, 0.01312861593235425),\n","  (19, 0.13684913217623498)],\n"," [(0, 0.05424450999672239),\n","  (1, 0.005954331913033978),\n","  (2, 0.008794930623839178),\n","  (3, 0.007374631268436578),\n","  (4, 0.00628209330274227),\n","  (5, 0.005845078116464547),\n","  (6, 0.017862995739101935),\n","  (7, 0.5304818092428711),\n","  (8, 0.006609854692450562),\n","  (9, 0.10340871845296624),\n","  (10, 0.015350158418005026),\n","  (11, 0.004534032557631378),\n","  (12, 0.05380749481044466),\n","  (13, 0.037528679121599476),\n","  (14, 0.007483885065006009),\n","  (15, 0.006172839506172839),\n","  (16, 0.10417349502895226),\n","  (17, 0.012181798317491534),\n","  (18, 0.005298809133617393),\n","  (19, 0.006609854692450562)],\n"," [(0, 0.038401166050005604),\n","  (1, 0.03750420450723175),\n","  (2, 0.19850880143513847),\n","  (3, 0.054210113241394776),\n","  (4, 0.020461935194528536),\n","  (5, 0.004540867810292634),\n","  (6, 0.26252943155062225),\n","  (7, 0.012052920731023658),\n","  (8, 0.006559031281533804),\n","  (9, 0.008801435138468438),\n","  (10, 0.01160443995963673),\n","  (11, 0.005774189931606682),\n","  (12, 0.028983069850880144),\n","  (13, 0.1572485704675412),\n","  (14, 0.010483238031169413),\n","  (15, 0.03133759390066151),\n","  (16, 0.04658594012781702),\n","  (17, 0.01620136786635273),\n","  (18, 0.029767911200807264),\n","  (19, 0.018443771723287365)],\n"," [(0, 0.023290003095017022),\n","  (1, 0.024837511606313836),\n","  (2, 0.009826679046734756),\n","  (3, 0.00874342308882699),\n","  (4, 0.012921696069328381),\n","  (5, 0.005029402661714639),\n","  (6, 0.08735685546270504),\n","  (7, 0.03922934076137419),\n","  (8, 0.08890436397400185),\n","  (9, 0.006731662024141133),\n","  (10, 0.011683689260290932),\n","  (11, 0.0045651501083255955),\n","  (12, 0.32180439492417207),\n","  (13, 0.1583874961312287),\n","  (14, 0.010290931600123801),\n","  (15, 0.010445682451253482),\n","  (16, 0.0127669452181987),\n","  (17, 0.017254719900959455),\n","  (18, 0.12093779015784586),\n","  (19, 0.024992262457443515)],\n"," [(0, 0.05746421267893661),\n","  (1, 0.017655078391274715),\n","  (2, 0.012201772324471713),\n","  (3, 0.01970006816632584),\n","  (4, 0.027198364008179963),\n","  (5, 0.03278800272665304),\n","  (6, 0.2068847989093388),\n","  (7, 0.025426039536468988),\n","  (8, 0.006475800954328563),\n","  (9, 0.029243353783231087),\n","  (10, 0.01888207225630539),\n","  (11, 0.008520790729379689),\n","  (12, 0.1481254260395365),\n","  (13, 0.0663258350374915),\n","  (14, 0.012065439672801638),\n","  (15, 0.08104976141785959),\n","  (16, 0.04492160872528971),\n","  (17, 0.15480572597137018),\n","  (18, 0.008929788684389914),\n","  (19, 0.02133605998636674)],\n"," [(0, 0.015865280485163526),\n","  (1, 0.009150963829326402),\n","  (2, 0.027452891487979206),\n","  (3, 0.00546891921160927),\n","  (4, 0.0050357374918778425),\n","  (5, 0.005902100931340698),\n","  (6, 0.20873944119558155),\n","  (7, 0.2512995451591943),\n","  (8, 0.0146740307559021),\n","  (9, 0.023337665150530647),\n","  (10, 0.0297270955165692),\n","  (11, 0.005577214641542127),\n","  (12, 0.053876976391596276),\n","  (13, 0.11582196231319038),\n","  (14, 0.012291531297379251),\n","  (15, 0.011858349577647823),\n","  (16, 0.14452025124539744),\n","  (17, 0.005252328351743557),\n","  (18, 0.0077431232401992635),\n","  (19, 0.046404591726229155)],\n"," [(0, 0.009455481972038266),\n","  (1, 0.024245768947755705),\n","  (2, 0.0065857247976453285),\n","  (3, 0.028145695364238416),\n","  (4, 0.009896983075791024),\n","  (5, 0.02542310522442973),\n","  (6, 0.010044150110375278),\n","  (7, 0.0036423841059602655),\n","  (8, 0.1299852832965416),\n","  (9, 0.006659308314937455),\n","  (10, 0.034253127299484924),\n","  (11, 0.006364974245768948),\n","  (12, 0.027262693156732895),\n","  (13, 0.11931567328918323),\n","  (14, 0.4335172921265637),\n","  (15, 0.008793230316409125),\n","  (16, 0.030721118469462842),\n","  (17, 0.01710816777041943),\n","  (18, 0.05286975717439294),\n","  (19, 0.015710080941869025)],\n"," [(0, 0.028664521983007507),\n","  (1, 0.02924193681431989),\n","  (2, 0.032376474470015675),\n","  (3, 0.023220325002062194),\n","  (4, 0.028334570650829),\n","  (5, 0.005732904396601501),\n","  (6, 0.017611152355027634),\n","  (7, 0.020993153509857296),\n","  (8, 0.005732904396601501),\n","  (9, 0.013156809370617834),\n","  (10, 0.029901839478676896),\n","  (11, 0.005402953064422998),\n","  (12, 0.3082982760042894),\n","  (13, 0.057782727047760454),\n","  (14, 0.059349995875608345),\n","  (15, 0.005732904396601501),\n","  (16, 0.17747257279551265),\n","  (17, 0.018188567186340016),\n","  (18, 0.10042893673183205),\n","  (19, 0.032376474470015675)],\n"," [(0, 0.042449648260576275),\n","  (1, 0.2565770453888407),\n","  (2, 0.00823937554206418),\n","  (3, 0.004384696925893804),\n","  (4, 0.006697504095596029),\n","  (5, 0.008721210369085478),\n","  (6, 0.01845427387491568),\n","  (7, 0.054206418039895926),\n","  (8, 0.007564806784234364),\n","  (9, 0.0057338344415534355),\n","  (10, 0.019321576563554013),\n","  (11, 0.08388744338440783),\n","  (12, 0.07887636118338634),\n","  (13, 0.008528476438276958),\n","  (14, 0.007564806784234364),\n","  (15, 0.004095596029681025),\n","  (16, 0.011323118435000482),\n","  (17, 0.20617712248241302),\n","  (18, 0.003613761202659728),\n","  (19, 0.16358292377373038)],\n"," [(0, 0.008960573476702507),\n","  (1, 0.010686313553697065),\n","  (2, 0.026350723483339965),\n","  (3, 0.020908004778972516),\n","  (4, 0.6346077260055754),\n","  (5, 0.015598035311296957),\n","  (6, 0.016792778441523958),\n","  (7, 0.04480286738351254),\n","  (8, 0.0071020841630160605),\n","  (9, 0.02435948493296163),\n","  (10, 0.02050975706889685),\n","  (11, 0.01904951546528607),\n","  (12, 0.013474047524226733),\n","  (13, 0.014536041417761844),\n","  (14, 0.03723616089207486),\n","  (15, 0.00988981813354573),\n","  (16, 0.014668790654453734),\n","  (17, 0.013208549050842955),\n","  (18, 0.011748307447232174),\n","  (19, 0.03551042081508031)],\n"," [(0, 0.012923488827103286),\n","  (1, 0.007568736484399135),\n","  (2, 0.05730614766759345),\n","  (3, 0.00746576047780867),\n","  (4, 0.4547935331067861),\n","  (5, 0.006436000411904026),\n","  (6, 0.011069920708474925),\n","  (7, 0.01467408093914118),\n","  (8, 0.009010400576665637),\n","  (9, 0.02208835341365462),\n","  (10, 0.07882813304500051),\n","  (11, 0.014159200906188859),\n","  (12, 0.007259808464627742),\n","  (13, 0.04968592317989908),\n","  (14, 0.022706209453197405),\n","  (15, 0.006024096385542169),\n","  (16, 0.015394912985274432),\n","  (17, 0.01302646483369375),\n","  (18, 0.005097312326227989),\n","  (19, 0.18448151580681701)],\n"," [(0, 0.011569416498993965),\n","  (1, 0.08053878828526717),\n","  (2, 0.130952380952381),\n","  (3, 0.05371115582383189),\n","  (4, 0.006651017214397497),\n","  (5, 0.03862061256427454),\n","  (6, 0.026659959758551312),\n","  (7, 0.06500111781801925),\n","  (8, 0.009669125866308966),\n","  (9, 0.009669125866308966),\n","  (10, 0.04622177509501454),\n","  (11, 0.21031746031746035),\n","  (12, 0.005309635591325733),\n","  (13, 0.18863179074446682),\n","  (14, 0.005644980997093674),\n","  (15, 0.008215962441314556),\n","  (16, 0.012463670914375141),\n","  (17, 0.07181980773530071),\n","  (18, 0.008327744243237203),\n","  (19, 0.010004471272076907)],\n"," [(0, 0.02261814505938842),\n","  (1, 0.03390615786370145),\n","  (2, 0.1987616881475865),\n","  (3, 0.006360037065116669),\n","  (4, 0.41668772639204776),\n","  (5, 0.005264931345295256),\n","  (6, 0.07105551343610478),\n","  (7, 0.03685451941706679),\n","  (8, 0.004422542330048016),\n","  (9, 0.009308398618482013),\n","  (10, 0.0061915592620672205),\n","  (11, 0.008550248504759496),\n","  (12, 0.02320781737006149),\n","  (13, 0.04839524892595399),\n","  (14, 0.003411675511751326),\n","  (15, 0.008971443012383116),\n","  (16, 0.07029736332238226),\n","  (17, 0.016974138657231908),\n","  (18, 0.0031589588071771536),\n","  (19, 0.005601886951394153)],\n"," [(0, 0.05045653978418118),\n","  (1, 0.004683979603936914),\n","  (2, 0.020811099252934895),\n","  (3, 0.014882011146685636),\n","  (4, 0.5154156290762479),\n","  (5, 0.004802561366061899),\n","  (6, 0.009782995375311276),\n","  (7, 0.04642475987193169),\n","  (8, 0.003616743744812047),\n","  (9, 0.0103759041859362),\n","  (10, 0.025910115024309253),\n","  (11, 0.005039724890311869),\n","  (12, 0.211490572749911),\n","  (13, 0.007411360132811572),\n","  (14, 0.007055614846436616),\n","  (15, 0.011798885331436021),\n","  (16, 0.010257322423811216),\n","  (17, 0.026147278548559225),\n","  (18, 0.007648523657061542),\n","  (19, 0.00598837898731175)],\n"," [(0, 0.004332565536909071),\n","  (1, 0.08275748601513656),\n","  (2, 0.014972030273116157),\n","  (3, 0.016288252714708785),\n","  (4, 0.16414390698694747),\n","  (5, 0.006416584402764067),\n","  (6, 0.018043215970165626),\n","  (7, 0.06948557639574421),\n","  (8, 0.005429417571569595),\n","  (9, 0.0066359548096961716),\n","  (10, 0.3168257102116924),\n","  (11, 0.007623121640890644),\n","  (12, 0.0801250411319513),\n","  (13, 0.048426017330262146),\n","  (14, 0.031095755182625865),\n","  (15, 0.028024569485576394),\n","  (16, 0.022979050126137983),\n","  (17, 0.013875178238455632),\n","  (18, 0.024843698585060876),\n","  (19, 0.03767686739058901)],\n"," [(0, 0.007373400111296605),\n","  (1, 0.35332034872936374),\n","  (2, 0.00811537748098683),\n","  (3, 0.008022630309775552),\n","  (4, 0.01052680393248006),\n","  (5, 0.046141717677610834),\n","  (6, 0.0042199962901131515),\n","  (7, 0.005611203858282322),\n","  (8, 0.11162122055277314),\n","  (9, 0.00524021517343721),\n","  (10, 0.019430532368762753),\n","  (11, 0.15048228529029864),\n","  (12, 0.07109070673344463),\n","  (13, 0.12516230754961974),\n","  (14, 0.010897792617325172),\n","  (15, 0.012196253014283065),\n","  (16, 0.00524021517343721),\n","  (17, 0.01859580782786125),\n","  (18, 0.013587460582452235),\n","  (19, 0.013123724726395844)],\n"," [(0, 0.024900505539421323),\n","  (1, 0.014144347639023343),\n","  (2, 0.005324298160697),\n","  (3, 0.005969667634720879),\n","  (4, 0.2576637625040336),\n","  (5, 0.007905776056792515),\n","  (6, 0.0070452834247606766),\n","  (7, 0.0809400882004948),\n","  (8, 0.0044638055286651615),\n","  (9, 0.011777992900935787),\n","  (10, 0.11191782295364097),\n","  (11, 0.008873830267828333),\n","  (12, 0.3806066473055825),\n","  (13, 0.007475529740776595),\n","  (14, 0.023502205012369586),\n","  (15, 0.014251909218027322),\n","  (16, 0.01059481553189201),\n","  (17, 0.005754544476712919),\n","  (18, 0.009196515004840273),\n","  (19, 0.007690652898784555)],\n"," [(0, 0.06499553514478888),\n","  (1, 0.23861461921163413),\n","  (2, 0.011672407194795254),\n","  (3, 0.014606454904962367),\n","  (4, 0.1079857124633244),\n","  (5, 0.005804311774461028),\n","  (6, 0.01384105115448399),\n","  (7, 0.028001020538333973),\n","  (8, 0.005931879066207424),\n","  (9, 0.016392396989411914),\n","  (10, 0.21182548794489092),\n","  (11, 0.006442148233193009),\n","  (12, 0.11270570225794106),\n","  (13, 0.03501722158438576),\n","  (14, 0.031190202831993877),\n","  (15, 0.011544839903048859),\n","  (16, 0.014989156780201556),\n","  (17, 0.04649827784156142),\n","  (18, 0.006314580941446613),\n","  (19, 0.01562699323893354)],\n"," [(0, 0.03240740740740741),\n","  (1, 0.11846405228758171),\n","  (2, 0.03850762527233116),\n","  (3, 0.012908496732026145),\n","  (4, 0.26051198257080616),\n","  (5, 0.011710239651416123),\n","  (6, 0.04449891067538127),\n","  (7, 0.05070806100217866),\n","  (8, 0.03534858387799565),\n","  (9, 0.007788671023965143),\n","  (10, 0.06955337690631809),\n","  (11, 0.010838779956427017),\n","  (12, 0.00866013071895425),\n","  (13, 0.08044662309368193),\n","  (14, 0.061383442265795216),\n","  (15, 0.010838779956427017),\n","  (16, 0.042755991285403055),\n","  (17, 0.03131808278867103),\n","  (18, 0.03306100217864925),\n","  (19, 0.03828976034858388)],\n"," [(0, 0.01803907467337888),\n","  (1, 0.023792400815054536),\n","  (2, 0.36072156298693514),\n","  (3, 0.012645331415557954),\n","  (4, 0.04920292460745535),\n","  (5, 0.027148507731032005),\n","  (6, 0.02762795157617164),\n","  (7, 0.021155459666786527),\n","  (8, 0.026309481002037637),\n","  (9, 0.033620999640417114),\n","  (10, 0.081445523193096),\n","  (11, 0.005333812777178473),\n","  (12, 0.03733668944024931),\n","  (13, 0.026429341963322545),\n","  (14, 0.00868991969315594),\n","  (15, 0.006172839506172839),\n","  (16, 0.011566582763993766),\n","  (17, 0.02487114946661872),\n","  (18, 0.014323384873546686),\n","  (19, 0.18356706220783892)],\n"," [(0, 0.008581752484191508),\n","  (1, 0.035782394860985646),\n","  (2, 0.03116531165311653),\n","  (3, 0.058667068152163),\n","  (4, 0.011793636454883067),\n","  (5, 0.07904245709123758),\n","  (6, 0.09470039144835893),\n","  (7, 0.005771354009836395),\n","  (8, 0.012596607447555957),\n","  (9, 0.08245508381009736),\n","  (10, 0.00787915286560273),\n","  (11, 0.15642878651008732),\n","  (12, 0.04401284753588277),\n","  (13, 0.25338753387533874),\n","  (14, 0.027451570812004417),\n","  (15, 0.027451570812004417),\n","  (16, 0.021730402489210078),\n","  (17, 0.026447857071163303),\n","  (18, 0.005068754391247616),\n","  (19, 0.00958546622503262)],\n"," [(0, 0.003179947624392068),\n","  (1, 0.23999251777029548),\n","  (2, 0.008193041526374858),\n","  (3, 0.05211372989150766),\n","  (4, 0.0056490834268612036),\n","  (5, 0.3320239431350542),\n","  (6, 0.008791619902731012),\n","  (7, 0.05847362514029179),\n","  (8, 0.0072951739618406265),\n","  (9, 0.008941264496820051),\n","  (10, 0.006546950991395434),\n","  (11, 0.12005237560793114),\n","  (12, 0.008567153011597455),\n","  (13, 0.09289188178077065),\n","  (14, 0.0239057239057239),\n","  (15, 0.006023194912083799),\n","  (16, 0.0032547699214365874),\n","  (17, 0.003554059109614664),\n","  (18, 0.004302282080059857),\n","  (19, 0.006247661803217357)],\n"," [(0, 0.020503314234355686),\n","  (1, 0.015335355578024937),\n","  (2, 0.026682395236490276),\n","  (3, 0.10465116279069765),\n","  (4, 0.005336479047298055),\n","  (5, 0.5203347938433883),\n","  (6, 0.005448825974609594),\n","  (7, 0.016571171778451854),\n","  (8, 0.172171666104932),\n","  (9, 0.01185260083136726),\n","  (10, 0.00792045837546343),\n","  (11, 0.009718009212448038),\n","  (12, 0.004100662846871137),\n","  (13, 0.004100662846871137),\n","  (14, 0.006459948320413435),\n","  (15, 0.004550050556117289),\n","  (16, 0.01185260083136726),\n","  (17, 0.03331086394787102),\n","  (18, 0.006684642175036511),\n","  (19, 0.012414335467924949)],\n"," [(0, 0.018174913194444444),\n","  (1, 0.011338975694444444),\n","  (2, 0.018283420138888888),\n","  (3, 0.005588107638888889),\n","  (4, 0.010362413194444444),\n","  (5, 0.05788845486111111),\n","  (6, 0.024034288194444444),\n","  (7, 0.009602864583333334),\n","  (8, 0.013292100694444444),\n","  (9, 0.04150390625),\n","  (10, 0.006564670138888889),\n","  (11, 0.52587890625),\n","  (12, 0.041829427083333336),\n","  (13, 0.09629991319444445),\n","  (14, 0.005805121527777778),\n","  (15, 0.013726128472222222),\n","  (16, 0.005479600694444444),\n","  (17, 0.006456163194444444),\n","  (18, 0.018934461805555556),\n","  (19, 0.06895616319444445)],\n"," [(0, 0.011991499696417727),\n","  (1, 0.01705120420967415),\n","  (2, 0.018872697834446464),\n","  (3, 0.003997166565472576),\n","  (4, 0.005717466099979761),\n","  (5, 0.006021048370775145),\n","  (6, 0.18503339404978747),\n","  (7, 0.07776765836875126),\n","  (8, 0.004300748836267961),\n","  (9, 0.2928051001821493),\n","  (10, 0.005211495648654117),\n","  (11, 0.15659785468528636),\n","  (12, 0.04730823719894757),\n","  (13, 0.06724347298117789),\n","  (14, 0.003491196114146933),\n","  (15, 0.014318963772515682),\n","  (16, 0.04538554948391013),\n","  (17, 0.026057478243270586),\n","  (18, 0.003491196114146933),\n","  (19, 0.007336571544221816)],\n"," [(0, 0.010609938153124334),\n","  (1, 0.020420132224354872),\n","  (2, 0.02948389848581787),\n","  (3, 0.00762422691405417),\n","  (4, 0.010929835785881852),\n","  (5, 0.006664534015781616),\n","  (6, 0.01711452335252719),\n","  (7, 0.054116016208146725),\n","  (8, 0.006131371294519087),\n","  (9, 0.10263382384303689),\n","  (10, 0.030017061207080403),\n","  (11, 0.3063019833653231),\n","  (12, 0.09133077415227127),\n","  (13, 0.02052676476860738),\n","  (14, 0.03598848368522073),\n","  (15, 0.013489016847941992),\n","  (16, 0.04590531030070377),\n","  (17, 0.006984431648539134),\n","  (18, 0.03289613990189806),\n","  (19, 0.15083173384516954)],\n"," [(0, 0.008459963358120487),\n","  (1, 0.00964543593059597),\n","  (2, 0.00792111218881345),\n","  (3, 0.07948054747278802),\n","  (4, 0.010723138269210045),\n","  (5, 0.46077163487444767),\n","  (6, 0.0067356396163379675),\n","  (7, 0.012986313180299601),\n","  (8, 0.08088156051298631),\n","  (9, 0.008459963358120487),\n","  (10, 0.0807737902791249),\n","  (11, 0.11418256277616122),\n","  (12, 0.00964543593059597),\n","  (13, 0.027427524517728202),\n","  (14, 0.012555232244853972),\n","  (15, 0.008028882422674858),\n","  (16, 0.019344756978122643),\n","  (17, 0.006843409850199375),\n","  (18, 0.027643064985451018),\n","  (19, 0.00749003125336782)],\n"," [(0, 0.005600358422939068),\n","  (1, 0.22665770609318997),\n","  (2, 0.038485663082437276),\n","  (3, 0.020922939068100357),\n","  (4, 0.0072132616487455194),\n","  (5, 0.3774641577060932),\n","  (6, 0.0038082437275985663),\n","  (7, 0.009363799283154122),\n","  (8, 0.1439516129032258),\n","  (9, 0.017338709677419354),\n","  (10, 0.023252688172043012),\n","  (11, 0.02271505376344086),\n","  (12, 0.006496415770609319),\n","  (13, 0.02620967741935484),\n","  (14, 0.019489247311827957),\n","  (15, 0.004077060931899641),\n","  (16, 0.008198924731182796),\n","  (17, 0.011245519713261648),\n","  (18, 0.010080645161290322),\n","  (19, 0.01742831541218638)],\n"," [(0, 0.009858197358197358),\n","  (1, 0.03530497280497281),\n","  (2, 0.18176961926961926),\n","  (3, 0.07502913752913754),\n","  (4, 0.015977078477078476),\n","  (5, 0.03229409479409479),\n","  (6, 0.14437645687645687),\n","  (7, 0.008304195804195804),\n","  (8, 0.003447940947940948),\n","  (9, 0.028797591297591296),\n","  (10, 0.018308080808080808),\n","  (11, 0.3013306138306138),\n","  (12, 0.00743006993006993),\n","  (13, 0.015297202797202798),\n","  (14, 0.005584693084693085),\n","  (15, 0.014617327117327118),\n","  (16, 0.003933566433566434),\n","  (17, 0.055798368298368296),\n","  (18, 0.013451825951825952),\n","  (19, 0.029088966588966588)],\n"," [(0, 0.041625207296849084),\n","  (1, 0.20956329463792145),\n","  (2, 0.007131011608623547),\n","  (3, 0.008347153123272525),\n","  (4, 0.00657822001105583),\n","  (5, 0.2824212271973465),\n","  (6, 0.010558319513543392),\n","  (7, 0.007462686567164177),\n","  (8, 0.13825317855168598),\n","  (9, 0.010116086235489218),\n","  (10, 0.0064676616915422865),\n","  (11, 0.05732448866777223),\n","  (12, 0.04892205638474294),\n","  (13, 0.012327252625760085),\n","  (14, 0.008236594803758981),\n","  (15, 0.004035378662244333),\n","  (16, 0.03079049198452183),\n","  (17, 0.0906025428413488),\n","  (18, 0.008015478164731894),\n","  (19, 0.011221669430624652)],\n"," [(0, 0.0058354682607458025),\n","  (1, 0.20338741816111588),\n","  (2, 0.03828636493025905),\n","  (3, 0.006784324888509347),\n","  (4, 0.007638295853496537),\n","  (5, 0.010389980074010818),\n","  (6, 0.02708985672264921),\n","  (7, 0.0059303539235221565),\n","  (8, 0.005550811272416739),\n","  (9, 0.008871809469589147),\n","  (10, 0.006594553562956638),\n","  (11, 0.06665717810038904),\n","  (12, 0.004222411993547776),\n","  (13, 0.11495398045355348),\n","  (14, 0.0032735553657842306),\n","  (15, 0.36336464560204956),\n","  (16, 0.016178005503368443),\n","  (17, 0.05470158459056837),\n","  (18, 0.009725780434576337),\n","  (19, 0.04056362083689155)],\n"," [(0, 0.026926472442931015),\n","  (1, 0.016584689115903647),\n","  (2, 0.030836171017782826),\n","  (3, 0.021124984235086394),\n","  (4, 0.016584689115903647),\n","  (5, 0.008513053348467652),\n","  (6, 0.010026485054861901),\n","  (7, 0.026043637280867706),\n","  (8, 0.0063690250977424655),\n","  (9, 0.1295875898600076),\n","  (10, 0.02125110354395258),\n","  (11, 0.004351116155883466),\n","  (12, 0.38901500819775514),\n","  (13, 0.015323496027241772),\n","  (14, 0.017719762895699334),\n","  (15, 0.007504098877538152),\n","  (16, 0.16338756463614582),\n","  (17, 0.05076302181864044),\n","  (18, 0.016710808424769834),\n","  (19, 0.021377222852818772)],\n"," [(0, 0.18778383287920067),\n","  (1, 0.009309718437783832),\n","  (2, 0.016878595216469873),\n","  (3, 0.007190432939751739),\n","  (4, 0.010066606115652436),\n","  (5, 0.009461095973357552),\n","  (6, 0.01294277929155313),\n","  (7, 0.07243415077202542),\n","  (8, 0.005525280048440811),\n","  (9, 0.04488343929760823),\n","  (10, 0.012640024220405688),\n","  (11, 0.008098698153194065),\n","  (12, 0.3620193763245534),\n","  (13, 0.11648501362397817),\n","  (14, 0.01097487132909476),\n","  (15, 0.01294277929155313),\n","  (16, 0.06471389645776565),\n","  (17, 0.009461095973357552),\n","  (18, 0.009309718437783832),\n","  (19, 0.016878595216469873)],\n"," [(0, 0.017560304961879766),\n","  (1, 0.016685414323209597),\n","  (2, 0.08855143107111611),\n","  (3, 0.006311711036120485),\n","  (4, 0.006311711036120485),\n","  (5, 0.0074365704286964126),\n","  (6, 0.00981127359080115),\n","  (7, 0.09692538432695913),\n","  (8, 0.010936132983377077),\n","  (9, 0.08455193100862392),\n","  (10, 0.007311586051743532),\n","  (11, 0.02118485189351331),\n","  (12, 0.21728533933258343),\n","  (13, 0.007561554805649294),\n","  (14, 0.004811898512685914),\n","  (15, 0.008311461067366578),\n","  (16, 0.09605049368828897),\n","  (17, 0.2731533558305212),\n","  (18, 0.006311711036120485),\n","  (19, 0.012935883014623173)],\n"," [(0, 0.011178519267498035),\n","  (1, 0.010055049994382656),\n","  (2, 0.0087068868666442),\n","  (3, 0.004999438265363443),\n","  (4, 0.03229974160206719),\n","  (5, 0.004325356701494215),\n","  (6, 0.11195371306594766),\n","  (7, 0.059263004156836316),\n","  (8, 0.007246376811594204),\n","  (9, 0.30024716324008544),\n","  (10, 0.045219638242894065),\n","  (11, 0.008145152230086508),\n","  (12, 0.12588473205257839),\n","  (13, 0.06297045275811708),\n","  (14, 0.011290866194809573),\n","  (15, 0.00926862150320189),\n","  (16, 0.14352319964048987),\n","  (17, 0.019941579597798003),\n","  (18, 0.009380968430513427),\n","  (19, 0.014099539377598025)],\n"," [(0, 0.3603591286117948),\n","  (1, 0.01617901740953423),\n","  (2, 0.1168079482271443),\n","  (3, 0.030398322851153042),\n","  (4, 0.022103728010208736),\n","  (5, 0.0035092516634764384),\n","  (6, 0.0878224409807675),\n","  (7, 0.07323853796372255),\n","  (8, 0.004238446814328686),\n","  (9, 0.04261234162792818),\n","  (10, 0.03650533223954061),\n","  (11, 0.00478534317746787),\n","  (12, 0.011530398322851155),\n","  (13, 0.011348099535138092),\n","  (14, 0.007064078023881142),\n","  (15, 0.005149940752893994),\n","  (16, 0.13731656184486377),\n","  (17, 0.011530398322851155),\n","  (18, 0.006334882873028895),\n","  (19, 0.011165800747425032)],\n"," [(0, 0.006127450980392157),\n","  (1, 0.03857376283846872),\n","  (2, 0.02456816059757236),\n","  (3, 0.011262838468720822),\n","  (4, 0.006711017740429505),\n","  (5, 0.03262138188608777),\n","  (6, 0.04872782446311858),\n","  (7, 0.03775676937441643),\n","  (8, 0.043242296918767506),\n","  (9, 0.007528011204481793),\n","  (10, 0.0717203548085901),\n","  (11, 0.024334733893557423),\n","  (12, 0.045809990662931836),\n","  (13, 0.12260737628384687),\n","  (14, 0.012896825396825396),\n","  (15, 0.05584733893557423),\n","  (16, 0.23955415499533148),\n","  (17, 0.006827731092436975),\n","  (18, 0.11723856209150327),\n","  (19, 0.04604341736694678)],\n"," [(0, 0.011073573573573574),\n","  (1, 0.007695195195195195),\n","  (2, 0.01707957957957958),\n","  (3, 0.012575075075075074),\n","  (4, 0.015202702702702704),\n","  (5, 0.09753503503503504),\n","  (6, 0.010698198198198198),\n","  (7, 0.04035285285285285),\n","  (8, 0.011824324324324325),\n","  (9, 0.1092967967967968),\n","  (10, 0.10316566566566567),\n","  (11, 0.033345845845845844),\n","  (12, 0.07426176176176176),\n","  (13, 0.1153028028028028),\n","  (14, 0.011574074074074073),\n","  (15, 0.034972472472472475),\n","  (16, 0.0381006006006006),\n","  (17, 0.14871121121121122),\n","  (18, 0.011949449449449449),\n","  (19, 0.09528278278278278)],\n"," [(0, 0.01637210576791341),\n","  (1, 0.029661597479106728),\n","  (2, 0.010617892862035896),\n","  (3, 0.007055761063159337),\n","  (4, 0.016098095629538293),\n","  (5, 0.009384847239347856),\n","  (6, 0.009110837100972735),\n","  (7, 0.02609946568023017),\n","  (8, 0.014454034799287573),\n","  (9, 0.1404986984518427),\n","  (10, 0.014865050006850254),\n","  (11, 0.015961090560350732),\n","  (12, 0.2729826003562132),\n","  (13, 0.07199616385806275),\n","  (14, 0.05158240854911632),\n","  (15, 0.012809973969036855),\n","  (16, 0.20338402520893273),\n","  (17, 0.03130565830935745),\n","  (18, 0.009110837100972735),\n","  (19, 0.03664885600767229)],\n"," [(0, 0.038961988304093566),\n","  (1, 0.008406432748538011),\n","  (2, 0.023757309941520467),\n","  (3, 0.042616959064327484),\n","  (4, 0.007821637426900585),\n","  (5, 0.012646198830409357),\n","  (6, 0.017178362573099414),\n","  (7, 0.007090643274853801),\n","  (8, 0.41001461988304094),\n","  (9, 0.17141812865497075),\n","  (10, 0.01644736842105263),\n","  (11, 0.007236842105263158),\n","  (12, 0.04641812865497076),\n","  (13, 0.01425438596491228),\n","  (14, 0.029751461988304095),\n","  (15, 0.011038011695906434),\n","  (16, 0.03559941520467836),\n","  (17, 0.024195906432748538),\n","  (18, 0.03720760233918129),\n","  (19, 0.03793859649122807)],\n"," [(0, 0.011123680241327302),\n","  (1, 0.007352941176470589),\n","  (2, 0.005718954248366014),\n","  (3, 0.0038335847159376574),\n","  (4, 0.020801910507792863),\n","  (5, 0.017282554047259934),\n","  (6, 0.0068501759678230275),\n","  (7, 0.008484162895927603),\n","  (8, 0.006347410759175466),\n","  (9, 0.006975867269984918),\n","  (10, 0.020676219205630973),\n","  (11, 0.007855706385118151),\n","  (12, 0.020676219205630973),\n","  (13, 0.12022373051784818),\n","  (14, 0.038147310206133746),\n","  (15, 0.008484162895927603),\n","  (16, 0.6349296128707894),\n","  (17, 0.005090497737556562),\n","  (18, 0.00747863247863248),\n","  (19, 0.04166666666666667)],\n"," [(0, 0.11850495183828517),\n","  (1, 0.010785510785510785),\n","  (2, 0.07482024148690815),\n","  (3, 0.042667209333876),\n","  (4, 0.01295617962284629),\n","  (5, 0.007122507122507123),\n","  (6, 0.20316103649436984),\n","  (7, 0.01010717677384344),\n","  (8, 0.009428842762176096),\n","  (9, 0.20031203364536698),\n","  (10, 0.01811151811151811),\n","  (11, 0.007122507122507123),\n","  (12, 0.13207163207163208),\n","  (13, 0.009835843169176503),\n","  (14, 0.017297517297517297),\n","  (15, 0.010785510785510785),\n","  (16, 0.01702618369285036),\n","  (17, 0.018654185320851988),\n","  (18, 0.02801519468186135),\n","  (19, 0.051214217880884545)],\n"," [(0, 0.20082842258092065),\n","  (1, 0.009607993850883935),\n","  (2, 0.03659578102314459),\n","  (3, 0.0066188402083867115),\n","  (4, 0.004996156802459646),\n","  (5, 0.006875053377743616),\n","  (6, 0.018319241609018704),\n","  (7, 0.049406439490989836),\n","  (8, 0.004739943633102741),\n","  (9, 0.39999145956102145),\n","  (10, 0.017038175762234178),\n","  (11, 0.004569134853531472),\n","  (12, 0.07314885985139637),\n","  (13, 0.01242633871380989),\n","  (14, 0.0038858997352463915),\n","  (15, 0.0052523699718165515),\n","  (16, 0.12618498590827568),\n","  (17, 0.0051669655820309164),\n","  (18, 0.004739943633102741),\n","  (19, 0.009607993850883935)],\n"," [(0, 0.032889426957223564),\n","  (1, 0.009079903147699755),\n","  (2, 0.007734732311003496),\n","  (3, 0.03894269572235673),\n","  (4, 0.012846381490449284),\n","  (5, 0.04459241323648103),\n","  (6, 0.01069410815173527),\n","  (7, 0.006793112725316113),\n","  (8, 0.14897767016411081),\n","  (9, 0.01230831315577078),\n","  (10, 0.01943771859026096),\n","  (11, 0.01298089857411891),\n","  (12, 0.033023944040893184),\n","  (13, 0.027508743610438518),\n","  (14, 0.3133575464083938),\n","  (15, 0.0054479418886198535),\n","  (16, 0.22363465160075324),\n","  (17, 0.021051923594296473),\n","  (18, 0.009887005649717513),\n","  (19, 0.008810868980360504)],\n"," [(0, 0.09090268976200536),\n","  (1, 0.0058442472891142095),\n","  (2, 0.011336431488522744),\n","  (3, 0.0071116744120546405),\n","  (4, 0.008942402478524151),\n","  (5, 0.02006759611322349),\n","  (6, 0.008660752006759611),\n","  (7, 0.023588227010280242),\n","  (8, 0.26707505985072527),\n","  (9, 0.04809181805379524),\n","  (10, 0.08583298127024362),\n","  (11, 0.009646528657935503),\n","  (12, 0.04302210956203352),\n","  (13, 0.013448810026756795),\n","  (14, 0.19919729615547105),\n","  (15, 0.004858470637938319),\n","  (16, 0.09217011688494578),\n","  (17, 0.023447401774397972),\n","  (18, 0.01922264469792987),\n","  (19, 0.01753274186734263)],\n"," [(0, 0.2401837928153718),\n","  (1, 0.007602339181286551),\n","  (2, 0.026148705096073525),\n","  (3, 0.010776942355889727),\n","  (4, 0.009774436090225566),\n","  (5, 0.015455304928989142),\n","  (6, 0.07861319966583126),\n","  (7, 0.02130325814536341),\n","  (8, 0.04218880534670009),\n","  (9, 0.08145363408521304),\n","  (10, 0.011111111111111113),\n","  (11, 0.007936507936507938),\n","  (12, 0.11403508771929827),\n","  (13, 0.03600668337510443),\n","  (14, 0.12690058479532165),\n","  (15, 0.015956558061821224),\n","  (16, 0.05087719298245615),\n","  (17, 0.023642439431913125),\n","  (18, 0.053049289891395165),\n","  (19, 0.026984126984126992)],\n"," [(0, 0.0766797947249075),\n","  (1, 0.02321279388948562),\n","  (2, 0.051855830051318774),\n","  (3, 0.03013486096192863),\n","  (4, 0.015455304928989139),\n","  (5, 0.013665115168874567),\n","  (6, 0.024764291681584914),\n","  (7, 0.02249671798543979),\n","  (8, 0.28040338942594584),\n","  (9, 0.11880892707960378),\n","  (10, 0.007936507936507936),\n","  (11, 0.007220432032462107),\n","  (12, 0.12226996061582528),\n","  (13, 0.011278195488721804),\n","  (14, 0.09863945578231292),\n","  (15, 0.019513068385248838),\n","  (16, 0.05137844611528822),\n","  (17, 0.007220432032462107),\n","  (18, 0.007459124000477384),\n","  (19, 0.00960735171261487)],\n"," [(0, 0.02689500941619585),\n","  (1, 0.006179378531073445),\n","  (2, 0.061970338983050835),\n","  (3, 0.02218691148775894),\n","  (4, 0.005826271186440677),\n","  (5, 0.004060734463276835),\n","  (6, 0.02336393596986817),\n","  (7, 0.05690913370998115),\n","  (8, 0.006179378531073445),\n","  (9, 0.006179378531073445),\n","  (10, 0.008298022598870055),\n","  (11, 0.09987052730696795),\n","  (12, 0.029955273069679843),\n","  (13, 0.24864642184557434),\n","  (14, 0.018891242937853103),\n","  (15, 0.016066384180790958),\n","  (16, 0.006767890772128059),\n","  (17, 0.32432909604519766),\n","  (18, 0.008533427495291901),\n","  (19, 0.018891242937853103)],\n"," [(0, 0.6156283710895363),\n","  (1, 0.007888349514563108),\n","  (2, 0.01921521035598706),\n","  (3, 0.005596008629989213),\n","  (4, 0.013956310679611653),\n","  (5, 0.007483818770226538),\n","  (6, 0.010989751887810142),\n","  (7, 0.019889428263214673),\n","  (8, 0.00546116504854369),\n","  (9, 0.07989482200647251),\n","  (10, 0.01341693635382956),\n","  (11, 0.006539913700107876),\n","  (12, 0.013282092772384036),\n","  (13, 0.08272653721682849),\n","  (14, 0.008562567421790724),\n","  (15, 0.004921790722761598),\n","  (16, 0.04591423948220066),\n","  (17, 0.006000539374325783),\n","  (18, 0.01179881337648328),\n","  (19, 0.020833333333333336)],\n"," [(0, 0.22665794066317627),\n","  (1, 0.004726585223967423),\n","  (2, 0.005453752181500873),\n","  (3, 0.006471785922047702),\n","  (4, 0.007489819662594531),\n","  (5, 0.008362420011634671),\n","  (6, 0.0108347876672484),\n","  (7, 0.33965968586387435),\n","  (8, 0.007198952879581152),\n","  (9, 0.07526178010471204),\n","  (10, 0.01287085514834206),\n","  (11, 0.003999418266433973),\n","  (12, 0.1683391506689936),\n","  (13, 0.006035485747527632),\n","  (14, 0.005308318789994182),\n","  (15, 0.01156195462478185),\n","  (16, 0.07511634671320536),\n","  (17, 0.01127108784176847),\n","  (18, 0.0058900523560209425),\n","  (19, 0.007489819662594531)],\n"," [(0, 0.2851753217931647),\n","  (1, 0.008359224737387188),\n","  (2, 0.015608817872466341),\n","  (3, 0.00954283177984909),\n","  (4, 0.008803077378310402),\n","  (5, 0.003920698328155053),\n","  (6, 0.019455540760467526),\n","  (7, 0.16518715786358928),\n","  (8, 0.007619470335848498),\n","  (9, 0.11917443408788282),\n","  (10, 0.008211273857079449),\n","  (11, 0.004808403610001479),\n","  (12, 0.060437934605710904),\n","  (13, 0.04357153425062879),\n","  (14, 0.008655126498002663),\n","  (15, 0.018863737239236573),\n","  (16, 0.1711051930758988),\n","  (17, 0.019159638999852048),\n","  (18, 0.008359224737387188),\n","  (19, 0.013981358189081226)],\n"," [(0, 0.5588594083562062),\n","  (1, 0.0036088238284029683),\n","  (2, 0.011436413540713633),\n","  (3, 0.007573447189183694),\n","  (4, 0.005845278031920301),\n","  (5, 0.004828707939412423),\n","  (6, 0.006455220087425028),\n","  (7, 0.07639524245196706),\n","  (8, 0.006150249059672664),\n","  (9, 0.004727050930161634),\n","  (10, 0.00777676120768527),\n","  (11, 0.0053369929856663615),\n","  (12, 0.011029785503710482),\n","  (13, 0.21302226288502593),\n","  (14, 0.006048592050421877),\n","  (15, 0.004625393920910847),\n","  (16, 0.006861848124428179),\n","  (17, 0.008386703263189996),\n","  (18, 0.021602114465792417),\n","  (19, 0.029429704178103082)],\n"," [(0, 0.6234412218018776),\n","  (1, 0.00819672131147541),\n","  (2, 0.03313717248143477),\n","  (3, 0.007496146840409136),\n","  (4, 0.009878100042034469),\n","  (5, 0.01366120218579235),\n","  (6, 0.005674653215636823),\n","  (7, 0.013240857503152586),\n","  (8, 0.006094997898276587),\n","  (9, 0.007496146840409136),\n","  (10, 0.029073840549250385),\n","  (11, 0.016323385175844193),\n","  (12, 0.09913128765587782),\n","  (13, 0.008056606417262155),\n","  (14, 0.02599131287655878),\n","  (15, 0.004133389379291019),\n","  (16, 0.019546027742749054),\n","  (17, 0.04574751296062771),\n","  (18, 0.011139134089953762),\n","  (19, 0.01254028303208631)],\n"," [(0, 0.03975188781014024),\n","  (1, 0.011380798274002157),\n","  (2, 0.11461704422869472),\n","  (3, 0.0065264293419633225),\n","  (4, 0.0232470334412082),\n","  (5, 0.00598705501618123),\n","  (6, 0.06165048543689321),\n","  (7, 0.02982740021574973),\n","  (8, 0.01192017259978425),\n","  (9, 0.009439050701186624),\n","  (10, 0.02745415318230852),\n","  (11, 0.007605177993527508),\n","  (12, 0.022707659115426104),\n","  (13, 0.09865156418554477),\n","  (14, 0.42950377562028047),\n","  (15, 0.004261057173678533),\n","  (16, 0.03586839266450917),\n","  (17, 0.020981661272923408),\n","  (18, 0.010302049622437972),\n","  (19, 0.02831715210355987)],\n"," [(0, 0.5471753398968591),\n","  (1, 0.01974917955930615),\n","  (2, 0.027015939990623544),\n","  (3, 0.0040436005625879064),\n","  (4, 0.0089662447257384),\n","  (5, 0.0042780121894045965),\n","  (6, 0.02584388185654009),\n","  (7, 0.021976090014064704),\n","  (8, 0.030532114392873896),\n","  (9, 0.00920065635255509),\n","  (10, 0.018459915611814353),\n","  (11, 0.011193155180496956),\n","  (12, 0.013888888888888893),\n","  (13, 0.12945382090951718),\n","  (14, 0.012834036568213787),\n","  (15, 0.011427566807313647),\n","  (16, 0.012130801687763716),\n","  (17, 0.03744725738396626),\n","  (18, 0.045886075949367104),\n","  (19, 0.00849742147210502)],\n"," [(0, 0.01732772435897436),\n","  (1, 0.004306891025641026),\n","  (2, 0.01545806623931624),\n","  (3, 0.007445245726495726),\n","  (4, 0.00938167735042735),\n","  (5, 0.0037727029914529915),\n","  (6, 0.02079994658119658),\n","  (7, 0.026275373931623932),\n","  (8, 0.0025040064102564105),\n","  (9, 0.09752270299145299),\n","  (10, 0.017194177350427352),\n","  (11, 0.0035056089743589745),\n","  (12, 0.012119391025641026),\n","  (13, 0.0758880876068376),\n","  (14, 0.029881143162393164),\n","  (15, 0.6236979166666666),\n","  (16, 0.007445245726495726),\n","  (17, 0.007512019230769231),\n","  (18, 0.00984909188034188),\n","  (19, 0.00811298076923077)],\n"," [(0, 0.012599624941397094),\n","  (1, 0.3002226910454759),\n","  (2, 0.041315049226441636),\n","  (3, 0.10671589310829818),\n","  (4, 0.012130801687763714),\n","  (5, 0.013068448195030476),\n","  (6, 0.009083450539146743),\n","  (7, 0.042604313173933435),\n","  (8, 0.008380215658696673),\n","  (9, 0.009552273792780123),\n","  (10, 0.11011486169714019),\n","  (11, 0.014006094702297236),\n","  (12, 0.009903891233005158),\n","  (13, 0.03592358180965777),\n","  (14, 0.018577121425222696),\n","  (15, 0.20352789498359122),\n","  (16, 0.008966244725738398),\n","  (17, 0.01295124238162213),\n","  (18, 0.019045944678856074),\n","  (19, 0.0113103609939053)],\n"," [(0, 0.011000491642084562),\n","  (1, 0.00841937069813176),\n","  (2, 0.04553834808259587),\n","  (3, 0.5413593903638152),\n","  (4, 0.009771386430678467),\n","  (5, 0.030789085545722714),\n","  (6, 0.009402654867256638),\n","  (7, 0.015794001966568338),\n","  (8, 0.03890117994100295),\n","  (9, 0.017637659783677484),\n","  (10, 0.03214110127826942),\n","  (11, 0.00854228121927237),\n","  (12, 0.007313176007866273),\n","  (13, 0.0051007866273352995),\n","  (14, 0.015794001966568338),\n","  (15, 0.04160521140609636),\n","  (16, 0.01198377581120944),\n","  (17, 0.020341691248770895),\n","  (18, 0.11805555555555555),\n","  (19, 0.010508849557522125)],\n"," [(0, 0.009860788863109048),\n","  (1, 0.006251611240010312),\n","  (2, 0.038734209847898944),\n","  (3, 0.010376385666408867),\n","  (4, 0.011149780871358598),\n","  (5, 0.010118587264758958),\n","  (6, 0.034609435421500384),\n","  (7, 0.016692446506831656),\n","  (8, 0.005349316834235627),\n","  (9, 0.044792472286671825),\n","  (10, 0.009602990461459139),\n","  (11, 0.005220417633410673),\n","  (12, 0.012180974477958236),\n","  (13, 0.08732920855890694),\n","  (14, 0.007798401649909771),\n","  (15, 0.6463650425367363),\n","  (16, 0.015403454498582108),\n","  (17, 0.007927300850734726),\n","  (18, 0.011794276875483373),\n","  (19, 0.008442897654034545)],\n"," [(0, 0.013091497667333144),\n","  (1, 0.010425592687803487),\n","  (2, 0.039369703894125495),\n","  (3, 0.2999619156431496),\n","  (4, 0.007378844139769591),\n","  (5, 0.005093782728744169),\n","  (6, 0.17999619156431498),\n","  (7, 0.026230600780729318),\n","  (8, 0.09706750452251739),\n","  (9, 0.010616014472055605),\n","  (10, 0.03746548605160431),\n","  (11, 0.010711225364181664),\n","  (12, 0.02270779777206513),\n","  (13, 0.11544320670284682),\n","  (14, 0.033276206798057706),\n","  (15, 0.02746834237836809),\n","  (16, 0.016138246215367042),\n","  (17, 0.005760258973626584),\n","  (18, 0.010235170903551367),\n","  (19, 0.03156241073978864)],\n"," [(0, 0.011089681774349084),\n","  (1, 0.004982320797171327),\n","  (2, 0.010125361620057859),\n","  (3, 0.00766098789242473),\n","  (4, 0.020518589949641057),\n","  (5, 0.011303975141969356),\n","  (6, 0.026411657559198543),\n","  (7, 0.014625522340083574),\n","  (8, 0.007446694524804457),\n","  (9, 0.010982535090538948),\n","  (10, 0.019982856530590377),\n","  (11, 0.006375227686703097),\n","  (12, 0.01912568306010929),\n","  (13, 0.14309439622843673),\n","  (14, 0.011089681774349084),\n","  (15, 0.6278259937854923),\n","  (16, 0.019768563162970106),\n","  (17, 0.010125361620057859),\n","  (18, 0.006910961105753777),\n","  (19, 0.010553948355298404)],\n"," [(0, 0.02875873289309981),\n","  (1, 0.004450186620729255),\n","  (2, 0.026079050626854238),\n","  (3, 0.008469710020097615),\n","  (4, 0.007608383577375824),\n","  (5, 0.007512680639295625),\n","  (6, 0.02100679490860369),\n","  (7, 0.07249497559575077),\n","  (8, 0.005215810125370848),\n","  (9, 0.016604459756914533),\n","  (10, 0.03105560340702459),\n","  (11, 0.00703416594889463),\n","  (12, 0.008469710020097615),\n","  (13, 0.036414967939515736),\n","  (14, 0.00588573069193224),\n","  (15, 0.6552301655660828),\n","  (16, 0.006172839506172837),\n","  (17, 0.021198200784764087),\n","  (18, 0.008086898267776818),\n","  (19, 0.02225093310364628)],\n"," [(0, 0.017188165176670923),\n","  (1, 0.0036717752234993613),\n","  (2, 0.00899318859088974),\n","  (3, 0.007396764580672626),\n","  (4, 0.012292464878671775),\n","  (5, 0.004097488292890592),\n","  (6, 0.011973180076628353),\n","  (7, 0.01952958705832269),\n","  (8, 0.006758194976585781),\n","  (9, 0.009738186462324393),\n","  (10, 0.009525329927628778),\n","  (11, 0.004629629629629629),\n","  (12, 0.02346743295019157),\n","  (13, 0.0927522349936143),\n","  (14, 0.009631758194976586),\n","  (15, 0.7147190293742018),\n","  (16, 0.013569604086845466),\n","  (17, 0.008780332056194126),\n","  (18, 0.007077479778629204),\n","  (19, 0.014208173690932312)],\n"," [(0, 0.01571561122709348),\n","  (1, 0.014903734632335885),\n","  (2, 0.043667362560890745),\n","  (3, 0.016063558339132453),\n","  (4, 0.011424263511946184),\n","  (5, 0.024298306657388077),\n","  (6, 0.026501971700301555),\n","  (7, 0.012932034330781722),\n","  (8, 0.00632103920204129),\n","  (9, 0.010380422175829273),\n","  (10, 0.009916492693110648),\n","  (11, 0.027661795407098122),\n","  (12, 0.00597309209000232),\n","  (13, 0.04784272790535839),\n","  (14, 0.006900951055439573),\n","  (15, 0.5978311296682904),\n","  (16, 0.008176757132915796),\n","  (17, 0.010728369287868245),\n","  (18, 0.027081883553699837),\n","  (19, 0.075678496868476)],\n"," [(0, 0.09132093466857416),\n","  (1, 0.009775870290891752),\n","  (2, 0.026847877920839296),\n","  (3, 0.011206485455412496),\n","  (4, 0.04172627563185504),\n","  (5, 0.004625655698617073),\n","  (6, 0.009489747257987603),\n","  (7, 0.011587982832618027),\n","  (8, 0.004625655698617073),\n","  (9, 0.0463996185026228),\n","  (10, 0.007677634716261326),\n","  (11, 0.08969957081545066),\n","  (12, 0.05269432522651407),\n","  (13, 0.007105388650453029),\n","  (14, 0.05803528850739152),\n","  (15, 0.4376251788268956),\n","  (16, 0.021602288984263238),\n","  (17, 0.02980448259418217),\n","  (18, 0.004244158321411541),\n","  (19, 0.033905579399141635)],\n"," [(0, 0.017717415604739547),\n","  (1, 0.010116253073999553),\n","  (2, 0.02207690587972278),\n","  (3, 0.03549072211044042),\n","  (4, 0.02431254191817572),\n","  (5, 0.008663089649005142),\n","  (6, 0.11452045606975185),\n","  (7, 0.26441985244802146),\n","  (8, 0.00687458081824279),\n","  (9, 0.011681198300916611),\n","  (10, 0.1379946344735077),\n","  (11, 0.009669125866308964),\n","  (12, 0.03180192264699307),\n","  (13, 0.0903755868544601),\n","  (14, 0.009221998658618377),\n","  (15, 0.014252179745137492),\n","  (16, 0.026771741560473954),\n","  (17, 0.06500111781801923),\n","  (18, 0.018723451822043373),\n","  (19, 0.08031522468142187)],\n"," [(0, 0.08744737849215463),\n","  (1, 0.010396734277331295),\n","  (2, 0.05836203597397628),\n","  (3, 0.03233830845771145),\n","  (4, 0.016137262405919124),\n","  (5, 0.020857252200535786),\n","  (6, 0.02787345324658758),\n","  (7, 0.02340859803546371),\n","  (8, 0.02302589616022452),\n","  (9, 0.10989922183952036),\n","  (10, 0.010524301569077691),\n","  (11, 0.010524301569077691),\n","  (12, 0.052876642428881244),\n","  (13, 0.012820512820512822),\n","  (14, 0.18567419313687974),\n","  (15, 0.19447633626738106),\n","  (16, 0.024939405536420465),\n","  (17, 0.011034570736063275),\n","  (18, 0.032210741165965055),\n","  (19, 0.055172853680316374)],\n"," [(0, 0.005677552552552554),\n","  (1, 0.05579016516516517),\n","  (2, 0.1852946696696697),\n","  (3, 0.05325638138138139),\n","  (4, 0.012903528528528531),\n","  (5, 0.05579016516516517),\n","  (6, 0.20443881381381385),\n","  (7, 0.06292229729729731),\n","  (8, 0.013748123123123126),\n","  (9, 0.012434309309309312),\n","  (10, 0.036833708708708716),\n","  (11, 0.1012105855855856),\n","  (12, 0.006146771771771773),\n","  (13, 0.12063626126126127),\n","  (14, 0.005677552552552554),\n","  (15, 0.012809684684684686),\n","  (16, 0.018534159159159163),\n","  (17, 0.0069913663663663675),\n","  (18, 0.018534159159159163),\n","  (19, 0.010369744744744747)],\n"," [(0, 0.012338789682539684),\n","  (1, 0.005890376984126985),\n","  (2, 0.006138392857142858),\n","  (3, 0.01643105158730159),\n","  (4, 0.008122519841269844),\n","  (5, 0.012710813492063494),\n","  (6, 0.010106646825396828),\n","  (7, 0.20070684523809526),\n","  (8, 0.009486607142857144),\n","  (9, 0.02399553571428572),\n","  (10, 0.017919146825396828),\n","  (11, 0.042100694444444454),\n","  (12, 0.014942956349206352),\n","  (13, 0.05673363095238096),\n","  (14, 0.008370535714285716),\n","  (15, 0.023127480158730163),\n","  (16, 0.02064732142857143),\n","  (17, 0.05698164682539683),\n","  (18, 0.4443824404761905),\n","  (19, 0.008866567460317462)],\n"," [(0, 0.024161007774825313),\n","  (1, 0.007331955516189351),\n","  (2, 0.017468753075484695),\n","  (3, 0.053390414329298294),\n","  (4, 0.015598858380080701),\n","  (5, 0.010087589804153134),\n","  (6, 0.02849129022733983),\n","  (7, 0.015795689400649543),\n","  (8, 0.07110520618049404),\n","  (9, 0.00939868123216219),\n","  (10, 0.03754551717350654),\n","  (11, 0.008906603680740085),\n","  (12, 0.009300265721877768),\n","  (13, 0.03173900206672572),\n","  (14, 0.04138372207459896),\n","  (15, 0.04187579962602106),\n","  (16, 0.020421218384017323),\n","  (17, 0.016779844503493752),\n","  (18, 0.5209625036905816),\n","  (19, 0.018256077157760064)],\n"," [(0, 0.0060831180017226535),\n","  (1, 0.2453165374677003),\n","  (2, 0.0042527993109388465),\n","  (3, 0.08392549526270458),\n","  (4, 0.013404392764857883),\n","  (5, 0.07961886304909561),\n","  (6, 0.005544788975021534),\n","  (7, 0.005544788975021534),\n","  (8, 0.07229758828596039),\n","  (9, 0.005221791559000862),\n","  (10, 0.010712747631352285),\n","  (11, 0.08683247200689062),\n","  (12, 0.01631136950904393),\n","  (13, 0.017818690783807066),\n","  (14, 0.01415805340223945),\n","  (15, 0.004468130921619295),\n","  (16, 0.008774763135228254),\n","  (17, 0.005221791559000862),\n","  (18, 0.30184108527131787),\n","  (19, 0.012650732127476316)],\n"," [(0, 0.0122592991268987),\n","  (1, 0.009269226169118528),\n","  (2, 0.01716301877765818),\n","  (3, 0.07983494797273055),\n","  (4, 0.014292548738189215),\n","  (5, 0.004365506518359049),\n","  (6, 0.009747637842363356),\n","  (7, 0.0789977275445521),\n","  (8, 0.004365506518359049),\n","  (9, 0.020033488817127142),\n","  (10, 0.03211338356655903),\n","  (11, 0.02768807558904438),\n","  (12, 0.23759119722521235),\n","  (13, 0.01823944504245904),\n","  (14, 0.006757564884583186),\n","  (15, 0.010226049515608182),\n","  (16, 0.06045927520631505),\n","  (17, 0.027329266834110758),\n","  (18, 0.30995096280349244),\n","  (19, 0.0193158713072599)],\n"," [(0, 0.017812887236679058),\n","  (1, 0.02886204047914085),\n","  (2, 0.13914704667492772),\n","  (3, 0.008002891367203635),\n","  (4, 0.04063403552251136),\n","  (5, 0.023905410987195373),\n","  (6, 0.038775299463031807),\n","  (7, 0.011100784799669559),\n","  (8, 0.005421313506815365),\n","  (9, 0.008415943824865758),\n","  (10, 0.09340148698884758),\n","  (11, 0.035161090458488226),\n","  (12, 0.016057414291615037),\n","  (13, 0.032166460140437836),\n","  (14, 0.014714993804213134),\n","  (15, 0.02865551425030979),\n","  (16, 0.007280049566294919),\n","  (17, 0.05364518793886824),\n","  (18, 0.24540479140850888),\n","  (19, 0.15143535729037588)],\n"," [(0, 0.034663756366282085),\n","  (1, 0.006288327616671862),\n","  (2, 0.016890136160482273),\n","  (3, 0.004833177424384159),\n","  (4, 0.008782870803450783),\n","  (5, 0.0060804490177736184),\n","  (6, 0.005560752520528011),\n","  (7, 0.017513771957177005),\n","  (8, 0.1047188441949901),\n","  (9, 0.005456813221078889),\n","  (10, 0.026244673110903227),\n","  (11, 0.014083775075355989),\n","  (12, 0.04058829643488202),\n","  (13, 0.010653778193534974),\n","  (14, 0.24493295915185526),\n","  (15, 0.005041056023282402),\n","  (16, 0.1837127117763226),\n","  (17, 0.03882132834424695),\n","  (18, 0.20720299345182408),\n","  (19, 0.017929529154973492)],\n"," [(0, 0.015346468301836214),\n","  (1, 0.021243801099048384),\n","  (2, 0.009851226377161239),\n","  (3, 0.01347004422999598),\n","  (4, 0.01601661975606487),\n","  (5, 0.005160166197560648),\n","  (6, 0.005428226779252111),\n","  (7, 0.010521377831389895),\n","  (8, 0.593687173301166),\n","  (9, 0.005830317651789305),\n","  (10, 0.06440155475137381),\n","  (11, 0.004758075325023456),\n","  (12, 0.005964347942635035),\n","  (13, 0.009047044632086852),\n","  (14, 0.008510923468703926),\n","  (15, 0.010521377831389895),\n","  (16, 0.010789438413081356),\n","  (17, 0.004355984452486262),\n","  (18, 0.17336818120895323),\n","  (19, 0.011727650449001474)],\n"," [(0, 0.006754289886820007),\n","  (1, 0.028781793842034806),\n","  (2, 0.0076061823049774855),\n","  (3, 0.008944870390653524),\n","  (4, 0.009918461725690641),\n","  (5, 0.03194596568090544),\n","  (6, 0.00529390288426433),\n","  (7, 0.006389193136181088),\n","  (8, 0.0198977729098211),\n","  (9, 0.0034684191310697336),\n","  (10, 0.009796762808811001),\n","  (11, 0.3914445661433613),\n","  (12, 0.01077035414384812),\n","  (13, 0.006267494219301448),\n","  (14, 0.028538396008275527),\n","  (15, 0.004807107216745771),\n","  (16, 0.018437385907265427),\n","  (17, 0.006510892053060728),\n","  (18, 0.34069611780455156),\n","  (19, 0.05373007180236096)],\n"," [(0, 0.03812299954275263),\n","  (1, 0.030006858710562415),\n","  (2, 0.021776406035665295),\n","  (3, 0.01651806127114769),\n","  (4, 0.007030178326474623),\n","  (5, 0.005086877000457247),\n","  (6, 0.01263145861911294),\n","  (7, 0.022576588934613628),\n","  (8, 0.010116598079561043),\n","  (9, 0.01343164151806127),\n","  (10, 0.030006858710562415),\n","  (11, 0.009087791495198902),\n","  (12, 0.2279949702789209),\n","  (13, 0.14694787379972565),\n","  (14, 0.02246227709190672),\n","  (15, 0.005544124371284865),\n","  (16, 0.1527777777777778),\n","  (17, 0.013317329675354367),\n","  (18, 0.20410379515317786),\n","  (19, 0.010459533607681756)],\n"," [(0, 0.03046182685753238),\n","  (1, 0.005666325835037491),\n","  (2, 0.35212167689161555),\n","  (3, 0.005836741649625085),\n","  (4, 0.005751533742331288),\n","  (5, 0.005836741649625085),\n","  (6, 0.0036213360599863667),\n","  (7, 0.004643830947511929),\n","  (8, 0.005581117927743694),\n","  (9, 0.010693592365371506),\n","  (10, 0.008478186775732787),\n","  (11, 0.004388207225630538),\n","  (12, 0.028672460804362646),\n","  (13, 0.024582481254260394),\n","  (14, 0.07690013633265166),\n","  (15, 0.0037065439672801636),\n","  (16, 0.00515507839127471),\n","  (17, 0.01248295841854124),\n","  (18, 0.005325494205862304),\n","  (19, 0.4000937286980232)],\n"," [(0, 0.005760945797014328),\n","  (1, 0.01968740607153592),\n","  (2, 0.6412684099789602),\n","  (3, 0.004458471095080654),\n","  (4, 0.007564372307384031),\n","  (5, 0.05385231940687307),\n","  (6, 0.0210900711351568),\n","  (7, 0.006362087967137563),\n","  (8, 0.005861136158701535),\n","  (9, 0.005660755435327122),\n","  (10, 0.01036970243462579),\n","  (11, 0.07419096282937583),\n","  (12, 0.005159803626891094),\n","  (13, 0.006261897605450357),\n","  (14, 0.04483518685502455),\n","  (15, 0.01036970243462579),\n","  (16, 0.00736399158400962),\n","  (17, 0.01437731690211402),\n","  (18, 0.0053601843502655054),\n","  (19, 0.05014527602444646)],\n"," [(0, 0.011698178364845031),\n","  (1, 0.013165846499179833),\n","  (2, 0.18799102132435466),\n","  (3, 0.004705171371838039),\n","  (4, 0.007985841319174652),\n","  (5, 0.005482172148838816),\n","  (6, 0.0093671760338427),\n","  (7, 0.005568505568505569),\n","  (8, 0.010144176810843477),\n","  (9, 0.027238193904860573),\n","  (10, 0.0052231718898385565),\n","  (11, 0.007122507122507123),\n","  (12, 0.017482517482517484),\n","  (13, 0.19075369075369075),\n","  (14, 0.4309332642665976),\n","  (15, 0.012475179141845809),\n","  (16, 0.007381507381507382),\n","  (17, 0.016964516964516965),\n","  (18, 0.006259172925839593),\n","  (19, 0.02205818872485539)],\n"," [(0, 0.018559084824145068),\n","  (1, 0.004685408299866132),\n","  (2, 0.15705245223317515),\n","  (3, 0.006997687720579289),\n","  (4, 0.03511013751977608),\n","  (5, 0.0072410855543385685),\n","  (6, 0.05823293172690765),\n","  (7, 0.22824631860776445),\n","  (8, 0.0063891931361810895),\n","  (9, 0.18054034319094564),\n","  (10, 0.03085067542898869),\n","  (11, 0.008701472556894246),\n","  (12, 0.05446026530363881),\n","  (13, 0.06273579165145432),\n","  (14, 0.013934525982718757),\n","  (15, 0.009796762808811003),\n","  (16, 0.07843495192892784),\n","  (17, 0.01697699890470975),\n","  (18, 0.006875988803699649),\n","  (19, 0.014177923816478036)],\n"," [(0, 0.017351162050302453),\n","  (1, 0.016502175527963493),\n","  (2, 0.21166295235063143),\n","  (3, 0.19478934521914465),\n","  (4, 0.0213838480314125),\n","  (5, 0.06330255757189855),\n","  (6, 0.012575612862145813),\n","  (7, 0.0066327072057731085),\n","  (8, 0.02233895786904383),\n","  (9, 0.015971558951501645),\n","  (10, 0.016608298843255865),\n","  (11, 0.009073543457497613),\n","  (12, 0.01703279210442534),\n","  (13, 0.010347023241006049),\n","  (14, 0.04961264989918285),\n","  (15, 0.1679401464501751),\n","  (16, 0.0824047543245251),\n","  (17, 0.03730234532526796),\n","  (18, 0.0078000636739891755),\n","  (19, 0.019367505040857477)],\n"," [(0, 0.011915015790984786),\n","  (1, 0.013063451047947174),\n","  (2, 0.5804861709254475),\n","  (3, 0.005885730691932243),\n","  (4, 0.023303665422528475),\n","  (5, 0.04062589721504451),\n","  (6, 0.09508086898267779),\n","  (7, 0.007799789453536225),\n","  (8, 0.005694324815771845),\n","  (9, 0.008469710020097618),\n","  (10, 0.011053689348262993),\n","  (11, 0.029237247583500818),\n","  (12, 0.004545889558809456),\n","  (13, 0.03871183845344053),\n","  (14, 0.020049765527801706),\n","  (15, 0.007512680639295628),\n","  (16, 0.02473920949373146),\n","  (17, 0.008852521772418414),\n","  (18, 0.006268542444253039),\n","  (19, 0.05670399081251795)],\n"," [(0, 0.035822449365890595),\n","  (1, 0.0145277304561802),\n","  (2, 0.31795381412076473),\n","  (3, 0.013108082528866175),\n","  (4, 0.017272383115653985),\n","  (5, 0.0038330494037478705),\n","  (6, 0.06913685406019307),\n","  (7, 0.024938481923149725),\n","  (8, 0.004874124550444823),\n","  (9, 0.006956274843838728),\n","  (10, 0.01216165057732349),\n","  (11, 0.033267083096725346),\n","  (12, 0.004211622184364944),\n","  (13, 0.006293772477758849),\n","  (14, 0.005536626916524702),\n","  (15, 0.22331061896649632),\n","  (16, 0.01036342986939239),\n","  (17, 0.1571550255536627),\n","  (18, 0.0073348476244558015),\n","  (19, 0.031942078364565585)],\n"," [(0, 0.022585452174722923),\n","  (1, 0.026497159355499678),\n","  (2, 0.06095743690043775),\n","  (3, 0.008987612927260875),\n","  (4, 0.005820992828536836),\n","  (5, 0.2588711930706902),\n","  (6, 0.006100400484306604),\n","  (7, 0.004330818664431406),\n","  (8, 0.1394709881717426),\n","  (9, 0.004703362205457764),\n","  (10, 0.04056067802924467),\n","  (11, 0.24965074043028782),\n","  (12, 0.004610226320201174),\n","  (13, 0.05639377852286487),\n","  (14, 0.04251653161963305),\n","  (15, 0.009173884697774054),\n","  (16, 0.007497438763155445),\n","  (17, 0.006938623451615908),\n","  (18, 0.019325696190742297),\n","  (19, 0.025006985191394247)],\n"," [(0, 0.01913297737096364),\n","  (1, 0.23538011695906433),\n","  (2, 0.013920671243325706),\n","  (3, 0.019895753877447242),\n","  (4, 0.007055682684973303),\n","  (5, 0.014047800661072972),\n","  (6, 0.00947114162217137),\n","  (7, 0.013666412407831173),\n","  (8, 0.2930968726163234),\n","  (9, 0.014810577167556573),\n","  (10, 0.1671116196287821),\n","  (11, 0.013539282990083905),\n","  (12, 0.047355708110856855),\n","  (13, 0.007182812102720569),\n","  (14, 0.04519450800915332),\n","  (15, 0.005657259089753369),\n","  (16, 0.015064836003051106),\n","  (17, 0.008454106280193236),\n","  (18, 0.030574624968217647),\n","  (19, 0.019387236206458176)],\n"," [(0, 0.011765280218707284),\n","  (1, 0.01586604178871314),\n","  (2, 0.02182190978324546),\n","  (3, 0.05228471001757469),\n","  (4, 0.005516500683460262),\n","  (5, 0.0551161882444835),\n","  (6, 0.0129369263815661),\n","  (7, 0.005614137863698496),\n","  (8, 0.4375610232376489),\n","  (9, 0.006199960945127905),\n","  (10, 0.29003124389767626),\n","  (11, 0.015573130247998437),\n","  (12, 0.013034563561804335),\n","  (13, 0.005321226322983793),\n","  (14, 0.0040519429798867405),\n","  (15, 0.0059070494044132),\n","  (16, 0.007469244288224956),\n","  (17, 0.004149580160124976),\n","  (18, 0.02367701620777192),\n","  (19, 0.00610232376488967)],\n"," [(0, 0.003703703703703704),\n","  (1, 0.21910112359550563),\n","  (2, 0.016937161880982106),\n","  (3, 0.02267998335414066),\n","  (4, 0.1173116937161881),\n","  (5, 0.07544735746982938),\n","  (6, 0.005368289637952559),\n","  (7, 0.006034124011652101),\n","  (8, 0.37748647523928425),\n","  (9, 0.00570120682480233),\n","  (10, 0.058551810237203496),\n","  (11, 0.02267998335414066),\n","  (12, 0.003121098626716604),\n","  (13, 0.0215980024968789),\n","  (14, 0.004369538077403246),\n","  (15, 0.0029546400332917185),\n","  (16, 0.0057844361215147735),\n","  (17, 0.008614232209737827),\n","  (18, 0.005201831044527674),\n","  (19, 0.01735330836454432)],\n"," [(0, 0.006960263224500125),\n","  (1, 0.006116594954863746),\n","  (2, 0.26149498017379563),\n","  (3, 0.01708428246013667),\n","  (4, 0.008057031975027417),\n","  (5, 0.20176326668353997),\n","  (6, 0.034801316122500627),\n","  (7, 0.006454062262718298),\n","  (8, 0.1717286762844849),\n","  (9, 0.006622795916645574),\n","  (10, 0.11300936471779294),\n","  (11, 0.006622795916645574),\n","  (12, 0.004597992069518265),\n","  (13, 0.09242385893866531),\n","  (14, 0.02223065890491858),\n","  (15, 0.005019826204336454),\n","  (16, 0.006538429089681936),\n","  (17, 0.004176157934700075),\n","  (18, 0.00594786130093647),\n","  (19, 0.01834978486459124)],\n"," [(0, 0.011020531400966186),\n","  (1, 0.012127616747181966),\n","  (2, 0.03547705314009662),\n","  (3, 0.049768518518518524),\n","  (4, 0.019072061191626414),\n","  (5, 0.1445752818035427),\n","  (6, 0.01836755233494364),\n","  (7, 0.014543075684380034),\n","  (8, 0.23575885668276975),\n","  (9, 0.110859500805153),\n","  (10, 0.03477254428341386),\n","  (11, 0.01192632850241546),\n","  (12, 0.008403784219001611),\n","  (13, 0.04010668276972625),\n","  (14, 0.04956723027375202),\n","  (15, 0.08378623188405798),\n","  (16, 0.0542975040257649),\n","  (17, 0.053291062801932375),\n","  (18, 0.005283816425120774),\n","  (19, 0.0069947665056360716)],\n"," [(0, 0.03266178266178267),\n","  (1, 0.004491540205825921),\n","  (2, 0.011119832548403978),\n","  (3, 0.15406418977847552),\n","  (4, 0.008765044479330195),\n","  (5, 0.0994679923251352),\n","  (6, 0.0060613989185417765),\n","  (7, 0.015393336821908252),\n","  (8, 0.010247688819117392),\n","  (9, 0.09414791557648702),\n","  (10, 0.01356183499040642),\n","  (11, 0.030917495203209494),\n","  (12, 0.0359759288330717),\n","  (13, 0.020538984824699114),\n","  (14, 0.007456828885400315),\n","  (15, 0.00388103959532531),\n","  (16, 0.1482208267922554),\n","  (17, 0.24136577708006282),\n","  (18, 0.00457875457875458),\n","  (19, 0.05708180708180709)],\n"," [(0, 0.005197132616487455),\n","  (1, 0.10758661887694146),\n","  (2, 0.03781362007168459),\n","  (3, 0.05872162485065711),\n","  (4, 0.008661887694145758),\n","  (5, 0.06039426523297491),\n","  (6, 0.00985663082437276),\n","  (7, 0.015113500597371566),\n","  (8, 0.17497013142174433),\n","  (9, 0.004121863799283154),\n","  (10, 0.29778972520908004),\n","  (11, 0.0508363201911589),\n","  (12, 0.03148148148148148),\n","  (13, 0.006152927120669056),\n","  (14, 0.016547192353643967),\n","  (15, 0.009020310633213858),\n","  (16, 0.0494026284348865),\n","  (17, 0.006989247311827957),\n","  (18, 0.007228195937873357),\n","  (19, 0.042114695340501794)],\n"," [(0, 0.004608096468561586),\n","  (1, 0.1240740740740741),\n","  (2, 0.06300602928509906),\n","  (3, 0.0595607235142119),\n","  (4, 0.1447459086993971),\n","  (5, 0.331223083548665),\n","  (6, 0.005469422911283378),\n","  (7, 0.026141257536606382),\n","  (8, 0.04836347975882861),\n","  (9, 0.01037898363479759),\n","  (10, 0.09616709732988805),\n","  (11, 0.011326442721791561),\n","  (12, 0.011670973298880278),\n","  (13, 0.015891472868217058),\n","  (14, 0.0051248923341946605),\n","  (15, 0.004091300602928511),\n","  (16, 0.010120585701981053),\n","  (17, 0.010981912144702843),\n","  (18, 0.004780361757105944),\n","  (19, 0.012273901808785534)],\n"," [(0, 0.018589538723766914),\n","  (1, 0.007723447320762758),\n","  (2, 0.01656546287418771),\n","  (3, 0.014115265793118144),\n","  (4, 0.006551613934164271),\n","  (5, 0.06748695003728562),\n","  (6, 0.007403856397144988),\n","  (7, 0.007616917012890168),\n","  (8, 0.4120059656972409),\n","  (9, 0.08197507190795783),\n","  (10, 0.06226696495152872),\n","  (11, 0.0114520080963034),\n","  (12, 0.010280174709704912),\n","  (13, 0.01123894748055822),\n","  (14, 0.15388302972195592),\n","  (15, 0.010493235325450092),\n","  (16, 0.07313305635453288),\n","  (17, 0.016991584105678068),\n","  (18, 0.0049536593160754245),\n","  (19, 0.005273250239693193)],\n"," [(0, 0.013733468972533058),\n","  (1, 0.015202893636260876),\n","  (2, 0.033288120266757086),\n","  (3, 0.018480840963038314),\n","  (4, 0.009099129648468405),\n","  (5, 0.653950491692099),\n","  (6, 0.009099129648468405),\n","  (7, 0.013620436306092457),\n","  (8, 0.03520967559624731),\n","  (9, 0.013394370973211254),\n","  (10, 0.021306657624053347),\n","  (11, 0.013281338306770652),\n","  (12, 0.014976828303379673),\n","  (13, 0.036679100259975124),\n","  (14, 0.010455521645755621),\n","  (15, 0.004690855657284955),\n","  (16, 0.04380015824573301),\n","  (17, 0.01983723296032553),\n","  (18, 0.006047247654572169),\n","  (19, 0.01384650163897366)],\n"," [(0, 0.023439382983851533),\n","  (1, 0.015365148228488794),\n","  (2, 0.04199807182453604),\n","  (3, 0.2067365630272355),\n","  (4, 0.008375512171607619),\n","  (5, 0.023680404916847437),\n","  (6, 0.008496023138105569),\n","  (7, 0.026211135213304416),\n","  (8, 0.2622921185827911),\n","  (9, 0.012954928898529767),\n","  (10, 0.06320800192817548),\n","  (11, 0.016570257893468308),\n","  (12, 0.021390696553386362),\n","  (13, 0.006206314774644494),\n","  (14, 0.033562304169679445),\n","  (15, 0.005121716076162932),\n","  (16, 0.13093516510002412),\n","  (17, 0.006326825741142445),\n","  (18, 0.07742829597493374),\n","  (19, 0.009701132803085082)],\n"," [(0, 0.01347068145800317),\n","  (1, 0.05326642014439162),\n","  (2, 0.014351118154604684),\n","  (3, 0.053618594823032224),\n","  (4, 0.009948934671597113),\n","  (5, 0.08003169572107766),\n","  (6, 0.010477196689558021),\n","  (7, 0.01100545870751893),\n","  (8, 0.4329107237189646),\n","  (9, 0.014175030815284382),\n","  (10, 0.09024476140165522),\n","  (11, 0.01787286494101074),\n","  (12, 0.020514175030815283),\n","  (13, 0.041996830427892234),\n","  (14, 0.06524035921817221),\n","  (15, 0.006251100545870752),\n","  (16, 0.019809825673534072),\n","  (17, 0.012942419440042261),\n","  (18, 0.014527205493924987),\n","  (19, 0.01734460292304983)],\n"," [(0, 0.029329785886228307),\n","  (1, 0.015266692005574562),\n","  (2, 0.01602685924236666),\n","  (3, 0.058089446344862544),\n","  (4, 0.00601799062460408),\n","  (5, 0.40257189915114666),\n","  (6, 0.028696313188901562),\n","  (7, 0.007031546940326872),\n","  (8, 0.10737362219688333),\n","  (9, 0.011212466742683391),\n","  (10, 0.025148866083871787),\n","  (11, 0.006651463321930826),\n","  (12, 0.03414417838591157),\n","  (13, 0.05783605726593184),\n","  (14, 0.006271379703534779),\n","  (15, 0.013112884834663628),\n","  (16, 0.12827822120866594),\n","  (17, 0.012226023058406185),\n","  (18, 0.013999746610921071),\n","  (19, 0.020714557202584572)],\n"," [(0, 0.009967935312979227),\n","  (1, 0.2982712951345322),\n","  (2, 0.007876760072494073),\n","  (3, 0.007040289976300012),\n","  (4, 0.011640875505367349),\n","  (5, 0.2146242855151261),\n","  (6, 0.005924996514707931),\n","  (7, 0.009270876899484175),\n","  (8, 0.17754077791718945),\n","  (9, 0.005785584832008922),\n","  (10, 0.11606022584692595),\n","  (11, 0.010525582043775267),\n","  (12, 0.006064408197406941),\n","  (13, 0.034783214833403034),\n","  (14, 0.027533807333054505),\n","  (15, 0.007876760072494073),\n","  (16, 0.006761466610901992),\n","  (17, 0.01484734420744458),\n","  (18, 0.016241461034434683),\n","  (19, 0.011362052139969327)],\n"," [(0, 0.021151141404305965),\n","  (1, 0.005247214107973602),\n","  (2, 0.00892567348263551),\n","  (3, 0.024937790760575573),\n","  (4, 0.005139024126365899),\n","  (5, 0.09223195932056692),\n","  (6, 0.018987341772151903),\n","  (7, 0.005896353997619821),\n","  (8, 0.5748674672725307),\n","  (9, 0.060856864654333015),\n","  (10, 0.018121821919290276),\n","  (11, 0.004057124310288868),\n","  (12, 0.007302823758519962),\n","  (13, 0.03792058855349995),\n","  (14, 0.01033214324353565),\n","  (15, 0.009466623390674025),\n","  (16, 0.0634534242129179),\n","  (17, 0.013145082765335932),\n","  (18, 0.00892567348263551),\n","  (19, 0.009033863464243213)],\n"," [(0, 0.04015646662705486),\n","  (1, 0.11274509803921569),\n","  (2, 0.029065161418102593),\n","  (3, 0.08333333333333333),\n","  (4, 0.03708655179243415),\n","  (5, 0.20751633986928106),\n","  (6, 0.010348583877995643),\n","  (7, 0.005892255892255892),\n","  (8, 0.30416914240443654),\n","  (9, 0.016785502079619728),\n","  (10, 0.019162210338680926),\n","  (11, 0.006288373935432759),\n","  (12, 0.01015052485640721),\n","  (13, 0.004901960784313725),\n","  (14, 0.004901960784313725),\n","  (15, 0.004208754208754209),\n","  (16, 0.03579916815210933),\n","  (17, 0.010843731431966727),\n","  (18, 0.028272925331748862),\n","  (19, 0.02837195484254308)],\n"," [(0, 0.0616865655587824),\n","  (1, 0.008551145530816392),\n","  (2, 0.008981391846832311),\n","  (3, 0.15720124771431646),\n","  (4, 0.00467892868667312),\n","  (5, 0.006077229213724857),\n","  (6, 0.17312036140690545),\n","  (7, 0.1866731203614069),\n","  (8, 0.008766268688824352),\n","  (9, 0.022103904485317846),\n","  (10, 0.01457459395503926),\n","  (11, 0.00951919974185221),\n","  (12, 0.05878240292567495),\n","  (13, 0.009411638162848231),\n","  (14, 0.036086909755835214),\n","  (15, 0.07039905345810477),\n","  (16, 0.023071958696353662),\n","  (17, 0.0542648166075078),\n","  (18, 0.025438313434441216),\n","  (19, 0.06061094976874261)],\n"," [(0, 0.0196046486369067),\n","  (1, 0.010589768654284784),\n","  (2, 0.021885521885521887),\n","  (3, 0.14038231780167265),\n","  (4, 0.020039100684261974),\n","  (5, 0.015368741175192787),\n","  (6, 0.005267731074182687),\n","  (7, 0.0046160530031497775),\n","  (8, 0.0564244596502661),\n","  (9, 0.021016617790811338),\n","  (10, 0.03654827848376235),\n","  (11, 0.00700553926360378),\n","  (12, 0.028836754643206255),\n","  (13, 0.04089279895731509),\n","  (14, 0.010372542630607147),\n","  (15, 0.011350059737156512),\n","  (16, 0.00820028239383078),\n","  (17, 0.01080699467796242),\n","  (18, 0.4822960790702726),\n","  (19, 0.04849570978603237)],\n"," [(0, 0.008494249234989976),\n","  (1, 0.17795715943864093),\n","  (2, 0.014825366677218529),\n","  (3, 0.026537933945341353),\n","  (4, 0.19895536562203228),\n","  (5, 0.004379022897541416),\n","  (6, 0.02643241532130421),\n","  (7, 0.015247441173367099),\n","  (8, 0.004906616017727129),\n","  (9, 0.004590060145615701),\n","  (10, 0.4038725335021631),\n","  (11, 0.01820196264640709),\n","  (12, 0.0052231718898385565),\n","  (13, 0.045531286272027016),\n","  (14, 0.009021842355175688),\n","  (15, 0.009760472723435686),\n","  (16, 0.009549435475361401),\n","  (17, 0.004695578769652844),\n","  (18, 0.00786113749076712),\n","  (19, 0.003956948401392846)],\n"," [(0, 0.032480561825934284),\n","  (1, 0.005141710559317783),\n","  (2, 0.007231836802942898),\n","  (3, 0.011495694339938132),\n","  (4, 0.030975670930524203),\n","  (5, 0.004305660061867737),\n","  (6, 0.004556475211102751),\n","  (7, 0.020023409413928602),\n","  (8, 0.004472870161357746),\n","  (9, 0.27710893737981773),\n","  (10, 0.006813811554217875),\n","  (11, 0.03039043558230917),\n","  (12, 0.03356742747261934),\n","  (13, 0.1343115124153499),\n","  (14, 0.0033023994649276816),\n","  (15, 0.010325223643508068),\n","  (16, 0.026042972995568934),\n","  (17, 0.29015132514003844),\n","  (18, 0.019688989214948584),\n","  (19, 0.04761307582978012)],\n"," [(0, 0.0068744271310724105),\n","  (1, 0.052398411243507484),\n","  (2, 0.018891944189836033),\n","  (3, 0.005957836846929423),\n","  (4, 0.006670740401262858),\n","  (5, 0.09364497402994194),\n","  (6, 0.006670740401262858),\n","  (7, 0.03457582238517161),\n","  (8, 0.00717995722578674),\n","  (9, 0.048833893471840306),\n","  (10, 0.008503920969548833),\n","  (11, 0.08264589062022609),\n","  (12, 0.00717995722578674),\n","  (13, 0.2369385884509624),\n","  (14, 0.00575415011711987),\n","  (15, 0.0037172828190243407),\n","  (16, 0.015632956512883187),\n","  (17, 0.24936347896934516),\n","  (18, 0.01094816172726347),\n","  (19, 0.09761686526122823)],\n"," [(0, 0.00784881171621503),\n","  (1, 0.00749802683504341),\n","  (2, 0.06309743050074541),\n","  (3, 0.00749802683504341),\n","  (4, 0.025388055774796105),\n","  (5, 0.007585723055336315),\n","  (6, 0.05687099885994914),\n","  (7, 0.027317372621240024),\n","  (8, 0.0032009120406910462),\n","  (9, 0.11957379636937648),\n","  (10, 0.009164255020608612),\n","  (11, 0.007147241953871788),\n","  (12, 0.013636762255546785),\n","  (13, 0.20727001666228184),\n","  (14, 0.2955801104972376),\n","  (15, 0.004867140226256248),\n","  (16, 0.027580461282118742),\n","  (17, 0.0839691309304569),\n","  (18, 0.007322634394457599),\n","  (19, 0.017583092168727527)],\n"," [(0, 0.019320297951582868),\n","  (1, 0.017768466790813158),\n","  (2, 0.07720360024829298),\n","  (3, 0.006905648665425201),\n","  (4, 0.009698944754810677),\n","  (5, 0.05330540037243948),\n","  (6, 0.032045313469894476),\n","  (7, 0.08418684047175667),\n","  (8, 0.009388578522656735),\n","  (9, 0.11987895716945997),\n","  (10, 0.014199255121042831),\n","  (11, 0.060133457479826194),\n","  (12, 0.01606145251396648),\n","  (13, 0.14827746741154563),\n","  (14, 0.007681564245810056),\n","  (15, 0.012957790192427064),\n","  (16, 0.008767846058348851),\n","  (17, 0.2783209186840472),\n","  (18, 0.008147113594040968),\n","  (19, 0.01575108628181254)],\n"," [(0, 0.030119825708061005),\n","  (1, 0.007352941176470589),\n","  (2, 0.014324618736383445),\n","  (3, 0.012908496732026145),\n","  (4, 0.012581699346405231),\n","  (5, 0.006917211328976036),\n","  (6, 0.028376906318082793),\n","  (7, 0.23981481481481484),\n","  (8, 0.011601307189542486),\n","  (9, 0.009531590413943357),\n","  (10, 0.007679738562091504),\n","  (11, 0.007679738562091504),\n","  (12, 0.04308278867102397),\n","  (13, 0.13458605664488021),\n","  (14, 0.004084967320261439),\n","  (15, 0.006699346405228759),\n","  (16, 0.23295206971677562),\n","  (17, 0.1382897603485839),\n","  (18, 0.011165577342047933),\n","  (19, 0.04025054466230937)],\n"," [(0, 0.010713031260976467),\n","  (1, 0.02745580142840417),\n","  (2, 0.04654021777309449),\n","  (3, 0.00942512586348203),\n","  (4, 0.006849315068493151),\n","  (5, 0.008605549701440115),\n","  (6, 0.03951527924130665),\n","  (7, 0.10800842992623816),\n","  (8, 0.00368809272918862),\n","  (9, 0.01832338133707997),\n","  (10, 0.03003161222339305),\n","  (11, 0.3326893806345862),\n","  (12, 0.024645826015689033),\n","  (13, 0.012703430511649691),\n","  (14, 0.014693829762322915),\n","  (15, 0.05824844865940757),\n","  (16, 0.01410841821800726),\n","  (17, 0.1612808804589627),\n","  (18, 0.00790305584826133),\n","  (19, 0.06457089333801663)],\n"," [(0, 0.04402603440260344),\n","  (1, 0.018735471873547186),\n","  (2, 0.04105067410506741),\n","  (3, 0.005160390516039052),\n","  (4, 0.0598326359832636),\n","  (5, 0.00599721059972106),\n","  (6, 0.12826592282659227),\n","  (7, 0.02803347280334728),\n","  (8, 0.00497443049744305),\n","  (9, 0.014737331473733148),\n","  (10, 0.035843793584379356),\n","  (11, 0.018921431892143188),\n","  (12, 0.005718270571827057),\n","  (13, 0.01613203161320316),\n","  (14, 0.019293351929335192),\n","  (15, 0.010088331008833102),\n","  (16, 0.06308693630869364),\n","  (17, 0.0060901906090190605),\n","  (18, 0.007763830776383078),\n","  (19, 0.46624825662482566)],\n"," [(0, 0.10429082240762813),\n","  (1, 0.01317706264070984),\n","  (2, 0.02205005959475566),\n","  (3, 0.008674347768507482),\n","  (4, 0.18322076546152827),\n","  (5, 0.004436498477022911),\n","  (6, 0.16746126340882003),\n","  (7, 0.22851277976426965),\n","  (8, 0.0036419017348695536),\n","  (9, 0.007350019864918554),\n","  (10, 0.02893656469341809),\n","  (11, 0.01225003310819759),\n","  (12, 0.03953118792212952),\n","  (13, 0.0306581909680837),\n","  (14, 0.03926632234141173),\n","  (15, 0.023904118659780163),\n","  (16, 0.015693285657528804),\n","  (17, 0.007482452655277447),\n","  (18, 0.011587869156403125),\n","  (19, 0.04787445371473977)],\n"," [(0, 0.009162607283692876),\n","  (1, 0.017513337972628155),\n","  (2, 0.00869867780097425),\n","  (3, 0.06406092940539704),\n","  (4, 0.005528493002396968),\n","  (5, 0.06599396891672464),\n","  (6, 0.005683136163303177),\n","  (7, 0.0028222376865383123),\n","  (8, 0.1326451712673007),\n","  (9, 0.0035954534910693564),\n","  (10, 0.06761772210623984),\n","  (11, 0.01202350576045774),\n","  (12, 0.020606201190752335),\n","  (13, 0.09359777313848294),\n","  (14, 0.39824480012371444),\n","  (15, 0.004832598778319028),\n","  (16, 0.015193690559035022),\n","  (17, 0.010167787829583235),\n","  (18, 0.030580685069202806),\n","  (19, 0.031431222454186954)],\n"," [(0, 0.039347867661120674),\n","  (1, 0.006358768406961178),\n","  (2, 0.01458213807611398),\n","  (3, 0.006071906674316313),\n","  (4, 0.045180722891566265),\n","  (5, 0.004541977433543698),\n","  (6, 0.03953910881621725),\n","  (7, 0.013721552878179385),\n","  (8, 0.004733218588640275),\n","  (9, 0.12196404666284184),\n","  (10, 0.018406961178045515),\n","  (11, 0.005880665519219736),\n","  (12, 0.1438611589213999),\n","  (13, 0.1602122776821572),\n","  (14, 0.23804742780646396),\n","  (15, 0.04795371964046663),\n","  (16, 0.028064639510422644),\n","  (17, 0.013243449990437943),\n","  (18, 0.005785044941671448),\n","  (19, 0.04250334672021419)],\n"," [(0, 0.011853226138940425),\n","  (1, 0.10296846011131726),\n","  (2, 0.023397237682951967),\n","  (3, 0.054937126365697794),\n","  (4, 0.080292723149866),\n","  (5, 0.010616367759224902),\n","  (6, 0.017212945784374357),\n","  (7, 0.016182230467944755),\n","  (8, 0.01927437641723356),\n","  (9, 0.14996907854050712),\n","  (10, 0.022160379303236447),\n","  (11, 0.006493506493506494),\n","  (12, 0.15450422593279736),\n","  (13, 0.1204906204906205),\n","  (14, 0.06998556998556998),\n","  (15, 0.05205112347969491),\n","  (16, 0.036178107606679034),\n","  (17, 0.01473922902494331),\n","  (18, 0.0196866625438054),\n","  (19, 0.017006802721088437)],\n"," [(0, 0.007107459572248305),\n","  (1, 0.04323161189358373),\n","  (2, 0.043622848200312994),\n","  (3, 0.010758998435054774),\n","  (4, 0.06474960876369329),\n","  (5, 0.009063641105894629),\n","  (6, 0.02458268127282212),\n","  (7, 0.009454877412623894),\n","  (8, 0.009194053208137718),\n","  (9, 0.01245435576421492),\n","  (10, 0.14808294209702663),\n","  (11, 0.005281690140845071),\n","  (12, 0.015845070422535214),\n","  (13, 0.13699791340636414),\n","  (14, 0.14456181533646326),\n","  (15, 0.01036776212832551),\n","  (16, 0.0248435054773083),\n","  (17, 0.006977047470005217),\n","  (18, 0.007759520083463746),\n","  (19, 0.26506259780907676)],\n"," [(0, 0.009767441860465118),\n","  (1, 0.0056330749354005175),\n","  (2, 0.020826873385012924),\n","  (3, 0.006046511627906978),\n","  (4, 0.009974160206718348),\n","  (5, 0.006149870801033594),\n","  (6, 0.02423772609819122),\n","  (7, 0.010490956072351423),\n","  (8, 0.004599483204134368),\n","  (9, 0.061033591731266164),\n","  (10, 0.02237726098191215),\n","  (11, 0.18299741602067185),\n","  (12, 0.1838242894056848),\n","  (13, 0.009147286821705429),\n","  (14, 0.019173126614987084),\n","  (15, 0.008113695090439278),\n","  (16, 0.023514211886304915),\n","  (17, 0.3029974160206719),\n","  (18, 0.005736434108527133),\n","  (19, 0.083359173126615)],\n"," [(0, 0.052243969948596286),\n","  (1, 0.007265717674970344),\n","  (2, 0.005387504942665085),\n","  (3, 0.008254250691973112),\n","  (4, 0.00953934361407671),\n","  (5, 0.009835903519177541),\n","  (6, 0.007858837485172005),\n","  (7, 0.027234084618426257),\n","  (8, 0.013790035587188613),\n","  (9, 0.1113582443653618),\n","  (10, 0.004596678529062871),\n","  (11, 0.022983392645314352),\n","  (12, 0.085359826018189),\n","  (13, 0.011516409648082246),\n","  (14, 0.01754646105179913),\n","  (15, 0.010231316725978648),\n","  (16, 0.08763345195729537),\n","  (17, 0.4286773428232503),\n","  (18, 0.03721826809015421),\n","  (19, 0.041468960063266114)],\n"," [(0, 0.013630932985771695),\n","  (1, 0.024818073205169978),\n","  (2, 0.013413706962094058),\n","  (3, 0.026447268382752253),\n","  (4, 0.026990333441946346),\n","  (5, 0.01830129249484088),\n","  (6, 0.4187574671445639),\n","  (7, 0.006679700228087325),\n","  (8, 0.021776908873683068),\n","  (9, 0.06750298685782556),\n","  (10, 0.021233843814488975),\n","  (11, 0.04089279895731509),\n","  (12, 0.0504507439991311),\n","  (13, 0.09052894536765504),\n","  (14, 0.008308895405669599),\n","  (15, 0.06131204518301293),\n","  (16, 0.005702183121537961),\n","  (17, 0.05729336374497665),\n","  (18, 0.006571087216248507),\n","  (19, 0.019387422613229066)],\n"," [(0, 0.022498974989749898),\n","  (1, 0.01522140221402214),\n","  (2, 0.050789257892578925),\n","  (3, 0.004048790487904879),\n","  (4, 0.019116441164411645),\n","  (5, 0.0032287822878228783),\n","  (6, 0.07016195161951619),\n","  (7, 0.0845120951209512),\n","  (8, 0.0033312833128331283),\n","  (9, 0.5542742927429274),\n","  (10, 0.01429889298892989),\n","  (11, 0.011941369413694137),\n","  (12, 0.03223657236572366),\n","  (13, 0.021678966789667898),\n","  (14, 0.00579130791307913),\n","  (15, 0.005996309963099631),\n","  (16, 0.008968839688396884),\n","  (17, 0.05632431324313243),\n","  (18, 0.003843788437884379),\n","  (19, 0.011736367363673637)],\n"," [(0, 0.009129759129759132),\n","  (1, 0.007834757834757837),\n","  (2, 0.032051282051282055),\n","  (3, 0.01754726754726755),\n","  (4, 0.005244755244755246),\n","  (5, 0.006410256410256411),\n","  (6, 0.08462833462833465),\n","  (7, 0.1414788914788915),\n","  (8, 0.004856254856254857),\n","  (9, 0.42340067340067344),\n","  (10, 0.0067987567987568),\n","  (11, 0.01987826987826988),\n","  (12, 0.07905982905982907),\n","  (13, 0.06300181300181301),\n","  (14, 0.007575757575757577),\n","  (15, 0.009777259777259779),\n","  (16, 0.04422429422429423),\n","  (17, 0.021173271173271176),\n","  (18, 0.0057627557627557635),\n","  (19, 0.010165760165760167)],\n"," [(0, 0.012171372930866602),\n","  (1, 0.005355404089581304),\n","  (2, 0.01801363193768257),\n","  (3, 0.1360489018716867),\n","  (4, 0.010656713188358757),\n","  (5, 0.038894298387969276),\n","  (6, 0.10186086768365249),\n","  (7, 0.024829600778967866),\n","  (8, 0.029373580006491398),\n","  (9, 0.00795196364816618),\n","  (10, 0.010332143243535648),\n","  (11, 0.050795196364816615),\n","  (12, 0.17607919506653683),\n","  (13, 0.02493779076057557),\n","  (14, 0.006437303905658336),\n","  (15, 0.007086443795304554),\n","  (16, 0.029589959969706803),\n","  (17, 0.275505788164016),\n","  (18, 0.02104295142269826),\n","  (19, 0.013036892783728226)],\n"," [(0, 0.042078044193700044),\n","  (1, 0.0358094342579533),\n","  (2, 0.011988716502115656),\n","  (3, 0.03173483779971791),\n","  (4, 0.012302146998902993),\n","  (5, 0.00791412004388027),\n","  (6, 0.006033537063156245),\n","  (7, 0.015749882463563705),\n","  (8, 0.014652875724808024),\n","  (9, 0.047563077887478454),\n","  (10, 0.09677166588309043),\n","  (11, 0.005876821814762577),\n","  (12, 0.1350101864911456),\n","  (13, 0.07561510734994514),\n","  (14, 0.008540981037454945),\n","  (15, 0.009481272527816956),\n","  (16, 0.027503526093088856),\n","  (17, 0.3917097633599749),\n","  (18, 0.010891709763359975),\n","  (19, 0.012772292744083999)],\n"," [(0, 0.009298245614035089),\n","  (1, 0.0060233918128654975),\n","  (2, 0.008713450292397661),\n","  (3, 0.012222222222222223),\n","  (4, 0.007543859649122807),\n","  (5, 0.029415204678362575),\n","  (6, 0.06497076023391812),\n","  (7, 0.03070175438596491),\n","  (8, 0.005204678362573099),\n","  (9, 0.006491228070175438),\n","  (10, 0.01128654970760234),\n","  (11, 0.059005847953216374),\n","  (12, 0.026491228070175437),\n","  (13, 0.13152046783625732),\n","  (14, 0.0077777777777777776),\n","  (15, 0.06964912280701754),\n","  (16, 0.011871345029239765),\n","  (17, 0.44380116959064325),\n","  (18, 0.013976608187134502),\n","  (19, 0.044035087719298247)],\n"," [(0, 0.03991159646385856),\n","  (1, 0.0636592130351881),\n","  (2, 0.030897902582769983),\n","  (3, 0.021884208701681403),\n","  (4, 0.1605564222568903),\n","  (5, 0.009057028947824581),\n","  (6, 0.017117351360721098),\n","  (7, 0.33927023747616575),\n","  (8, 0.004376841740336281),\n","  (9, 0.022837580169873466),\n","  (10, 0.08870688160859769),\n","  (11, 0.019717455364881266),\n","  (12, 0.025611024440977646),\n","  (13, 0.009490379615184608),\n","  (14, 0.04043161726469059),\n","  (15, 0.005156872941584331),\n","  (16, 0.01590396949211302),\n","  (17, 0.04935864101230717),\n","  (18, 0.012957184954064832),\n","  (19, 0.02309759057028948)],\n"," [(0, 0.017885406774295665),\n","  (1, 0.005434209137912842),\n","  (2, 0.009443916851324261),\n","  (3, 0.038039463965389896),\n","  (4, 0.013559143188772821),\n","  (5, 0.010077028595547115),\n","  (6, 0.28453096971615494),\n","  (7, 0.013664661812809965),\n","  (8, 0.004167985649467132),\n","  (9, 0.007650100242692837),\n","  (10, 0.006067320882135699),\n","  (11, 0.08773873588688405),\n","  (12, 0.011976363828215684),\n","  (13, 0.008705286483064262),\n","  (14, 0.01071014033976997),\n","  (15, 0.006383876754247127),\n","  (16, 0.006383876754247127),\n","  (17, 0.2794660757623721),\n","  (18, 0.013770180436847107),\n","  (19, 0.16434525693784957)],\n"," [(0, 0.13692528735632184),\n","  (1, 0.003879310344827586),\n","  (2, 0.029837164750957854),\n","  (3, 0.019300766283524905),\n","  (4, 0.009147509578544061),\n","  (5, 0.004837164750957855),\n","  (6, 0.021024904214559388),\n","  (7, 0.07772988505747126),\n","  (8, 0.0037835249042145595),\n","  (9, 0.07725095785440612),\n","  (10, 0.008381226053639846),\n","  (11, 0.007519157088122605),\n","  (12, 0.007040229885057471),\n","  (13, 0.004166666666666667),\n","  (14, 0.2791666666666667),\n","  (15, 0.08328544061302683),\n","  (16, 0.038553639846743294),\n","  (17, 0.013362068965517242),\n","  (18, 0.0037835249042145595),\n","  (19, 0.17102490421455938)],\n"," [(0, 0.010885504794134237),\n","  (1, 0.006373378454596729),\n","  (2, 0.0730400451212634),\n","  (3, 0.014382402707275803),\n","  (4, 0.007727016356457981),\n","  (5, 0.008291032148900168),\n","  (6, 0.5210941906373379),\n","  (7, 0.10338409475465313),\n","  (8, 0.005019740552735476),\n","  (9, 0.010772701635645797),\n","  (10, 0.015397631133671742),\n","  (11, 0.01787930062041737),\n","  (12, 0.006034968979131416),\n","  (13, 0.011449520586576424),\n","  (14, 0.016300056401579246),\n","  (15, 0.019684151156232375),\n","  (16, 0.012577552171460801),\n","  (17, 0.07100958826847152),\n","  (18, 0.006373378454596729),\n","  (19, 0.06232374506486182)],\n"," [(0, 0.07490997942386832),\n","  (1, 0.008551954732510287),\n","  (2, 0.008037551440329218),\n","  (3, 0.010352366255144033),\n","  (4, 0.005336934156378601),\n","  (5, 0.006108539094650206),\n","  (6, 0.008551954732510287),\n","  (7, 0.46238425925925924),\n","  (8, 0.01099537037037037),\n","  (9, 0.2201003086419753),\n","  (10, 0.012409979423868314),\n","  (11, 0.0055941358024691355),\n","  (12, 0.05150462962962963),\n","  (13, 0.029513888888888888),\n","  (14, 0.005722736625514403),\n","  (15, 0.009709362139917695),\n","  (16, 0.03530092592592592),\n","  (17, 0.013824588477366255),\n","  (18, 0.005208333333333333),\n","  (19, 0.015882201646090534)],\n"," [(0, 0.02645292351174704),\n","  (1, 0.0060501678148736976),\n","  (2, 0.011702879349938173),\n","  (3, 0.002958841194135312),\n","  (4, 0.009318141671082848),\n","  (5, 0.004813637166578343),\n","  (6, 0.015147500441618089),\n","  (7, 0.0533033033033033),\n","  (8, 0.043764352587882),\n","  (9, 0.18949832185126303),\n","  (10, 0.007463345698639817),\n","  (11, 0.007286698463169051),\n","  (12, 0.05922098569157393),\n","  (13, 0.19046988164635223),\n","  (14, 0.004283695460166048),\n","  (15, 0.010731319554848967),\n","  (16, 0.18437555202261086),\n","  (17, 0.14772125066242714),\n","  (18, 0.008788199964670554),\n","  (19, 0.01664900194311959)],\n"," [(0, 0.015479686735193345),\n","  (1, 0.005567792462065591),\n","  (2, 0.029919236417033777),\n","  (3, 0.020129711209006366),\n","  (4, 0.016336270190895744),\n","  (5, 0.0187836514929026),\n","  (6, 0.056718061674008814),\n","  (7, 0.05598384728340676),\n","  (8, 0.0066691140479686745),\n","  (9, 0.3589696524718552),\n","  (10, 0.010829662261380324),\n","  (11, 0.005323054331864905),\n","  (12, 0.007770435633871758),\n","  (13, 0.11937102300538426),\n","  (14, 0.008994126284875186),\n","  (15, 0.007525697503671073),\n","  (16, 0.19303720019579054),\n","  (17, 0.019150758688203626),\n","  (18, 0.0066691140479686745),\n","  (19, 0.03677190406265297)],\n"," [(0, 0.015077424612876932),\n","  (1, 0.008919677623834102),\n","  (2, 0.03925563705514805),\n","  (3, 0.002943040840351353),\n","  (4, 0.01498686951009689),\n","  (5, 0.008557457212713934),\n","  (6, 0.15054785837181922),\n","  (7, 0.09793534365661502),\n","  (8, 0.09023815992031149),\n","  (9, 0.018880738929638683),\n","  (10, 0.016345196051797517),\n","  (11, 0.2385674182740197),\n","  (12, 0.08109209453952729),\n","  (13, 0.032916779860545135),\n","  (14, 0.006203024540432852),\n","  (15, 0.022955718554740556),\n","  (16, 0.008466902109933894),\n","  (17, 0.09177759666757221),\n","  (18, 0.0056596939237526026),\n","  (19, 0.048673367744272375)],\n"," [(0, 0.014053840063341254),\n","  (1, 0.006532066508313541),\n","  (2, 0.10616257587754027),\n","  (3, 0.011546582211665348),\n","  (4, 0.008115597783056218),\n","  (5, 0.007851675903932438),\n","  (6, 0.0391264185801003),\n","  (7, 0.35345737661652155),\n","  (8, 0.03463974663499605),\n","  (9, 0.007983636843494327),\n","  (10, 0.033452098178939044),\n","  (11, 0.007851675903932438),\n","  (12, 0.03767484824491951),\n","  (13, 0.1095935603061494),\n","  (14, 0.041765637371338094),\n","  (15, 0.01115069939297968),\n","  (16, 0.1338743731855371),\n","  (17, 0.009963050936922672),\n","  (18, 0.006532066508313541),\n","  (19, 0.018672472948007394)],\n"," [(0, 0.015967153284671534),\n","  (1, 0.004207218167072182),\n","  (2, 0.0064375506893755065),\n","  (3, 0.02458434712084347),\n","  (4, 0.018197485806974858),\n","  (5, 0.006944444444444444),\n","  (6, 0.05641727493917275),\n","  (7, 0.01525750202757502),\n","  (8, 0.004004460665044607),\n","  (9, 0.005930656934306569),\n","  (10, 0.008667883211678832),\n","  (11, 0.019109894566098944),\n","  (12, 0.18314071370640714),\n","  (13, 0.19165652879156528),\n","  (14, 0.03198499594484996),\n","  (15, 0.022252635847526358),\n","  (16, 0.10903284671532847),\n","  (17, 0.007755474452554745),\n","  (18, 0.2574513381995134),\n","  (19, 0.010999594484995944)],\n"," [(0, 0.012264044308805245),\n","  (1, 0.0168983836328699),\n","  (2, 0.014072566971854866),\n","  (3, 0.007403639651859388),\n","  (4, 0.005821182321690968),\n","  (5, 0.011133717644399231),\n","  (6, 0.16689273199954788),\n","  (7, 0.16417994800497343),\n","  (8, 0.00593421498813157),\n","  (9, 0.011585848310161637),\n","  (10, 0.010568554312196225),\n","  (11, 0.0046908556572849555),\n","  (12, 0.02277608228778117),\n","  (13, 0.2114276025771448),\n","  (14, 0.00593421498813157),\n","  (15, 0.05035605289928789),\n","  (16, 0.036226969594212725),\n","  (17, 0.038261557590143554),\n","  (18, 0.007177574318978185),\n","  (19, 0.19639425794054483)],\n"," [(0, 0.0090761937796277),\n","  (1, 0.04306856283963463),\n","  (2, 0.006763787721123828),\n","  (3, 0.007341889235749796),\n","  (4, 0.011388599838131574),\n","  (5, 0.03416579951439472),\n","  (6, 0.01416348710833622),\n","  (7, 0.4105098855359),\n","  (8, 0.005145103480171117),\n","  (9, 0.02167880679847381),\n","  (10, 0.010348017111804832),\n","  (11, 0.033587697999768754),\n","  (12, 0.06214591282229158),\n","  (13, 0.07937333795814544),\n","  (14, 0.005723204994797086),\n","  (15, 0.008498092265001733),\n","  (16, 0.033587697999768754),\n","  (17, 0.16886345242224532),\n","  (18, 0.005260723783096311),\n","  (19, 0.029309746791536587)],\n"," [(0, 0.015265804597701152),\n","  (1, 0.03825431034482759),\n","  (2, 0.017061781609195407),\n","  (3, 0.013948754789272032),\n","  (4, 0.01011733716475096),\n","  (5, 0.021731321839080463),\n","  (6, 0.04938936781609196),\n","  (7, 0.09404932950191572),\n","  (8, 0.005208333333333334),\n","  (9, 0.031908524904214565),\n","  (10, 0.04663553639846744),\n","  (11, 0.011075191570881228),\n","  (12, 0.01682231800766284),\n","  (13, 0.376257183908046),\n","  (14, 0.005567528735632185),\n","  (15, 0.013948754789272032),\n","  (16, 0.011075191570881228),\n","  (17, 0.08303400383141764),\n","  (18, 0.004849137931034484),\n","  (19, 0.13380028735632188)],\n"," [(0, 0.038170065752814),\n","  (1, 0.004624986069319069),\n","  (2, 0.022679148556781457),\n","  (3, 0.007076785913295443),\n","  (4, 0.011757494706341248),\n","  (5, 0.006965340465841971),\n","  (6, 0.15039563133845985),\n","  (7, 0.04507968349492924),\n","  (8, 0.004402095174412126),\n","  (9, 0.036052602251198045),\n","  (10, 0.023013484899141873),\n","  (11, 0.009751476652178761),\n","  (12, 0.10659757048924552),\n","  (13, 0.12119692410565029),\n","  (14, 0.008079794940376687),\n","  (15, 0.00540510420149337),\n","  (16, 0.3314944834503511),\n","  (17, 0.041959210966232036),\n","  (18, 0.01409784910286415),\n","  (19, 0.01120026746907389)],\n"," [(0, 0.009364002011060836),\n","  (1, 0.008609854198089495),\n","  (2, 0.05021367521367522),\n","  (3, 0.019042232277526397),\n","  (4, 0.0067244846656611375),\n","  (5, 0.008107088989441933),\n","  (6, 0.10111865258924084),\n","  (7, 0.03450226244343892),\n","  (8, 0.009992458521870288),\n","  (9, 0.11544746103569635),\n","  (10, 0.03135997988939166),\n","  (11, 0.005216189039718452),\n","  (12, 0.03726747109100051),\n","  (13, 0.424396681749623),\n","  (14, 0.02570387129210659),\n","  (15, 0.016025641025641028),\n","  (16, 0.027589240824534945),\n","  (17, 0.05323026646556059),\n","  (18, 0.006975867269984918),\n","  (19, 0.009112619406737056)],\n"," [(0, 0.012629237010862455),\n","  (1, 0.006739955503206388),\n","  (2, 0.01904201020808795),\n","  (3, 0.029380971077084154),\n","  (4, 0.011843999476508313),\n","  (5, 0.011713126554115955),\n","  (6, 0.08081402957728047),\n","  (7, 0.008964795183876459),\n","  (8, 0.024015181258997518),\n","  (9, 0.03553199842952494),\n","  (10, 0.027025258474021728),\n","  (11, 0.006216463813636959),\n","  (12, 0.020481612354403878),\n","  (13, 0.3288182175107971),\n","  (14, 0.1130087684858003),\n","  (15, 0.13643502159403223),\n","  (16, 0.033830650438424294),\n","  (17, 0.008179557649522315),\n","  (18, 0.015508441303494309),\n","  (19, 0.06982070409632249)],\n"," [(0, 0.011213439072226486),\n","  (1, 0.017694209942866888),\n","  (2, 0.028694465762769676),\n","  (3, 0.006949774025752537),\n","  (4, 0.005755947812739831),\n","  (5, 0.006097041016457747),\n","  (6, 0.02238424149398823),\n","  (7, 0.010190159461072738),\n","  (8, 0.004817941502515562),\n","  (9, 0.18261277394047923),\n","  (10, 0.006011767715528268),\n","  (11, 0.21697791421505927),\n","  (12, 0.023578067707000938),\n","  (13, 0.054532275944401805),\n","  (14, 0.016329837127995225),\n","  (15, 0.019826042466103862),\n","  (16, 0.015562377419629915),\n","  (17, 0.04941587788863307),\n","  (18, 0.006949774025752537),\n","  (19, 0.2944060714590262)],\n"," [(0, 0.11613756613756611),\n","  (1, 0.00802469135802469),\n","  (2, 0.007319223985890651),\n","  (3, 0.010846560846560846),\n","  (4, 0.007319223985890651),\n","  (5, 0.007319223985890651),\n","  (6, 0.014373897707231038),\n","  (7, 0.2801587301587301),\n","  (8, 0.006613756613756612),\n","  (9, 0.05617283950617283),\n","  (10, 0.01719576719576719),\n","  (11, 0.048941798941798925),\n","  (12, 0.1847442680776014),\n","  (13, 0.07363315696649028),\n","  (14, 0.009964726631393297),\n","  (15, 0.00802469135802469),\n","  (16, 0.0997354497354497),\n","  (17, 0.016313932980599643),\n","  (18, 0.010670194003527336),\n","  (19, 0.016490299823633155)],\n"," [(0, 0.36243176615601336),\n","  (1, 0.004974467335798556),\n","  (2, 0.009640781827786582),\n","  (3, 0.0068233843986617365),\n","  (4, 0.010433174854727945),\n","  (5, 0.004886423666138405),\n","  (6, 0.039311498503257616),\n","  (7, 0.08958443387920409),\n","  (8, 0.021526677231907026),\n","  (9, 0.03103539355520338),\n","  (10, 0.009376650818806127),\n","  (11, 0.004270117978517345),\n","  (12, 0.049436520514175034),\n","  (13, 0.023815812643070963),\n","  (14, 0.009640781827786582),\n","  (15, 0.005590773023419616),\n","  (16, 0.27887832364852966),\n","  (17, 0.022231026589188237),\n","  (18, 0.006207078711040676),\n","  (19, 0.009904912836767036)],\n"," [(0, 0.631619937694704),\n","  (1, 0.0051401869158878505),\n","  (2, 0.007736240913811007),\n","  (3, 0.004724818276220145),\n","  (4, 0.0112668743509865),\n","  (5, 0.0036863966770508826),\n","  (6, 0.005763239875389408),\n","  (7, 0.009190031152647975),\n","  (8, 0.00347871235721703),\n","  (9, 0.006593977154724818),\n","  (10, 0.00711318795430945),\n","  (11, 0.004620976116303219),\n","  (12, 0.028920041536863967),\n","  (13, 0.07751817237798546),\n","  (14, 0.014693665628245067),\n","  (15, 0.006178608515057113),\n","  (16, 0.01407061266874351),\n","  (17, 0.013136033229491173),\n","  (18, 0.045119418483904467),\n","  (19, 0.09942886812045691)],\n"," [(0, 0.5490550864495377),\n","  (1, 0.006232408524326499),\n","  (2, 0.008108832596166736),\n","  (3, 0.006634499396863692),\n","  (4, 0.012665862484921594),\n","  (5, 0.005294196488406381),\n","  (6, 0.013470044229995981),\n","  (7, 0.11707545905374617),\n","  (8, 0.044297011124514146),\n","  (9, 0.014542286556761829),\n","  (10, 0.010253317249698433),\n","  (11, 0.007304650851092348),\n","  (12, 0.08115534110709022),\n","  (13, 0.05796810079077872),\n","  (14, 0.024594558370191668),\n","  (15, 0.0060983782334807675),\n","  (16, 0.010387347540544165),\n","  (17, 0.013604074520841712),\n","  (18, 0.005964347942635036),\n","  (19, 0.005294196488406381)],\n"," [(0, 0.036825726141078846),\n","  (1, 0.005474873213462426),\n","  (2, 0.027835408022130022),\n","  (3, 0.01204472106961734),\n","  (4, 0.008241124942369758),\n","  (5, 0.006857999077916092),\n","  (6, 0.34560857538035966),\n","  (7, 0.09111341632088521),\n","  (8, 0.005820654679575842),\n","  (9, 0.03532733978792071),\n","  (10, 0.015617796219455974),\n","  (11, 0.021726602120792995),\n","  (12, 0.02345550945136008),\n","  (13, 0.030255878284923935),\n","  (14, 0.0036307053941908723),\n","  (15, 0.014234670355002308),\n","  (16, 0.2931650530198249),\n","  (17, 0.006973259566620565),\n","  (18, 0.008010603964960812),\n","  (19, 0.007780082987551869)],\n"," [(0, 0.3991228070175439),\n","  (1, 0.00947114162217137),\n","  (2, 0.013285024154589372),\n","  (3, 0.006292906178489702),\n","  (4, 0.00947114162217137),\n","  (5, 0.01595474192728197),\n","  (6, 0.006038647342995169),\n","  (7, 0.005911517925247902),\n","  (8, 0.01061530638189677),\n","  (9, 0.00908975336892957),\n","  (10, 0.014683447749809305),\n","  (11, 0.020150012712941773),\n","  (12, 0.061848461734045256),\n","  (13, 0.1461352657004831),\n","  (14, 0.008454106280193236),\n","  (15, 0.005784388507500636),\n","  (16, 0.07481566234426647),\n","  (17, 0.08829138062547673),\n","  (18, 0.012140859394863972),\n","  (19, 0.08244342740910246)],\n"," [(0, 0.16496772001359158),\n","  (1, 0.005379997734737796),\n","  (2, 0.014554309661343301),\n","  (3, 0.028145882885944048),\n","  (4, 0.009004417261297996),\n","  (5, 0.007871786159247933),\n","  (6, 0.009570732812323027),\n","  (7, 0.1766338203647072),\n","  (8, 0.00436062974289274),\n","  (9, 0.009117680371503002),\n","  (10, 0.05430966134330049),\n","  (11, 0.006172839506172839),\n","  (12, 0.4182240344319855),\n","  (13, 0.01931136028995356),\n","  (14, 0.021689885604258694),\n","  (15, 0.009570732812323027),\n","  (16, 0.015233888322573337),\n","  (17, 0.012289047457243176),\n","  (18, 0.009344206591913014),\n","  (19, 0.004247366632687734)],\n"," [(0, 0.38214545077290174),\n","  (1, 0.005135387488328665),\n","  (2, 0.01104886399004046),\n","  (3, 0.00866272434899886),\n","  (4, 0.006691565515094927),\n","  (5, 0.004409171075837742),\n","  (6, 0.007210291524017014),\n","  (7, 0.00824774354186119),\n","  (8, 0.0037866998651312376),\n","  (9, 0.007729017532939101),\n","  (10, 0.027751841477331674),\n","  (11, 0.004720406681190995),\n","  (12, 0.0160286336756925),\n","  (13, 0.070079883805374),\n","  (14, 0.1412490922294844),\n","  (15, 0.010322647577549538),\n","  (16, 0.00980392156862745),\n","  (17, 0.00866272434899886),\n","  (18, 0.06613756613756613),\n","  (19, 0.2001763668430335)],\n"," [(0, 0.2378901373283396),\n","  (1, 0.004431960049937579),\n","  (2, 0.012172284644194759),\n","  (3, 0.006928838951310862),\n","  (4, 0.004681647940074907),\n","  (5, 0.0078027465667915115),\n","  (6, 0.01254681647940075),\n","  (7, 0.2155430711610487),\n","  (8, 0.01279650436953808),\n","  (9, 0.07122347066167292),\n","  (10, 0.01579275905118602),\n","  (11, 0.004931335830212236),\n","  (12, 0.02565543071161049),\n","  (13, 0.11367041198501875),\n","  (14, 0.005181023720349564),\n","  (15, 0.004556803995006243),\n","  (16, 0.20143570536828967),\n","  (17, 0.016416978776529343),\n","  (18, 0.006928838951310862),\n","  (19, 0.019413233458177283)],\n"," [(0, 0.5815402038505098),\n","  (1, 0.02132880332200831),\n","  (2, 0.012017113376116776),\n","  (3, 0.008116270290675728),\n","  (4, 0.012394614319869135),\n","  (5, 0.0058512646281615715),\n","  (6, 0.028752988549138044),\n","  (7, 0.014030451742796025),\n","  (8, 0.0449855291304895),\n","  (9, 0.0089971058260979),\n","  (10, 0.01818296212407198),\n","  (11, 0.006983767459418649),\n","  (12, 0.015288788221970556),\n","  (13, 0.11369070089341891),\n","  (14, 0.022461306153265387),\n","  (15, 0.009122939474015353),\n","  (16, 0.025229646407449357),\n","  (17, 0.024474644519944636),\n","  (18, 0.017176292940732356),\n","  (19, 0.00937460676985026)],\n"," [(0, 0.10837696335078534),\n","  (1, 0.0038976148923792905),\n","  (2, 0.03170447934845841),\n","  (3, 0.00447934845840605),\n","  (4, 0.018440954043048283),\n","  (5, 0.004828388598022106),\n","  (6, 0.014368819080860965),\n","  (7, 0.030890052356020943),\n","  (8, 0.0075043630017452),\n","  (9, 0.006108202443280977),\n","  (10, 0.010296684118673648),\n","  (11, 0.006922629435718441),\n","  (12, 0.4502036067481094),\n","  (13, 0.19377545084351366),\n","  (14, 0.015299592786503781),\n","  (15, 0.004595695171611402),\n","  (16, 0.016230366492146597),\n","  (17, 0.008435136707388016),\n","  (18, 0.04496800465386853),\n","  (19, 0.018673647469458987)],\n"," [(0, 0.25474854837122324),\n","  (1, 0.004379490207656726),\n","  (2, 0.006741462454482825),\n","  (3, 0.0040842436768034635),\n","  (4, 0.011170160417281761),\n","  (5, 0.007135124495620508),\n","  (6, 0.026522980021651404),\n","  (7, 0.45955122527310294),\n","  (8, 0.003985828166519043),\n","  (9, 0.07543548863300854),\n","  (10, 0.00979234327329987),\n","  (11, 0.014319456746383227),\n","  (12, 0.03301840370042318),\n","  (13, 0.02612931798051372),\n","  (14, 0.005462060820785355),\n","  (15, 0.0072335400059049286),\n","  (16, 0.02475150083653183),\n","  (17, 0.010776498376144078),\n","  (18, 0.010186005314437554),\n","  (19, 0.004576321228225567)],\n"," [(0, 0.005657259089753369),\n","  (1, 0.004513094330027968),\n","  (2, 0.03616831934909738),\n","  (3, 0.01061530638189677),\n","  (4, 0.00947114162217137),\n","  (5, 0.02142130689041444),\n","  (6, 0.0068014238494787695),\n","  (7, 0.016336130180523774),\n","  (8, 0.15389016018306637),\n","  (9, 0.02345537757437071),\n","  (10, 0.022946859903381644),\n","  (11, 0.008072718026951437),\n","  (12, 0.09706331045003815),\n","  (13, 0.08028222730739894),\n","  (14, 0.06985761505212307),\n","  (15, 0.0038774472412916347),\n","  (16, 0.12032799389778795),\n","  (17, 0.2930968726163234),\n","  (18, 0.007182812102720569),\n","  (19, 0.008962623951182304)],\n"," [(0, 0.08305443998215083),\n","  (1, 0.006191432396251673),\n","  (2, 0.030510932619366354),\n","  (3, 0.008311021865238733),\n","  (4, 0.008422579205711735),\n","  (5, 0.007418563141454708),\n","  (6, 0.09755689424364124),\n","  (7, 0.39786925479696567),\n","  (8, 0.005410531012940652),\n","  (9, 0.11217090584560464),\n","  (10, 0.024152164212405177),\n","  (11, 0.021251673360107096),\n","  (12, 0.09688755020080321),\n","  (13, 0.005856760374832664),\n","  (14, 0.0051874163319946456),\n","  (15, 0.006637661758143686),\n","  (16, 0.025156180276662204),\n","  (17, 0.042336010709504684),\n","  (18, 0.00429495760821062),\n","  (19, 0.011323070058009817)],\n"," [(0, 0.34303227808814407),\n","  (1, 0.01016449410304159),\n","  (2, 0.02645872129112353),\n","  (3, 0.03608007448789572),\n","  (4, 0.015751086281812543),\n","  (5, 0.00830229671011794),\n","  (6, 0.007371198013656115),\n","  (7, 0.005664183736809436),\n","  (8, 0.009388578522656737),\n","  (9, 0.012337057728119183),\n","  (10, 0.02242396027312229),\n","  (11, 0.00628491620111732),\n","  (12, 0.06463376784605836),\n","  (13, 0.12763811297330854),\n","  (14, 0.10187771570453136),\n","  (15, 0.008767846058348853),\n","  (16, 0.015130353817504657),\n","  (17, 0.01016449410304159),\n","  (18, 0.011095592799503416),\n","  (19, 0.15743327126008694)],\n"," [(0, 0.19981732215774764),\n","  (1, 0.012948635289060817),\n","  (2, 0.061734364925854275),\n","  (3, 0.012841177734794753),\n","  (4, 0.025628626692456472),\n","  (5, 0.012411347517730494),\n","  (6, 0.041854717386632274),\n","  (7, 0.019933376316355035),\n","  (8, 0.007898130238555769),\n","  (9, 0.015527616591446376),\n","  (10, 0.048624543305394355),\n","  (11, 0.012626262626262623),\n","  (12, 0.01456049860305179),\n","  (13, 0.006716097141629055),\n","  (14, 0.012411347517730494),\n","  (15, 0.00639372447883086),\n","  (16, 0.3445626477541371),\n","  (17, 0.012841177734794753),\n","  (18, 0.013485923060391142),\n","  (19, 0.11718246292714375)],\n"," [(0, 0.018372127067779243),\n","  (1, 0.006514419557897819),\n","  (2, 0.007978334065290587),\n","  (3, 0.01617625530669009),\n","  (4, 0.012223686136729615),\n","  (5, 0.011930903235251061),\n","  (6, 0.004757722149026497),\n","  (7, 0.00783194261455131),\n","  (8, 0.11938222807788025),\n","  (9, 0.017493778363343582),\n","  (10, 0.022471087688478993),\n","  (11, 0.00739276826233348),\n","  (12, 0.06609574000878349),\n","  (13, 0.00739276826233348),\n","  (14, 0.11001317523056653),\n","  (15, 0.009881422924901186),\n","  (16, 0.5203484116527595),\n","  (17, 0.010320597277119016),\n","  (18, 0.0091494656712048),\n","  (19, 0.01427316644707949)],\n"," [(0, 0.34395496818404303),\n","  (1, 0.006020558002936856),\n","  (2, 0.007293196279980419),\n","  (3, 0.008663729809104256),\n","  (4, 0.011796377875673026),\n","  (5, 0.02559960841899167),\n","  (6, 0.004160548213411649),\n","  (7, 0.004454233969652471),\n","  (8, 0.009838472834067547),\n","  (9, 0.006510034263338227),\n","  (10, 0.02961331375428291),\n","  (11, 0.02863436123348017),\n","  (12, 0.06240822320117473),\n","  (13, 0.03137542829172785),\n","  (14, 0.2673029858051884),\n","  (15, 0.005041605482134116),\n","  (16, 0.009251101321585901),\n","  (17, 0.08355359765051394),\n","  (18, 0.039402838962310324),\n","  (19, 0.015124816446402347)],\n"," [(0, 0.030948509485094852),\n","  (1, 0.005691056910569106),\n","  (2, 0.01924119241192412),\n","  (3, 0.009485094850948509),\n","  (4, 0.01024390243902439),\n","  (5, 0.023143631436314364),\n","  (6, 0.0071002710027100275),\n","  (7, 0.01078590785907859),\n","  (8, 0.5238482384823848),\n","  (9, 0.016747967479674795),\n","  (10, 0.01078590785907859),\n","  (11, 0.005365853658536586),\n","  (12, 0.024444444444444446),\n","  (13, 0.00937669376693767),\n","  (14, 0.05468834688346883),\n","  (15, 0.01002710027100271),\n","  (16, 0.18964769647696478),\n","  (17, 0.01100271002710027),\n","  (18, 0.0040650406504065045),\n","  (19, 0.023360433604336043)],\n"," [(0, 0.012544802867383515),\n","  (1, 0.008337229234844945),\n","  (2, 0.007713884992987378),\n","  (3, 0.009116409537166902),\n","  (4, 0.009583917718560077),\n","  (5, 0.060386473429951695),\n","  (6, 0.008025557113916162),\n","  (7, 0.011921458625525948),\n","  (8, 0.34385226741467984),\n","  (9, 0.006155524388343463),\n","  (10, 0.007713884992987378),\n","  (11, 0.0064671965092722465),\n","  (12, 0.047919588592800384),\n","  (13, 0.005843852267414681),\n","  (14, 0.1319152251831074),\n","  (15, 0.005064671965092723),\n","  (16, 0.29195885928003745),\n","  (17, 0.010830606202275208),\n","  (18, 0.006311360448807855),\n","  (19, 0.008337229234844945)],\n"," [(0, 0.018136693394425352),\n","  (1, 0.01177294132620593),\n","  (2, 0.007827415043909889),\n","  (3, 0.010754740995290824),\n","  (4, 0.0176275932289678),\n","  (5, 0.0041364388443426246),\n","  (6, 0.005027364133893344),\n","  (7, 0.054664630266004835),\n","  (8, 0.007954690085274278),\n","  (9, 0.06179203258241059),\n","  (10, 0.02411862033855161),\n","  (11, 0.0038818887616138475),\n","  (12, 0.4101438207967418),\n","  (13, 0.021318569428535064),\n","  (14, 0.021318569428535064),\n","  (15, 0.009354715540282551),\n","  (16, 0.23832251495481735),\n","  (17, 0.019791268932162404),\n","  (18, 0.011263841160748377),\n","  (19, 0.040791650757286496)],\n"," [(0, 0.060045498084291174),\n","  (1, 0.0177801724137931),\n","  (2, 0.04005028735632183),\n","  (3, 0.04076867816091953),\n","  (4, 0.016223659003831416),\n","  (5, 0.02053400383141762),\n","  (6, 0.021012931034482756),\n","  (7, 0.008800287356321837),\n","  (8, 0.28681752873563215),\n","  (9, 0.04220545977011494),\n","  (10, 0.030830938697318),\n","  (11, 0.004609674329501915),\n","  (12, 0.12745450191570878),\n","  (13, 0.01909722222222222),\n","  (14, 0.13799090038314174),\n","  (15, 0.010715996168582374),\n","  (16, 0.05058668582375477),\n","  (17, 0.009638409961685822),\n","  (18, 0.024964080459770107),\n","  (19, 0.02987308429118773)],\n"," [(0, 0.1754638515674984),\n","  (1, 0.0046385156749840055),\n","  (2, 0.014661974834719557),\n","  (3, 0.017541053529537214),\n","  (4, 0.007304329281296652),\n","  (5, 0.009223715077841758),\n","  (6, 0.004425250586478993),\n","  (7, 0.005171678396246535),\n","  (8, 0.04857112390701642),\n","  (9, 0.007517594369801663),\n","  (10, 0.01636809554275965),\n","  (11, 0.004745148219236511),\n","  (12, 0.3601514182128386),\n","  (13, 0.003145660055448923),\n","  (14, 0.23688419705694178),\n","  (15, 0.00399872040946897),\n","  (16, 0.007837492002559182),\n","  (17, 0.02660481979100021),\n","  (18, 0.03822776711452335),\n","  (19, 0.007517594369801663)],\n"," [(0, 0.03806772100567721),\n","  (1, 0.06523722627737226),\n","  (2, 0.13772303325223034),\n","  (3, 0.011709245742092457),\n","  (4, 0.03391119221411192),\n","  (5, 0.015460259529602595),\n","  (6, 0.021745742092457422),\n","  (7, 0.012317518248175183),\n","  (8, 0.007045823195458232),\n","  (9, 0.016676804541768047),\n","  (10, 0.02752433090024331),\n","  (11, 0.017082319545823194),\n","  (12, 0.05205798864557989),\n","  (13, 0.04262976480129765),\n","  (14, 0.3879257907542579),\n","  (15, 0.012317518248175183),\n","  (16, 0.009985806974858069),\n","  (17, 0.04070356853203568),\n","  (18, 0.009073398215733983),\n","  (19, 0.040804947283049474)],\n"," [(0, 0.006754289886820008),\n","  (1, 0.009553364975051723),\n","  (2, 0.008458074723134966),\n","  (3, 0.007727881221857126),\n","  (4, 0.029146890592673728),\n","  (5, 0.012960934647681638),\n","  (6, 0.006389193136181089),\n","  (7, 0.0065108920530607284),\n","  (8, 0.7179627601314349),\n","  (9, 0.013204332481440918),\n","  (10, 0.08123402701715957),\n","  (11, 0.010892053060727762),\n","  (12, 0.007606182304977486),\n","  (13, 0.004198612632347573),\n","  (14, 0.02841669709139589),\n","  (15, 0.003833515881708653),\n","  (16, 0.008214676889375687),\n","  (17, 0.004685408299866132),\n","  (18, 0.024644030668127058),\n","  (19, 0.007606182304977486)],\n"," [(0, 0.015187493816167016),\n","  (1, 0.011625606015632731),\n","  (2, 0.015187493816167016),\n","  (3, 0.010042544770950828),\n","  (4, 0.004303947758978926),\n","  (5, 0.08029088750371031),\n","  (6, 0.004007123775601069),\n","  (7, 0.010636192737706543),\n","  (8, 0.6258533689522114),\n","  (9, 0.010537251409913923),\n","  (10, 0.05951320866726033),\n","  (11, 0.006579598298209164),\n","  (12, 0.0073711289205501155),\n","  (13, 0.02587315721776987),\n","  (14, 0.006678539626001782),\n","  (15, 0.0038092411200158316),\n","  (16, 0.024191154645295348),\n","  (17, 0.013802315227070351),\n","  (18, 0.04862966261007224),\n","  (19, 0.01588008311071535)],\n"," [(0, 0.004491725768321513),\n","  (1, 0.008747044917257684),\n","  (2, 0.02794326241134752),\n","  (3, 0.024066193853427897),\n","  (4, 0.004869976359338062),\n","  (5, 0.24534278959810873),\n","  (6, 0.006666666666666667),\n","  (7, 0.008085106382978723),\n","  (8, 0.478628841607565),\n","  (9, 0.02614657210401891),\n","  (10, 0.05612293144208038),\n","  (11, 0.02160756501182033),\n","  (12, 0.0058156028368794325),\n","  (13, 0.014799054373522458),\n","  (14, 0.030307328605200946),\n","  (15, 0.00524822695035461),\n","  (16, 0.006288416075650118),\n","  (17, 0.011583924349881796),\n","  (18, 0.006761229314420804),\n","  (19, 0.006477541371158392)],\n"," [(0, 0.0077477477477477475),\n","  (1, 0.035975975975975974),\n","  (2, 0.011351351351351352),\n","  (3, 0.04858858858858859),\n","  (4, 0.010630630630630631),\n","  (5, 0.011231231231231232),\n","  (6, 0.006066066066066066),\n","  (7, 0.014474474474474475),\n","  (8, 0.6213213213213213),\n","  (9, 0.0046246246246246245),\n","  (10, 0.08006006006006006),\n","  (11, 0.009309309309309309),\n","  (12, 0.005585585585585586),\n","  (13, 0.016156156156156155),\n","  (14, 0.03465465465465466),\n","  (15, 0.006066066066066066),\n","  (16, 0.03657657657657658),\n","  (17, 0.007147147147147147),\n","  (18, 0.012192192192192192),\n","  (19, 0.02024024024024024)],\n"," [(0, 0.009117082533589251),\n","  (1, 0.009010449989336745),\n","  (2, 0.017754318618042227),\n","  (3, 0.06264661974834719),\n","  (4, 0.007837492002559182),\n","  (5, 0.6948709746214544),\n","  (6, 0.010503305608871827),\n","  (7, 0.014128812113457028),\n","  (8, 0.010503305608871827),\n","  (9, 0.0065579014715291106),\n","  (10, 0.029910428662827896),\n","  (11, 0.03566858605246321),\n","  (12, 0.011569631051396887),\n","  (13, 0.005918106206014076),\n","  (14, 0.006771166560034123),\n","  (15, 0.005811473661761569),\n","  (16, 0.02596502452548518),\n","  (17, 0.017221155896779698),\n","  (18, 0.011782896139901898),\n","  (19, 0.006451268927276605)],\n"," [(0, 0.035470580661774165),\n","  (1, 0.025556843053946177),\n","  (2, 0.0070168662289172125),\n","  (3, 0.18700914123857343),\n","  (4, 0.006501866872666407),\n","  (5, 0.1312604609244238),\n","  (6, 0.006888116389854511),\n","  (7, 0.011651860435174454),\n","  (8, 0.04551306810866486),\n","  (9, 0.09018926226342215),\n","  (10, 0.027616840478949394),\n","  (11, 0.21417535728080334),\n","  (12, 0.015256855928930085),\n","  (13, 0.007274365907042614),\n","  (14, 0.011523110596111753),\n","  (15, 0.006888116389854511),\n","  (16, 0.11516673104158617),\n","  (17, 0.012166859791425257),\n","  (18, 0.01255310930861336),\n","  (19, 0.03032058709926612)],\n"," [(0, 0.0048999309868875095),\n","  (1, 0.02491373360938579),\n","  (2, 0.006280193236714977),\n","  (3, 0.09337474120082817),\n","  (4, 0.023947550034506562),\n","  (5, 0.2457556935817806),\n","  (6, 0.007936507936507938),\n","  (7, 0.006556245686680471),\n","  (8, 0.24506556245686686),\n","  (9, 0.00531400966183575),\n","  (10, 0.03595583160800553),\n","  (11, 0.14430641821946172),\n","  (12, 0.006556245686680471),\n","  (13, 0.046445824706694276),\n","  (14, 0.011387163561076606),\n","  (15, 0.0048999309868875095),\n","  (16, 0.019944789510006906),\n","  (17, 0.00572808833678399),\n","  (18, 0.05320910973084887),\n","  (19, 0.007522429261559698)],\n"," [(0, 0.015181649429859454),\n","  (1, 0.039313179527976666),\n","  (2, 0.025391142932909044),\n","  (3, 0.009612834791832406),\n","  (4, 0.013457968708565367),\n","  (5, 0.010673561389551844),\n","  (6, 0.011601697162556351),\n","  (7, 0.04183240519756033),\n","  (8, 0.4673163617077698),\n","  (9, 0.029634049323786795),\n","  (10, 0.03109254839565102),\n","  (11, 0.008419517369398038),\n","  (12, 0.1152877220896314),\n","  (13, 0.04382126756828428),\n","  (14, 0.04315831344470963),\n","  (15, 0.008552108194112968),\n","  (16, 0.02658446035534341),\n","  (17, 0.010806152214266772),\n","  (18, 0.03228586581808539),\n","  (19, 0.015977194378149033)],\n"," [(0, 0.019307659932659933),\n","  (1, 0.07244318181818182),\n","  (2, 0.06160563973063973),\n","  (3, 0.12115951178451179),\n","  (4, 0.007733585858585859),\n","  (5, 0.12663089225589225),\n","  (6, 0.01520412457912458),\n","  (7, 0.008890993265993265),\n","  (8, 0.16924452861952863),\n","  (9, 0.009732744107744109),\n","  (10, 0.16829755892255893),\n","  (11, 0.01320496632996633),\n","  (12, 0.027409511784511783),\n","  (13, 0.12115951178451179),\n","  (14, 0.004471801346801346),\n","  (15, 0.01015361952861953),\n","  (16, 0.00794402356902357),\n","  (17, 0.013941498316498317),\n","  (18, 0.011837121212121212),\n","  (19, 0.009627525252525252)],\n"," [(0, 0.00710666172290199),\n","  (1, 0.26010381905821284),\n","  (2, 0.01007292052898282),\n","  (3, 0.040724261525151406),\n","  (4, 0.012050426399703374),\n","  (5, 0.06939809665059943),\n","  (6, 0.007971820541342232),\n","  (7, 0.008836979359782475),\n","  (8, 0.0981955258929675),\n","  (9, 0.004387591150661229),\n","  (10, 0.12563341984921517),\n","  (11, 0.03849956742059078),\n","  (12, 0.007724632307502163),\n","  (13, 0.006735879372141886),\n","  (14, 0.006365097021381782),\n","  (15, 0.005129155852181436),\n","  (16, 0.01007292052898282),\n","  (17, 0.006859473489061921),\n","  (18, 0.2673958719564949),\n","  (19, 0.006735879372141886)],\n"," [(0, 0.020304870787185902),\n","  (1, 0.007085863999047279),\n","  (2, 0.02983208288674527),\n","  (3, 0.11140883648922235),\n","  (4, 0.01720852685482911),\n","  (5, 0.00875312611647017),\n","  (6, 0.10211980469215197),\n","  (7, 0.053054662379421226),\n","  (8, 0.2344289627247827),\n","  (9, 0.25002977253781117),\n","  (10, 0.011492199595093488),\n","  (11, 0.021019411694652854),\n","  (12, 0.02530665713945457),\n","  (13, 0.006609503394069311),\n","  (14, 0.014469453376205789),\n","  (15, 0.011730379897582471),\n","  (16, 0.024830296534476604),\n","  (17, 0.029593902584256284),\n","  (18, 0.006847683696558296),\n","  (19, 0.013874002619983328)],\n"," [(0, 0.009956355701036552),\n","  (1, 0.004318967084924532),\n","  (2, 0.09233496999454446),\n","  (3, 0.022776868521549372),\n","  (4, 0.17607746863066012),\n","  (5, 0.11152027641389343),\n","  (6, 0.0049554464448081465),\n","  (7, 0.08387888707037643),\n","  (8, 0.12415893798872522),\n","  (9, 0.02541371158392435),\n","  (10, 0.22935988361520276),\n","  (11, 0.02368612474995454),\n","  (12, 0.03868885251863975),\n","  (13, 0.020321876704855427),\n","  (14, 0.0049554464448081465),\n","  (15, 0.004500818330605565),\n","  (16, 0.008592471358428805),\n","  (17, 0.004137115839243499),\n","  (18, 0.0061374795417348605),\n","  (19, 0.004228041462084015)],\n"," [(0, 0.02490234375),\n","  (1, 0.052897135416666664),\n","  (2, 0.1012912326388889),\n","  (3, 0.14610460069444445),\n","  (4, 0.012641059027777778),\n","  (5, 0.21761067708333334),\n","  (6, 0.012749565972222222),\n","  (7, 0.01220703125),\n","  (8, 0.010796440972222222),\n","  (9, 0.0901150173611111),\n","  (10, 0.09879557291666667),\n","  (11, 0.009928385416666666),\n","  (12, 0.024251302083333332),\n","  (13, 0.1325412326388889),\n","  (14, 0.005696614583333333),\n","  (15, 0.007215711805555556),\n","  (16, 0.008409288194444444),\n","  (17, 0.010904947916666666),\n","  (18, 0.009711371527777778),\n","  (19, 0.01123046875)],\n"," [(0, 0.046876674166934536),\n","  (1, 0.260634308368156),\n","  (2, 0.024911603985856637),\n","  (3, 0.04376942033644059),\n","  (4, 0.011732561877209901),\n","  (5, 0.1017357762777242),\n","  (6, 0.00573234758384228),\n","  (7, 0.024911603985856637),\n","  (8, 0.015161255759134254),\n","  (9, 0.014304082288653167),\n","  (10, 0.1406300225008036),\n","  (11, 0.003910853959069966),\n","  (12, 0.02426872388299582),\n","  (13, 0.03648344583735134),\n","  (14, 0.008518161362905818),\n","  (15, 0.004339440694310511),\n","  (16, 0.020304296582020785),\n","  (17, 0.1967748848173149),\n","  (18, 0.009268188149576771),\n","  (19, 0.00573234758384228)],\n"," [(0, 0.005400207131232431),\n","  (1, 0.02123095132416038),\n","  (2, 0.015312916111850865),\n","  (3, 0.019603491640775263),\n","  (4, 0.009246930019233615),\n","  (5, 0.01901168811954431),\n","  (6, 0.005696108891847906),\n","  (7, 0.004808403610001479),\n","  (8, 0.4647876904867584),\n","  (9, 0.01294570202692706),\n","  (10, 0.18575233022636484),\n","  (11, 0.009838733540464566),\n","  (12, 0.005252256250924693),\n","  (13, 0.006879715934309809),\n","  (14, 0.011466193223849681),\n","  (15, 0.004512501849386004),\n","  (16, 0.006731765054002072),\n","  (17, 0.011466193223849681),\n","  (18, 0.17288060363959165),\n","  (19, 0.0071756176949252844)],\n"," [(0, 0.013849092645654247),\n","  (1, 0.021171601400827758),\n","  (2, 0.004828610845802822),\n","  (3, 0.11127029608404963),\n","  (4, 0.006632707205773107),\n","  (5, 0.16093600764087868),\n","  (6, 0.003767377692879125),\n","  (7, 0.008755173511620501),\n","  (8, 0.37753369415260524),\n","  (9, 0.009391913403374719),\n","  (10, 0.007906186989281543),\n","  (11, 0.0051469807916799315),\n","  (12, 0.010240899925713677),\n","  (13, 0.021808341292581978),\n","  (14, 0.037196222009975584),\n","  (15, 0.006208213944603628),\n","  (16, 0.10235593759949058),\n","  (17, 0.0722169160564576),\n","  (18, 0.006314337259895998),\n","  (19, 0.01246948954685344)],\n"," [(0, 0.015480043149946065),\n","  (1, 0.036947141316073365),\n","  (2, 0.019363538295577135),\n","  (3, 0.008036677454153185),\n","  (4, 0.6153721682847898),\n","  (5, 0.005016181229773464),\n","  (6, 0.010409924487594393),\n","  (7, 0.010409924487594393),\n","  (8, 0.006310679611650486),\n","  (9, 0.005016181229773464),\n","  (10, 0.10005393743257822),\n","  (11, 0.014617044228694715),\n","  (12, 0.02637540453074434),\n","  (13, 0.026699029126213594),\n","  (14, 0.039859762675296666),\n","  (15, 0.017206040992448764),\n","  (16, 0.007605177993527509),\n","  (17, 0.00900755124056095),\n","  (18, 0.007173678532901835),\n","  (19, 0.01903991370010788)],\n"," [(0, 0.008926591261920603),\n","  (1, 0.024894655134176092),\n","  (2, 0.027999556442670215),\n","  (3, 0.021457085828343315),\n","  (4, 0.022122421823020627),\n","  (5, 0.014803725881570194),\n","  (6, 0.017354180527833222),\n","  (7, 0.15153027278775782),\n","  (8, 0.009591927256597915),\n","  (9, 0.007817697937458417),\n","  (10, 0.22305389221556887),\n","  (11, 0.009037480594366822),\n","  (12, 0.13378797959636282),\n","  (13, 0.06658904413395432),\n","  (14, 0.17381902860944778),\n","  (15, 0.011587935240629852),\n","  (16, 0.027999556442670215),\n","  (17, 0.029773785761809713),\n","  (18, 0.01003548458638279),\n","  (19, 0.007817697937458417)],\n"," [(0, 0.012870204730669847),\n","  (1, 0.11294971178692109),\n","  (2, 0.014659113496322798),\n","  (3, 0.005317034386801828),\n","  (4, 0.6250745378652356),\n","  (5, 0.00919300337904989),\n","  (6, 0.006211488769628305),\n","  (7, 0.01644802226197575),\n","  (8, 0.0043231961836613),\n","  (9, 0.016547406082289804),\n","  (10, 0.0302623732856291),\n","  (11, 0.013764659113496322),\n","  (12, 0.037418008348240904),\n","  (13, 0.010882528324388789),\n","  (14, 0.005515802027429934),\n","  (15, 0.005118266746173723),\n","  (16, 0.028374080699662095),\n","  (17, 0.016944941363546014),\n","  (18, 0.012174517988471478),\n","  (19, 0.015951103160405485)],\n"," [(0, 0.011926165647848496),\n","  (1, 0.1064964641016421),\n","  (2, 0.0132446362219825),\n","  (3, 0.0035358983579048303),\n","  (4, 0.4724319789044708),\n","  (5, 0.02978544887930001),\n","  (6, 0.0054536737384633825),\n","  (7, 0.031942946182428385),\n","  (8, 0.011446721802708858),\n","  (9, 0.005813256622318111),\n","  (10, 0.03134364137600384),\n","  (11, 0.06382596188421431),\n","  (12, 0.1230372767589596),\n","  (13, 0.007011866235167206),\n","  (14, 0.00833033680930121),\n","  (15, 0.007611171041591753),\n","  (16, 0.00845019777058612),\n","  (17, 0.011926165647848496),\n","  (18, 0.010248112189859764),\n","  (19, 0.036138079827400214)],\n"," [(0, 0.007104454685099846),\n","  (1, 0.009792626728110598),\n","  (2, 0.0311699948796723),\n","  (3, 0.02297747055811572),\n","  (4, 0.4818868407578085),\n","  (5, 0.010944700460829493),\n","  (6, 0.010944700460829493),\n","  (7, 0.012608806963645674),\n","  (8, 0.012224782386072709),\n","  (9, 0.005952380952380952),\n","  (10, 0.1292242703533026),\n","  (11, 0.009280593958013313),\n","  (12, 0.007104454685099846),\n","  (13, 0.028353814644137224),\n","  (14, 0.024385560675883256),\n","  (15, 0.012608806963645674),\n","  (16, 0.01145673323092678),\n","  (17, 0.008128520225294419),\n","  (18, 0.013760880696364568),\n","  (19, 0.150089605734767)],\n"," [(0, 0.01233984457046839),\n","  (1, 0.09583070783448856),\n","  (2, 0.013180004200798153),\n","  (3, 0.007929006511237137),\n","  (4, 0.23498214660785552),\n","  (5, 0.004673387943709305),\n","  (6, 0.029668136946019746),\n","  (7, 0.24285864314219704),\n","  (8, 0.006248687250577611),\n","  (9, 0.054137786179374085),\n","  (10, 0.08133795421130016),\n","  (11, 0.009924385633270322),\n","  (12, 0.04038017223272423),\n","  (13, 0.055503045578659954),\n","  (14, 0.005513547574039069),\n","  (15, 0.010659525309808866),\n","  (16, 0.051092207519428695),\n","  (17, 0.008349086326402019),\n","  (18, 0.019796261289645036),\n","  (19, 0.01559546313799622)],\n"," [(0, 0.0896540252827678),\n","  (1, 0.00504546462630295),\n","  (2, 0.012807717897538257),\n","  (3, 0.005821689953426481),\n","  (4, 0.6034043025060989),\n","  (5, 0.0033821246396096697),\n","  (6, 0.007817697937458417),\n","  (7, 0.011366156575737415),\n","  (8, 0.0038256819693945443),\n","  (9, 0.022233311155466843),\n","  (10, 0.055611000221778666),\n","  (11, 0.003936571301840763),\n","  (12, 0.12569305832778888),\n","  (13, 0.005821689953426481),\n","  (14, 0.01325127522732313),\n","  (15, 0.00504546462630295),\n","  (16, 0.007374140607673542),\n","  (17, 0.005599911288534043),\n","  (18, 0.004934575293856731),\n","  (19, 0.007374140607673542)],\n"," [(0, 0.23686706984465788),\n","  (1, 0.004921143128186885),\n","  (2, 0.01950669986956006),\n","  (3, 0.00503972489031187),\n","  (4, 0.3701529704731412),\n","  (5, 0.004683979603936915),\n","  (6, 0.005632633700936796),\n","  (7, 0.042748725246057155),\n","  (8, 0.005632633700936796),\n","  (9, 0.015237756433060596),\n","  (10, 0.011087394758686114),\n","  (11, 0.0039724890311870036),\n","  (12, 0.22382307601090953),\n","  (13, 0.005632633700936796),\n","  (14, 0.009190086564686352),\n","  (15, 0.004446816079686944),\n","  (16, 0.011561721807186055),\n","  (17, 0.009190086564686352),\n","  (18, 0.004209652555436974),\n","  (19, 0.0064627060358116925)],\n"," [(0, 0.012104283054003724),\n","  (1, 0.05183116076970826),\n","  (2, 0.013842333954065797),\n","  (3, 0.01707014276846679),\n","  (4, 0.48075729360645564),\n","  (5, 0.016573556797020484),\n","  (6, 0.011980136561142147),\n","  (7, 0.028119180633147115),\n","  (8, 0.006641837368094351),\n","  (9, 0.006269397889509622),\n","  (10, 0.04078212290502793),\n","  (11, 0.02799503414028554),\n","  (12, 0.006021104903786468),\n","  (13, 0.008628181253879577),\n","  (14, 0.012973308504034761),\n","  (15, 0.009745499689633768),\n","  (16, 0.006021104903786468),\n","  (17, 0.020670391061452513),\n","  (18, 0.00887647423960273),\n","  (19, 0.21309745499689633)],\n"," [(0, 0.004346004817258352),\n","  (1, 0.03796209027123259),\n","  (2, 0.008744371138339093),\n","  (3, 0.008953817153628653),\n","  (4, 0.05702167766258247),\n","  (5, 0.004660173840192691),\n","  (6, 0.01209550738297204),\n","  (7, 0.1444653890459734),\n","  (8, 0.004241281809613572),\n","  (9, 0.1249869096240444),\n","  (10, 0.3053199287883548),\n","  (11, 0.029898418682584565),\n","  (12, 0.05670750863964813),\n","  (13, 0.09158027018535972),\n","  (14, 0.06361922714420358),\n","  (15, 0.00916326316891821),\n","  (16, 0.011257723321813802),\n","  (17, 0.012828568436485496),\n","  (18, 0.00696408000837784),\n","  (19, 0.0051837888784165885)],\n"," [(0, 0.009649983644095519),\n","  (1, 0.40088321884200195),\n","  (2, 0.05272053211209247),\n","  (3, 0.02088103805473776),\n","  (4, 0.0055064878421110025),\n","  (5, 0.009649983644095519),\n","  (6, 0.008341511285574092),\n","  (7, 0.05032166612146985),\n","  (8, 0.02186239232362883),\n","  (9, 0.011285574092247301),\n","  (10, 0.01510195180460146),\n","  (11, 0.01826409333769491),\n","  (12, 0.10418711154726856),\n","  (13, 0.006923999563842547),\n","  (14, 0.027205321120924653),\n","  (15, 0.1841129647802857),\n","  (16, 0.02099007741794788),\n","  (17, 0.009213826191255043),\n","  (18, 0.007360157016683023),\n","  (19, 0.015538109257441937)],\n"," [(0, 0.006046511627906977),\n","  (1, 0.24852713178294578),\n","  (2, 0.02237726098191215),\n","  (3, 0.013591731266149872),\n","  (4, 0.006666666666666668),\n","  (5, 0.012248062015503878),\n","  (6, 0.2053229974160207),\n","  (7, 0.013488372093023258),\n","  (8, 0.005736434108527133),\n","  (9, 0.014005167958656332),\n","  (10, 0.0192764857881137),\n","  (11, 0.08397932816537469),\n","  (12, 0.007906976744186047),\n","  (13, 0.08015503875968993),\n","  (14, 0.012661498708010338),\n","  (15, 0.12914728682170545),\n","  (16, 0.011834625322997418),\n","  (17, 0.0551421188630491),\n","  (18, 0.023410852713178297),\n","  (19, 0.028475452196382433)],\n"," [(0, 0.015205459130692777),\n","  (1, 0.013276961875092717),\n","  (2, 0.04887998813232459),\n","  (3, 0.01090342679127726),\n","  (4, 0.016837264500815907),\n","  (5, 0.02766651832072393),\n","  (6, 0.06742323097463286),\n","  (7, 0.011496810562231124),\n","  (8, 0.010458388963061862),\n","  (9, 0.018765761756415967),\n","  (10, 0.010458388963061862),\n","  (11, 0.35328586263165707),\n","  (12, 0.007194778222815607),\n","  (13, 0.08848835484349504),\n","  (14, 0.0058596647381694116),\n","  (15, 0.16117786678534346),\n","  (16, 0.006749740394600208),\n","  (17, 0.014315383474261981),\n","  (18, 0.09293873312564903),\n","  (19, 0.0186174158136775)],\n"," [(0, 0.0052588996763754045),\n","  (1, 0.28258719884933475),\n","  (2, 0.08436713412441568),\n","  (3, 0.01991190219345559),\n","  (4, 0.006877022653721683),\n","  (5, 0.010382955771305286),\n","  (6, 0.12679791441927366),\n","  (7, 0.01982200647249191),\n","  (8, 0.003910463861920173),\n","  (9, 0.005348795397339087),\n","  (10, 0.014518158935634663),\n","  (11, 0.12185364976627112),\n","  (12, 0.003460985257101762),\n","  (13, 0.19646709816612729),\n","  (14, 0.004180151024811219),\n","  (15, 0.053443006112909024),\n","  (16, 0.007416396979503775),\n","  (17, 0.004449838187702265),\n","  (18, 0.020361380798274),\n","  (19, 0.008585041352031643)],\n"," [(0, 0.03995889251601982),\n","  (1, 0.012029984282432592),\n","  (2, 0.12531737395719983),\n","  (3, 0.05180752025148107),\n","  (4, 0.010216418812719138),\n","  (5, 0.015294402127916815),\n","  (6, 0.05253294643936645),\n","  (7, 0.011425462459194775),\n","  (8, 0.04310240599685648),\n","  (9, 0.028593882239148827),\n","  (10, 0.023274090194656025),\n","  (11, 0.02520856002901704),\n","  (12, 0.009732801354128882),\n","  (13, 0.021097811630999875),\n","  (14, 0.26000483617458586),\n","  (15, 0.15239995163825412),\n","  (16, 0.01456897594003143),\n","  (17, 0.010216418812719138),\n","  (18, 0.015415306492564378),\n","  (19, 0.07780195865070727)],\n"," [(0, 0.004356417856822061),\n","  (1, 0.03076439414353723),\n","  (2, 0.012171023084523488),\n","  (3, 0.03597413096200485),\n","  (4, 0.01585376807688853),\n","  (5, 0.0732506961286266),\n","  (6, 0.06992724333063864),\n","  (7, 0.010913500404203719),\n","  (8, 0.041902452169226626),\n","  (9, 0.005613940537141831),\n","  (10, 0.07082547381658133),\n","  (11, 0.34900745531303334),\n","  (12, 0.014326776250785952),\n","  (13, 0.004985179196981946),\n","  (14, 0.0030988951765022906),\n","  (15, 0.023758196353184227),\n","  (16, 0.010913500404203719),\n","  (17, 0.0963352196173538),\n","  (18, 0.08043654001616815),\n","  (19, 0.045585197161591665)],\n"," [(0, 0.008376068376068378),\n","  (1, 0.044957264957264965),\n","  (2, 0.010997150997151),\n","  (3, 0.015213675213675216),\n","  (4, 0.009743589743589746),\n","  (5, 0.012250712250712252),\n","  (6, 0.07823361823361824),\n","  (7, 0.007692307692307694),\n","  (8, 0.005754985754985756),\n","  (9, 0.00700854700854701),\n","  (10, 0.006894586894586895),\n","  (11, 0.036410256410256414),\n","  (12, 0.008262108262108265),\n","  (13, 0.009515669515669517),\n","  (14, 0.017720797720797724),\n","  (15, 0.5412535612535614),\n","  (16, 0.006894586894586895),\n","  (17, 0.052478632478632485),\n","  (18, 0.007236467236467237),\n","  (19, 0.11310541310541312)],\n"," [(0, 0.012813396243870852),\n","  (1, 0.016051438615968177),\n","  (2, 0.03150152650568971),\n","  (3, 0.017994264039226573),\n","  (4, 0.016143954112313817),\n","  (5, 0.01864187251364604),\n","  (6, 0.0453788509575354),\n","  (7, 0.007169950966786938),\n","  (8, 0.0035618466093070597),\n","  (9, 0.0037468776019983357),\n","  (10, 0.0070774354704413005),\n","  (11, 0.3703857896197614),\n","  (12, 0.005782218521602369),\n","  (13, 0.0056897030252567316),\n","  (14, 0.00883522990100842),\n","  (15, 0.158155241002868),\n","  (16, 0.020214635951521884),\n","  (17, 0.02215746137478028),\n","  (18, 0.2081136090295125),\n","  (19, 0.020584697936904435)],\n"," [(0, 0.04506573023590852),\n","  (1, 0.003286511795425896),\n","  (2, 0.040023410768953716),\n","  (3, 0.02237529263461192),\n","  (4, 0.004547091662164595),\n","  (5, 0.017423014586709886),\n","  (6, 0.4269313884386818),\n","  (7, 0.05352962362686836),\n","  (8, 0.008418872681433459),\n","  (9, 0.012920943634071673),\n","  (10, 0.05046821537907437),\n","  (11, 0.007338375652800288),\n","  (12, 0.005357464433639474),\n","  (13, 0.13258598955519538),\n","  (14, 0.0771204754186926),\n","  (15, 0.014091482081757608),\n","  (16, 0.009589411129119395),\n","  (17, 0.04551593733117234),\n","  (18, 0.010579866738699801),\n","  (19, 0.012830902215018909)],\n"," [(0, 0.011147264401060498),\n","  (1, 0.017895878524945775),\n","  (2, 0.2508435767654857),\n","  (3, 0.009219088937093277),\n","  (4, 0.14587852494577008),\n","  (5, 0.007290913473126055),\n","  (6, 0.06405157869366114),\n","  (7, 0.03175463967221018),\n","  (8, 0.013075439865027719),\n","  (9, 0.005965292841648591),\n","  (10, 0.03633405639913233),\n","  (11, 0.006206314774644494),\n","  (12, 0.20842371655820682),\n","  (13, 0.09514340805013258),\n","  (14, 0.00861653410460352),\n","  (15, 0.013677994697517477),\n","  (16, 0.010062665702578936),\n","  (17, 0.022475295251867925),\n","  (18, 0.007772957339117861),\n","  (19, 0.0341648590021692)],\n"," [(0, 0.04120125416421713),\n","  (1, 0.009455222418185383),\n","  (2, 0.19826572604350384),\n","  (3, 0.016901822457378018),\n","  (4, 0.007103664511071919),\n","  (5, 0.01121889084852048),\n","  (6, 0.18543013913384288),\n","  (7, 0.04188712522045856),\n","  (8, 0.012002743484224967),\n","  (9, 0.02219282774838331),\n","  (10, 0.00906329610033314),\n","  (11, 0.007495590828924163),\n","  (12, 0.03767391730354694),\n","  (13, 0.1274250440917108),\n","  (14, 0.15809327846364887),\n","  (15, 0.010826964530668236),\n","  (16, 0.025230256711738196),\n","  (17, 0.049823633156966494),\n","  (18, 0.006319811875367432),\n","  (19, 0.022388790907309428)],\n"," [(0, 0.0050302706552706545),\n","  (1, 0.02176816239316239),\n","  (2, 0.11516203703703701),\n","  (3, 0.011084401709401708),\n","  (4, 0.3451299857549857),\n","  (5, 0.06201032763532762),\n","  (6, 0.017494658119658116),\n","  (7, 0.03699252136752136),\n","  (8, 0.006098646723646722),\n","  (9, 0.007256054131054129),\n","  (10, 0.027288105413105408),\n","  (11, 0.0877403846153846),\n","  (12, 0.005475427350427349),\n","  (13, 0.10020477207977205),\n","  (14, 0.004941239316239315),\n","  (15, 0.01624821937321937),\n","  (16, 0.017316595441595438),\n","  (17, 0.015624999999999997),\n","  (18, 0.02265847578347578),\n","  (19, 0.07447471509971508)],\n"," [(0, 0.026615428408767437),\n","  (1, 0.014280292247841352),\n","  (2, 0.03496536673308663),\n","  (3, 0.0777588006452225),\n","  (4, 0.03895056456969352),\n","  (5, 0.005455925609640384),\n","  (6, 0.11713635069740962),\n","  (7, 0.033162539140335894),\n","  (8, 0.004222411993547775),\n","  (9, 0.22511623493690103),\n","  (10, 0.16040421292342727),\n","  (11, 0.005930353923522156),\n","  (12, 0.015324034538381251),\n","  (13, 0.01570357718948667),\n","  (14, 0.06580320713540184),\n","  (15, 0.10166998766486383),\n","  (16, 0.00963089477179998),\n","  (17, 0.013900749596735934),\n","  (18, 0.02671031407154379),\n","  (19, 0.007258753202391118)],\n"," [(0, 0.011740209108630158),\n","  (1, 0.10769980506822609),\n","  (2, 0.02804359383306751),\n","  (3, 0.057194754563175604),\n","  (4, 0.03708133971291865),\n","  (5, 0.005006202374623426),\n","  (6, 0.007664362927520821),\n","  (7, 0.018474215842636893),\n","  (8, 0.011740209108630158),\n","  (9, 0.11275031011873114),\n","  (10, 0.030701754385964904),\n","  (11, 0.01599326599326599),\n","  (12, 0.012360446570972882),\n","  (13, 0.009879496721601983),\n","  (14, 0.19169767853978376),\n","  (15, 0.2287347155768208),\n","  (16, 0.031056175793017893),\n","  (17, 0.012626262626262623),\n","  (18, 0.015461633882686511),\n","  (19, 0.05409356725146197)],\n"," [(0, 0.017188103514870608),\n","  (1, 0.012553109308613364),\n","  (2, 0.008433114458606927),\n","  (3, 0.008561864297669627),\n","  (4, 0.019376850778936527),\n","  (5, 0.011523110596111755),\n","  (6, 0.07422428221964722),\n","  (7, 0.03456933178833527),\n","  (8, 0.007531865585168019),\n","  (9, 0.005343118321102099),\n","  (10, 0.019376850778936527),\n","  (11, 0.015128106089867388),\n","  (12, 0.012939358825801467),\n","  (13, 0.03186558516801854),\n","  (14, 0.038045577443028195),\n","  (15, 0.011651860435174455),\n","  (16, 0.005085618642976696),\n","  (17, 0.10769924037594952),\n","  (18, 0.47991502510621864),\n","  (19, 0.07898802626496716)],\n"," [(0, 0.004993167244822874),\n","  (1, 0.004257332071901609),\n","  (2, 0.022442972774098603),\n","  (3, 0.022758330705350573),\n","  (4, 0.030326921055397877),\n","  (5, 0.00804162724692526),\n","  (6, 0.01865867759907495),\n","  (7, 0.14753495217071377),\n","  (8, 0.007200672763586671),\n","  (9, 0.004677809313570903),\n","  (10, 0.029906443813728582),\n","  (11, 0.005413644486492169),\n","  (12, 0.015399978976137917),\n","  (13, 0.0850940817828235),\n","  (14, 0.010144013455271734),\n","  (15, 0.11768106801219384),\n","  (16, 0.0068853148323347),\n","  (17, 0.004362451382318932),\n","  (18, 0.4448123620309051),\n","  (19, 0.009408178282350467)],\n"," [(0, 0.014552521805081532),\n","  (1, 0.024981039059537354),\n","  (2, 0.009433067880166856),\n","  (3, 0.07712362533181646),\n","  (4, 0.013794084186575654),\n","  (5, 0.017396662874478573),\n","  (6, 0.006209708001516876),\n","  (7, 0.015216154721274175),\n","  (8, 0.0775028441410694),\n","  (9, 0.005735684489950702),\n","  (10, 0.030195297686765262),\n","  (11, 0.004218809252938945),\n","  (12, 0.16045695866514978),\n","  (13, 0.04233029958285931),\n","  (14, 0.09703261281759576),\n","  (15, 0.006683731513083049),\n","  (16, 0.020999241562381494),\n","  (17, 0.02033560864618885),\n","  (18, 0.3458949563898369),\n","  (19, 0.00990709139173303)],\n"," [(0, 0.006594724220623499),\n","  (1, 0.011257660538236076),\n","  (2, 0.008726352251532106),\n","  (3, 0.05642152944311216),\n","  (4, 0.011524114042099652),\n","  (5, 0.0053956834532374095),\n","  (6, 0.0879962696509459),\n","  (7, 0.006727950972555287),\n","  (8, 0.028843591793232076),\n","  (9, 0.006461497468691711),\n","  (10, 0.05335731414868104),\n","  (11, 0.005262456701305622),\n","  (12, 0.03164135358379962),\n","  (13, 0.09425792699173992),\n","  (14, 0.006461497468691711),\n","  (15, 0.019384492406075135),\n","  (16, 0.025379696243005588),\n","  (17, 0.00592859046096456),\n","  (18, 0.5147215560884625),\n","  (19, 0.013655742073008256)],\n"," [(0, 0.03619281045751634),\n","  (1, 0.008251633986928105),\n","  (2, 0.017401960784313726),\n","  (3, 0.006617647058823529),\n","  (4, 0.013643790849673203),\n","  (5, 0.018545751633986928),\n","  (6, 0.005964052287581699),\n","  (7, 0.008741830065359477),\n","  (8, 0.008905228758169934),\n","  (9, 0.03799019607843137),\n","  (10, 0.01919934640522876),\n","  (11, 0.04534313725490196),\n","  (12, 0.23031045751633986),\n","  (13, 0.020179738562091504),\n","  (14, 0.012990196078431373),\n","  (15, 0.009558823529411765),\n","  (16, 0.020016339869281044),\n","  (17, 0.061356209150326795),\n","  (18, 0.37165032679738563),\n","  (19, 0.04714052287581699)],\n"," [(0, 0.009203624225083452),\n","  (1, 0.004339532665712923),\n","  (2, 0.0569861707200763),\n","  (3, 0.014639961850262279),\n","  (4, 0.020648545541249404),\n","  (5, 0.005579399141630901),\n","  (6, 0.03752980448259418),\n","  (7, 0.010061993323795898),\n","  (8, 0.009012875536480686),\n","  (9, 0.006342393896041965),\n","  (10, 0.0642346208869814),\n","  (11, 0.015784453981878874),\n","  (12, 0.009012875536480686),\n","  (13, 0.07310443490701002),\n","  (14, 0.023032904148783978),\n","  (15, 0.030090605627086313),\n","  (16, 0.0077730090605627085),\n","  (17, 0.015784453981878874),\n","  (18, 0.49256080114449213),\n","  (19, 0.09427753934191703)],\n"," [(0, 0.011965048158077649),\n","  (1, 0.016036143381987885),\n","  (2, 0.006503822857710257),\n","  (3, 0.008191837950551088),\n","  (4, 0.01831992850759607),\n","  (5, 0.0063052328467878066),\n","  (6, 0.01593684837652666),\n","  (7, 0.0036242676993347234),\n","  (8, 0.01047562307615927),\n","  (9, 0.008390427961473539),\n","  (10, 0.01236222817992255),\n","  (11, 0.006801707874093933),\n","  (12, 0.0706483963856618),\n","  (13, 0.14740343560718897),\n","  (14, 0.08772713732499256),\n","  (15, 0.006404527852249032),\n","  (16, 0.13171482474431537),\n","  (17, 0.004915102770330653),\n","  (18, 0.40646410485552575),\n","  (19, 0.019809353589514446)],\n"," [(0, 0.01300400123114805),\n","  (1, 0.013311788242536168),\n","  (2, 0.0319329024315174),\n","  (3, 0.013773468759618348),\n","  (4, 0.01915974145891044),\n","  (5, 0.056863650353955075),\n","  (6, 0.01238842720837181),\n","  (7, 0.014235149276700526),\n","  (8, 0.27554632194521395),\n","  (9, 0.019005847953216377),\n","  (10, 0.01885195444752232),\n","  (11, 0.008387196060326256),\n","  (12, 0.0074638350261618976),\n","  (13, 0.026854416743613427),\n","  (14, 0.009618344105878734),\n","  (15, 0.010849492151431212),\n","  (16, 0.074715297014466),\n","  (17, 0.007309941520467838),\n","  (18, 0.25646352723915056),\n","  (19, 0.11026469682979381)],\n"," [(0, 0.09575477291592553),\n","  (1, 0.005988378987311751),\n","  (2, 0.050219376259931225),\n","  (3, 0.010020158899561248),\n","  (4, 0.03634531009130796),\n","  (5, 0.003735325506937033),\n","  (6, 0.22785485592315902),\n","  (7, 0.010968812996561128),\n","  (8, 0.0062255425115617215),\n","  (9, 0.047491995731056565),\n","  (10, 0.005988378987311751),\n","  (11, 0.04013992647930748),\n","  (12, 0.009071504802561366),\n","  (13, 0.028993240839558877),\n","  (14, 0.007648523657061544),\n","  (15, 0.01879520929681015),\n","  (16, 0.010968812996561128),\n","  (17, 0.012866121190560891),\n","  (18, 0.09907506225542512),\n","  (19, 0.27184868967152853)],\n"," [(0, 0.02396228249246622),\n","  (1, 0.009089141635073393),\n","  (2, 0.03339165937591135),\n","  (3, 0.012005443763973948),\n","  (4, 0.0073393603577330616),\n","  (5, 0.019490619228152036),\n","  (6, 0.0074365704286964126),\n","  (7, 0.010741712841450373),\n","  (8, 0.11971420239136775),\n","  (9, 0.005881209293282784),\n","  (10, 0.0336832895888014),\n","  (11, 0.014532905609021094),\n","  (12, 0.006464469719062895),\n","  (13, 0.009283561777000097),\n","  (14, 0.016282686886361426),\n","  (15, 0.0037425877320890442),\n","  (16, 0.008505881209293282),\n","  (17, 0.005492369009429377),\n","  (18, 0.5825313502478857),\n","  (19, 0.07042869641294838)],\n"," [(0, 0.009225589225589225),\n","  (1, 0.010976430976430977),\n","  (2, 0.02511784511784512),\n","  (3, 0.042222222222222223),\n","  (4, 0.012457912457912458),\n","  (5, 0.017845117845117844),\n","  (6, 0.00962962962962963),\n","  (7, 0.00531986531986532),\n","  (8, 0.38781144781144783),\n","  (9, 0.012323232323232323),\n","  (10, 0.08316498316498316),\n","  (11, 0.019326599326599327),\n","  (12, 0.006666666666666667),\n","  (13, 0.00734006734006734),\n","  (14, 0.05542087542087542),\n","  (15, 0.008282828282828282),\n","  (16, 0.007609427609427609),\n","  (17, 0.005589225589225589),\n","  (18, 0.25542087542087544),\n","  (19, 0.018249158249158248)],\n"," [(0, 0.0072693307342430145),\n","  (1, 0.04511858349577648),\n","  (2, 0.01392949967511371),\n","  (3, 0.0034519168291098114),\n","  (4, 0.007512995451591942),\n","  (5, 0.007025666016894087),\n","  (6, 0.01303606237816764),\n","  (7, 0.02887426900584795),\n","  (8, 0.004832683560753736),\n","  (9, 0.05250974658869396),\n","  (10, 0.007350552306692657),\n","  (11, 0.09027777777777778),\n","  (12, 0.04203216374269006),\n","  (13, 0.05096653671215075),\n","  (14, 0.012954840805717998),\n","  (15, 0.49281189083820665),\n","  (16, 0.012548732943469785),\n","  (17, 0.023432423651721896),\n","  (18, 0.010680636777128005),\n","  (19, 0.07338369070825211)],\n"," [(0, 0.012543164736949015),\n","  (1, 0.08079423115986187),\n","  (2, 0.047074954296160876),\n","  (3, 0.11624009750152346),\n","  (4, 0.019652650822669104),\n","  (5, 0.02361365021328458),\n","  (6, 0.06820028437944343),\n","  (7, 0.007972780824700387),\n","  (8, 0.006043063172862076),\n","  (9, 0.006043063172862076),\n","  (10, 0.01101970343286614),\n","  (11, 0.3624314442413163),\n","  (12, 0.011222831606743856),\n","  (13, 0.032348161690026404),\n","  (14, 0.019855778996546822),\n","  (15, 0.08191143611618931),\n","  (16, 0.004316473694901483),\n","  (17, 0.07256754011781434),\n","  (18, 0.00908998578102783),\n","  (19, 0.00705870404225066)],\n"," [(0, 0.008690614136732331),\n","  (1, 0.014741856572679285),\n","  (2, 0.04100682374147033),\n","  (3, 0.2441740697824128),\n","  (4, 0.005858117677352904),\n","  (5, 0.22138534826831469),\n","  (6, 0.006888116389854514),\n","  (7, 0.010493111883610148),\n","  (8, 0.008819363975795032),\n","  (9, 0.012424359469550665),\n","  (10, 0.038303077121153606),\n","  (11, 0.29091026136217335),\n","  (12, 0.024140594824256473),\n","  (13, 0.007918115102356123),\n","  (14, 0.004570619286725893),\n","  (15, 0.00972061284923394),\n","  (16, 0.010235612205484745),\n","  (17, 0.01860435174456032),\n","  (18, 0.011136861078923654),\n","  (19, 0.009978112527359343)],\n"," [(0, 0.03527777777777777),\n","  (1, 0.024388888888888884),\n","  (2, 0.02616666666666666),\n","  (3, 0.0040555555555555544),\n","  (4, 0.010833333333333332),\n","  (5, 0.005944444444444442),\n","  (6, 0.17538888888888887),\n","  (7, 0.04827777777777777),\n","  (8, 0.005499999999999999),\n","  (9, 0.17861111111111108),\n","  (10, 0.006055555555555554),\n","  (11, 0.2849444444444444),\n","  (12, 0.020388888888888887),\n","  (13, 0.06738888888888887),\n","  (14, 0.005833333333333333),\n","  (15, 0.010388888888888887),\n","  (16, 0.01683333333333333),\n","  (17, 0.04449999999999999),\n","  (18, 0.00561111111111111),\n","  (19, 0.023611111111111104)],\n"," [(0, 0.025225546058879392),\n","  (1, 0.17384852801519468),\n","  (2, 0.011455365622032289),\n","  (3, 0.0763888888888889),\n","  (4, 0.004926400759734093),\n","  (5, 0.328525641025641),\n","  (6, 0.005638651471984805),\n","  (7, 0.009199905033238366),\n","  (8, 0.046711775878442545),\n","  (9, 0.010743114909781576),\n","  (10, 0.0068257359924026595),\n","  (11, 0.11853038936372269),\n","  (12, 0.11105175688509022),\n","  (13, 0.00397673314339981),\n","  (14, 0.004570275403608737),\n","  (15, 0.004451566951566952),\n","  (16, 0.009556030389363722),\n","  (17, 0.03495963912630579),\n","  (18, 0.008250237416904083),\n","  (19, 0.005163817663817663)],\n"," [(0, 0.03837404214559387),\n","  (1, 0.1303280651340996),\n","  (2, 0.00748323754789272),\n","  (3, 0.0073635057471264365),\n","  (4, 0.01191331417624521),\n","  (5, 0.006884578544061303),\n","  (6, 0.012871168582375478),\n","  (7, 0.029274425287356323),\n","  (8, 0.006046455938697318),\n","  (9, 0.07944204980842912),\n","  (10, 0.005088601532567049),\n","  (11, 0.40738745210727967),\n","  (12, 0.11524185823754789),\n","  (13, 0.004011015325670498),\n","  (14, 0.007004310344827586),\n","  (15, 0.004849137931034483),\n","  (16, 0.038613505747126436),\n","  (17, 0.019456417624521074),\n","  (18, 0.004130747126436782),\n","  (19, 0.0642361111111111)],\n"," [(0, 0.015313390313390313),\n","  (1, 0.012606837606837607),\n","  (2, 0.012891737891737892),\n","  (3, 0.00819088319088319),\n","  (4, 0.009188034188034188),\n","  (5, 0.0054843304843304845),\n","  (6, 0.005626780626780627),\n","  (7, 0.01787749287749288),\n","  (8, 0.005626780626780627),\n","  (9, 0.07115384615384615),\n","  (10, 0.008903133903133903),\n","  (11, 0.4420940170940171),\n","  (12, 0.1881054131054131),\n","  (13, 0.008048433048433048),\n","  (14, 0.016595441595441594),\n","  (15, 0.007478632478632479),\n","  (16, 0.048504273504273505),\n","  (17, 0.04024216524216524),\n","  (18, 0.013746438746438746),\n","  (19, 0.062321937321937325)],\n"," [(0, 0.024551320830812663),\n","  (1, 0.073653962492438),\n","  (2, 0.05328695301472071),\n","  (3, 0.01073805202661827),\n","  (4, 0.005696713046985279),\n","  (5, 0.30837870538415),\n","  (6, 0.00509175236942932),\n","  (7, 0.004184311353095382),\n","  (8, 0.08706392417826174),\n","  (9, 0.01678765880217786),\n","  (10, 0.008721516434765073),\n","  (11, 0.21218995765275256),\n","  (12, 0.006906634402097197),\n","  (13, 0.02142569066344021),\n","  (14, 0.004990925589836661),\n","  (15, 0.009729784230691672),\n","  (16, 0.006402500504133898),\n","  (17, 0.03302077031659609),\n","  (18, 0.048548094373865695),\n","  (19, 0.05863077233313168)],\n"," [(0, 0.06091288974606237),\n","  (1, 0.013768348869602487),\n","  (2, 0.037233472624022296),\n","  (3, 0.06916318439944286),\n","  (4, 0.007875281260045002),\n","  (5, 0.08223507982427945),\n","  (6, 0.00658952105432337),\n","  (7, 0.027483124397299906),\n","  (8, 0.019125683060109294),\n","  (9, 0.3257794921247188),\n","  (10, 0.025340190721097184),\n","  (11, 0.13237972784742316),\n","  (12, 0.058984249437479916),\n","  (13, 0.02298296367727419),\n","  (14, 0.005518054216222009),\n","  (15, 0.005839494267652417),\n","  (16, 0.04248366013071896),\n","  (17, 0.03198328511732563),\n","  (18, 0.015161255759134256),\n","  (19, 0.009161041465766637)],\n"," [(0, 0.05802469135802469),\n","  (1, 0.007675791733762748),\n","  (2, 0.04954374664519592),\n","  (3, 0.17933440687063876),\n","  (4, 0.022168545356951154),\n","  (5, 0.0067096081588835215),\n","  (6, 0.011111111111111112),\n","  (7, 0.051798174986580785),\n","  (8, 0.005421363392377885),\n","  (9, 0.004455179817498658),\n","  (10, 0.04804079441760601),\n","  (11, 0.017767042404723564),\n","  (12, 0.07638217928073),\n","  (13, 0.039667203435319376),\n","  (14, 0.2288244766505636),\n","  (15, 0.02431561996779388),\n","  (16, 0.011540526033279656),\n","  (17, 0.109447128287708),\n","  (18, 0.018518518518518517),\n","  (19, 0.029253891572732154)],\n"," [(0, 0.004409171075837741),\n","  (1, 0.01898264178965933),\n","  (2, 0.012577740647916082),\n","  (3, 0.16267520653485562),\n","  (4, 0.039868188990996),\n","  (5, 0.004780469692750393),\n","  (6, 0.07439896036387263),\n","  (7, 0.01434140907825118),\n","  (8, 0.005151768309663045),\n","  (9, 0.03420588508307806),\n","  (10, 0.013320337881741386),\n","  (11, 0.014434233732479342),\n","  (12, 0.07300659055045018),\n","  (13, 0.06270305393112409),\n","  (14, 0.015455304928989135),\n","  (15, 0.3770073331476839),\n","  (16, 0.02121043349113524),\n","  (17, 0.005058943655434882),\n","  (18, 0.008214981899192423),\n","  (19, 0.038197345214889065)],\n"," [(0, 0.015893719806763285),\n","  (1, 0.0542512077294686),\n","  (2, 0.05821256038647343),\n","  (3, 0.12932367149758453),\n","  (4, 0.0042995169082125605),\n","  (5, 0.04545893719806763),\n","  (6, 0.03367149758454106),\n","  (7, 0.005942028985507246),\n","  (8, 0.009420289855072464),\n","  (9, 0.05541062801932367),\n","  (10, 0.008937198067632851),\n","  (11, 0.21618357487922704),\n","  (12, 0.011739130434782608),\n","  (13, 0.052222222222222225),\n","  (14, 0.004975845410628019),\n","  (15, 0.2528985507246377),\n","  (16, 0.016666666666666666),\n","  (17, 0.00826086956521739),\n","  (18, 0.010966183574879227),\n","  (19, 0.005265700483091788)],\n"," [(0, 0.01693766937669377),\n","  (1, 0.004638315613925371),\n","  (2, 0.02246195538878466),\n","  (3, 0.01474880133416719),\n","  (4, 0.006306024598707527),\n","  (5, 0.004534083802376486),\n","  (6, 0.2385345007296227),\n","  (7, 0.01693766937669377),\n","  (8, 0.008182197206587452),\n","  (9, 0.012455701480091726),\n","  (10, 0.014644569522618304),\n","  (11, 0.014019178653324997),\n","  (12, 0.026318532416093394),\n","  (13, 0.01120491974150511),\n","  (14, 0.00682718365645195),\n","  (15, 0.41781321659370446),\n","  (16, 0.009328747133625184),\n","  (17, 0.049041067333750264),\n","  (18, 0.0076610381488430275),\n","  (19, 0.09740462789243279)],\n"," [(0, 0.008419370698131761),\n","  (1, 0.3490044247787611),\n","  (2, 0.01591691248770895),\n","  (3, 0.09285889872173059),\n","  (4, 0.00927974434611603),\n","  (5, 0.007436086529006884),\n","  (6, 0.011000491642084564),\n","  (7, 0.010508849557522126),\n","  (8, 0.011000491642084564),\n","  (9, 0.00878810226155359),\n","  (10, 0.12370943952802362),\n","  (11, 0.01628564405113078),\n","  (12, 0.0355825958702065),\n","  (13, 0.018866764995083583),\n","  (14, 0.028576696165191744),\n","  (15, 0.20532202556538842),\n","  (16, 0.013704523107177976),\n","  (17, 0.01210668633235005),\n","  (18, 0.014564896755162244),\n","  (19, 0.007067354965585055)],\n"," [(0, 0.04549151163972756),\n","  (1, 0.006455220087425028),\n","  (2, 0.03776557893666768),\n","  (3, 0.17449425637897734),\n","  (4, 0.019467317271525873),\n","  (5, 0.003710480837653756),\n","  (6, 0.01153807054996442),\n","  (7, 0.06999085086916743),\n","  (8, 0.0053369929856663615),\n","  (9, 0.020788858391786114),\n","  (10, 0.019365660262275083),\n","  (11, 0.010013215411202602),\n","  (12, 0.012757954660973875),\n","  (13, 0.07232896208193555),\n","  (14, 0.06633119853613907),\n","  (15, 0.39183694215716175),\n","  (16, 0.007878418216936058),\n","  (17, 0.009911558401951815),\n","  (18, 0.0033038528006506048),\n","  (19, 0.011233099522212056)],\n"," [(0, 0.031212982705520018),\n","  (1, 0.00929874437337124),\n","  (2, 0.013089315328121298),\n","  (3, 0.005034352049277422),\n","  (4, 0.007877280265339967),\n","  (5, 0.01818289504856669),\n","  (6, 0.023394930111348022),\n","  (7, 0.18603411513859275),\n","  (8, 0.006337360814972755),\n","  (9, 0.01687988628287136),\n","  (10, 0.008588012319355603),\n","  (11, 0.00550817341862118),\n","  (12, 0.024105662165363657),\n","  (13, 0.12763563136697464),\n","  (14, 0.007995735607675906),\n","  (15, 0.4478204217010187),\n","  (16, 0.007995735607675906),\n","  (17, 0.0353589196872779),\n","  (18, 0.008469556977019664),\n","  (19, 0.0091802890310353)],\n"," [(0, 0.010188564476885644),\n","  (1, 0.02468572587185726),\n","  (2, 0.20645782643957827),\n","  (3, 0.3140206812652068),\n","  (4, 0.028436739659367397),\n","  (5, 0.08348540145985402),\n","  (6, 0.008160989456609894),\n","  (7, 0.009073398215733983),\n","  (8, 0.02539537712895377),\n","  (9, 0.0057278994322789946),\n","  (10, 0.06402068126520681),\n","  (11, 0.05236212489862125),\n","  (12, 0.01637266828872668),\n","  (13, 0.017183698296836983),\n","  (14, 0.006944444444444444),\n","  (15, 0.04202149229521492),\n","  (16, 0.005930656934306569),\n","  (17, 0.00917477696674777),\n","  (18, 0.06229724249797242),\n","  (19, 0.008059610705596108)],\n"," [(0, 0.010862098574617894),\n","  (1, 0.38927528765241287),\n","  (2, 0.04048600377812124),\n","  (3, 0.009230637128627855),\n","  (4, 0.04254679718358235),\n","  (5, 0.0463249184269277),\n","  (6, 0.012235960844925297),\n","  (7, 0.03086896788596943),\n","  (8, 0.017044478791001204),\n","  (9, 0.004937317533917225),\n","  (10, 0.2145371801476902),\n","  (11, 0.02331272539927872),\n","  (12, 0.020307401682981282),\n","  (13, 0.01069036579082947),\n","  (14, 0.05482569122445475),\n","  (15, 0.0056242486690709255),\n","  (16, 0.027691911385883564),\n","  (17, 0.022024729520865535),\n","  (18, 0.012064228061136871),\n","  (19, 0.00510905031770565)],\n"," [(0, 0.01269706409893326),\n","  (1, 0.011941848390446524),\n","  (2, 0.4248560370055698),\n","  (3, 0.15014632304351933),\n","  (4, 0.017133956386292837),\n","  (5, 0.030727839139054094),\n","  (6, 0.010809024827716418),\n","  (7, 0.01316907391673747),\n","  (8, 0.017889172094779575),\n","  (9, 0.007882563957330314),\n","  (10, 0.01722835834985368),\n","  (11, 0.009959407155668839),\n","  (12, 0.003728877560653262),\n","  (13, 0.08963466440101955),\n","  (14, 0.08359293873312566),\n","  (15, 0.015529123005758521),\n","  (16, 0.006655338431039367),\n","  (17, 0.03960162371377325),\n","  (18, 0.00684414235816105),\n","  (19, 0.02997262343056736)],\n"," [(0, 0.08593750000000003),\n","  (1, 0.008833741830065363),\n","  (2, 0.15895629084967325),\n","  (3, 0.004646650326797387),\n","  (4, 0.007812500000000003),\n","  (5, 0.004544526143790852),\n","  (6, 0.14394403594771246),\n","  (7, 0.12117034313725494),\n","  (8, 0.05070465686274511),\n","  (9, 0.007097630718954251),\n","  (10, 0.027113970588235305),\n","  (11, 0.007097630718954251),\n","  (12, 0.0398794934640523),\n","  (13, 0.18581495098039222),\n","  (14, 0.028543709150326807),\n","  (15, 0.019046160130718963),\n","  (16, 0.03467116013071897),\n","  (17, 0.014552696078431378),\n","  (18, 0.005872140522875819),\n","  (19, 0.043760212418300665)],\n"," [(0, 0.006766381766381766),\n","  (1, 0.01694139194139194),\n","  (2, 0.1714997964997965),\n","  (3, 0.023555148555148554),\n","  (4, 0.07402319902319902),\n","  (5, 0.17963980463980464),\n","  (6, 0.006257631257631258),\n","  (7, 0.010531135531135532),\n","  (8, 0.016330891330891332),\n","  (9, 0.004019129019129019),\n","  (10, 0.013888888888888888),\n","  (11, 0.01561864061864062),\n","  (12, 0.007376882376882377),\n","  (13, 0.1994810744810745),\n","  (14, 0.017755392755392757),\n","  (15, 0.005952380952380952),\n","  (16, 0.0607956857956858),\n","  (17, 0.047466422466422464),\n","  (18, 0.004934879934879935),\n","  (19, 0.11716524216524217)],\n"," [(0, 0.007259147364887544),\n","  (1, 0.004741524001342732),\n","  (2, 0.5126300772071164),\n","  (3, 0.007175226586102717),\n","  (4, 0.004909365558912386),\n","  (5, 0.008434038267875124),\n","  (6, 0.013888888888888885),\n","  (7, 0.027735817388385358),\n","  (8, 0.00448976166498825),\n","  (9, 0.015147700570661293),\n","  (10, 0.020266868076535747),\n","  (11, 0.06944444444444443),\n","  (12, 0.008434038267875124),\n","  (13, 0.008182275931520643),\n","  (14, 0.03000167841557569),\n","  (15, 0.007343068143672371),\n","  (16, 0.01254615642833165),\n","  (17, 0.13112621685129236),\n","  (18, 0.020602551191675054),\n","  (19, 0.08564115474991607)],\n"," [(0, 0.008378281193815174),\n","  (1, 0.007227615965480043),\n","  (2, 0.2263574253865516),\n","  (3, 0.006292700467457749),\n","  (4, 0.0032722042430780294),\n","  (5, 0.011902193455591514),\n","  (6, 0.0062207838906868035),\n","  (7, 0.005501618122977346),\n","  (8, 0.005429701546206401),\n","  (9, 0.00391945343401654),\n","  (10, 0.005213951815893563),\n","  (11, 0.011254944264653002),\n","  (12, 0.0079467817331895),\n","  (13, 0.44606256742179073),\n","  (14, 0.007299532542250989),\n","  (15, 0.005717367853290183),\n","  (16, 0.012046026609133405),\n","  (17, 0.1884573894282632),\n","  (18, 0.022473930240920533),\n","  (19, 0.009025530384753686)],\n"," [(0, 0.04571916089531266),\n","  (1, 0.010187694469537288),\n","  (2, 0.15482284452474154),\n","  (3, 0.005570611261668172),\n","  (4, 0.005871725383920506),\n","  (5, 0.004868011643079394),\n","  (6, 0.027451570812004417),\n","  (7, 0.006273210880256951),\n","  (8, 0.004466526146742949),\n","  (9, 0.004767640268995283),\n","  (10, 0.006775067750677507),\n","  (11, 0.00707618187292984),\n","  (12, 0.029659741041854863),\n","  (13, 0.1302318578741343),\n","  (14, 0.09971896015256448),\n","  (15, 0.009183980728696176),\n","  (16, 0.005570611261668172),\n","  (17, 0.053648499447957444),\n","  (18, 0.00707618187292984),\n","  (19, 0.3810599217103282)],\n"," [(0, 0.009116074837724322),\n","  (1, 0.0043432607865597555),\n","  (2, 0.01627529591447117),\n","  (3, 0.006538755250095456),\n","  (4, 0.06133066055746468),\n","  (5, 0.005679648720885834),\n","  (6, 0.30388507063764797),\n","  (7, 0.2881347842688049),\n","  (8, 0.005202367315769378),\n","  (9, 0.013697976326842306),\n","  (10, 0.008543337151584575),\n","  (11, 0.005775105001909126),\n","  (12, 0.030402825505918288),\n","  (13, 0.13215922107674685),\n","  (14, 0.016657121038564336),\n","  (15, 0.004915998472699503),\n","  (16, 0.020666284841542572),\n","  (17, 0.004438717067583047),\n","  (18, 0.013602520045819015),\n","  (19, 0.04863497518136693)],\n"," [(0, 0.018028322440087147),\n","  (1, 0.013671023965141612),\n","  (2, 0.06650326797385621),\n","  (3, 0.25953159041394336),\n","  (4, 0.007788671023965142),\n","  (5, 0.007570806100217865),\n","  (6, 0.1764161220043573),\n","  (7, 0.05266884531590414),\n","  (8, 0.057135076252723314),\n","  (9, 0.006045751633986928),\n","  (10, 0.010729847494553376),\n","  (11, 0.011056644880174292),\n","  (12, 0.04297385620915033),\n","  (13, 0.055392156862745096),\n","  (14, 0.011492374727668845),\n","  (15, 0.11062091503267973),\n","  (16, 0.018681917211328974),\n","  (17, 0.057352941176470586),\n","  (18, 0.011165577342047931),\n","  (19, 0.0051742919389978215)],\n"," [(0, 0.00885755603759942),\n","  (1, 0.07429501084598697),\n","  (2, 0.013075439865027714),\n","  (3, 0.019342010122921183),\n","  (4, 0.005844781875150638),\n","  (5, 0.07007712701855867),\n","  (6, 0.008616534104603517),\n","  (7, 0.008375512171607615),\n","  (8, 0.31652205350686907),\n","  (9, 0.007170402506628102),\n","  (10, 0.022595806218365867),\n","  (11, 0.13274282959749334),\n","  (12, 0.006808869607134248),\n","  (13, 0.013798505664015422),\n","  (14, 0.009098577970595323),\n","  (15, 0.00512171607616293),\n","  (16, 0.01078573150156664),\n","  (17, 0.009339599903591225),\n","  (18, 0.23879248011569046),\n","  (19, 0.018739455290431428)],\n"," [(0, 0.015573937908496734),\n","  (1, 0.011488970588235297),\n","  (2, 0.14149305555555558),\n","  (3, 0.04723243464052288),\n","  (4, 0.011999591503267976),\n","  (5, 0.00862949346405229),\n","  (6, 0.3062193627450981),\n","  (7, 0.008016748366013073),\n","  (8, 0.09237132352941178),\n","  (9, 0.0036254084967320267),\n","  (10, 0.01393995098039216),\n","  (11, 0.14486315359477128),\n","  (12, 0.005361519607843138),\n","  (13, 0.06530841503267976),\n","  (14, 0.014042075163398695),\n","  (15, 0.006995506535947714),\n","  (16, 0.00607638888888889),\n","  (17, 0.01925040849673203),\n","  (18, 0.01342933006535948),\n","  (19, 0.06408292483660132)],\n"," [(0, 0.10509997468995193),\n","  (1, 0.00563148569982283),\n","  (2, 0.020437863831941284),\n","  (3, 0.009174892432295623),\n","  (4, 0.4021133890154392),\n","  (5, 0.006390787142495572),\n","  (6, 0.012465198683877502),\n","  (7, 0.06903315616299673),\n","  (8, 0.006896988104277399),\n","  (9, 0.08333333333333334),\n","  (10, 0.017147557580359407),\n","  (11, 0.010187294355859277),\n","  (12, 0.13154897494305243),\n","  (13, 0.0070235383447228555),\n","  (14, 0.01006074411541382),\n","  (15, 0.005504935459377374),\n","  (16, 0.03170083523158695),\n","  (17, 0.04511516071880537),\n","  (18, 0.007529739306504683),\n","  (19, 0.013604150847886612)],\n"," [(0, 0.16148263348885433),\n","  (1, 0.0061344392604112665),\n","  (2, 0.011491273544150683),\n","  (3, 0.020304129946431656),\n","  (4, 0.005443234836702955),\n","  (5, 0.0044064282011404875),\n","  (6, 0.00924485916709867),\n","  (7, 0.16545705892517712),\n","  (8, 0.005443234836702955),\n","  (9, 0.040521859339899775),\n","  (10, 0.0061344392604112665),\n","  (11, 0.005097632624848799),\n","  (12, 0.2753585622947987),\n","  (13, 0.11378952825298082),\n","  (14, 0.01252808017971315),\n","  (15, 0.010627268014515292),\n","  (16, 0.1155175393122516),\n","  (17, 0.012700881285640227),\n","  (18, 0.0061344392604112665),\n","  (19, 0.012182477967858995)],\n"," [(0, 0.017842149454240133),\n","  (1, 0.012944304506017352),\n","  (2, 0.009026028547439127),\n","  (3, 0.004687937307584663),\n","  (4, 0.012664427651833194),\n","  (5, 0.009165966974531207),\n","  (6, 0.026938147215225302),\n","  (7, 0.024839070808844108),\n","  (8, 0.004827875734676742),\n","  (9, 0.009445843828715366),\n","  (10, 0.011265043380912398),\n","  (11, 0.006507136859781696),\n","  (12, 0.020221102714805487),\n","  (13, 0.14210747271200672),\n","  (14, 0.02120067170445004),\n","  (15, 0.009026028547439127),\n","  (16, 0.6211167086481948),\n","  (17, 0.011544920235096557),\n","  (18, 0.0073467674223341725),\n","  (19, 0.017282395745871815)],\n"," [(0, 0.04072479368496591),\n","  (1, 0.03115656022006937),\n","  (2, 0.030678148546824543),\n","  (3, 0.00651835904796077),\n","  (4, 0.008192799904317665),\n","  (5, 0.006877167802894391),\n","  (6, 0.22646812582227005),\n","  (7, 0.168580313359646),\n","  (8, 0.007235976557828011),\n","  (9, 0.014053342901566799),\n","  (10, 0.0439540724793685),\n","  (11, 0.005322329864848702),\n","  (12, 0.014292548738189212),\n","  (13, 0.16343738787226408),\n","  (14, 0.005083124028226289),\n","  (15, 0.011182872862097836),\n","  (16, 0.18388948690348045),\n","  (17, 0.009986843678985767),\n","  (18, 0.005322329864848702),\n","  (19, 0.017043415859346968)],\n"," [(0, 0.007693211181583275),\n","  (1, 0.05467465351186281),\n","  (2, 0.00804557199906037),\n","  (3, 0.019908386187455954),\n","  (4, 0.006401221517500587),\n","  (5, 0.317066008926474),\n","  (6, 0.013918252290345313),\n","  (7, 0.00922010805731736),\n","  (8, 0.4050387596899225),\n","  (9, 0.004287056612638008),\n","  (10, 0.016737138830162087),\n","  (11, 0.012156448202959831),\n","  (12, 0.013800798684519614),\n","  (13, 0.005813953488372093),\n","  (14, 0.0254287056612638),\n","  (15, 0.004521963824289405),\n","  (16, 0.021200375851538642),\n","  (17, 0.006753582334977684),\n","  (18, 0.04163730326521024),\n","  (19, 0.005696499882546394)],\n"," [(0, 0.014131701631701634),\n","  (1, 0.014811577311577314),\n","  (2, 0.2727758352758353),\n","  (3, 0.008304195804195806),\n","  (4, 0.09212315462315464),\n","  (5, 0.07172688422688424),\n","  (6, 0.009761072261072264),\n","  (7, 0.005001942501942503),\n","  (8, 0.02073620823620824),\n","  (9, 0.005196192696192697),\n","  (10, 0.02277583527583528),\n","  (11, 0.026272338772338776),\n","  (12, 0.017239704739704743),\n","  (13, 0.10096153846153848),\n","  (14, 0.23120629370629373),\n","  (15, 0.03365384615384616),\n","  (16, 0.017239704739704743),\n","  (17, 0.006555944055944057),\n","  (18, 0.006750194250194251),\n","  (19, 0.02277583527583528)],\n"," [(0, 0.005073761405303998),\n","  (1, 0.36262471220260933),\n","  (2, 0.13741792444785536),\n","  (3, 0.009507973053636905),\n","  (4, 0.03619851624456382),\n","  (5, 0.02622154003581478),\n","  (6, 0.00498848810437452),\n","  (7, 0.05052443080071628),\n","  (8, 0.009166879849918989),\n","  (9, 0.020081862368892298),\n","  (10, 0.20017907393195186),\n","  (11, 0.003027202182996503),\n","  (12, 0.01394218470196981),\n","  (13, 0.017864756544725843),\n","  (14, 0.047625138569113996),\n","  (15, 0.009763792956425342),\n","  (16, 0.019484949262385944),\n","  (17, 0.007973053636906284),\n","  (18, 0.011383985674085443),\n","  (19, 0.006949774025752535)],\n"," [(0, 0.013727764887857692),\n","  (1, 0.02352410415055426),\n","  (2, 0.37877030162412983),\n","  (3, 0.008185099252384634),\n","  (4, 0.016692446506831653),\n","  (5, 0.032418149007476145),\n","  (6, 0.05329981954111883),\n","  (7, 0.0053493168342356266),\n","  (8, 0.009345192059809227),\n","  (9, 0.0059938128383604005),\n","  (10, 0.01179427687548337),\n","  (11, 0.006638308842485175),\n","  (12, 0.10150812064965195),\n","  (13, 0.01630574890435679),\n","  (14, 0.17034029389017785),\n","  (15, 0.05097963392626964),\n","  (16, 0.010763083268883731),\n","  (17, 0.03873420984789894),\n","  (18, 0.017981438515081202),\n","  (19, 0.027648878576952817)],\n"," [(0, 0.016814252582753528),\n","  (1, 0.004796542272823107),\n","  (2, 0.14405439595192912),\n","  (3, 0.00648323845667299),\n","  (4, 0.008486190174994728),\n","  (5, 0.3599515074847143),\n","  (6, 0.009645793801391522),\n","  (7, 0.008486190174994728),\n","  (8, 0.038319628926839544),\n","  (9, 0.005745308876238667),\n","  (10, 0.06931267130508116),\n","  (11, 0.015443811933375497),\n","  (12, 0.011227071473750789),\n","  (13, 0.013967952772506849),\n","  (14, 0.06267130508117225),\n","  (15, 0.015232974910394262),\n","  (16, 0.17199030149694283),\n","  (17, 0.011121652962260172),\n","  (18, 0.013335441703563142),\n","  (19, 0.012913767657600671)],\n"," [(0, 0.006015258215962441),\n","  (1, 0.47403169014084506),\n","  (2, 0.027924491392801253),\n","  (3, 0.0282179186228482),\n","  (4, 0.06323356807511737),\n","  (5, 0.008264866979655712),\n","  (6, 0.006015258215962441),\n","  (7, 0.10724765258215962),\n","  (8, 0.008851721439749608),\n","  (9, 0.016089593114241),\n","  (10, 0.125929186228482),\n","  (11, 0.010710093896713615),\n","  (12, 0.013253129890453835),\n","  (13, 0.007678012519561815),\n","  (14, 0.03291275430359938),\n","  (15, 0.003961267605633803),\n","  (16, 0.024598982785602505),\n","  (17, 0.023229655712050077),\n","  (18, 0.006015258215962441),\n","  (19, 0.005819640062597809)],\n"," [(0, 0.00703405017921147),\n","  (1, 0.007840501792114696),\n","  (2, 0.3111559139784947),\n","  (3, 0.02199820788530466),\n","  (4, 0.0055107526881720435),\n","  (5, 0.046818996415770614),\n","  (6, 0.006406810035842295),\n","  (7, 0.014023297491039428),\n","  (8, 0.09807347670250897),\n","  (9, 0.02513440860215054),\n","  (10, 0.010259856630824375),\n","  (11, 0.004435483870967743),\n","  (12, 0.011514336917562725),\n","  (13, 0.04672939068100359),\n","  (14, 0.1349014336917563),\n","  (15, 0.10622759856630826),\n","  (16, 0.05837813620071685),\n","  (17, 0.004614695340501793),\n","  (18, 0.031048387096774197),\n","  (19, 0.047894265232974916)],\n"," [(0, 0.04282267792521109),\n","  (1, 0.005160166197560647),\n","  (2, 0.011995711030692933),\n","  (3, 0.013201983648304513),\n","  (4, 0.010789438413081354),\n","  (5, 0.03786355716391904),\n","  (6, 0.03478086047446722),\n","  (7, 0.014408256265916094),\n","  (8, 0.45737836751105737),\n","  (9, 0.05756600991824151),\n","  (10, 0.09951749095295534),\n","  (11, 0.00676852968770942),\n","  (12, 0.03585310280123307),\n","  (13, 0.09147567350221149),\n","  (14, 0.01883125586382522),\n","  (15, 0.01025331724969843),\n","  (16, 0.014542286556761824),\n","  (17, 0.008778984050395388),\n","  (18, 0.009851226377161237),\n","  (19, 0.018161104409596566)],\n"," [(0, 0.038806115493232055),\n","  (1, 0.037929691303924436),\n","  (2, 0.03568994059791606),\n","  (3, 0.05682150160677768),\n","  (4, 0.008715551660336936),\n","  (5, 0.041532768526633554),\n","  (6, 0.010663160969909435),\n","  (7, 0.008715551660336936),\n","  (8, 0.3534423994546694),\n","  (9, 0.007352225143636187),\n","  (10, 0.034716135943129806),\n","  (11, 0.007936507936507936),\n","  (12, 0.02127763170707956),\n","  (13, 0.01582432564027656),\n","  (14, 0.006281040023371312),\n","  (15, 0.005599376765020937),\n","  (16, 0.06870191839516993),\n","  (17, 0.07103904956665692),\n","  (18, 0.1282013828026098),\n","  (19, 0.04075372480280456)],\n"," [(0, 0.0065270188221007875),\n","  (1, 0.005919854280510017),\n","  (2, 0.08141064561829588),\n","  (3, 0.007235377453956687),\n","  (4, 0.010574782432705928),\n","  (5, 0.13150172029953447),\n","  (6, 0.008348512446873102),\n","  (7, 0.005515077919449504),\n","  (8, 0.4959016393442622),\n","  (9, 0.009259259259259257),\n","  (10, 0.0495345071847804),\n","  (11, 0.006122242461040274),\n","  (12, 0.009461647439789514),\n","  (13, 0.010878364703501314),\n","  (14, 0.11784051811374213),\n","  (15, 0.005717466099979761),\n","  (16, 0.007235377453956687),\n","  (17, 0.008753288807933616),\n","  (18, 0.011283141064561828),\n","  (19, 0.010979558793766442)],\n"," [(0, 0.009417040358744395),\n","  (1, 0.019581464872944692),\n","  (2, 0.04788241155954161),\n","  (3, 0.0552566018933732),\n","  (4, 0.012506228201295466),\n","  (5, 0.050672645739910316),\n","  (6, 0.006826108619830593),\n","  (7, 0.011609367214748381),\n","  (8, 0.3233183856502242),\n","  (9, 0.004135525660189337),\n","  (10, 0.041405082212257104),\n","  (11, 0.005430991529646238),\n","  (12, 0.00931738913801694),\n","  (13, 0.14075734927752864),\n","  (14, 0.05754857997010463),\n","  (15, 0.040308918784255104),\n","  (16, 0.009715994020926755),\n","  (17, 0.03572496263079223),\n","  (18, 0.03652217239661186),\n","  (19, 0.08206278026905829)],\n"," [(0, 0.009402654867256636),\n","  (1, 0.02734759095378564),\n","  (2, 0.032632743362831854),\n","  (3, 0.037549164208456234),\n","  (4, 0.008419370698131758),\n","  (5, 0.589540314650934),\n","  (6, 0.04492379547689281),\n","  (7, 0.025258112094395272),\n","  (8, 0.008419370698131758),\n","  (9, 0.006329891838741395),\n","  (10, 0.02624139626352015),\n","  (11, 0.015056538839724676),\n","  (12, 0.01395034414945919),\n","  (13, 0.009402654867256636),\n","  (14, 0.018620943952802355),\n","  (15, 0.006821533923303833),\n","  (16, 0.0575835791543756),\n","  (17, 0.03742625368731563),\n","  (18, 0.007313176007866271),\n","  (19, 0.017760570304818087)],\n"," [(0, 0.007045283424760677),\n","  (1, 0.08115521135850276),\n","  (2, 0.05243626976444016),\n","  (3, 0.29133053673227927),\n","  (4, 0.09277186189093257),\n","  (5, 0.15989028718941597),\n","  (6, 0.012208239216951708),\n","  (7, 0.009304076583844252),\n","  (8, 0.07222760030117244),\n","  (9, 0.0076906528987845555),\n","  (10, 0.07545444767129182),\n","  (11, 0.011347746584919868),\n","  (12, 0.023287081854361624),\n","  (13, 0.016833387114122838),\n","  (14, 0.026944175540496943),\n","  (15, 0.012530923953963647),\n","  (16, 0.004248682370657202),\n","  (17, 0.019092180273206413),\n","  (18, 0.01048725395288803),\n","  (19, 0.013714101323007426)],\n"," [(0, 0.004223042230422304),\n","  (1, 0.006929069290692907),\n","  (2, 0.01119311193111931),\n","  (3, 0.004305043050430504),\n","  (4, 0.01029110291102911),\n","  (5, 0.6432554325543255),\n","  (6, 0.0049610496104961046),\n","  (7, 0.004059040590405904),\n","  (8, 0.15494054940549407),\n","  (9, 0.007831078310783108),\n","  (10, 0.022263222632226323),\n","  (11, 0.014145141451414513),\n","  (12, 0.011111111111111112),\n","  (13, 0.013161131611316114),\n","  (14, 0.017097170971709718),\n","  (15, 0.004141041410414104),\n","  (16, 0.009143091430914308),\n","  (17, 0.03341533415334153),\n","  (18, 0.005617056170561706),\n","  (19, 0.017917179171791718)],\n"," [(0, 0.037627873999797425),\n","  (1, 0.01919376076167325),\n","  (2, 0.016560316013369797),\n","  (3, 0.05271953813430568),\n","  (4, 0.006938114048414869),\n","  (5, 0.47548870657348324),\n","  (6, 0.009976704142611162),\n","  (7, 0.028208244707788918),\n","  (8, 0.11014889091461562),\n","  (9, 0.0280056720348425),\n","  (10, 0.016762888686316216),\n","  (11, 0.0529221108072521),\n","  (12, 0.012002430872075358),\n","  (13, 0.01564873898511091),\n","  (14, 0.006532968702522029),\n","  (15, 0.005722678010736352),\n","  (16, 0.02354907323002127),\n","  (17, 0.04603463992707384),\n","  (18, 0.012812721563861035),\n","  (19, 0.023143927884128432)],\n"," [(0, 0.0062108262108262115),\n","  (1, 0.03390313390313391),\n","  (2, 0.022621082621082624),\n","  (3, 0.12951566951566953),\n","  (4, 0.09977207977207979),\n","  (5, 0.28370370370370374),\n","  (6, 0.0071225071225071235),\n","  (7, 0.0062108262108262115),\n","  (8, 0.07834757834757836),\n","  (9, 0.011111111111111113),\n","  (10, 0.045868945868945875),\n","  (11, 0.018632478632478636),\n","  (12, 0.053618233618233625),\n","  (13, 0.13794871794871796),\n","  (14, 0.006552706552706553),\n","  (15, 0.005527065527065528),\n","  (16, 0.021481481481481483),\n","  (17, 0.010541310541310543),\n","  (18, 0.00985754985754986),\n","  (19, 0.011452991452991454)],\n"," [(0, 0.018387491264849752),\n","  (1, 0.03472222222222222),\n","  (2, 0.010525856044723968),\n","  (3, 0.004149196366177497),\n","  (4, 0.011748777078965754),\n","  (5, 0.5645090845562543),\n","  (6, 0.008429419986023758),\n","  (7, 0.004411250873515024),\n","  (8, 0.08110587002096435),\n","  (9, 0.008429419986023758),\n","  (10, 0.033761355695317956),\n","  (11, 0.025288259958071272),\n","  (12, 0.044767645003494054),\n","  (13, 0.012360237596086648),\n","  (14, 0.06118972746331235),\n","  (15, 0.0031009783368273927),\n","  (16, 0.042758560447239684),\n","  (17, 0.013146401118099229),\n","  (18, 0.007119147449336127),\n","  (19, 0.010089098532494758)],\n"," [(0, 0.003821907013396374),\n","  (1, 0.0078408195429472),\n","  (2, 0.274428684003152),\n","  (3, 0.04219858156028368),\n","  (4, 0.015248226950354606),\n","  (5, 0.10894405043341211),\n","  (6, 0.013435776201733645),\n","  (7, 0.008392434988179667),\n","  (8, 0.1585894405043341),\n","  (9, 0.011308116627265562),\n","  (10, 0.1205279747832939),\n","  (11, 0.003112687155240346),\n","  (12, 0.008234830575256105),\n","  (13, 0.05212765957446807),\n","  (14, 0.13305752561071707),\n","  (15, 0.019109535066981872),\n","  (16, 0.006737588652482268),\n","  (17, 0.0026398739164696606),\n","  (18, 0.004846335697399526),\n","  (19, 0.005397951142631993)],\n"," [(0, 0.010906941758005587),\n","  (1, 0.008865248226950355),\n","  (2, 0.07000859660434129),\n","  (3, 0.02122286696754782),\n","  (4, 0.012626262626262626),\n","  (5, 0.14813023855577048),\n","  (6, 0.011014399312271652),\n","  (7, 0.005748979153234472),\n","  (8, 0.26880507199656134),\n","  (9, 0.01004728132387707),\n","  (10, 0.059047926069202665),\n","  (11, 0.011229314420803783),\n","  (12, 0.0067160971416290564),\n","  (13, 0.0045669460563077585),\n","  (14, 0.21475392220073072),\n","  (15, 0.007038469804427251),\n","  (16, 0.1018160326670965),\n","  (17, 0.010477111540941329),\n","  (18, 0.0055340640447023425),\n","  (19, 0.011444229529335912)],\n"," [(0, 0.011736367363673639),\n","  (1, 0.14509020090200905),\n","  (2, 0.04105166051660517),\n","  (3, 0.008046330463304635),\n","  (4, 0.005688806888068882),\n","  (5, 0.41466789667896686),\n","  (6, 0.00425379253792538),\n","  (7, 0.05458179581795819),\n","  (8, 0.05939934399343994),\n","  (9, 0.0202439524395244),\n","  (10, 0.016656416564165645),\n","  (11, 0.04791922919229193),\n","  (12, 0.011838868388683888),\n","  (13, 0.016963919639196395),\n","  (14, 0.006098810988109882),\n","  (15, 0.004868798687986881),\n","  (16, 0.009686346863468637),\n","  (17, 0.10050225502255024),\n","  (18, 0.01409389093890939),\n","  (19, 0.006611316113161132)],\n"," [(0, 0.007324687800192124),\n","  (1, 0.21938040345821327),\n","  (2, 0.0314201088696766),\n","  (3, 0.015890169708613518),\n","  (4, 0.010286583413384568),\n","  (5, 0.32240634005763696),\n","  (6, 0.00580371437720141),\n","  (7, 0.012047710534742236),\n","  (8, 0.033421389689401226),\n","  (9, 0.020533141210374643),\n","  (10, 0.011087095741274417),\n","  (11, 0.13460614793467823),\n","  (12, 0.008285302593659944),\n","  (13, 0.09546109510086456),\n","  (14, 0.01500960614793468),\n","  (15, 0.005563560678834455),\n","  (16, 0.006284021773935319),\n","  (17, 0.03510246557796991),\n","  (18, 0.005563560678834455),\n","  (19, 0.00452289465257765)],\n"," [(0, 0.01014734500973033),\n","  (1, 0.14247984431470667),\n","  (2, 0.02034102492818089),\n","  (3, 0.015336854786396067),\n","  (4, 0.009313316652766192),\n","  (5, 0.3272634602909832),\n","  (6, 0.005884533407469186),\n","  (7, 0.004679825780743211),\n","  (8, 0.2802798628486701),\n","  (9, 0.005977203224909645),\n","  (10, 0.0542581781113891),\n","  (11, 0.02395514780835881),\n","  (12, 0.014039477342229632),\n","  (13, 0.02682791214901306),\n","  (14, 0.0175609304049671),\n","  (15, 0.008571958113242515),\n","  (16, 0.008942637383004354),\n","  (17, 0.01218608099342044),\n","  (18, 0.006162542859790564),\n","  (19, 0.005791863590028727)],\n"," [(0, 0.4584332533972822),\n","  (1, 0.006528110844657607),\n","  (2, 0.014699351629807265),\n","  (3, 0.009636735056399325),\n","  (4, 0.0075051070254907185),\n","  (5, 0.006883382183142375),\n","  (6, 0.008837374544808598),\n","  (7, 0.0039523936406430414),\n","  (8, 0.27040589750421884),\n","  (9, 0.015054622968292034),\n","  (10, 0.019673150368594013),\n","  (11, 0.007149835687005951),\n","  (12, 0.007949196198596678),\n","  (13, 0.010969002575717203),\n","  (14, 0.05582200905941913),\n","  (15, 0.011324273914201972),\n","  (16, 0.006439293010036416),\n","  (17, 0.01256772359889866),\n","  (18, 0.04329869437783107),\n","  (19, 0.022870592414956925)],\n"," [(0, 0.16247466371844485),\n","  (1, 0.0034549474847982314),\n","  (2, 0.00824580799705178),\n","  (3, 0.020775750875253365),\n","  (4, 0.00824580799705178),\n","  (5, 0.005205454210429335),\n","  (6, 0.00308641975308642),\n","  (7, 0.21535839321908976),\n","  (8, 0.0038234752165100428),\n","  (9, 0.042795282845034095),\n","  (10, 0.034042749216878575),\n","  (11, 0.004929058411645477),\n","  (12, 0.40837479270315097),\n","  (13, 0.01598489036299982),\n","  (14, 0.016721945826423442),\n","  (15, 0.008337939929979733),\n","  (16, 0.011931085314169892),\n","  (17, 0.019025244149622263),\n","  (18, 0.004099871015293902),\n","  (19, 0.00308641975308642)],\n"," [(0, 0.026871980676328504),\n","  (1, 0.004097653554175293),\n","  (2, 0.01410455486542443),\n","  (3, 0.01048136645962733),\n","  (4, 0.006254313319530711),\n","  (5, 0.12866632160110422),\n","  (6, 0.009187370600414078),\n","  (7, 0.009187370600414078),\n","  (8, 0.36650276052449965),\n","  (9, 0.005650448585231194),\n","  (10, 0.02902864044168392),\n","  (11, 0.17559523809523808),\n","  (12, 0.043780193236714976),\n","  (13, 0.011085231193926846),\n","  (14, 0.05447722567287785),\n","  (15, 0.01281055900621118),\n","  (16, 0.026699447895100068),\n","  (17, 0.005736714975845411),\n","  (18, 0.020229468599033816),\n","  (19, 0.03955314009661836)],\n"," [(0, 0.05313770744037807),\n","  (1, 0.004341136388614134),\n","  (2, 0.00928673480602264),\n","  (3, 0.01719969227387625),\n","  (4, 0.004011429827453566),\n","  (5, 0.018408616331464997),\n","  (6, 0.017749203209143864),\n","  (7, 0.016320474777448073),\n","  (8, 0.00741839762611276),\n","  (9, 0.033355313770744036),\n","  (10, 0.0056599626332564015),\n","  (11, 0.004341136388614134),\n","  (12, 0.09204308165732498),\n","  (13, 0.0052203538850423125),\n","  (14, 0.0042312342015606115),\n","  (15, 0.007967908561380372),\n","  (16, 0.6644136718320695),\n","  (17, 0.01994724695021431),\n","  (18, 0.005000549510935268),\n","  (19, 0.009946147928343774)],\n"," [(0, 0.6160203827989064),\n","  (1, 0.011123539647029581),\n","  (2, 0.007519264230673628),\n","  (3, 0.007519264230673628),\n","  (4, 0.0182078051205568),\n","  (5, 0.008016405667412381),\n","  (6, 0.01385781754909272),\n","  (7, 0.040579169773800654),\n","  (8, 0.023179219487944325),\n","  (9, 0.02616206810837684),\n","  (10, 0.008264976385781758),\n","  (11, 0.010502112851106141),\n","  (12, 0.007146408153119564),\n","  (13, 0.018704946557295554),\n","  (14, 0.013484961471538656),\n","  (15, 0.020942083022619937),\n","  (16, 0.10234899328859064),\n","  (17, 0.019450658712403684),\n","  (18, 0.008762117822520508),\n","  (19, 0.0182078051205568)],\n"," [(0, 0.05229863459852186),\n","  (1, 0.010083928347738946),\n","  (2, 0.012589252160841788),\n","  (3, 0.0067017412000501065),\n","  (4, 0.006075410246774396),\n","  (5, 0.0069522735813603905),\n","  (6, 0.022109482650632593),\n","  (7, 0.20374545910058875),\n","  (8, 0.004822748340222974),\n","  (9, 0.024239007891770012),\n","  (10, 0.012338719779531505),\n","  (11, 0.06394839032945009),\n","  (12, 0.1252035575598146),\n","  (13, 0.039020418389076786),\n","  (14, 0.00920706501315295),\n","  (15, 0.011587122635600652),\n","  (16, 0.20286859576600275),\n","  (17, 0.15827383189277214),\n","  (18, 0.00632594262808468),\n","  (19, 0.021608417888012026)],\n"," [(0, 0.01833696441539579),\n","  (1, 0.004296780440571291),\n","  (2, 0.02087872185911402),\n","  (3, 0.005386105059307675),\n","  (4, 0.009380295328007748),\n","  (5, 0.0065964657467925455),\n","  (6, 0.17604696199467443),\n","  (7, 0.20533769063180832),\n","  (8, 0.006112321471798597),\n","  (9, 0.012890341321713872),\n","  (10, 0.005628177196804649),\n","  (11, 0.008533042846768339),\n","  (12, 0.025478092471556527),\n","  (13, 0.038549987896393134),\n","  (14, 0.008169934640522878),\n","  (15, 0.01180101670297749),\n","  (16, 0.4073468893730332),\n","  (17, 0.007927862503025903),\n","  (18, 0.010469619946744132),\n","  (19, 0.010832728152989593)],\n"," [(0, 0.024466986005447538),\n","  (1, 0.005024889640274255),\n","  (2, 0.022682445759368834),\n","  (3, 0.045224006762468294),\n","  (4, 0.030478068939607396),\n","  (5, 0.0031464262233492995),\n","  (6, 0.37611533765379906),\n","  (7, 0.03977646285338592),\n","  (8, 0.010002817695125385),\n","  (9, 0.0077486615948154395),\n","  (10, 0.06250587019817788),\n","  (11, 0.008875739644970413),\n","  (12, 0.005306659152812998),\n","  (13, 0.04447262139569831),\n","  (14, 0.04109138724523339),\n","  (15, 0.03019629942706865),\n","  (16, 0.12581008734854887),\n","  (17, 0.010848126232741616),\n","  (18, 0.019582981121442657),\n","  (19, 0.08664412510566355)],\n"," [(0, 0.06491416309012876),\n","  (1, 0.004589890319504054),\n","  (2, 0.03236766809728184),\n","  (3, 0.019849785407725324),\n","  (4, 0.006139723414401527),\n","  (5, 0.014246542680019077),\n","  (6, 0.00590128755364807),\n","  (7, 0.007927992370052457),\n","  (8, 0.02485693848354793),\n","  (9, 0.08053171196948022),\n","  (10, 0.011146876490224132),\n","  (11, 0.003993800667620411),\n","  (12, 0.17638292799237007),\n","  (13, 0.08923462088698142),\n","  (14, 0.016273247496423466),\n","  (15, 0.006258941344778256),\n","  (16, 0.06360276585598476),\n","  (17, 0.3605746304244159),\n","  (18, 0.00673581306628517),\n","  (19, 0.004470672389127326)],\n"," [(0, 0.012416928996152503),\n","  (1, 0.0055380669231666095),\n","  (2, 0.0441296490614434),\n","  (3, 0.004838521627608722),\n","  (4, 0.006237612218724497),\n","  (5, 0.006237612218724497),\n","  (6, 0.02710738020286814),\n","  (7, 0.058586918502973075),\n","  (8, 0.004838521627608722),\n","  (9, 0.2593564183280868),\n","  (10, 0.016031246356534922),\n","  (11, 0.008103066340212198),\n","  (12, 0.011017838405036728),\n","  (13, 0.01311647429171039),\n","  (14, 0.004488748979829778),\n","  (15, 0.009268975166142009),\n","  (16, 0.4661886440480355),\n","  (17, 0.010551474874664802),\n","  (18, 0.006820566631689403),\n","  (19, 0.025125335198787457)],\n"," [(0, 0.3518518518518518),\n","  (1, 0.005817056249332905),\n","  (2, 0.022254242715337814),\n","  (3, 0.009659515423204182),\n","  (4, 0.012648094780659619),\n","  (5, 0.0048564414558650865),\n","  (6, 0.06366741381150602),\n","  (7, 0.042640623332265974),\n","  (8, 0.011153805101931901),\n","  (9, 0.020653218059558113),\n","  (10, 0.009232575514996262),\n","  (11, 0.004216031593553207),\n","  (12, 0.180648948660476),\n","  (13, 0.045842672643825376),\n","  (14, 0.01211441989539972),\n","  (15, 0.008592165652684383),\n","  (16, 0.15172376987938946),\n","  (17, 0.02545629202689721),\n","  (18, 0.004216031593553207),\n","  (19, 0.012754829757711599)],\n"," [(0, 0.04151307705810075),\n","  (1, 0.012550465157100225),\n","  (2, 0.0327365279971915),\n","  (3, 0.006231349833245566),\n","  (4, 0.025890819729682282),\n","  (5, 0.011497279269791116),\n","  (6, 0.008162190626645601),\n","  (7, 0.01623661576268211),\n","  (8, 0.11664033701948391),\n","  (9, 0.05274705985606458),\n","  (10, 0.0222046691241004),\n","  (11, 0.01272599613831841),\n","  (12, 0.2056345444971037),\n","  (13, 0.01307705810075478),\n","  (14, 0.24354923644023166),\n","  (15, 0.006582411795681936),\n","  (16, 0.10681060207126555),\n","  (17, 0.026768474635773205),\n","  (18, 0.01272599613831841),\n","  (19, 0.025715288748464096)],\n"," [(0, 0.6126415671870218),\n","  (1, 0.005458626670747883),\n","  (2, 0.014641363126211612),\n","  (3, 0.005866748290990716),\n","  (4, 0.013212937455361697),\n","  (5, 0.0051525354555657585),\n","  (6, 0.024640342822161005),\n","  (7, 0.02861952861952862),\n","  (8, 0.009233751657994083),\n","  (9, 0.12656871747780837),\n","  (10, 0.007499234771962045),\n","  (11, 0.008723599632690543),\n","  (12, 0.01943679216406489),\n","  (13, 0.028415467809407204),\n","  (14, 0.005458626670747883),\n","  (15, 0.006376900316294256),\n","  (16, 0.016477910417304356),\n","  (17, 0.03300683603713907),\n","  (18, 0.007805325987144169),\n","  (19, 0.020763187429854096)],\n"," [(0, 0.054776511831726556),\n","  (1, 0.009202453987730062),\n","  (2, 0.07571331190963093),\n","  (3, 0.003846528386405687),\n","  (4, 0.005112474437627812),\n","  (5, 0.006475800954328562),\n","  (6, 0.03588470152887331),\n","  (7, 0.061982666277144806),\n","  (8, 0.003846528386405687),\n","  (9, 0.017479793553413184),\n","  (10, 0.011052682831823936),\n","  (11, 0.003651767455448437),\n","  (12, 0.02887330801441231),\n","  (13, 0.016116467036712434),\n","  (14, 0.004333430713798812),\n","  (15, 0.004333430713798812),\n","  (16, 0.6206543967280164),\n","  (17, 0.026536176842925308),\n","  (18, 0.004236050248320187),\n","  (19, 0.0058915181614568116)],\n"," [(0, 0.10000986777185711),\n","  (1, 0.0050819025064140514),\n","  (2, 0.02708703374777975),\n","  (3, 0.010213143872113677),\n","  (4, 0.008140911782119597),\n","  (5, 0.0033057035721334124),\n","  (6, 0.009325044404973356),\n","  (7, 0.33436944937833035),\n","  (8, 0.0032070258535622656),\n","  (9, 0.04257943556344977),\n","  (10, 0.016627195579238207),\n","  (11, 0.004193803039273732),\n","  (12, 0.021955792382080126),\n","  (13, 0.008831655812117624),\n","  (14, 0.004193803039273732),\n","  (15, 0.007450167752121571),\n","  (16, 0.3252910992697849),\n","  (17, 0.04593447799486876),\n","  (18, 0.003996447602131439),\n","  (19, 0.018206039076376555)],\n"," [(0, 0.15162334571074929),\n","  (1, 0.019375416547653047),\n","  (2, 0.019470627439779106),\n","  (3, 0.005284204512996286),\n","  (4, 0.01918499476340093),\n","  (5, 0.004141673807483575),\n","  (6, 0.018232885842140337),\n","  (7, 0.006426735218508996),\n","  (8, 0.0036656193468532793),\n","  (9, 0.0066171570027611145),\n","  (10, 0.004236884699609635),\n","  (11, 0.005188993620870227),\n","  (12, 0.19456345805960196),\n","  (13, 0.013662763020089494),\n","  (14, 0.00842616395315624),\n","  (15, 0.006331524326382936),\n","  (16, 0.49314481576692365),\n","  (17, 0.008902218413786536),\n","  (18, 0.006045891650004759),\n","  (19, 0.0054746262972484045)],\n"," [(0, 0.03112509992006394),\n","  (1, 0.028327338129496397),\n","  (2, 0.01653677058353317),\n","  (3, 0.051209032773780964),\n","  (4, 0.016137090327737805),\n","  (5, 0.0068445243804956014),\n","  (6, 0.012939648281374897),\n","  (7, 0.30690447641886487),\n","  (8, 0.005045963229416466),\n","  (9, 0.011640687450039966),\n","  (10, 0.0381195043964828),\n","  (11, 0.004646282973621102),\n","  (12, 0.06539768185451637),\n","  (13, 0.037919664268585125),\n","  (14, 0.23366306954436444),\n","  (15, 0.02652877697841726),\n","  (16, 0.0454136690647482),\n","  (17, 0.03961830535571542),\n","  (18, 0.007444044764188648),\n","  (19, 0.014538369304556351)],\n"," [(0, 0.06749789562289563),\n","  (1, 0.00594486531986532),\n","  (2, 0.019518097643097643),\n","  (3, 0.005629208754208755),\n","  (4, 0.004787457912457912),\n","  (5, 0.28288089225589225),\n","  (6, 0.005734427609427609),\n","  (7, 0.006786616161616162),\n","  (8, 0.07149621212121213),\n","  (9, 0.01383627946127946),\n","  (10, 0.010048400673400673),\n","  (11, 0.005103114478114478),\n","  (12, 0.1913404882154882),\n","  (13, 0.011521464646464646),\n","  (14, 0.12179082491582492),\n","  (15, 0.004892676767676768),\n","  (16, 0.1362058080808081),\n","  (17, 0.023832070707070708),\n","  (18, 0.006786616161616162),\n","  (19, 0.004366582491582492)],\n"," [(0, 0.27131915866483763),\n","  (1, 0.022119341563786005),\n","  (2, 0.022347965249199814),\n","  (3, 0.012174211248285318),\n","  (4, 0.012974394147233649),\n","  (5, 0.010802469135802467),\n","  (6, 0.046124828532235936),\n","  (7, 0.03697988111568358),\n","  (8, 0.00725880201188843),\n","  (9, 0.1541495198902606),\n","  (10, 0.010688157293095564),\n","  (11, 0.021090534979423866),\n","  (12, 0.06430041152263373),\n","  (13, 0.05675582990397804),\n","  (14, 0.005544124371284864),\n","  (15, 0.00805898491083676),\n","  (16, 0.19290123456790118),\n","  (17, 0.013660265203475077),\n","  (18, 0.007716049382716047),\n","  (19, 0.02303383630544124)],\n"," [(0, 0.26327985739750454),\n","  (1, 0.0033868092691622114),\n","  (2, 0.008021390374331553),\n","  (3, 0.016696375519904934),\n","  (4, 0.017290552584670237),\n","  (5, 0.005050505050505052),\n","  (6, 0.03357100415923946),\n","  (7, 0.05306001188354131),\n","  (8, 0.004812834224598932),\n","  (9, 0.09310754604872254),\n","  (10, 0.02465834818775996),\n","  (11, 0.004931669637551992),\n","  (12, 0.21883541295306005),\n","  (13, 0.059001782531194306),\n","  (14, 0.03618538324420678),\n","  (15, 0.008734402852049912),\n","  (16, 0.11782531194295903),\n","  (17, 0.009922756981580513),\n","  (18, 0.006833036244800953),\n","  (19, 0.014795008912655975)],\n"," [(0, 0.09204571169947952),\n","  (1, 0.007976917854718261),\n","  (2, 0.010466168816474315),\n","  (3, 0.007411178999773705),\n","  (4, 0.5497284453496266),\n","  (5, 0.004582484725050916),\n","  (6, 0.0093346911065852),\n","  (7, 0.018725956098664856),\n","  (8, 0.00480878026702874),\n","  (9, 0.08276759447838877),\n","  (10, 0.010918759900429962),\n","  (11, 0.005940257976917855),\n","  (12, 0.04904955872369314),\n","  (13, 0.00480878026702874),\n","  (14, 0.00786377008372935),\n","  (15, 0.012502828694274723),\n","  (16, 0.04633401221995927),\n","  (17, 0.008090065625707174),\n","  (18, 0.006732292373840236),\n","  (19, 0.05991174473862865)],\n"," [(0, 0.0161415791520809),\n","  (1, 0.0346817062102943),\n","  (2, 0.014845066770387653),\n","  (3, 0.007843899909244131),\n","  (4, 0.48547906132503554),\n","  (5, 0.0053805263840269665),\n","  (6, 0.012770646959678462),\n","  (7, 0.03584856735381822),\n","  (8, 0.00693634124205886),\n","  (9, 0.006158433813042913),\n","  (10, 0.016011927913911574),\n","  (11, 0.009010761052768053),\n","  (12, 0.23706728899260981),\n","  (13, 0.016789835342927523),\n","  (14, 0.014326461817710356),\n","  (15, 0.014585764294049005),\n","  (16, 0.01873460391546739),\n","  (17, 0.03286658887592376),\n","  (18, 0.008103202385582781),\n","  (19, 0.0064177362893815615)],\n"," [(0, 0.18317584446616705),\n","  (1, 0.004833279026827414),\n","  (2, 0.011893124796350603),\n","  (3, 0.003638535896600413),\n","  (4, 0.47784294558488105),\n","  (5, 0.004398826979472141),\n","  (6, 0.007765830346475508),\n","  (7, 0.016020419246225697),\n","  (8, 0.003638535896600413),\n","  (9, 0.03426740523514717),\n","  (10, 0.034593244270663626),\n","  (11, 0.003964374932116868),\n","  (12, 0.07760399695883567),\n","  (13, 0.0060280221570544155),\n","  (14, 0.010698381666123602),\n","  (15, 0.004507439991310959),\n","  (16, 0.012001737808189422),\n","  (17, 0.08737916802432931),\n","  (18, 0.004724666014988596),\n","  (19, 0.011024220701640056)],\n"," [(0, 0.05511229314420804),\n","  (1, 0.014529156816390859),\n","  (2, 0.01679472025216706),\n","  (3, 0.0033983451536643027),\n","  (4, 0.5387608353033885),\n","  (5, 0.007436958234830576),\n","  (6, 0.013248620961386919),\n","  (7, 0.11027383766745469),\n","  (8, 0.003595350669818755),\n","  (9, 0.009407013396375098),\n","  (10, 0.0308806146572104),\n","  (11, 0.011968085106382979),\n","  (12, 0.08525413711583925),\n","  (13, 0.050384160756501185),\n","  (14, 0.009013002364066195),\n","  (15, 0.011968085106382979),\n","  (16, 0.008520488573680063),\n","  (17, 0.005959416863672183),\n","  (18, 0.0052698975571315995),\n","  (19, 0.008224980299448384)],\n"," [(0, 0.011614255765199161),\n","  (1, 0.005995807127882599),\n","  (2, 0.06570230607966457),\n","  (3, 0.013710691823899371),\n","  (4, 0.5257442348008385),\n","  (5, 0.0038155136268343817),\n","  (6, 0.04255765199161426),\n","  (7, 0.022180293501048217),\n","  (8, 0.003228511530398323),\n","  (9, 0.004905660377358491),\n","  (10, 0.0220125786163522),\n","  (11, 0.02192872117400419),\n","  (12, 0.00440251572327044),\n","  (13, 0.00960167714884696),\n","  (14, 0.14251572327044026),\n","  (15, 0.006331236897274633),\n","  (16, 0.011949685534591196),\n","  (17, 0.010775681341719077),\n","  (18, 0.004737945492662474),\n","  (19, 0.06628930817610063)],\n"," [(0, 0.03403508771929824),\n","  (1, 0.008226120857699804),\n","  (2, 0.10265107212475631),\n","  (3, 0.004249512670565302),\n","  (4, 0.6450292397660817),\n","  (5, 0.005575048732943469),\n","  (6, 0.01282651072124756),\n","  (7, 0.007134502923976606),\n","  (8, 0.003547758284600389),\n","  (9, 0.00580896686159844),\n","  (10, 0.010877192982456138),\n","  (11, 0.013294346978557501),\n","  (12, 0.055243664717348914),\n","  (13, 0.012436647173489275),\n","  (14, 0.009161793372319687),\n","  (15, 0.005263157894736841),\n","  (16, 0.020545808966861596),\n","  (17, 0.007056530214424949),\n","  (18, 0.0046393762183235855),\n","  (19, 0.032397660818713446)],\n"," [(0, 0.045221169036334906),\n","  (1, 0.004673512374934175),\n","  (2, 0.015073723012111634),\n","  (3, 0.007701421800947865),\n","  (4, 0.5776066350710899),\n","  (5, 0.004541864139020536),\n","  (6, 0.010992627698788834),\n","  (7, 0.018496577145866243),\n","  (8, 0.006384939441811478),\n","  (9, 0.026132174828857286),\n","  (10, 0.0370589784096893),\n","  (11, 0.004936808846761453),\n","  (12, 0.16199315429173247),\n","  (13, 0.014020537124802524),\n","  (14, 0.012572406529752498),\n","  (15, 0.007043180621379672),\n","  (16, 0.022051079515534488),\n","  (17, 0.00717482885729331),\n","  (18, 0.005595050026329647),\n","  (19, 0.010729331226961557)],\n"," [(0, 0.07370801033591731),\n","  (1, 0.007041343669250646),\n","  (2, 0.010142118863049096),\n","  (3, 0.006524547803617571),\n","  (4, 0.6266795865633075),\n","  (5, 0.00562015503875969),\n","  (6, 0.011046511627906977),\n","  (7, 0.006782945736434108),\n","  (8, 0.005749354005167959),\n","  (9, 0.03171834625322997),\n","  (10, 0.08068475452196383),\n","  (11, 0.005749354005167959),\n","  (12, 0.05135658914728682),\n","  (13, 0.011563307493540051),\n","  (14, 0.00962532299741602),\n","  (15, 0.008979328165374677),\n","  (16, 0.010788113695090439),\n","  (17, 0.009108527131782946),\n","  (18, 0.015439276485788113),\n","  (19, 0.01169250645994832)],\n"," [(0, 0.047732551032881064),\n","  (1, 0.022674489671189343),\n","  (2, 0.1444199975553111),\n","  (3, 0.016562767387849897),\n","  (4, 0.02181884855152182),\n","  (5, 0.0049504950495049506),\n","  (6, 0.0605671678278939),\n","  (7, 0.43350446155726685),\n","  (8, 0.013751375137513752),\n","  (9, 0.005072729495171739),\n","  (10, 0.008739762865175407),\n","  (11, 0.016807236279183475),\n","  (12, 0.011306686224177973),\n","  (13, 0.033431120889866765),\n","  (14, 0.009595403984842928),\n","  (15, 0.01839628407285173),\n","  (16, 0.0957706881799291),\n","  (17, 0.0049504950495049506),\n","  (18, 0.01155115511551155),\n","  (19, 0.01839628407285173)],\n"," [(0, 0.01210751940678948),\n","  (1, 0.01697369945545128),\n","  (2, 0.052311435523114354),\n","  (3, 0.008399953655428108),\n","  (4, 0.01535163943923068),\n","  (5, 0.00851581508515815),\n","  (6, 0.05937898273664697),\n","  (7, 0.38448615455914725),\n","  (8, 0.009558567952728537),\n","  (9, 0.13086548488008343),\n","  (10, 0.014308886571660294),\n","  (11, 0.013613717993280037),\n","  (12, 0.056019001274475726),\n","  (13, 0.02462055381763411),\n","  (14, 0.07374580002317228),\n","  (15, 0.049067315490673155),\n","  (16, 0.03307843818792724),\n","  (17, 0.024504692387904068),\n","  (18, 0.0056192793419070795),\n","  (19, 0.0074730622175877654)],\n"," [(0, 0.028509842199446883),\n","  (1, 0.005653164145111436),\n","  (2, 0.04762485765414023),\n","  (3, 0.005815845127704571),\n","  (4, 0.03916544655929722),\n","  (5, 0.0037823328452903857),\n","  (6, 0.016390108996258337),\n","  (7, 0.03997885147226289),\n","  (8, 0.003294289897510981),\n","  (9, 0.1915975272490646),\n","  (10, 0.02021311208719701),\n","  (11, 0.004351716284366358),\n","  (12, 0.075687327151456),\n","  (13, 0.09878802667968115),\n","  (14, 0.26065560435985036),\n","  (15, 0.10008947454042623),\n","  (16, 0.006222547584187409),\n","  (17, 0.013949894257361314),\n","  (18, 0.00427037579306979),\n","  (19, 0.033959655116316906)],\n"," [(0, 0.017156071851134776),\n","  (1, 0.010379692373884048),\n","  (2, 0.04243304291707002),\n","  (3, 0.016725825535118855),\n","  (4, 0.013498978164999462),\n","  (5, 0.012530923953963644),\n","  (6, 0.008766268688824352),\n","  (7, 0.018124126062170592),\n","  (8, 0.011025061847907927),\n","  (9, 0.3016564483166613),\n","  (10, 0.017909002904162634),\n","  (11, 0.017693879746154672),\n","  (12, 0.021781219748305904),\n","  (13, 0.09395503925997634),\n","  (14, 0.12966548348929763),\n","  (15, 0.10137678821125094),\n","  (16, 0.10761535979348177),\n","  (17, 0.00930407658384425),\n","  (18, 0.01833924922017855),\n","  (19, 0.03006346133161235)],\n"," [(0, 0.10901787734781625),\n","  (1, 0.011145055442407785),\n","  (2, 0.03694274722787961),\n","  (3, 0.007750622312740439),\n","  (4, 0.09600588368409142),\n","  (5, 0.008316361167684997),\n","  (6, 0.015670966281964245),\n","  (7, 0.2549785019235121),\n","  (8, 0.006279701289884589),\n","  (9, 0.19161575016972165),\n","  (10, 0.05391491287621634),\n","  (11, 0.010579316587463227),\n","  (12, 0.01680244399185336),\n","  (13, 0.021102059289432),\n","  (14, 0.006958587915818058),\n","  (15, 0.007411178999773705),\n","  (16, 0.09758995247793618),\n","  (17, 0.009674134419551934),\n","  (18, 0.0122765331522969),\n","  (19, 0.025967413441955193)],\n"," [(0, 0.0033564814814814816),\n","  (1, 0.05081018518518519),\n","  (2, 0.22689043209876544),\n","  (3, 0.0251929012345679),\n","  (4, 0.014699074074074074),\n","  (5, 0.022723765432098766),\n","  (6, 0.036535493827160495),\n","  (7, 0.03236882716049383),\n","  (8, 0.005208333333333333),\n","  (9, 0.004128086419753086),\n","  (10, 0.04510030864197531),\n","  (11, 0.22982253086419754),\n","  (12, 0.003896604938271605),\n","  (13, 0.20173611111111112),\n","  (14, 0.011149691358024691),\n","  (15, 0.010686728395061728),\n","  (16, 0.052739197530864194),\n","  (17, 0.006751543209876543),\n","  (18, 0.008371913580246913),\n","  (19, 0.00783179012345679)],\n"," [(0, 0.04237891737891738),\n","  (1, 0.013888888888888888),\n","  (2, 0.04553317053317053),\n","  (3, 0.003917378917378918),\n","  (4, 0.012464387464387465),\n","  (5, 0.007580382580382581),\n","  (6, 0.015415140415140416),\n","  (7, 0.11095848595848595),\n","  (8, 0.00819088319088319),\n","  (9, 0.1534900284900285),\n","  (10, 0.004934879934879935),\n","  (11, 0.013481888481888481),\n","  (12, 0.11492673992673992),\n","  (13, 0.007682132682132682),\n","  (14, 0.023046398046398048),\n","  (15, 0.006257631257631258),\n","  (16, 0.07738095238095238),\n","  (17, 0.3003154253154253),\n","  (18, 0.006257631257631258),\n","  (19, 0.031898656898656896)],\n"," [(0, 0.03367846804720552),\n","  (1, 0.006513026052104208),\n","  (2, 0.025439768425740368),\n","  (3, 0.06941661099977733),\n","  (4, 0.005622355822756624),\n","  (5, 0.02532843464707192),\n","  (6, 0.006290358494767312),\n","  (7, 0.007737697617457137),\n","  (8, 0.02610777109775106),\n","  (9, 0.10114673792028502),\n","  (10, 0.0180917390336228),\n","  (11, 0.00862836784680472),\n","  (12, 0.13109552438209754),\n","  (13, 0.07821197951458472),\n","  (14, 0.007181028724114896),\n","  (15, 0.004843019372077488),\n","  (16, 0.10827209975506569),\n","  (17, 0.29909819639278556),\n","  (18, 0.011634379870852817),\n","  (19, 0.025662435983077266)],\n"," [(0, 0.017514661594547468),\n","  (1, 0.00847994927880805),\n","  (2, 0.03907116817245205),\n","  (3, 0.011808527500396257),\n","  (4, 0.012601046124583924),\n","  (5, 0.21659533999048972),\n","  (6, 0.026232366460611816),\n","  (7, 0.020526232366460607),\n","  (8, 0.038595656997939445),\n","  (9, 0.07330797273735931),\n","  (10, 0.04763036931367886),\n","  (11, 0.02670787763512442),\n","  (12, 0.06173720082421936),\n","  (13, 0.13274686955143444),\n","  (14, 0.013869075923284194),\n","  (15, 0.018148676493897602),\n","  (16, 0.013869075923284194),\n","  (17, 0.11642098589316846),\n","  (18, 0.015929624346172132),\n","  (19, 0.08820732287208748)],\n"," [(0, 0.018448438978240302),\n","  (1, 0.097918637653737),\n","  (2, 0.010564490696941028),\n","  (3, 0.04788184589509093),\n","  (4, 0.008777462419846525),\n","  (5, 0.005203405865657522),\n","  (6, 0.006569956901082729),\n","  (7, 0.04567434037632713),\n","  (8, 0.013823189319878061),\n","  (9, 0.005413644486492169),\n","  (10, 0.01865867759907495),\n","  (11, 0.009408178282350467),\n","  (12, 0.09592137075580784),\n","  (13, 0.2126038053190371),\n","  (14, 0.25591296121097445),\n","  (15, 0.00457269000315358),\n","  (16, 0.05408388520971302),\n","  (17, 0.0695364238410596),\n","  (18, 0.007516030694838642),\n","  (19, 0.01151056449069694)],\n"," [(0, 0.009485094850948509),\n","  (1, 0.005543237250554324),\n","  (2, 0.023774328652377434),\n","  (3, 0.02032520325203252),\n","  (4, 0.019750349018641703),\n","  (5, 0.006364457583969779),\n","  (6, 0.013837562618050423),\n","  (7, 0.26562371684322905),\n","  (8, 0.0064465796173113245),\n","  (9, 0.1679806192001314),\n","  (10, 0.013098464317976514),\n","  (11, 0.02073581341874025),\n","  (12, 0.0280446743861378),\n","  (13, 0.18662232076866223),\n","  (14, 0.012195121951219513),\n","  (15, 0.004722016917138868),\n","  (16, 0.13907366346390737),\n","  (17, 0.03379321672004599),\n","  (18, 0.007185677917385234),\n","  (19, 0.015397881251539788)],\n"," [(0, 0.020304339996398343),\n","  (1, 0.0072483342337475234),\n","  (2, 0.017963263101026472),\n","  (3, 0.004096884566900774),\n","  (4, 0.010219701062488745),\n","  (5, 0.07910138663785342),\n","  (6, 0.07423915000900415),\n","  (7, 0.031559517377993875),\n","  (8, 0.02003421573924005),\n","  (9, 0.008328831262380695),\n","  (10, 0.043354943273905995),\n","  (11, 0.008598955519538988),\n","  (12, 0.010669908157752567),\n","  (13, 0.21605438501710786),\n","  (14, 0.016792724653340536),\n","  (15, 0.018503511615343058),\n","  (16, 0.016342517558076714),\n","  (17, 0.25423194669547994),\n","  (18, 0.009679452548172159),\n","  (19, 0.13267603097424815)],\n"," [(0, 0.1088563613513713),\n","  (1, 0.058438678199157226),\n","  (2, 0.31341021660382934),\n","  (3, 0.004916093738449027),\n","  (4, 0.024358690027352695),\n","  (5, 0.0038811266356176526),\n","  (6, 0.050602498706291105),\n","  (7, 0.0060989132845420254),\n","  (8, 0.005729282176387964),\n","  (9, 0.005803208398018777),\n","  (10, 0.013417609225992456),\n","  (11, 0.02051452650255045),\n","  (12, 0.12157167147187105),\n","  (13, 0.14973756191321058),\n","  (14, 0.01474828121534708),\n","  (15, 0.01600502698307089),\n","  (16, 0.005729282176387964),\n","  (17, 0.04468840097582612),\n","  (18, 0.0038811266356176526),\n","  (19, 0.027611443779108444)],\n"," [(0, 0.010898215733982157),\n","  (1, 0.003092051905920519),\n","  (2, 0.023874695863746957),\n","  (3, 0.007349959448499595),\n","  (4, 0.023671938361719384),\n","  (5, 0.03462084347120843),\n","  (6, 0.011810624493106246),\n","  (7, 0.012723033252230332),\n","  (8, 0.16732562854825628),\n","  (9, 0.026510543390105435),\n","  (10, 0.017791970802919707),\n","  (11, 0.009681670721816707),\n","  (12, 0.008667883211678832),\n","  (13, 0.16195255474452555),\n","  (14, 0.1899330900243309),\n","  (15, 0.004105839416058394),\n","  (16, 0.09595498783454988),\n","  (17, 0.01120235198702352),\n","  (18, 0.1664132197891322),\n","  (19, 0.01241889699918897)],\n"," [(0, 0.023464832911086975),\n","  (1, 0.008746531547834478),\n","  (2, 0.012607069610326935),\n","  (3, 0.013934129569308718),\n","  (4, 0.007660755217758474),\n","  (5, 0.036976716129810586),\n","  (6, 0.2064784654361201),\n","  (7, 0.11382555193630109),\n","  (8, 0.008625889733381588),\n","  (9, 0.016708891301725176),\n","  (10, 0.022379056581010973),\n","  (11, 0.006695620702135359),\n","  (12, 0.028893714561467),\n","  (13, 0.22409217034624196),\n","  (14, 0.02479189287006876),\n","  (15, 0.10453613222342861),\n","  (16, 0.02141392206538786),\n","  (17, 0.09138617444806368),\n","  (18, 0.008263964290022921),\n","  (19, 0.018518518518518514)],\n"," [(0, 0.0677183356195702),\n","  (1, 0.0118427069044353),\n","  (2, 0.014951989026063102),\n","  (3, 0.006447187928669411),\n","  (4, 0.008184727937814358),\n","  (5, 0.012757201646090536),\n","  (6, 0.04037494284407865),\n","  (7, 0.18550525834476456),\n","  (8, 0.005989940557841794),\n","  (9, 0.0839963420210334),\n","  (10, 0.014494741655235484),\n","  (11, 0.003978052126200275),\n","  (12, 0.11893004115226338),\n","  (13, 0.2292181069958848),\n","  (14, 0.01742112482853224),\n","  (15, 0.0047096479195244635),\n","  (16, 0.10201188843164154),\n","  (17, 0.02491998171010517),\n","  (18, 0.017238225880201193),\n","  (19, 0.0293095564700503)],\n"," [(0, 0.032042015996916265),\n","  (1, 0.006215669268574734),\n","  (2, 0.013154090777681413),\n","  (3, 0.009973980919340852),\n","  (4, 0.007275705888021587),\n","  (5, 0.009106678230702516),\n","  (6, 0.052279078731810744),\n","  (7, 0.01353955863929845),\n","  (8, 0.006215669268574734),\n","  (9, 0.014503228293341044),\n","  (10, 0.010648549677170667),\n","  (11, 0.006022935337766214),\n","  (12, 0.07367254505155633),\n","  (13, 0.5148405126722561),\n","  (14, 0.02664546593427773),\n","  (15, 0.01363592560470271),\n","  (16, 0.03454755709742701),\n","  (17, 0.014985063120362341),\n","  (18, 0.014985063120362341),\n","  (19, 0.12571070636985643)],\n"," [(0, 0.010724753530508929),\n","  (1, 0.017252864375166536),\n","  (2, 0.014988009592326141),\n","  (3, 0.00672795097255529),\n","  (4, 0.009792166266986412),\n","  (5, 0.011524114042099656),\n","  (6, 0.0099253930189182),\n","  (7, 0.028843591793232086),\n","  (8, 0.003930189181987744),\n","  (9, 0.46609379163336),\n","  (10, 0.014055422328803625),\n","  (11, 0.016187050359712234),\n","  (12, 0.18098854249933388),\n","  (13, 0.008593125499600321),\n","  (14, 0.014321875832667202),\n","  (15, 0.009792166266986412),\n","  (16, 0.07933653077537971),\n","  (17, 0.05229150013322676),\n","  (18, 0.006328270716759926),\n","  (19, 0.03830269118038903)],\n"," [(0, 0.06049960967993755),\n","  (1, 0.004033307311995837),\n","  (2, 0.011492757394396739),\n","  (3, 0.005768063145112325),\n","  (4, 0.05390753751409489),\n","  (5, 0.003165929395437592),\n","  (6, 0.0095845259779686),\n","  (7, 0.006548703270014745),\n","  (8, 0.00429352068696331),\n","  (9, 0.06266805447133315),\n","  (10, 0.015829646977187962),\n","  (11, 0.005247636395177379),\n","  (12, 0.1958973024546795),\n","  (13, 0.030748547141989765),\n","  (14, 0.34144331685315293),\n","  (15, 0.04679503859831729),\n","  (16, 0.009931477144591899),\n","  (17, 0.07906149709428398),\n","  (18, 0.004900685228554081),\n","  (19, 0.048182843264810475)],\n"," [(0, 0.00954296915299701),\n","  (1, 0.03481894150417828),\n","  (2, 0.19658516455173838),\n","  (3, 0.008511296812132469),\n","  (4, 0.014185494686887446),\n","  (5, 0.03739812235633963),\n","  (6, 0.01253481894150418),\n","  (7, 0.047611678530898593),\n","  (8, 0.004797276385020119),\n","  (9, 0.12457443515939338),\n","  (10, 0.008098627875786652),\n","  (11, 0.17461054369132367),\n","  (12, 0.0291447436294233),\n","  (13, 0.026771897245434852),\n","  (14, 0.007892293407613743),\n","  (15, 0.01129681213246673),\n","  (16, 0.04451666150830497),\n","  (17, 0.17203136283916232),\n","  (18, 0.004075105746414939),\n","  (19, 0.031001753842979474)],\n"," [(0, 0.12073530909461248),\n","  (1, 0.005143089927691211),\n","  (2, 0.008707607699358386),\n","  (3, 0.004837559832976882),\n","  (4, 0.04506568897036358),\n","  (5, 0.00575415011711987),\n","  (6, 0.017160606986454833),\n","  (7, 0.1154394541195641),\n","  (8, 0.03355738873612384),\n","  (9, 0.034881352479885934),\n","  (10, 0.0341684489255525),\n","  (11, 0.005244933292595988),\n","  (12, 0.42239535594256034),\n","  (13, 0.02316936551583664),\n","  (14, 0.00921682452388227),\n","  (15, 0.015632956512883187),\n","  (16, 0.011966595376311234),\n","  (17, 0.02388226907017008),\n","  (18, 0.013596089214787657),\n","  (19, 0.04944495366126897)],\n"," [(0, 0.008251332513325133),\n","  (1, 0.010403854038540385),\n","  (2, 0.07641451414514146),\n","  (3, 0.021473964739647397),\n","  (4, 0.00527880278802788),\n","  (5, 0.06841943419434195),\n","  (6, 0.01419639196391964),\n","  (7, 0.004151291512915129),\n","  (8, 0.013888888888888888),\n","  (9, 0.004766297662976629),\n","  (10, 0.019833948339483393),\n","  (11, 0.006406314063140631),\n","  (12, 0.00527880278802788),\n","  (13, 0.09312218122181222),\n","  (14, 0.1745079950799508),\n","  (15, 0.14714022140221403),\n","  (16, 0.005483804838048381),\n","  (17, 0.20976834768347682),\n","  (18, 0.006816318163181631),\n","  (19, 0.10439729397293973)],\n"," [(0, 0.06309561210597968),\n","  (1, 0.006754633993088281),\n","  (2, 0.02602366739972772),\n","  (3, 0.010419939260655566),\n","  (4, 0.017541103780500576),\n","  (5, 0.006231018954864383),\n","  (6, 0.03932348937061472),\n","  (7, 0.02790868153733375),\n","  (8, 0.0058121269242852654),\n","  (9, 0.13231752015917897),\n","  (10, 0.01199078437532726),\n","  (11, 0.008325479107759975),\n","  (12, 0.3652214891611687),\n","  (13, 0.08079380039794742),\n","  (14, 0.00947743219185255),\n","  (15, 0.02581422138443816),\n","  (16, 0.030212587705518903),\n","  (17, 0.09042831710126714),\n","  (18, 0.006545187977798722),\n","  (19, 0.035762907110692216)],\n"," [(0, 0.12981248753241573),\n","  (1, 0.04054458408138839),\n","  (2, 0.04513265509674846),\n","  (3, 0.006134051466187912),\n","  (4, 0.04832435667265111),\n","  (5, 0.00862756832236186),\n","  (6, 0.05660283263514861),\n","  (7, 0.30934570117693994),\n","  (8, 0.005435866746459206),\n","  (9, 0.18586674645920606),\n","  (10, 0.014412527428685418),\n","  (11, 0.006632754837422701),\n","  (12, 0.022591262716935966),\n","  (13, 0.015908637542389786),\n","  (14, 0.004438460003989627),\n","  (15, 0.005435866746459206),\n","  (16, 0.056004388589666866),\n","  (17, 0.010323159784560144),\n","  (18, 0.0068322361859166165),\n","  (19, 0.021593855974466388)],\n"," [(0, 0.007914446760867886),\n","  (1, 0.006833449154505443),\n","  (2, 0.21585205775615782),\n","  (3, 0.006447378580804571),\n","  (4, 0.00590687977762335),\n","  (5, 0.006833449154505443),\n","  (6, 0.009072658481970504),\n","  (7, 0.00590687977762335),\n","  (8, 0.006061308007103698),\n","  (9, 0.010848583120994518),\n","  (10, 0.006987877383985793),\n","  (11, 0.029379970658636397),\n","  (12, 0.004903096286001081),\n","  (13, 0.34572619874913135),\n","  (14, 0.0071423056134661414),\n","  (15, 0.003822098679638638),\n","  (16, 0.015095359431704116),\n","  (17, 0.28997760790672533),\n","  (18, 0.010153656088332947),\n","  (19, 0.005134738630221605)],\n"," [(0, 0.08394495412844036),\n","  (1, 0.004841997961264015),\n","  (2, 0.4391946992864423),\n","  (3, 0.006065239551478082),\n","  (4, 0.0082059123343527),\n","  (5, 0.005963302752293577),\n","  (6, 0.11636085626911312),\n","  (7, 0.021865443425076448),\n","  (8, 0.003618756371049948),\n","  (9, 0.007900101936799183),\n","  (10, 0.014627930682976551),\n","  (11, 0.010448521916411824),\n","  (12, 0.00912334352701325),\n","  (13, 0.17507645259938834),\n","  (14, 0.007696228338430171),\n","  (15, 0.004332313965341487),\n","  (16, 0.025229357798165132),\n","  (17, 0.027471967380224255),\n","  (18, 0.007186544342507643),\n","  (19, 0.020846075433231394)],\n"," [(0, 0.0322596981667173),\n","  (1, 0.01858604274283399),\n","  (2, 0.010888281170870049),\n","  (3, 0.007140686721361287),\n","  (4, 0.026688949660690766),\n","  (5, 0.006938114048414867),\n","  (6, 0.0050136736554238824),\n","  (7, 0.02243492352881596),\n","  (8, 0.006836827711941658),\n","  (9, 0.486528917249063),\n","  (10, 0.00987541780613795),\n","  (11, 0.031651980147878046),\n","  (12, 0.06487389851109084),\n","  (13, 0.08644788817988452),\n","  (14, 0.014838448293325228),\n","  (15, 0.008254836422566594),\n","  (16, 0.01797832472399473),\n","  (17, 0.12311354198318644),\n","  (18, 0.007545832067254126),\n","  (19, 0.012103717208548564)],\n"," [(0, 0.017051705170517052),\n","  (1, 0.026708226378193374),\n","  (2, 0.008984231756508984),\n","  (3, 0.03257547977019924),\n","  (4, 0.017662877398850998),\n","  (5, 0.04919936438088253),\n","  (6, 0.010817748441510818),\n","  (7, 0.02585258525852585),\n","  (8, 0.015462657376848796),\n","  (9, 0.015340422931182008),\n","  (10, 0.08513629140691847),\n","  (11, 0.005072729495171739),\n","  (12, 0.008739762865175407),\n","  (13, 0.12816281628162815),\n","  (14, 0.00825082508250825),\n","  (15, 0.010328810658843663),\n","  (16, 0.017785111844517785),\n","  (17, 0.23181762620706514),\n","  (18, 0.008128590636841461),\n","  (19, 0.2769221366581103)],\n"," [(0, 0.01111111111111111),\n","  (1, 0.0050712250712250705),\n","  (2, 0.008490028490028488),\n","  (3, 0.0041595441595441585),\n","  (4, 0.005299145299145299),\n","  (5, 0.003247863247863247),\n","  (6, 0.0364102564102564),\n","  (7, 0.2072364672364672),\n","  (8, 0.0038176638176638166),\n","  (9, 0.3235897435897435),\n","  (10, 0.04415954415954415),\n","  (11, 0.005413105413105412),\n","  (12, 0.19002849002848998),\n","  (13, 0.08769230769230768),\n","  (14, 0.0050712250712250705),\n","  (15, 0.007122507122507121),\n","  (16, 0.01088319088319088),\n","  (17, 0.028091168091168083),\n","  (18, 0.006780626780626779),\n","  (19, 0.0063247863247863235)],\n"," [(0, 0.023254170200735082),\n","  (1, 0.06538026576194514),\n","  (2, 0.014489680520214867),\n","  (3, 0.03046366977664687),\n","  (4, 0.00728018094430308),\n","  (5, 0.011945151258128353),\n","  (6, 0.015337856940910373),\n","  (7, 0.1598105739327113),\n","  (8, 0.006714729997172744),\n","  (9, 0.39192818772971433),\n","  (10, 0.01491376873056262),\n","  (11, 0.011662425784563188),\n","  (12, 0.033856375459428885),\n","  (13, 0.022405993780039577),\n","  (14, 0.04417585524455753),\n","  (15, 0.006007916313259823),\n","  (16, 0.015196494204127789),\n","  (17, 0.10213457732541699),\n","  (18, 0.009824710206389593),\n","  (19, 0.013217415889171611)],\n"," [(0, 0.005829278183292782),\n","  (1, 0.004511354420113544),\n","  (2, 0.014243714517437145),\n","  (3, 0.018805758313057582),\n","  (4, 0.015764395782643957),\n","  (5, 0.0038017031630170318),\n","  (6, 0.019211273317112733),\n","  (7, 0.04496147607461476),\n","  (8, 0.023976074614760746),\n","  (9, 0.006944444444444444),\n","  (10, 0.027727088402270884),\n","  (11, 0.005829278183292782),\n","  (12, 0.11572384428223845),\n","  (13, 0.12119829683698297),\n","  (14, 0.14502230332522303),\n","  (15, 0.014040957015409571),\n","  (16, 0.33460056772100566),\n","  (17, 0.004004460665044607),\n","  (18, 0.03563463098134631),\n","  (19, 0.038169099756691)],\n"," [(0, 0.03126273742561343),\n","  (1, 0.0264530855139806),\n","  (2, 0.2949783973261596),\n","  (3, 0.02376294122442325),\n","  (4, 0.016833781690714925),\n","  (5, 0.009741583109154642),\n","  (6, 0.0747941632020869),\n","  (7, 0.043001548870954594),\n","  (8, 0.0041167359582620034),\n","  (9, 0.004442814053965925),\n","  (10, 0.012594766446563953),\n","  (11, 0.09574468085106383),\n","  (12, 0.004442814053965925),\n","  (13, 0.29424472161082577),\n","  (14, 0.004687372625743866),\n","  (15, 0.014388195972935518),\n","  (16, 0.028164995516426185),\n","  (17, 0.003953696910410043),\n","  (18, 0.006399282628189451),\n","  (19, 0.00599168500855955)],\n"," [(0, 0.007477667369266285),\n","  (1, 0.016812205159088628),\n","  (2, 0.025042657833985748),\n","  (3, 0.008581752484191508),\n","  (4, 0.010488808591789621),\n","  (5, 0.005771354009836395),\n","  (6, 0.2510789922714042),\n","  (7, 0.004767640268995283),\n","  (8, 0.004667268894911172),\n","  (9, 0.0075780387433503965),\n","  (10, 0.013801063936565291),\n","  (11, 0.05113921509585466),\n","  (12, 0.03708722272407909),\n","  (13, 0.03337348188296698),\n","  (14, 0.01781591889992974),\n","  (15, 0.029960855164107198),\n","  (16, 0.009786208973200844),\n","  (17, 0.25258456288266584),\n","  (18, 0.023135601726387633),\n","  (19, 0.18904948308742348)],\n"," [(0, 0.013233321136723993),\n","  (1, 0.0068910842785705586),\n","  (2, 0.012501524576167827),\n","  (3, 0.018233930967191125),\n","  (4, 0.007622880839126723),\n","  (5, 0.006769118185144531),\n","  (6, 0.015062812538114406),\n","  (7, 0.176911818514453),\n","  (8, 0.01286742285644591),\n","  (9, 0.007013050371996586),\n","  (10, 0.034577387486278824),\n","  (11, 0.004573728503476034),\n","  (12, 0.13056470301256254),\n","  (13, 0.20411025734845717),\n","  (14, 0.11897792413708991),\n","  (15, 0.012135626295889745),\n","  (16, 0.18727893645566535),\n","  (17, 0.01030613489449933),\n","  (18, 0.009940236614221248),\n","  (19, 0.010428100987925359)],\n"," [(0, 0.007918968692449353),\n","  (1, 0.005463474524248004),\n","  (2, 0.010006138735420502),\n","  (3, 0.004726826273787599),\n","  (4, 0.008655616942909759),\n","  (5, 0.012829957028852052),\n","  (6, 0.27004297114794346),\n","  (7, 0.015653775322283608),\n","  (8, 0.0049723756906077336),\n","  (9, 0.24524248004910984),\n","  (10, 0.010497237569060772),\n","  (11, 0.005586249232658071),\n","  (12, 0.007918968692449353),\n","  (13, 0.008901166359729894),\n","  (14, 0.004604051565377532),\n","  (15, 0.10687538367096375),\n","  (16, 0.007673419275629219),\n","  (17, 0.2060773480662983),\n","  (18, 0.04511970534069981),\n","  (19, 0.011233885819521177)],\n"," [(0, 0.037775093934514226),\n","  (1, 0.02100107353730542),\n","  (2, 0.009997316156736446),\n","  (3, 0.009594739667203435),\n","  (4, 0.0073134728931830385),\n","  (5, 0.005703166935050993),\n","  (6, 0.13358829844337092),\n","  (7, 0.1640499194847021),\n","  (8, 0.006776704240472356),\n","  (9, 0.30401234567901236),\n","  (10, 0.010534084809447128),\n","  (11, 0.025966183574879228),\n","  (12, 0.09011003757380569),\n","  (13, 0.04582662372517445),\n","  (14, 0.022477187332259795),\n","  (15, 0.013486312399355877),\n","  (16, 0.044753086419753084),\n","  (17, 0.03012614063338701),\n","  (18, 0.0073134728931830385),\n","  (19, 0.009594739667203435)],\n"," [(0, 0.012141672589433784),\n","  (1, 0.004205164652925847),\n","  (2, 0.00906183368869936),\n","  (3, 0.0498104714522625),\n","  (4, 0.006811182184316512),\n","  (5, 0.00550817341862118),\n","  (6, 0.13272921108742003),\n","  (7, 0.03559583037194977),\n","  (8, 0.005034352049277422),\n","  (9, 0.01486614546316039),\n","  (10, 0.052653399668325045),\n","  (11, 0.004323619995261786),\n","  (12, 0.05289031035299692),\n","  (13, 0.16376451077943616),\n","  (14, 0.2683605780620706),\n","  (15, 0.06651267472162994),\n","  (16, 0.02351338545368396),\n","  (17, 0.005626628760957119),\n","  (18, 0.008469556977019664),\n","  (19, 0.078121298270552)],\n"," [(0, 0.017733280984164376),\n","  (1, 0.035270252584740217),\n","  (2, 0.014984949613924879),\n","  (3, 0.013676220390001308),\n","  (4, 0.046787069755267634),\n","  (5, 0.005169480434498102),\n","  (6, 0.009095668106268813),\n","  (7, 0.5123020547048815),\n","  (8, 0.004122497055359246),\n","  (9, 0.18472712995681193),\n","  (10, 0.0313440649129695),\n","  (11, 0.006739955503206387),\n","  (12, 0.015770187148279022),\n","  (13, 0.03160581075775422),\n","  (14, 0.006739955503206387),\n","  (15, 0.011582253631723596),\n","  (16, 0.027025258474021725),\n","  (17, 0.010011778563015312),\n","  (18, 0.006739955503206387),\n","  (19, 0.008572176416699384)],\n"," [(0, 0.01795580110497238),\n","  (1, 0.004655207693881728),\n","  (2, 0.04895641497851444),\n","  (3, 0.010180069572334768),\n","  (4, 0.009156947002250872),\n","  (5, 0.005780642520974014),\n","  (6, 0.02644771843666872),\n","  (7, 0.0296193984039288),\n","  (8, 0.004245958665848169),\n","  (9, 0.049877225291589944),\n","  (10, 0.012328626969510951),\n","  (11, 0.23946183752813593),\n","  (12, 0.08435645590341724),\n","  (13, 0.015602619193779418),\n","  (14, 0.009975445058317989),\n","  (15, 0.008338448946183755),\n","  (16, 0.030847145488029474),\n","  (17, 0.1933190096173522),\n","  (18, 0.006394516063024352),\n","  (19, 0.1925005115612851)],\n"," [(0, 0.007189542483660132),\n","  (1, 0.006485671191553547),\n","  (2, 0.06551030668677729),\n","  (3, 0.013423831070889897),\n","  (4, 0.2904474610356964),\n","  (5, 0.006787330316742083),\n","  (6, 0.019255907491201613),\n","  (7, 0.26480643539467075),\n","  (8, 0.005178481649069885),\n","  (9, 0.005882352941176472),\n","  (10, 0.00497737556561086),\n","  (11, 0.011211664152840625),\n","  (12, 0.01875314228255405),\n","  (13, 0.16153846153846158),\n","  (14, 0.0076923076923076945),\n","  (15, 0.00970336852689794),\n","  (16, 0.07606837606837608),\n","  (17, 0.007390648567119157),\n","  (18, 0.009401709401709403),\n","  (19, 0.008295625942684768)],\n"," [(0, 0.017508100763039616),\n","  (1, 0.04134002299571443),\n","  (2, 0.009982230584300198),\n","  (3, 0.05471934775791784),\n","  (4, 0.01280443190132748),\n","  (5, 0.01228180202780391),\n","  (6, 0.004128776000836208),\n","  (7, 0.00391972405142678),\n","  (8, 0.26356224521793664),\n","  (9, 0.004337827950245636),\n","  (10, 0.022316295599456464),\n","  (11, 0.010609386432528484),\n","  (12, 0.028587854081739313),\n","  (13, 0.02022577610536218),\n","  (14, 0.006323821469635204),\n","  (15, 0.0070555032925682035),\n","  (16, 0.030887425525243022),\n","  (17, 0.00569666562140692),\n","  (18, 0.3864847914706805),\n","  (19, 0.05722797115083098)],\n"," [(0, 0.06730324074074076),\n","  (1, 0.005266203703703704),\n","  (2, 0.01950231481481482),\n","  (3, 0.005960648148148149),\n","  (4, 0.009780092592592594),\n","  (5, 0.007581018518518519),\n","  (6, 0.07876157407407408),\n","  (7, 0.10306712962962965),\n","  (8, 0.01961805555555556),\n","  (9, 0.021932870370370373),\n","  (10, 0.018807870370370374),\n","  (11, 0.018807870370370374),\n","  (12, 0.03940972222222223),\n","  (13, 0.11672453703703706),\n","  (14, 0.011747685185185187),\n","  (15, 0.08894675925925927),\n","  (16, 0.08061342592592594),\n","  (17, 0.19369212962962964),\n","  (18, 0.07563657407407409),\n","  (19, 0.01684027777777778)],\n"," [(0, 0.014773965141612203),\n","  (1, 0.005242374727668846),\n","  (2, 0.015318627450980393),\n","  (3, 0.020765250544662314),\n","  (4, 0.010688997821350764),\n","  (5, 0.009191176470588237),\n","  (6, 0.028662854030501093),\n","  (7, 0.009463507625272333),\n","  (8, 0.014910130718954251),\n","  (9, 0.005514705882352942),\n","  (10, 0.03737745098039216),\n","  (11, 0.02430555555555556),\n","  (12, 0.07141884531590416),\n","  (13, 0.027301198257080612),\n","  (14, 0.06447440087145971),\n","  (15, 0.013684640522875819),\n","  (16, 0.005514705882352942),\n","  (17, 0.050449346405228766),\n","  (18, 0.5262118736383443),\n","  (19, 0.04473039215686275)],\n"," [(0, 0.0069944044764188645),\n","  (1, 0.009126032507327471),\n","  (2, 0.009392486011191047),\n","  (3, 0.027111644018118837),\n","  (4, 0.012589928057553957),\n","  (5, 0.009392486011191047),\n","  (6, 0.016586730615507594),\n","  (7, 0.009259259259259259),\n","  (8, 0.018185451638689048),\n","  (9, 0.00792699173994138),\n","  (10, 0.05642152944311218),\n","  (11, 0.006461497468691713),\n","  (12, 0.015920596855848654),\n","  (13, 0.005795363709032774),\n","  (14, 0.07294164668265388),\n","  (15, 0.008193445243804956),\n","  (16, 0.006861177724487077),\n","  (17, 0.007260857980282441),\n","  (18, 0.6630029309885425),\n","  (19, 0.030575539568345324)],\n"," [(0, 0.021808510638297876),\n","  (1, 0.06648936170212767),\n","  (2, 0.027718676122931444),\n","  (3, 0.05620567375886525),\n","  (4, 0.006914893617021277),\n","  (5, 0.029137115839243503),\n","  (6, 0.031146572104018916),\n","  (7, 0.06542553191489363),\n","  (8, 0.010579196217494091),\n","  (9, 0.010579196217494091),\n","  (10, 0.042730496453900715),\n","  (11, 0.027836879432624115),\n","  (12, 0.12228132387706857),\n","  (13, 0.17913711583924352),\n","  (14, 0.02641843971631206),\n","  (15, 0.008806146572104021),\n","  (16, 0.02381796690307329),\n","  (17, 0.010579196217494091),\n","  (18, 0.21991725768321516),\n","  (19, 0.012470449172576835)],\n"," [(0, 0.005222776853392494),\n","  (1, 0.010861527084488815),\n","  (2, 0.010306895914217046),\n","  (3, 0.047189868737289704),\n","  (4, 0.004760584211499353),\n","  (5, 0.04321501201700869),\n","  (6, 0.01160103531151784),\n","  (7, 0.03877796265483453),\n","  (8, 0.015113699389905713),\n","  (9, 0.009659826215566648),\n","  (10, 0.012525420595304123),\n","  (11, 0.011508596783139213),\n","  (12, 0.04182843409132927),\n","  (13, 0.005222776853392494),\n","  (14, 0.020752449621002034),\n","  (15, 0.008827879460158995),\n","  (16, 0.009474949158809393),\n","  (17, 0.15876317249029395),\n","  (18, 0.5136346829358477),\n","  (19, 0.020752449621002034)],\n"," [(0, 0.011935763888888888),\n","  (1, 0.17961516203703703),\n","  (2, 0.01077835648148148),\n","  (3, 0.1984230324074074),\n","  (4, 0.007595486111111111),\n","  (5, 0.1414207175925926),\n","  (6, 0.005714699074074074),\n","  (7, 0.009910300925925927),\n","  (8, 0.03334780092592592),\n","  (9, 0.007450810185185185),\n","  (10, 0.017578125),\n","  (11, 0.011067708333333334),\n","  (12, 0.005135995370370371),\n","  (13, 0.031322337962962965),\n","  (14, 0.007161458333333333),\n","  (15, 0.014973958333333334),\n","  (16, 0.009620949074074073),\n","  (17, 0.043041087962962965),\n","  (18, 0.2474681712962963),\n","  (19, 0.006438078703703704)],\n"," [(0, 0.030909461248599647),\n","  (1, 0.010948161727263468),\n","  (2, 0.010337101537834808),\n","  (3, 0.014410836134025865),\n","  (4, 0.008911294429167937),\n","  (5, 0.005855993482024645),\n","  (6, 0.008300234239739279),\n","  (7, 0.009827884713310926),\n","  (8, 0.05290762806803136),\n","  (9, 0.009318667888787043),\n","  (10, 0.021234341582645887),\n","  (11, 0.024900702719217835),\n","  (12, 0.20597820551991033),\n","  (13, 0.013901619309501982),\n","  (14, 0.14466849984723493),\n","  (15, 0.0072818005906915145),\n","  (16, 0.030400244424075764),\n","  (17, 0.071748650575415),\n","  (18, 0.3013035950707811),\n","  (19, 0.0168550768917405)],\n"," [(0, 0.03705779770802192),\n","  (1, 0.027092675635276533),\n","  (2, 0.038054309915296465),\n","  (3, 0.010650224215246636),\n","  (4, 0.013141504733432985),\n","  (5, 0.004920279023418037),\n","  (6, 0.044656203288490286),\n","  (7, 0.020490782262082712),\n","  (8, 0.01002740408570005),\n","  (9, 0.008034379671150971),\n","  (10, 0.0325734927752865),\n","  (11, 0.007536123567513702),\n","  (12, 0.37400348779272546),\n","  (13, 0.12387892376681614),\n","  (14, 0.01912057797708022),\n","  (15, 0.008781763826606876),\n","  (16, 0.10818385650224215),\n","  (17, 0.013390632785251618),\n","  (18, 0.0805306427503737),\n","  (19, 0.017874937717987045)],\n"," [(0, 0.011904761904761906),\n","  (1, 0.006613756613756614),\n","  (2, 0.010846560846560849),\n","  (3, 0.04470899470899472),\n","  (4, 0.006261022927689595),\n","  (5, 0.00908289241622575),\n","  (6, 0.06940035273368608),\n","  (7, 0.018783068783068787),\n","  (8, 0.015255731922398591),\n","  (9, 0.005908289241622576),\n","  (10, 0.03465608465608466),\n","  (11, 0.012610229276895946),\n","  (12, 0.008906525573192242),\n","  (13, 0.0281305114638448),\n","  (14, 0.019664902998236335),\n","  (15, 0.11366843033509702),\n","  (16, 0.010846560846560849),\n","  (17, 0.012433862433862436),\n","  (18, 0.5528218694885363),\n","  (19, 0.007495590828924163)],\n"," [(0, 0.008890993265993267),\n","  (1, 0.010784932659932661),\n","  (2, 0.012889309764309766),\n","  (3, 0.038773148148148154),\n","  (4, 0.008049242424242426),\n","  (5, 0.019307659932659937),\n","  (6, 0.0065761784511784525),\n","  (7, 0.0053135521885521895),\n","  (8, 0.014046717171717174),\n","  (9, 0.004892676767676769),\n","  (10, 0.04645412457912459),\n","  (11, 0.0049978956228956236),\n","  (12, 0.030250420875420878),\n","  (13, 0.01541456228956229),\n","  (14, 0.0065761784511784525),\n","  (15, 0.0053135521885521895),\n","  (16, 0.04855850168350169),\n","  (17, 0.007102272727272728),\n","  (18, 0.694497053872054),\n","  (19, 0.011311026936026938)],\n"," [(0, 0.00918737060041408),\n","  (1, 0.012551759834368533),\n","  (2, 0.06741718426501037),\n","  (3, 0.5412784679089028),\n","  (4, 0.01574361628709455),\n","  (5, 0.015571083505866118),\n","  (6, 0.011775362318840583),\n","  (7, 0.011085231193926848),\n","  (8, 0.005219116632160111),\n","  (9, 0.013414423740510701),\n","  (10, 0.03187543133195308),\n","  (11, 0.02997757073844031),\n","  (12, 0.03834541062801933),\n","  (13, 0.01712387853692202),\n","  (14, 0.014622153209109734),\n","  (15, 0.046368184955141487),\n","  (16, 0.007979641131815046),\n","  (17, 0.064484126984127),\n","  (18, 0.02350759144237406),\n","  (19, 0.022472394755003455)],\n"," [(0, 0.010127016821146584),\n","  (1, 0.00520654537132395),\n","  (2, 0.013903192584963954),\n","  (3, 0.03976427508868292),\n","  (4, 0.014246481290765533),\n","  (5, 0.007838425449136056),\n","  (6, 0.011042453369950795),\n","  (7, 0.3817942556356563),\n","  (8, 0.006694129763130793),\n","  (9, 0.09686462982034558),\n","  (10, 0.017107220505778693),\n","  (11, 0.0060075523515276345),\n","  (12, 0.013331044741961322),\n","  (13, 0.03804783155967502),\n","  (14, 0.00966929854674448),\n","  (15, 0.14835793569058245),\n","  (16, 0.019166952740588168),\n","  (17, 0.1081931571117977),\n","  (18, 0.013216615173360797),\n","  (19, 0.039420986382881336)],\n"," [(0, 0.006256656017039404),\n","  (1, 0.18073304934327297),\n","  (2, 0.050630102946396874),\n","  (3, 0.10050585729499467),\n","  (4, 0.015575079872204472),\n","  (5, 0.005724174653887114),\n","  (6, 0.016640042598509052),\n","  (7, 0.03687433439829606),\n","  (8, 0.00457046503372382),\n","  (9, 0.008919062832800853),\n","  (10, 0.07379304224352147),\n","  (11, 0.015131345402910898),\n","  (12, 0.009274050408235712),\n","  (13, 0.12633120340788073),\n","  (14, 0.007055378061767838),\n","  (15, 0.2876730564430245),\n","  (16, 0.017971246006389777),\n","  (17, 0.009274050408235712),\n","  (18, 0.016018814341498048),\n","  (19, 0.01104898828541001)],\n"," [(0, 0.005683025281495646),\n","  (1, 0.034363713618015726),\n","  (2, 0.08014659018483111),\n","  (3, 0.12019332908434248),\n","  (4, 0.03298279158699809),\n","  (5, 0.03478861270448269),\n","  (6, 0.022785213511790952),\n","  (7, 0.007807520713830466),\n","  (8, 0.006001699596345869),\n","  (9, 0.06527512215848737),\n","  (10, 0.02459103462927555),\n","  (11, 0.19253239855534313),\n","  (12, 0.007701295942213725),\n","  (13, 0.01226896112173359),\n","  (14, 0.015561929041852562),\n","  (15, 0.26784576163161256),\n","  (16, 0.03595708519226684),\n","  (17, 0.02352878691310814),\n","  (18, 0.0051519014234119405),\n","  (19, 0.004833227108561718)],\n"," [(0, 0.05046403712296984),\n","  (1, 0.004833720030935808),\n","  (2, 0.008958494457334364),\n","  (3, 0.005478216035060582),\n","  (4, 0.021977313740654807),\n","  (5, 0.010118587264758958),\n","  (6, 0.06915442124258829),\n","  (7, 0.12174529517916989),\n","  (8, 0.004575921629285899),\n","  (9, 0.14700953854086105),\n","  (10, 0.010763083268883733),\n","  (11, 0.010891982469708688),\n","  (12, 0.0337071410157257),\n","  (13, 0.023781902552204175),\n","  (14, 0.012825470482083011),\n","  (15, 0.3974606857437484),\n","  (16, 0.0063805104408352666),\n","  (17, 0.011278680072183552),\n","  (18, 0.006251611240010312),\n","  (19, 0.04234338747099768)],\n"," [(0, 0.01888796527200517),\n","  (1, 0.004941350327883994),\n","  (2, 0.011499030202272097),\n","  (3, 0.037822111388196175),\n","  (4, 0.005310797081370647),\n","  (5, 0.00457190357439734),\n","  (6, 0.021658815923155075),\n","  (7, 0.09970444259721067),\n","  (8, 0.003740648379052369),\n","  (9, 0.01741017825805856),\n","  (10, 0.014546965918536992),\n","  (11, 0.005495520458113974),\n","  (12, 0.044010344509097625),\n","  (13, 0.033573473723099656),\n","  (14, 0.00808164773252055),\n","  (15, 0.5210584649487393),\n","  (16, 0.0042948185092823495),\n","  (17, 0.13286228872263786),\n","  (18, 0.004017733444167359),\n","  (19, 0.006511499030202272)],\n"," [(0, 0.004805661174482467),\n","  (1, 0.007234896493451624),\n","  (2, 0.015050697084917613),\n","  (3, 0.2417089142374313),\n","  (4, 0.008607942543303758),\n","  (5, 0.004805661174482467),\n","  (6, 0.10609421208280521),\n","  (7, 0.020120405576679337),\n","  (8, 0.00829108576256865),\n","  (9, 0.005650612589776087),\n","  (10, 0.0071292775665399225),\n","  (11, 0.023288973384030416),\n","  (12, 0.005967469370511194),\n","  (13, 0.025929446556822975),\n","  (14, 0.005333755809040979),\n","  (15, 0.44196239966201933),\n","  (16, 0.01325517532741867),\n","  (17, 0.04018800168990282),\n","  (18, 0.006073088297422896),\n","  (19, 0.008502323616392056)],\n"," [(0, 0.04151819457941907),\n","  (1, 0.006424792139077854),\n","  (2, 0.011715797430083144),\n","  (3, 0.1712018140589569),\n","  (4, 0.008692365835222978),\n","  (5, 0.007504589137242198),\n","  (6, 0.040654356980887596),\n","  (7, 0.06797322103444553),\n","  (8, 0.006640751538710722),\n","  (9, 0.01614296512255696),\n","  (10, 0.07391210452434942),\n","  (11, 0.0048050966418313355),\n","  (12, 0.3517438721520354),\n","  (13, 0.07455998272324803),\n","  (14, 0.01592700572292409),\n","  (15, 0.03460749379116726),\n","  (16, 0.007180650037792895),\n","  (17, 0.02305366591080877),\n","  (18, 0.022189828312277293),\n","  (19, 0.01355145232696253)],\n"," [(0, 0.011784511784511786),\n","  (1, 0.2290794120062413),\n","  (2, 0.030015603186334897),\n","  (3, 0.04381210478771455),\n","  (4, 0.04151268785415128),\n","  (5, 0.12601626016260165),\n","  (6, 0.00858175248419151),\n","  (7, 0.0069393118173605986),\n","  (8, 0.009567216884290056),\n","  (9, 0.012523610084585695),\n","  (10, 0.03707809805370782),\n","  (11, 0.2260408967726041),\n","  (12, 0.02057156935205716),\n","  (13, 0.012195121951219514),\n","  (14, 0.01802578631846925),\n","  (15, 0.004393528783772687),\n","  (16, 0.010142071117680876),\n","  (17, 0.10688182639402152),\n","  (18, 0.019175494785250886),\n","  (19, 0.025663135419232985)],\n"," [(0, 0.006451930654058312),\n","  (1, 0.21675531914893612),\n","  (2, 0.011672576832151298),\n","  (3, 0.023886918833727337),\n","  (4, 0.005072892040977147),\n","  (5, 0.015809692671394794),\n","  (6, 0.10938731284475962),\n","  (7, 0.0203408195429472),\n","  (8, 0.007141449960598895),\n","  (9, 0.0038908589440504325),\n","  (10, 0.02359141055949566),\n","  (11, 0.25753546099290775),\n","  (12, 0.0034968479117415277),\n","  (13, 0.07904846335697398),\n","  (14, 0.005762411347517729),\n","  (15, 0.16021473601260833),\n","  (16, 0.012460598896769107),\n","  (17, 0.0038908589440504325),\n","  (18, 0.025955476753349086),\n","  (19, 0.007633963750985026)],\n"," [(0, 0.006928697404887882),\n","  (1, 0.1051902242378433),\n","  (2, 0.13744016124968508),\n","  (3, 0.018434534307550186),\n","  (4, 0.023389602754682125),\n","  (5, 0.009700176366843035),\n","  (6, 0.0074326026706979095),\n","  (7, 0.032543881750230964),\n","  (8, 0.005165028974552785),\n","  (9, 0.005332997396489461),\n","  (10, 0.048416897623246836),\n","  (11, 0.3909884941630974),\n","  (12, 0.00701268161585622),\n","  (13, 0.02515327118501722),\n","  (14, 0.044721592340639965),\n","  (15, 0.04195011337868482),\n","  (16, 0.03313177122700933),\n","  (17, 0.0284286554127824),\n","  (18, 0.01809859746367683),\n","  (19, 0.010540018476526416)],\n"," [(0, 0.00962778699097107),\n","  (1, 0.257831214298876),\n","  (2, 0.0890455131748664),\n","  (3, 0.017090473558135248),\n","  (4, 0.013773723972728948),\n","  (5, 0.26197715128063387),\n","  (6, 0.052561267735397085),\n","  (7, 0.009074995393403354),\n","  (8, 0.027409250046065967),\n","  (9, 0.009167127326331306),\n","  (10, 0.06288004422332781),\n","  (11, 0.0138658559056569),\n","  (12, 0.014050119771512807),\n","  (13, 0.02068361894232541),\n","  (14, 0.005942509673852957),\n","  (15, 0.0105491063202506),\n","  (16, 0.06011608623548922),\n","  (17, 0.03957066519255574),\n","  (18, 0.014142251704440759),\n","  (19, 0.010641238253178552)],\n"," [(0, 0.010943466517649249),\n","  (1, 0.006566079910589548),\n","  (2, 0.012154233025984909),\n","  (3, 0.007869982304181799),\n","  (4, 0.008428797615721335),\n","  (5, 0.502700940672441),\n","  (6, 0.007590574648412032),\n","  (7, 0.015320853124708947),\n","  (8, 0.06431032876967495),\n","  (9, 0.011595417714445375),\n","  (10, 0.1390053087454596),\n","  (11, 0.012154233025984909),\n","  (12, 0.07008475365558348),\n","  (13, 0.005634721058023656),\n","  (14, 0.016717891403557786),\n","  (15, 0.00507590574648412),\n","  (16, 0.079491478066499),\n","  (17, 0.01075719474713607),\n","  (18, 0.008615069386234514),\n","  (19, 0.00498276986122753)],\n"," [(0, 0.007466583124477861),\n","  (1, 0.041614452798663326),\n","  (2, 0.03952589807852966),\n","  (3, 0.024070593149540517),\n","  (4, 0.010808270676691729),\n","  (5, 0.04881996658312448),\n","  (6, 0.06699039264828739),\n","  (7, 0.008928571428571428),\n","  (8, 0.006526733500417711),\n","  (9, 0.13622598162071847),\n","  (10, 0.006317878028404344),\n","  (11, 0.218828320802005),\n","  (12, 0.05414578111946533),\n","  (13, 0.24075814536340853),\n","  (14, 0.008406432748538011),\n","  (15, 0.005482456140350877),\n","  (16, 0.01237468671679198),\n","  (17, 0.04046574770258981),\n","  (18, 0.004124895572263993),\n","  (19, 0.018118212197159565)],\n"," [(0, 0.017735257916526803),\n","  (1, 0.004084144567528254),\n","  (2, 0.023329976502181944),\n","  (3, 0.00945507440975719),\n","  (4, 0.007552870090634442),\n","  (5, 0.014714109880273024),\n","  (6, 0.012700011189437173),\n","  (7, 0.014602215508559922),\n","  (8, 0.008000447577486854),\n","  (9, 0.009007496922904779),\n","  (10, 0.019525567863936447),\n","  (11, 0.3162694416470852),\n","  (12, 0.03261720935436948),\n","  (13, 0.14898735593599644),\n","  (14, 0.010126440640035808),\n","  (15, 0.015273581738838539),\n","  (16, 0.04817052702249078),\n","  (17, 0.2218305919212264),\n","  (18, 0.010014546268322705),\n","  (19, 0.056003133042407977)],\n"," [(0, 0.005839336389434621),\n","  (1, 0.05779305828421742),\n","  (2, 0.004529578694608164),\n","  (3, 0.008240558829949793),\n","  (4, 0.005184457542021393),\n","  (5, 0.0054027504911591355),\n","  (6, 0.10123335516262824),\n","  (7, 0.022320454049334207),\n","  (8, 0.006275922287710107),\n","  (9, 0.014571054354944336),\n","  (10, 0.016317397948046278),\n","  (11, 0.3426653569089718),\n","  (12, 0.020574110456232263),\n","  (13, 0.26735428945645057),\n","  (14, 0.00496616459288365),\n","  (15, 0.007476533507967693),\n","  (16, 0.012497271338135779),\n","  (17, 0.0834424798079022),\n","  (18, 0.00573018991486575),\n","  (19, 0.007585679982536564)],\n"," [(0, 0.023559555055618048),\n","  (1, 0.012435945506811649),\n","  (2, 0.044181977252843396),\n","  (3, 0.022809648793900763),\n","  (4, 0.008686414198225223),\n","  (5, 0.0064366954130733655),\n","  (6, 0.021684789401324834),\n","  (7, 0.02218472690913636),\n","  (8, 0.008936382952130983),\n","  (9, 0.03405824271966004),\n","  (10, 0.009186351706036745),\n","  (11, 0.3737657792775903),\n","  (12, 0.14891888513935758),\n","  (13, 0.0140607424071991),\n","  (14, 0.0074365704286964126),\n","  (15, 0.031558555180602424),\n","  (16, 0.05793025871766029),\n","  (17, 0.07467816522934634),\n","  (18, 0.01968503937007874),\n","  (19, 0.05780527434070741)],\n"," [(0, 0.008886946386946384),\n","  (1, 0.016268453768453765),\n","  (2, 0.008304195804195802),\n","  (3, 0.00937257187257187),\n","  (4, 0.02704933954933954),\n","  (5, 0.006653069153069151),\n","  (6, 0.04598873348873348),\n","  (7, 0.13184731934731933),\n","  (8, 0.005778943278943278),\n","  (9, 0.12485431235431232),\n","  (10, 0.028991841491841485),\n","  (11, 0.2527680652680652),\n","  (12, 0.025980963480963473),\n","  (13, 0.16778360528360525),\n","  (14, 0.005681818181818181),\n","  (15, 0.06123737373737372),\n","  (16, 0.01811383061383061),\n","  (17, 0.005099067599067598),\n","  (18, 0.006458818958818957),\n","  (19, 0.04288073038073038)],\n"," [(0, 0.008147419072615923),\n","  (1, 0.02575459317585302),\n","  (2, 0.02334864391951006),\n","  (3, 0.011318897637795276),\n","  (4, 0.019083552055993002),\n","  (5, 0.012849956255468066),\n","  (6, 0.008584864391951006),\n","  (7, 0.1272419072615923),\n","  (8, 0.0065069991251093615),\n","  (9, 0.011865704286964129),\n","  (10, 0.022583114610673666),\n","  (11, 0.33666885389326334),\n","  (12, 0.023895450568678915),\n","  (13, 0.010225284339457567),\n","  (14, 0.022692475940507436),\n","  (15, 0.042705599300087486),\n","  (16, 0.00781933508311461),\n","  (17, 0.24907042869641294),\n","  (18, 0.013943569553805775),\n","  (19, 0.015693350831146105)],\n"," [(0, 0.01192693092134433),\n","  (1, 0.008734592533475215),\n","  (2, 0.044293695131683956),\n","  (3, 0.041544737075463335),\n","  (4, 0.006961071206881263),\n","  (5, 0.010685465992728562),\n","  (6, 0.06601933138245987),\n","  (7, 0.020528509355325),\n","  (8, 0.00261594395672608),\n","  (9, 0.004389465283320032),\n","  (10, 0.010242085661080074),\n","  (11, 0.4373060211049038),\n","  (12, 0.005542254145606101),\n","  (13, 0.08393189678105879),\n","  (14, 0.010685465992728562),\n","  (15, 0.11284029440454021),\n","  (16, 0.00891194466613461),\n","  (17, 0.02824332712600869),\n","  (18, 0.006783719074221868),\n","  (19, 0.07781324820430965)],\n"," [(0, 0.015094123048668502),\n","  (1, 0.00809228650137741),\n","  (2, 0.02094811753902663),\n","  (3, 0.004878328741965106),\n","  (4, 0.03173783287419651),\n","  (5, 0.004878328741965106),\n","  (6, 0.23995638200183655),\n","  (7, 0.011765381083562901),\n","  (8, 0.004074839302112029),\n","  (9, 0.009010560146923783),\n","  (10, 0.01314279155188246),\n","  (11, 0.04654499540863177),\n","  (12, 0.051939853076216715),\n","  (13, 0.10244490358126722),\n","  (14, 0.020144628099173553),\n","  (15, 0.11725206611570248),\n","  (16, 0.017389807162534434),\n","  (17, 0.24707300275482094),\n","  (18, 0.018652433425160698),\n","  (19, 0.014979338842975207)],\n"," [(0, 0.04086510434883993),\n","  (1, 0.01754692783024368),\n","  (2, 0.019295791069138396),\n","  (3, 0.008686020753177103),\n","  (4, 0.012300338113559522),\n","  (5, 0.007986475457619217),\n","  (6, 0.15908825929812292),\n","  (7, 0.0360848781625277),\n","  (8, 0.004721930745015741),\n","  (9, 0.013349656056896352),\n","  (10, 0.008103066340212198),\n","  (11, 0.1287746298239478),\n","  (12, 0.0764253235396992),\n","  (13, 0.006703975749096421),\n","  (14, 0.017080564299871752),\n","  (15, 0.023726244607671684),\n","  (16, 0.01568147370875598),\n","  (17, 0.3828261629940539),\n","  (18, 0.014981928413198089),\n","  (19, 0.005771248688352572)],\n"," [(0, 0.007599858607281726),\n","  (1, 0.04530458348061742),\n","  (2, 0.01584776717332391),\n","  (3, 0.06286084599976435),\n","  (4, 0.006892895015906682),\n","  (5, 0.009602922116177685),\n","  (6, 0.06981265464828562),\n","  (7, 0.03163662071403323),\n","  (8, 0.01030988570755273),\n","  (9, 0.03505361140567928),\n","  (10, 0.17738894780252154),\n","  (11, 0.08960763520678687),\n","  (12, 0.061800400612701784),\n","  (13, 0.03246141157063745),\n","  (14, 0.00948509485094851),\n","  (15, 0.01266643101213621),\n","  (16, 0.05614469188170143),\n","  (17, 0.061446918817014265),\n","  (18, 0.1884647107340639),\n","  (19, 0.01561211264286556)],\n"," [(0, 0.006885890949971894),\n","  (1, 0.0066985197676597335),\n","  (2, 0.01438073824245831),\n","  (3, 0.009040659546561738),\n","  (4, 0.012600712010492785),\n","  (5, 0.004262694397601649),\n","  (6, 0.16708825182686904),\n","  (7, 0.03958216226344388),\n","  (8, 0.003419524077196927),\n","  (9, 0.19191493348323027),\n","  (10, 0.0066985197676597335),\n","  (11, 0.09663668727749672),\n","  (12, 0.015223908562863032),\n","  (13, 0.05953719317968897),\n","  (14, 0.016441821247892074),\n","  (15, 0.040800074948472924),\n","  (16, 0.006979576541127974),\n","  (17, 0.27623196552370244),\n","  (18, 0.008759602773093499),\n","  (19, 0.016816563612516395)],\n"," [(0, 0.06318756875687567),\n","  (1, 0.2063393839383938),\n","  (2, 0.010794829482948293),\n","  (3, 0.009144664466446644),\n","  (4, 0.013820132013201317),\n","  (5, 0.02303355335533553),\n","  (6, 0.011344884488448843),\n","  (7, 0.0343096809680968),\n","  (8, 0.027709020902090202),\n","  (9, 0.01478272827282728),\n","  (10, 0.18942519251925186),\n","  (11, 0.016570407040704067),\n","  (12, 0.049711221122112195),\n","  (13, 0.012857535753575355),\n","  (14, 0.012857535753575355),\n","  (15, 0.05974972497249723),\n","  (16, 0.014095159515951592),\n","  (17, 0.2166529152915291),\n","  (18, 0.004194169416941693),\n","  (19, 0.009419691969196918)],\n"," [(0, 0.00617283950617284),\n","  (1, 0.006455564979738008),\n","  (2, 0.008434643294694188),\n","  (3, 0.009000094241824524),\n","  (4, 0.02125153142964848),\n","  (5, 0.0057016303835642265),\n","  (6, 0.010413721609650364),\n","  (7, 0.04584864762981812),\n","  (8, 0.0030628592969559896),\n","  (9, 0.005607388559042504),\n","  (10, 0.6185562152483273),\n","  (11, 0.0050419376119121674),\n","  (12, 0.05725190839694657),\n","  (13, 0.06761850909433607),\n","  (14, 0.058382810291207246),\n","  (15, 0.012392799924606542),\n","  (16, 0.03519932145886345),\n","  (17, 0.006078597681651117),\n","  (18, 0.007586466873998682),\n","  (19, 0.009942512487041751)],\n"," [(0, 0.005276225946617007),\n","  (1, 0.012725015518311605),\n","  (2, 0.3610446040613638),\n","  (3, 0.010508113860069166),\n","  (4, 0.008379888268156423),\n","  (5, 0.004655493482309124),\n","  (6, 0.1198457036445863),\n","  (7, 0.0238981998758535),\n","  (8, 0.004832845614968519),\n","  (9, 0.008911944666134609),\n","  (10, 0.02531701693712866),\n","  (11, 0.003946084951671543),\n","  (12, 0.003414028553693357),\n","  (13, 0.14405426975259375),\n","  (14, 0.006872395140551563),\n","  (15, 0.19530903609115893),\n","  (16, 0.0050101977476279145),\n","  (17, 0.004744169548638821),\n","  (18, 0.021503946084951667),\n","  (19, 0.029750820253613544)],\n"," [(0, 0.0561300700907733),\n","  (1, 0.004883373549350799),\n","  (2, 0.022693324141100774),\n","  (3, 0.04946570148224751),\n","  (4, 0.014420314833965301),\n","  (5, 0.004423761921176606),\n","  (6, 0.016258761346662074),\n","  (7, 0.10864069860967483),\n","  (8, 0.008904975295874987),\n","  (9, 0.045903711363897516),\n","  (10, 0.01062851890152821),\n","  (11, 0.004423761921176606),\n","  (12, 0.012352062507181433),\n","  (13, 0.0858899230150523),\n","  (14, 0.012466965414224982),\n","  (15, 0.1974606457543376),\n","  (16, 0.01258186832126853),\n","  (17, 0.13219579455360222),\n","  (18, 0.07796162242904747),\n","  (19, 0.12231414454785708)],\n"," [(0, 0.02159483542462266),\n","  (1, 0.004591743953446081),\n","  (2, 0.01104746317512275),\n","  (3, 0.01332060374613566),\n","  (4, 0.009410801963993453),\n","  (5, 0.012411347517730497),\n","  (6, 0.34456264775413714),\n","  (7, 0.2367248590652846),\n","  (8, 0.005046372067648663),\n","  (9, 0.01977632296781233),\n","  (10, 0.032960538279687215),\n","  (11, 0.0049554464448081465),\n","  (12, 0.010501909438079651),\n","  (13, 0.06314784506273868),\n","  (14, 0.008228768867066739),\n","  (15, 0.12052191307510457),\n","  (16, 0.013502454991816694),\n","  (17, 0.006046553918894345),\n","  (18, 0.01332060374613566),\n","  (19, 0.0483269685397345)],\n"," [(0, 0.012873682391567305),\n","  (1, 0.011145671332296526),\n","  (2, 0.06868843960601348),\n","  (3, 0.017193710039744254),\n","  (4, 0.01581130119232763),\n","  (5, 0.03136340072576464),\n","  (6, 0.33618455158113014),\n","  (7, 0.16372904786590634),\n","  (8, 0.015465698980473475),\n","  (9, 0.035683428373941596),\n","  (10, 0.04259547261102471),\n","  (11, 0.017712113357525486),\n","  (12, 0.04484188698807672),\n","  (13, 0.03637463279764991),\n","  (14, 0.007344047001900812),\n","  (15, 0.01339208570934854),\n","  (16, 0.02255054432348367),\n","  (17, 0.02687057197166062),\n","  (18, 0.009590461378952825),\n","  (19, 0.07058925177121134)],\n"," [(0, 0.01587919695396331),\n","  (1, 0.0054084458290065765),\n","  (2, 0.1995067497403946),\n","  (3, 0.00661993769470405),\n","  (4, 0.4486413984077535),\n","  (5, 0.011552440290758048),\n","  (6, 0.07221356870889581),\n","  (7, 0.040628245067497404),\n","  (8, 0.004283489096573208),\n","  (9, 0.008004499826929733),\n","  (10, 0.014927310488058152),\n","  (11, 0.0140619591554171),\n","  (12, 0.016311872620283836),\n","  (13, 0.06979058497750086),\n","  (14, 0.0053219106957424715),\n","  (15, 0.009648667358947732),\n","  (16, 0.017956040152301834),\n","  (17, 0.006793007961232261),\n","  (18, 0.020811699550017305),\n","  (19, 0.011638975424022154)],\n"," [(0, 0.010716049382716052),\n","  (1, 0.007950617283950618),\n","  (2, 0.05160493827160494),\n","  (3, 0.010617283950617286),\n","  (4, 0.04597530864197531),\n","  (5, 0.00834567901234568),\n","  (6, 0.2377777777777778),\n","  (7, 0.07096296296296298),\n","  (8, 0.00814814814814815),\n","  (9, 0.36943209876543215),\n","  (10, 0.03807407407407408),\n","  (11, 0.00834567901234568),\n","  (12, 0.02651851851851852),\n","  (13, 0.020000000000000004),\n","  (14, 0.008740740740740742),\n","  (15, 0.006962962962962964),\n","  (16, 0.007654320987654322),\n","  (17, 0.03758024691358025),\n","  (18, 0.00488888888888889),\n","  (19, 0.019703703703703706)],\n"," [(0, 0.028934010152284265),\n","  (1, 0.0069373942470389175),\n","  (2, 0.17760857304004513),\n","  (3, 0.04032712915961647),\n","  (4, 0.0069373942470389175),\n","  (5, 0.008629441624365483),\n","  (6, 0.16835871404399322),\n","  (7, 0.025775521714608007),\n","  (8, 0.014156796390298928),\n","  (9, 0.2093062605752961),\n","  (10, 0.013479977439368303),\n","  (11, 0.19858996051889452),\n","  (12, 0.01505922165820643),\n","  (13, 0.006260575296108291),\n","  (14, 0.006598984771573604),\n","  (15, 0.027918781725888325),\n","  (16, 0.015397631133671742),\n","  (17, 0.010772701635645797),\n","  (18, 0.013028764805414551),\n","  (19, 0.005922165820642978)],\n"," [(0, 0.01528709917971663),\n","  (1, 0.0385107062959412),\n","  (2, 0.009534462554596784),\n","  (3, 0.008362629167998297),\n","  (4, 0.013689144561627785),\n","  (5, 0.008575689783743477),\n","  (6, 0.012091189943538938),\n","  (7, 0.010280174709704912),\n","  (8, 0.004847129008202835),\n","  (9, 0.05022904016192607),\n","  (10, 0.0066581442420368606),\n","  (11, 0.009960583786087144),\n","  (12, 0.0156066901033344),\n","  (13, 0.03936294875892192),\n","  (14, 0.06620858634281454),\n","  (15, 0.6222967934377331),\n","  (16, 0.009960583786087144),\n","  (17, 0.048950676467455),\n","  (18, 0.0049536593160754245),\n","  (19, 0.004634068392457655)],\n"," [(0, 0.0166951566951567),\n","  (1, 0.09452991452991455),\n","  (2, 0.009743589743589746),\n","  (3, 0.006666666666666668),\n","  (4, 0.00700854700854701),\n","  (5, 0.009743589743589746),\n","  (6, 0.1275783475783476),\n","  (7, 0.0071225071225071235),\n","  (8, 0.0736752136752137),\n","  (9, 0.012250712250712252),\n","  (10, 0.02455840455840456),\n","  (11, 0.11846153846153848),\n","  (12, 0.19925925925925927),\n","  (13, 0.10980056980056982),\n","  (14, 0.012136752136752138),\n","  (15, 0.09863247863247865),\n","  (16, 0.011111111111111113),\n","  (17, 0.020683760683760686),\n","  (18, 0.008831908831908833),\n","  (19, 0.031509971509971514)],\n"," [(0, 0.04238095238095238),\n","  (1, 0.0059788359788359785),\n","  (2, 0.045132275132275135),\n","  (3, 0.0053439153439153435),\n","  (4, 0.004074074074074074),\n","  (5, 0.0037566137566137567),\n","  (6, 0.25317460317460316),\n","  (7, 0.028412698412698414),\n","  (8, 0.015396825396825397),\n","  (9, 0.17962962962962964),\n","  (10, 0.006084656084656085),\n","  (11, 0.013915343915343915),\n","  (12, 0.019735449735449734),\n","  (13, 0.03222222222222222),\n","  (14, 0.004920634920634921),\n","  (15, 0.29074074074074074),\n","  (16, 0.017830687830687832),\n","  (17, 0.016031746031746032),\n","  (18, 0.005132275132275132),\n","  (19, 0.010105820105820106)],\n"," [(0, 0.008409975971497225),\n","  (1, 0.4640401027425636),\n","  (2, 0.11794680586626895),\n","  (3, 0.00940425884497473),\n","  (4, 0.018767089236887895),\n","  (5, 0.039564172673792364),\n","  (6, 0.010895683155190985),\n","  (7, 0.01561852680420913),\n","  (8, 0.029704200845140444),\n","  (9, 0.009984257187836605),\n","  (10, 0.11280967768663518),\n","  (11, 0.002858563261247825),\n","  (12, 0.017524235645041013),\n","  (13, 0.016115668240947883),\n","  (14, 0.06756980694340874),\n","  (15, 0.011972822934791615),\n","  (16, 0.017689949457287264),\n","  (17, 0.018435661612395394),\n","  (18, 0.004847129008202834),\n","  (19, 0.0058414118816803385)],\n"," [(0, 0.006253994339450379),\n","  (1, 0.3888888888888889),\n","  (2, 0.11937368757418058),\n","  (3, 0.013010134209805532),\n","  (4, 0.012188441522870446),\n","  (5, 0.15945403085912535),\n","  (6, 0.0074408837761343926),\n","  (7, 0.033461152195745456),\n","  (8, 0.007166986213822697),\n","  (9, 0.014288322833926777),\n","  (10, 0.10604400620834474),\n","  (11, 0.0370218205057975),\n","  (12, 0.008719072400255637),\n","  (13, 0.015566511458048023),\n","  (14, 0.009632064274627955),\n","  (15, 0.01036245777412581),\n","  (16, 0.018579384643476672),\n","  (17, 0.020405368392221308),\n","  (18, 0.006071395964575915),\n","  (19, 0.006071395964575915)],\n"," [(0, 0.00930407658384425),\n","  (1, 0.008981391846832311),\n","  (2, 0.008981391846832311),\n","  (3, 0.11191782295364096),\n","  (4, 0.04221791975906206),\n","  (5, 0.004248682370657201),\n","  (6, 0.2202323330106486),\n","  (7, 0.007905776056792513),\n","  (8, 0.022103904485317846),\n","  (9, 0.007260406582768635),\n","  (10, 0.04243304291707002),\n","  (11, 0.006722598687748736),\n","  (12, 0.01962998816822631),\n","  (13, 0.03146176185866408),\n","  (14, 0.00467892868667312),\n","  (15, 0.00930407658384425),\n","  (16, 0.010487253952888029),\n","  (17, 0.006830160266752716),\n","  (18, 0.41362805205980424),\n","  (19, 0.011670431321931806)],\n"," [(0, 0.025400496873126018),\n","  (1, 0.004583226248607898),\n","  (2, 0.19185299408892315),\n","  (3, 0.009466289728433136),\n","  (4, 0.00835260858391159),\n","  (5, 0.04810245866529598),\n","  (6, 0.08545361089694166),\n","  (7, 0.023687141266169793),\n","  (8, 0.004840229589651332),\n","  (9, 0.2219223849910049),\n","  (10, 0.02051743339330078),\n","  (11, 0.013921014306519318),\n","  (12, 0.05178617322025186),\n","  (13, 0.007410263000085668),\n","  (14, 0.004754561809303521),\n","  (15, 0.03071189925469031),\n","  (16, 0.04039235843399298),\n","  (17, 0.1339415745738028),\n","  (18, 0.0065535851966075555),\n","  (19, 0.06634969587937976)],\n"," [(0, 0.009148653773227154),\n","  (1, 0.13239476678043233),\n","  (2, 0.09475729996207813),\n","  (3, 0.009338263177853624),\n","  (4, 0.0061149032992036416),\n","  (5, 0.4669605612438377),\n","  (6, 0.01066552901023891),\n","  (7, 0.05200037921880926),\n","  (8, 0.03597838452787259),\n","  (9, 0.006399317406143345),\n","  (10, 0.04498483124762989),\n","  (11, 0.06186006825938567),\n","  (12, 0.00810580204778157),\n","  (13, 0.01066552901023891),\n","  (14, 0.005166856276071294),\n","  (15, 0.007726583238528632),\n","  (16, 0.009717481987106561),\n","  (17, 0.011518771331058022),\n","  (18, 0.007916192643155103),\n","  (19, 0.008579825559347746)],\n"," [(0, 0.02567449956483899),\n","  (1, 0.07006092254134029),\n","  (2, 0.09355961705831158),\n","  (3, 0.021999806595106856),\n","  (4, 0.007494439609322116),\n","  (5, 0.06184121458272894),\n","  (6, 0.09433323663088676),\n","  (7, 0.01832511362537472),\n","  (8, 0.005270283338168456),\n","  (9, 0.004690068658737066),\n","  (10, 0.011652644811913741),\n","  (11, 0.02683492892370177),\n","  (12, 0.012039454598201334),\n","  (13, 0.07131805434677498),\n","  (14, 0.018808625858234212),\n","  (15, 0.3703220191470844),\n","  (16, 0.025867904457982788),\n","  (17, 0.014360313315926894),\n","  (18, 0.0049801759984527605),\n","  (19, 0.04056667633691132)],\n"," [(0, 0.03455674244499223),\n","  (1, 0.005523600839952523),\n","  (2, 0.25321829635716236),\n","  (3, 0.008445174837943941),\n","  (4, 0.00789737971332055),\n","  (5, 0.003423719528896192),\n","  (6, 0.30334155026020265),\n","  (7, 0.01794029033141605),\n","  (8, 0.0037889162786451194),\n","  (9, 0.011549347210809822),\n","  (10, 0.015292613895736324),\n","  (11, 0.015931708207796946),\n","  (12, 0.005980096777138681),\n","  (13, 0.031087373322377425),\n","  (14, 0.028987492011321093),\n","  (15, 0.007440883776134391),\n","  (16, 0.003971514653519583),\n","  (17, 0.009632064274627953),\n","  (18, 0.004245412215831278),\n","  (19, 0.2277458230621747)],\n"," [(0, 0.024728588661037394),\n","  (1, 0.007438681141938078),\n","  (2, 0.25083768931778583),\n","  (3, 0.006098378233480767),\n","  (4, 0.008242862887012465),\n","  (5, 0.007572711432783809),\n","  (6, 0.007036590269400885),\n","  (7, 0.006232408524326498),\n","  (8, 0.004892105615869186),\n","  (9, 0.007974802305321002),\n","  (10, 0.011593620158155743),\n","  (11, 0.0069025599785551535),\n","  (12, 0.3296475003350757),\n","  (13, 0.018563195282133762),\n","  (14, 0.07378367511057499),\n","  (15, 0.0071706205602466155),\n","  (16, 0.01347004422999598),\n","  (17, 0.053411070902023856),\n","  (18, 0.007036590269400885),\n","  (19, 0.14736630478488139)],\n"," [(0, 0.011099112071034317),\n","  (1, 0.05501559875209983),\n","  (2, 0.029937604991600673),\n","  (3, 0.009779217662586994),\n","  (4, 0.041456683465322774),\n","  (5, 0.10697144228461723),\n","  (6, 0.007139428845692345),\n","  (7, 0.008219342452603792),\n","  (8, 0.011339092872570195),\n","  (9, 0.020578353731701464),\n","  (10, 0.1769258459323254),\n","  (11, 0.016858651307895368),\n","  (12, 0.06545476361891049),\n","  (13, 0.08537317014638829),\n","  (14, 0.21376289896808257),\n","  (15, 0.005099592032637389),\n","  (16, 0.04685625149988001),\n","  (17, 0.013498920086393088),\n","  (18, 0.009899208063354932),\n","  (19, 0.06473482121430285)],\n"," [(0, 0.025108577633007615),\n","  (1, 0.0023977560622511778),\n","  (2, 0.21077633007600446),\n","  (3, 0.01099348534201955),\n","  (4, 0.010903003981179882),\n","  (5, 0.003935939196525518),\n","  (6, 0.4553474484256246),\n","  (7, 0.04790988056460372),\n","  (8, 0.003935939196525518),\n","  (9, 0.016512848353239244),\n","  (10, 0.009998190372783211),\n","  (11, 0.016512848353239244),\n","  (12, 0.006197973217517195),\n","  (13, 0.017960550126673917),\n","  (14, 0.006559898660875863),\n","  (15, 0.006197973217517195),\n","  (16, 0.011264929424538552),\n","  (17, 0.01253166847629389),\n","  (18, 0.005021715526601522),\n","  (19, 0.11993304379297871)],\n"," [(0, 0.011992414100847835),\n","  (1, 0.015785363676929943),\n","  (2, 0.08852074966532798),\n","  (3, 0.31297411869701025),\n","  (4, 0.011769299419901829),\n","  (5, 0.22328201695671576),\n","  (6, 0.01321954484605087),\n","  (7, 0.005745203034359661),\n","  (8, 0.09186746987951808),\n","  (9, 0.012996430165104864),\n","  (10, 0.06007362784471218),\n","  (11, 0.025490852298081214),\n","  (12, 0.018128067826863006),\n","  (13, 0.012773315484158857),\n","  (14, 0.029506916555109328),\n","  (15, 0.014223560910307898),\n","  (16, 0.011769299419901829),\n","  (17, 0.015562248995983935),\n","  (18, 0.010653726015171798),\n","  (19, 0.013665774207942882)],\n"," [(0, 0.006799336650082917),\n","  (1, 0.26694306246545046),\n","  (2, 0.16390270867882806),\n","  (3, 0.008126036484245439),\n","  (4, 0.03609729132117191),\n","  (5, 0.14223327805417355),\n","  (6, 0.008899944720840242),\n","  (7, 0.024488667772249856),\n","  (8, 0.12078496406854614),\n","  (9, 0.007573244886677721),\n","  (10, 0.02891100055279159),\n","  (11, 0.049143173023770026),\n","  (12, 0.0056937534549474835),\n","  (13, 0.020066334991708124),\n","  (14, 0.03178551686014372),\n","  (15, 0.005804311774461027),\n","  (16, 0.00724156992813709),\n","  (17, 0.018186843559977885),\n","  (18, 0.009894969596462131),\n","  (19, 0.03742399115533443)],\n"," [(0, 0.013832429990966573),\n","  (1, 0.030318428184281834),\n","  (2, 0.016542457091237577),\n","  (3, 0.22397244805781386),\n","  (4, 0.007847786811201444),\n","  (5, 0.20726061427280934),\n","  (6, 0.007960704607046069),\n","  (7, 0.0162037037037037),\n","  (8, 0.3733626919602529),\n","  (9, 0.008073622402890694),\n","  (10, 0.007847786811201444),\n","  (11, 0.004347335140018066),\n","  (12, 0.0063798554652213174),\n","  (13, 0.0050248419150858165),\n","  (14, 0.006041102077687442),\n","  (15, 0.006718608852755193),\n","  (16, 0.03731933152664859),\n","  (17, 0.006831526648599817),\n","  (18, 0.006266937669376692),\n","  (19, 0.007847786811201444)],\n"," [(0, 0.07182244279018472),\n","  (1, 0.004273504273504274),\n","  (2, 0.007398217075636431),\n","  (3, 0.17695983825016084),\n","  (4, 0.007306313757926662),\n","  (5, 0.007122507122507123),\n","  (6, 0.003630181049535888),\n","  (7, 0.00473302086205312),\n","  (8, 0.3710596452531936),\n","  (9, 0.10683760683760683),\n","  (10, 0.007030603804797354),\n","  (11, 0.003630181049535888),\n","  (12, 0.01162576969028582),\n","  (13, 0.003262567778696811),\n","  (14, 0.00611157062769966),\n","  (15, 0.004181600955794504),\n","  (16, 0.16602334344269828),\n","  (17, 0.0075820237110559695),\n","  (18, 0.013923352633030052),\n","  (19, 0.01548570903409613)],\n"," [(0, 0.04096866096866097),\n","  (1, 0.06854700854700854),\n","  (2, 0.005185185185185185),\n","  (3, 0.0056410256410256415),\n","  (4, 0.006666666666666667),\n","  (5, 0.3435327635327635),\n","  (6, 0.010883190883190882),\n","  (7, 0.014415954415954417),\n","  (8, 0.14797720797720798),\n","  (9, 0.025242165242165243),\n","  (10, 0.06524216524216524),\n","  (11, 0.0150997150997151),\n","  (12, 0.09213675213675214),\n","  (13, 0.028319088319088317),\n","  (14, 0.011908831908831909),\n","  (15, 0.00905982905982906),\n","  (16, 0.08735042735042735),\n","  (17, 0.0074643874643874645),\n","  (18, 0.007008547008547009),\n","  (19, 0.007350427350427351)],\n"," [(0, 0.01876742333731581),\n","  (1, 0.39182596575069695),\n","  (2, 0.02334727200318598),\n","  (3, 0.03220828355236957),\n","  (4, 0.004729191557148547),\n","  (5, 0.06894663480684986),\n","  (6, 0.004828753484667463),\n","  (7, 0.010603345280764636),\n","  (8, 0.07302867383512544),\n","  (9, 0.013092393468737555),\n","  (10, 0.14521107128634012),\n","  (11, 0.05361409796893668),\n","  (12, 0.01996216646754281),\n","  (13, 0.005724810832337714),\n","  (14, 0.016776184786937477),\n","  (15, 0.00950816407805655),\n","  (16, 0.026732377538829153),\n","  (17, 0.06815013938669853),\n","  (18, 0.009109916367980884),\n","  (19, 0.0038331342094782956)],\n"," [(0, 0.026257861635220127),\n","  (1, 0.07017819706498951),\n","  (2, 0.03265199161425576),\n","  (3, 0.008752620545073374),\n","  (4, 0.008962264150943396),\n","  (5, 0.16567085953878408),\n","  (6, 0.007075471698113208),\n","  (7, 0.012735849056603774),\n","  (8, 0.39837526205450735),\n","  (9, 0.009276729559748427),\n","  (10, 0.052882599580712786),\n","  (11, 0.00550314465408805),\n","  (12, 0.01409853249475891),\n","  (13, 0.010115303983228512),\n","  (14, 0.019444444444444445),\n","  (15, 0.007075471698113208),\n","  (16, 0.03967505241090147),\n","  (17, 0.0038259958071278828),\n","  (18, 0.007389937106918239),\n","  (19, 0.1000524109014675)],\n"," [(0, 0.0458242556281772),\n","  (1, 0.006463326071169209),\n","  (2, 0.005591866376180102),\n","  (3, 0.06456063907044299),\n","  (4, 0.015177923021060276),\n","  (5, 0.01299927378358751),\n","  (6, 0.0058823529411764705),\n","  (7, 0.006608569353667393),\n","  (8, 0.4420479302832244),\n","  (9, 0.07312999273783588),\n","  (10, 0.01140159767610748),\n","  (11, 0.009949164851125635),\n","  (12, 0.04960058097312999),\n","  (13, 0.010675381263616559),\n","  (14, 0.007915758896151053),\n","  (15, 0.007625272331154684),\n","  (16, 0.1822076978939724),\n","  (17, 0.0077705156136528685),\n","  (18, 0.008351488743645606),\n","  (19, 0.026216412490922294)],\n"," [(0, 0.014481050818260122),\n","  (1, 0.021909991386735578),\n","  (2, 0.11654823428079243),\n","  (3, 0.4120908699397072),\n","  (4, 0.02363264427217916),\n","  (5, 0.06293066322136091),\n","  (6, 0.00543712316968131),\n","  (7, 0.011035745047372956),\n","  (8, 0.09684539190353145),\n","  (9, 0.005006459948320414),\n","  (10, 0.04419681309216194),\n","  (11, 0.007159776055124893),\n","  (12, 0.0060831180017226535),\n","  (13, 0.06971360895779502),\n","  (14, 0.02147932816537468),\n","  (15, 0.009313092161929373),\n","  (16, 0.007267441860465117),\n","  (17, 0.04645779500430664),\n","  (18, 0.01006675279931094),\n","  (19, 0.008344099913867357)],\n"," ...]"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"1SKdaeGYaUtp","colab_type":"code","outputId":"4a425236-6b5a-448e-df06-589d6670908c","executionInfo":{"status":"ok","timestamp":1590837052734,"user_tz":-180,"elapsed":635,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(tm_results)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1740"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"W27QDA-SYHqA","colab_type":"code","outputId":"e3de547e-7418-4f44-9266-a551c7c3121d","executionInfo":{"status":"ok","timestamp":1590837054095,"user_tz":-180,"elapsed":701,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["corpus_topics = [sorted(topics, key=lambda record: -record[1])[0] for topics in tm_results]\n","\n","corpus_topics"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 0.38040123456790126),\n"," (12, 0.38552580519984486),\n"," (12, 0.13461184694628403),\n"," (0, 0.3781515460414543),\n"," (13, 0.21812780989081568),\n"," (5, 0.5503110532407408),\n"," (13, 0.3700989602906176),\n"," (8, 0.689757058437295),\n"," (8, 0.28962281679672985),\n"," (3, 0.33202658303464755),\n"," (8, 0.47356653648044383),\n"," (18, 0.3535194514413658),\n"," (0, 0.3613519588953115),\n"," (15, 0.30802891933028914),\n"," (8, 0.5985128350857252),\n"," (12, 0.3125826719576719),\n"," (18, 0.4283036122542295),\n"," (13, 0.22725240330874139),\n"," (14, 0.33194112967382655),\n"," (8, 0.3744131455399061),\n"," (10, 0.3359734861251545),\n"," (8, 0.5487411568872244),\n"," (14, 0.23701201201201205),\n"," (12, 0.3213199513381995),\n"," (2, 0.26198574667962427),\n"," (0, 0.22680672875604652),\n"," (13, 0.20893879348304706),\n"," (13, 0.46958671517865475),\n"," (10, 0.31695671575189643),\n"," (18, 0.24874948749487497),\n"," (12, 0.3925566343042071),\n"," (10, 0.2760141093474427),\n"," (14, 0.3564871481028152),\n"," (14, 0.3356085050053777),\n"," (7, 0.21718058808222737),\n"," (5, 0.259280303030303),\n"," (12, 0.33072916666666663),\n"," (5, 0.3196538246789503),\n"," (13, 0.2865666309396213),\n"," (18, 0.6143872277593208),\n"," (14, 0.32466305062458906),\n"," (12, 0.21259943546317683),\n"," (1, 0.3074835761211083),\n"," (12, 0.46890697207152904),\n"," (18, 0.70923814694173),\n"," (18, 0.5158833297234857),\n"," (14, 0.1800976800976801),\n"," (19, 0.20332439242461184),\n"," (12, 0.452191405581236),\n"," (12, 0.5619321497040927),\n"," (11, 0.33792352829831535),\n"," (13, 0.31485276259866424),\n"," (1, 0.17586717428087986),\n"," (14, 0.23217345459338348),\n"," (5, 0.4286835748792271),\n"," (5, 0.23521011831905345),\n"," (2, 0.41910583103764926),\n"," (8, 0.17427516657662523),\n"," (13, 0.21106075864384022),\n"," (1, 0.13709902370990237),\n"," (19, 0.2942441774217475),\n"," (3, 0.2072167017966476),\n"," (2, 0.3041253496058988),\n"," (19, 0.3732253549290142),\n"," (18, 0.25325539397395685),\n"," (8, 0.4005887681159419),\n"," (12, 0.19171348314606745),\n"," (18, 0.2692830445124023),\n"," (6, 0.3368853767560664),\n"," (14, 0.2559937169312169),\n"," (8, 0.44142730710435263),\n"," (14, 0.2667994281045752),\n"," (11, 0.21713421342134212),\n"," (8, 0.4636642954947734),\n"," (19, 0.20227808326787122),\n"," (0, 0.18298472036789795),\n"," (18, 0.27971958925750395),\n"," (10, 0.31221429877491313),\n"," (3, 0.19495658960101453),\n"," (11, 0.29897022610252966),\n"," (10, 0.1841281607775307),\n"," (5, 0.2981537768771812),\n"," (4, 0.4654038705820819),\n"," (8, 0.4956243162994218),\n"," (8, 0.16741038226982002),\n"," (0, 0.3516540572615339),\n"," (10, 0.38935325903411),\n"," (8, 0.39666125979994593),\n"," (0, 0.3484126984126984),\n"," (17, 0.2112669600293363),\n"," (2, 0.27411280951103967),\n"," (17, 0.28554191585353056),\n"," (1, 0.22702894150981584),\n"," (19, 0.19833025546321947),\n"," (12, 0.24977578475336323),\n"," (13, 0.43640350877192985),\n"," (2, 0.3639648130694313),\n"," (13, 0.5076299475441107),\n"," (13, 0.14239356178608512),\n"," (3, 0.31450000000000006),\n"," (13, 0.2492305324715297),\n"," (13, 0.38355391402482814),\n"," (13, 0.32687122268712226),\n"," (13, 0.32241655235482397),\n"," (13, 0.30224024719969095),\n"," (16, 0.239024618552841),\n"," (15, 0.3440724742020638),\n"," (15, 0.41917604957667814),\n"," (15, 0.34188469815381123),\n"," (15, 0.266104626026805),\n"," (14, 0.140365993763335),\n"," (2, 0.357496194824962),\n"," (3, 0.3466790813159529),\n"," (6, 0.2661935062816121),\n"," (5, 0.322015503875969),\n"," (11, 0.27369718609198335),\n"," (18, 0.3381801125703564),\n"," (11, 0.2669893960350392),\n"," (1, 0.4405033238366572),\n"," (1, 0.28349875930521096),\n"," (5, 0.2685514110913756),\n"," (5, 0.5413394710713963),\n"," (8, 0.5273163528977484),\n"," (8, 0.7259170227920229),\n"," (10, 0.2848035389018995),\n"," (10, 0.2450047573739296),\n"," (8, 0.20485476335567745),\n"," (5, 0.4428432191590087),\n"," (14, 0.24326993036066938),\n"," (5, 0.4244888888888889),\n"," (8, 0.36882896964960266),\n"," (8, 0.2585493489107947),\n"," (0, 0.40418781725888325),\n"," (15, 0.3736168267032465),\n"," (14, 0.2785094596007108),\n"," (13, 0.44167377095765853),\n"," (2, 0.32665470494417864),\n"," (2, 0.2988928081520674),\n"," (2, 0.4094147582697201),\n"," (14, 0.26837131248895957),\n"," (10, 0.5481232353429663),\n"," (14, 0.2788333333333333),\n"," (13, 0.2348165196266462),\n"," (11, 0.3198495370370371),\n"," (13, 0.20008929710422244),\n"," (12, 0.23906485671191555),\n"," (14, 0.3496760895170789),\n"," (14, 0.41141768859160166),\n"," (3, 0.4107974910394264),\n"," (18, 0.597636815920398),\n"," (18, 0.35019055995309295),\n"," (18, 0.354043392504931),\n"," (18, 0.30569072722357393),\n"," (18, 0.6003070624360287),\n"," (18, 0.6016499365409023),\n"," (18, 0.3882843670656511),\n"," (18, 0.1634304207119741),\n"," (18, 0.5660083622453519),\n"," (18, 0.5234040573236554),\n"," (14, 0.355458454709391),\n"," (13, 0.22832805668626563),\n"," (13, 0.239195791055994),\n"," (18, 0.3608190976784932),\n"," (19, 0.529925187032419),\n"," (12, 0.2994734931009441),\n"," (8, 0.3105794358835722),\n"," (8, 0.5964542157080466),\n"," (11, 0.2658187800556821),\n"," (8, 0.22715543412264727),\n"," (8, 0.5400688298918388),\n"," (2, 0.5052190633154785),\n"," (8, 0.48865378718184604),\n"," (6, 0.21458633958633958),\n"," (15, 0.24464747139165738),\n"," (11, 0.568272386310361),\n"," (2, 0.26139661143873444),\n"," (16, 0.3264873379377196),\n"," (0, 0.5318260120585704),\n"," (19, 0.2762146671877036),\n"," (17, 0.25169880624426083),\n"," (18, 0.3993921944977607),\n"," (8, 0.2797737857618098),\n"," (3, 0.386762360446571),\n"," (15, 0.3005177721802407),\n"," (8, 0.30875044373446936),\n"," (3, 0.43917720115324904),\n"," (8, 0.36988964133433655),\n"," (8, 0.7226669901342392),\n"," (8, 0.5732165825665381),\n"," (3, 0.33037825059101655),\n"," (10, 0.5631420224295541),\n"," (5, 0.19806108097247344),\n"," (8, 0.24478890655361246),\n"," (8, 0.46885064412238325),\n"," (8, 0.495361875637105),\n"," (5, 0.5413744027650705),\n"," (3, 0.17416803055100927),\n"," (8, 0.37527177022542624),\n"," (5, 0.35944048023558733),\n"," (5, 0.42658561869757805),\n"," (5, 0.38768443239987954),\n"," (8, 0.5616228764376913),\n"," (6, 0.5369782013944048),\n"," (15, 0.3056867065365649),\n"," (15, 0.5977056962025317),\n"," (15, 0.5461277008982763),\n"," (9, 0.2847147470398278),\n"," (15, 0.3997083450936118),\n"," (18, 0.41643402196166013),\n"," (3, 0.2737003058103975),\n"," (3, 0.27061766735679776),\n"," (3, 0.27437753647033014),\n"," (11, 0.24667466746674668),\n"," (11, 0.5621597822606469),\n"," (11, 0.32692493013395),\n"," (1, 0.43149381541389153),\n"," (11, 0.351138353765324),\n"," (1, 0.2659406565656565),\n"," (7, 0.28305555555555556),\n"," (10, 0.34829313299064196),\n"," (10, 0.2117023126412798),\n"," (10, 0.4277054832610388),\n"," (17, 0.2210829647500335),\n"," (12, 0.5534793814432991),\n"," (2, 0.39017162066525124),\n"," (2, 0.4144002789400279),\n"," (14, 0.5584352294878611),\n"," (9, 0.278786799620133),\n"," (13, 0.25386012715712986),\n"," (13, 0.24682801613494681),\n"," (15, 0.3163733039970663),\n"," (2, 0.37064896755162247),\n"," (4, 0.3658989598811293),\n"," (19, 0.22014803969415975),\n"," (13, 0.17125984251968504),\n"," (8, 0.29053465691396724),\n"," (13, 0.18386627906976746),\n"," (13, 0.17025518341307816),\n"," (12, 0.41855500821018066),\n"," (13, 0.3277633101851852),\n"," (13, 0.466012419605234),\n"," (13, 0.6595754672677748),\n"," (13, 0.2644824311490978),\n"," (0, 0.26540207522697795),\n"," (13, 0.20037010788251045),\n"," (12, 0.22056549591340843),\n"," (13, 0.2767866975002753),\n"," (6, 0.3382607753952784),\n"," (6, 0.25503549612019155),\n"," (13, 0.24392361111111113),\n"," (13, 0.3780431521864544),\n"," (11, 0.2584771419921284),\n"," (0, 0.4315350032113038),\n"," (4, 0.6507490636704121),\n"," (5, 0.23596967639737498),\n"," (0, 0.6338410061151494),\n"," (17, 0.18869823111958478),\n"," (13, 0.4405886056027699),\n"," (18, 0.33626073380171745),\n"," (18, 0.37274453941120606),\n"," (18, 0.674862583431488),\n"," (12, 0.41474611204796713),\n"," (18, 0.614693313222725),\n"," (18, 0.41937765205091937),\n"," (18, 0.3085564789165897),\n"," (18, 0.398840885142255),\n"," (18, 0.4601223002862347),\n"," (10, 0.20040022870211552),\n"," (0, 0.6154226174436361),\n"," (13, 0.3593044263775972),\n"," (13, 0.20790849673202608),\n"," (2, 0.2517527815881726),\n"," (18, 0.5883950617283951),\n"," (15, 0.5968551150269212),\n"," (12, 0.3721975308641976),\n"," (13, 0.32899207248018114),\n"," (12, 0.194807078619089),\n"," (12, 0.29054834054834056),\n"," (12, 0.2559216192937124),\n"," (13, 0.21436781609195402),\n"," (19, 0.20078066516950058),\n"," (19, 0.19873579215959175),\n"," (11, 0.3529580043483236),\n"," (5, 0.20071355759429155),\n"," (8, 0.7539049919484702),\n"," (5, 0.49114959114959106),\n"," (5, 0.4835884945488286),\n"," (5, 0.4751259122211944),\n"," (5, 0.6416926272066459),\n"," (5, 0.4056935038876348),\n"," (8, 0.25125661375661373),\n"," (8, 0.6083581103402708),\n"," (8, 0.3577253531381972),\n"," (8, 0.3018278018278019),\n"," (14, 0.2087987458357829),\n"," (12, 0.19743225653768784),\n"," (12, 0.3995147255689425),\n"," (8, 0.47495222495222483),\n"," (14, 0.3806423611111111),\n"," (12, 0.25848843767625496),\n"," (12, 0.3663799283154122),\n"," (12, 0.441826643397324),\n"," (8, 0.3379687283796873),\n"," (10, 0.17391050356350043),\n"," (16, 0.2876230661040788),\n"," (15, 0.1912090959511925),\n"," (13, 0.31529117541979285),\n"," (14, 0.20999831394368576),\n"," (13, 0.23009565857247982),\n"," (14, 0.2584017515051998),\n"," (0, 0.5411355212006149),\n"," (13, 0.47944630872483224),\n"," (15, 0.6015693659761457),\n"," (15, 0.5554312701963708),\n"," (15, 0.5919679935176745),\n"," (15, 0.5934601552148401),\n"," (15, 0.5460895678286983),\n"," (15, 0.3388162240718721),\n"," (17, 0.1803191489361702),\n"," (15, 0.5261437908496732),\n"," (15, 0.3286484951731971),\n"," (15, 0.4650624461670974),\n"," (10, 0.2008242999096658),\n"," (8, 0.5153091735438345),\n"," (3, 0.36894015379471745),\n"," (11, 0.45317529997073447),\n"," (11, 0.3250402576489533),\n"," (5, 0.3977834855403349),\n"," (11, 0.2934611602753196),\n"," (19, 0.3449512335054504),\n"," (5, 0.35679992161473645),\n"," (5, 0.24768009768009772),\n"," (11, 0.303567483984979),\n"," (5, 0.19426937441643327),\n"," (11, 0.38669467787114853),\n"," (3, 0.32527957320201084),\n"," (8, 0.350660950339407),\n"," (8, 0.30504430811179284),\n"," (18, 0.4790404040404041),\n"," (18, 0.3469522438204944),\n"," (10, 0.37599872163630554),\n"," (18, 0.29365079365079366),\n"," (13, 0.21934025006650704),\n"," (1, 0.5715852130325815),\n"," (18, 0.343037112700034),\n"," (4, 0.446422516107155),\n"," (4, 0.27301759460110875),\n"," (4, 0.39007979040133384),\n"," (4, 0.6734643974960877),\n"," (4, 0.35038379262614816),\n"," (10, 0.2740793201133145),\n"," (4, 0.26579168164928685),\n"," (4, 0.2895545314900154),\n"," (10, 0.3360100513492844),\n"," (8, 0.4290555555555556),\n"," (14, 0.2261430090377459),\n"," (2, 0.24155405405405406),\n"," (10, 0.318349469801752),\n"," (3, 0.3351788756388415),\n"," (7, 0.38525048978449483),\n"," (13, 0.2578259373925009),\n"," (2, 0.2130685089234312),\n"," (11, 0.27461240310077517),\n"," (2, 0.3671282558522915),\n"," (19, 0.17914046121593294),\n"," (2, 0.3991902419024191),\n"," (2, 0.493028880352824),\n"," (2, 0.4077184892402284),\n"," (2, 0.47710830363891576),\n"," (14, 0.3967243443247361),\n"," (13, 0.27061750964858833),\n"," (2, 0.4947044917257682),\n"," (14, 0.3267331878183141),\n"," (2, 0.45316470316470325),\n"," (2, 0.2348302190074342),\n"," (2, 0.23002799310938846),\n"," (7, 0.4420511253691822),\n"," (13, 0.2526726726726727),\n"," (19, 0.3044223327805418),\n"," (7, 0.24918471931050548),\n"," (7, 0.3850756081525311),\n"," (13, 0.32947154471544715),\n"," (7, 0.395131086142322),\n"," (11, 0.18314055380301436),\n"," (0, 0.29888167388167386),\n"," (0, 0.24412611793239353),\n"," (7, 0.18381642512077295),\n"," (7, 0.20440499671268908),\n"," (13, 0.2593608976704477),\n"," (13, 0.24206767357452294),\n"," (12, 0.24328295549958018),\n"," (2, 0.4765479219677693),\n"," (6, 0.49770740029857113),\n"," (19, 0.21425255338904364),\n"," (0, 0.279291553133515),\n"," (0, 0.22856341189674517),\n"," (0, 0.42665104863900033),\n"," (12, 0.3124557678697806),\n"," (12, 0.26335222393955005),\n"," (16, 0.320230607966457),\n"," (9, 0.19700103412616335),\n"," (13, 0.16801994301994302),\n"," (12, 0.2773268398268398),\n"," (7, 0.2652404921700224),\n"," (0, 0.3896649851248222),\n"," (16, 0.22435428849902533),\n"," (16, 0.5894965277777778),\n"," (13, 0.4180424528301888),\n"," (0, 0.28478912438450016),\n"," (12, 0.2798330842052815),\n"," (13, 0.23526936026936027),\n"," (6, 0.3135004708097929),\n"," (0, 0.5749851455733807),\n"," (0, 0.5774853801169592),\n"," (6, 0.3553724053724054),\n"," (6, 0.5751417769376183),\n"," (12, 0.18722422783794626),\n"," (17, 0.27108505093579716),\n"," (18, 0.5419426048565122),\n"," (18, 0.6205915813424346),\n"," (18, 0.38105852766346593),\n"," (18, 0.2653508771929825),\n"," (18, 0.33191837950551084),\n"," (18, 0.45522533022533024),\n"," (18, 0.6362550920073043),\n"," (18, 0.2810266406757635),\n"," (18, 0.4260549748354627),\n"," (0, 0.4386574074074074),\n"," (11, 0.42665214233841686),\n"," (8, 0.30303610154356425),\n"," (8, 0.5454939341421144),\n"," (12, 0.3636756890372019),\n"," (3, 0.27218408339689804),\n"," (8, 0.4281691485299733),\n"," (8, 0.569623784473139),\n"," (8, 0.36454753722794964),\n"," (5, 0.44021921017402943),\n"," (5, 0.5710090506450993),\n"," (5, 0.6051124081279724),\n"," (8, 0.2746745382985165),\n"," (8, 0.44788387548093744),\n"," (8, 0.43280364621828027),\n"," (15, 0.6566627125345986),\n"," (14, 0.2092813289996389),\n"," (15, 0.4416500899876859),\n"," (15, 0.5206309556850772),\n"," (15, 0.329929835584878),\n"," (15, 0.3882744495647722),\n"," (15, 0.5121261115602262),\n"," (1, 0.42333333333333323),\n"," (15, 0.6718749999999999),\n"," (2, 0.3390479030013914),\n"," (19, 0.2497283950617284),\n"," (2, 0.6390251880929015),\n"," (2, 0.4242708892467347),\n"," (15, 0.633398885611275),\n"," (4, 0.4465896589658966),\n"," (4, 0.3631025704196436),\n"," (2, 0.4113279946613281),\n"," (13, 0.2721985815602837),\n"," (14, 0.22727694305323687),\n"," (7, 0.32786596119929456),\n"," (14, 0.4456622593365809),\n"," (14, 0.40842822636300896),\n"," (14, 0.32206591116321476),\n"," (12, 0.40681921675774135),\n"," (13, 0.3787680922842529),\n"," (1, 0.26813655761024185),\n"," (5, 0.19177417117492024),\n"," (11, 0.31309926677946986),\n"," (11, 0.4289345367776741),\n"," (1, 0.3082326358188427),\n"," (11, 0.41263710061993314),\n"," (19, 0.2736505286588759),\n"," (11, 0.5110819480898221),\n"," (5, 0.4264654873395769),\n"," (5, 0.33267768161552586),\n"," (11, 0.33647342995169083),\n"," (11, 0.3707473760405357),\n"," (11, 0.21119800152058216),\n"," (11, 0.3634038034559321),\n"," (6, 0.18128916741271264),\n"," (11, 0.3607884023433622),\n"," (15, 0.45701685097419703),\n"," (15, 0.29871248986212495),\n"," (15, 0.38990903681033756),\n"," (11, 0.33220355634148735),\n"," (4, 0.28833114610673666),\n"," (4, 0.44408535959782486),\n"," (12, 0.21946985094850952),\n"," (1, 0.2756450270495215),\n"," (10, 0.302652414885194),\n"," (4, 0.2973208072372999),\n"," (4, 0.27743457743457745),\n"," (13, 0.31892837910608507),\n"," (1, 0.24812353882121324),\n"," (1, 0.36701662292213477),\n"," (1, 0.21395026150069377),\n"," (1, 0.3913807189542485),\n"," (1, 0.6699564451858031),\n"," (1, 0.5863190445724034),\n"," (6, 0.3407795907795908),\n"," (6, 0.28276945831022365),\n"," (3, 0.36335125448028666),\n"," (7, 0.22501833516685002),\n"," (6, 0.20495179256763085),\n"," (15, 0.10508935508935509),\n"," (7, 0.5290925366454067),\n"," (19, 0.2914607880400355),\n"," (7, 0.34606866002214837),\n"," (3, 0.24071075123706698),\n"," (1, 0.22167397660818716),\n"," (19, 0.2441305712492153),\n"," (3, 0.32114828583230076),\n"," (18, 0.5370849504096593),\n"," (18, 0.6332633788037776),\n"," (5, 0.24487221673796206),\n"," (18, 0.30741648511256353),\n"," (18, 0.5730429292929293),\n"," (18, 0.3248445719751315),\n"," (18, 0.32831359278059785),\n"," (18, 0.2992831541218638),\n"," (3, 0.3972878390201225),\n"," (3, 0.34313327926466614),\n"," (18, 0.22672815945716712),\n"," (7, 0.25616230565036024),\n"," (9, 0.3756049840387189),\n"," (7, 0.4429716036290799),\n"," (0, 0.5164808294209704),\n"," (19, 0.3316175600520649),\n"," (18, 0.3430426289785132),\n"," (0, 0.6319607843137255),\n"," (12, 0.39534730459997364),\n"," (11, 0.23765432098765435),\n"," (0, 0.8123442949676132),\n"," (9, 0.31613549285963083),\n"," (0, 0.4634759413152765),\n"," (0, 0.5313031850230976),\n"," (7, 0.3088568760837669),\n"," (0, 0.686524493895843),\n"," (16, 0.41557466557466555),\n"," (19, 0.27929984779299843),\n"," (2, 0.46552807224185316),\n"," (2, 0.42998672734167614),\n"," (7, 0.26409101409101415),\n"," (7, 0.23829246741771593),\n"," (13, 0.30392156862745096),\n"," (12, 0.3321271463033915),\n"," (17, 0.191405829837582),\n"," (12, 0.32440817105765557),\n"," (7, 0.37519612038225647),\n"," (0, 0.5730244414454939),\n"," (7, 0.285412406271302),\n"," (12, 0.4257523148148148),\n"," (0, 0.2125140924464487),\n"," (19, 0.2296328586651168),\n"," (13, 0.3838091563786008),\n"," (7, 0.3908066642694473),\n"," (9, 0.23486005089058526),\n"," (6, 0.2153050108932462),\n"," (6, 0.3408161068044789),\n"," (15, 0.2373593279442067),\n"," (0, 0.20442134005461038),\n"," (6, 0.21952718676122931),\n"," (17, 0.4032112166440525),\n"," (6, 0.3127042483660131),\n"," (7, 0.364841745081266),\n"," (13, 0.31192718141870684),\n"," (16, 0.26274643099932027),\n"," (11, 0.4961542459203825),\n"," (8, 0.6511523110596111),\n"," (8, 0.2634009888108249),\n"," (8, 0.7140612327656123),\n"," (7, 0.2519747235387046),\n"," (19, 0.26296296296296295),\n"," (0, 0.727777777777778),\n"," (11, 0.3653690186536902),\n"," (14, 0.2836017836017837),\n"," (6, 0.5568542568542569),\n"," (17, 0.27485609628466773),\n"," (0, 0.5758317639673571),\n"," (14, 0.4463364293085656),\n"," (7, 0.30807053238385895),\n"," (4, 0.32194538819680385),\n"," (13, 0.2592771342771343),\n"," (19, 0.19749971969951788),\n"," (2, 0.3443146164665152),\n"," (13, 0.4429929929929931),\n"," (17, 0.3522176022176023),\n"," (7, 0.2959615181308807),\n"," (6, 0.46946606139213154),\n"," (6, 0.43438208616780033),\n"," (12, 0.336113549897571),\n"," (16, 0.34248737373737376),\n"," (13, 0.2214556277056277),\n"," (14, 0.2943686687405281),\n"," (13, 0.32821539830885627),\n"," (7, 0.5304818092428711),\n"," (6, 0.26252943155062225),\n"," (12, 0.32180439492417207),\n"," (6, 0.2068847989093388),\n"," (7, 0.2512995451591943),\n"," (14, 0.4335172921265637),\n"," (12, 0.3082982760042894),\n"," (1, 0.2565770453888407),\n"," (4, 0.6346077260055754),\n"," (4, 0.4547935331067861),\n"," (11, 0.21031746031746035),\n"," (4, 0.41668772639204776),\n"," (4, 0.5154156290762479),\n"," (10, 0.3168257102116924),\n"," (1, 0.35332034872936374),\n"," (12, 0.3806066473055825),\n"," (1, 0.23861461921163413),\n"," (4, 0.26051198257080616),\n"," (2, 0.36072156298693514),\n"," (13, 0.25338753387533874),\n"," (5, 0.3320239431350542),\n"," (5, 0.5203347938433883),\n"," (11, 0.52587890625),\n"," (9, 0.2928051001821493),\n"," (11, 0.3063019833653231),\n"," (5, 0.46077163487444767),\n"," (5, 0.3774641577060932),\n"," (11, 0.3013306138306138),\n"," (5, 0.2824212271973465),\n"," (15, 0.36336464560204956),\n"," (12, 0.38901500819775514),\n"," (12, 0.3620193763245534),\n"," (17, 0.2731533558305212),\n"," (9, 0.30024716324008544),\n"," (0, 0.3603591286117948),\n"," (16, 0.23955415499533148),\n"," (17, 0.14871121121121122),\n"," (12, 0.2729826003562132),\n"," (8, 0.41001461988304094),\n"," (16, 0.6349296128707894),\n"," (6, 0.20316103649436984),\n"," (9, 0.39999145956102145),\n"," (14, 0.3133575464083938),\n"," (8, 0.26707505985072527),\n"," (0, 0.2401837928153718),\n"," (8, 0.28040338942594584),\n"," (17, 0.32432909604519766),\n"," (0, 0.6156283710895363),\n"," (7, 0.33965968586387435),\n"," (0, 0.2851753217931647),\n"," (0, 0.5588594083562062),\n"," (0, 0.6234412218018776),\n"," (14, 0.42950377562028047),\n"," (0, 0.5471753398968591),\n"," (15, 0.6236979166666666),\n"," (1, 0.3002226910454759),\n"," (3, 0.5413593903638152),\n"," (15, 0.6463650425367363),\n"," (3, 0.2999619156431496),\n"," (15, 0.6278259937854923),\n"," (15, 0.6552301655660828),\n"," (15, 0.7147190293742018),\n"," (15, 0.5978311296682904),\n"," (15, 0.4376251788268956),\n"," (7, 0.26441985244802146),\n"," (15, 0.19447633626738106),\n"," (6, 0.20443881381381385),\n"," (18, 0.4443824404761905),\n"," (18, 0.5209625036905816),\n"," (18, 0.30184108527131787),\n"," (18, 0.30995096280349244),\n"," (18, 0.24540479140850888),\n"," (14, 0.24493295915185526),\n"," (8, 0.593687173301166),\n"," (11, 0.3914445661433613),\n"," (12, 0.2279949702789209),\n"," (19, 0.4000937286980232),\n"," (2, 0.6412684099789602),\n"," (14, 0.4309332642665976),\n"," (7, 0.22824631860776445),\n"," (2, 0.21166295235063143),\n"," (2, 0.5804861709254475),\n"," (2, 0.31795381412076473),\n"," (5, 0.2588711930706902),\n"," (8, 0.2930968726163234),\n"," (8, 0.4375610232376489),\n"," (8, 0.37748647523928425),\n"," (2, 0.26149498017379563),\n"," (8, 0.23575885668276975),\n"," (17, 0.24136577708006282),\n"," (10, 0.29778972520908004),\n"," (5, 0.331223083548665),\n"," (8, 0.4120059656972409),\n"," (5, 0.653950491692099),\n"," (8, 0.2622921185827911),\n"," (8, 0.4329107237189646),\n"," (5, 0.40257189915114666),\n"," (1, 0.2982712951345322),\n"," (8, 0.5748674672725307),\n"," (8, 0.30416914240443654),\n"," (7, 0.1866731203614069),\n"," (18, 0.4822960790702726),\n"," (10, 0.4038725335021631),\n"," (17, 0.29015132514003844),\n"," (17, 0.24936347896934516),\n"," (14, 0.2955801104972376),\n"," (17, 0.2783209186840472),\n"," (7, 0.23981481481481484),\n"," (11, 0.3326893806345862),\n"," (19, 0.46624825662482566),\n"," (7, 0.22851277976426965),\n"," (14, 0.39824480012371444),\n"," (14, 0.23804742780646396),\n"," (12, 0.15450422593279736),\n"," (19, 0.26506259780907676),\n"," (17, 0.3029974160206719),\n"," (17, 0.4286773428232503),\n"," (6, 0.4187574671445639),\n"," (9, 0.5542742927429274),\n"," (9, 0.42340067340067344),\n"," (17, 0.275505788164016),\n"," (17, 0.3917097633599749),\n"," (17, 0.44380116959064325),\n"," (7, 0.33927023747616575),\n"," (6, 0.28453096971615494),\n"," (14, 0.2791666666666667),\n"," (6, 0.5210941906373379),\n"," (7, 0.46238425925925924),\n"," (13, 0.19046988164635223),\n"," (9, 0.3589696524718552),\n"," (11, 0.2385674182740197),\n"," (7, 0.35345737661652155),\n"," (18, 0.2574513381995134),\n"," (13, 0.2114276025771448),\n"," (7, 0.4105098855359),\n"," (13, 0.376257183908046),\n"," (16, 0.3314944834503511),\n"," (13, 0.424396681749623),\n"," (13, 0.3288182175107971),\n"," (19, 0.2944060714590262),\n"," (7, 0.2801587301587301),\n"," (0, 0.36243176615601336),\n"," (0, 0.631619937694704),\n"," (0, 0.5490550864495377),\n"," (6, 0.34560857538035966),\n"," (0, 0.3991228070175439),\n"," (12, 0.4182240344319855),\n"," (0, 0.38214545077290174),\n"," (0, 0.2378901373283396),\n"," (0, 0.5815402038505098),\n"," (12, 0.4502036067481094),\n"," (7, 0.45955122527310294),\n"," (17, 0.2930968726163234),\n"," (7, 0.39786925479696567),\n"," (0, 0.34303227808814407),\n"," (16, 0.3445626477541371),\n"," (16, 0.5203484116527595),\n"," (0, 0.34395496818404303),\n"," (8, 0.5238482384823848),\n"," (8, 0.34385226741467984),\n"," (12, 0.4101438207967418),\n"," (8, 0.28681752873563215),\n"," (12, 0.3601514182128386),\n"," (14, 0.3879257907542579),\n"," (8, 0.7179627601314349),\n"," (8, 0.6258533689522114),\n"," (8, 0.478628841607565),\n"," (8, 0.6213213213213213),\n"," (5, 0.6948709746214544),\n"," (11, 0.21417535728080334),\n"," (5, 0.2457556935817806),\n"," (8, 0.4673163617077698),\n"," (8, 0.16924452861952863),\n"," (18, 0.2673958719564949),\n"," (9, 0.25002977253781117),\n"," (10, 0.22935988361520276),\n"," (5, 0.21761067708333334),\n"," (1, 0.260634308368156),\n"," (8, 0.4647876904867584),\n"," (8, 0.37753369415260524),\n"," (4, 0.6153721682847898),\n"," (10, 0.22305389221556887),\n"," (4, 0.6250745378652356),\n"," (4, 0.4724319789044708),\n"," (4, 0.4818868407578085),\n"," (7, 0.24285864314219704),\n"," (4, 0.6034043025060989),\n"," (4, 0.3701529704731412),\n"," (4, 0.48075729360645564),\n"," (10, 0.3053199287883548),\n"," (1, 0.40088321884200195),\n"," (1, 0.24852713178294578),\n"," (11, 0.35328586263165707),\n"," (1, 0.28258719884933475),\n"," (14, 0.26000483617458586),\n"," (11, 0.34900745531303334),\n"," (15, 0.5412535612535614),\n"," (11, 0.3703857896197614),\n"," (6, 0.4269313884386818),\n"," (2, 0.2508435767654857),\n"," (2, 0.19826572604350384),\n"," (4, 0.3451299857549857),\n"," (9, 0.22511623493690103),\n"," (15, 0.2287347155768208),\n"," (18, 0.47991502510621864),\n"," (18, 0.4448123620309051),\n"," (18, 0.3458949563898369),\n"," (18, 0.5147215560884625),\n"," (18, 0.37165032679738563),\n"," (18, 0.49256080114449213),\n"," (18, 0.40646410485552575),\n"," (8, 0.27554632194521395),\n"," (19, 0.27184868967152853),\n"," (18, 0.5825313502478857),\n"," (8, 0.38781144781144783),\n"," (15, 0.49281189083820665),\n"," (11, 0.3624314442413163),\n"," (11, 0.29091026136217335),\n"," (11, 0.2849444444444444),\n"," (5, 0.328525641025641),\n"," (11, 0.40738745210727967),\n"," (11, 0.4420940170940171),\n"," (5, 0.30837870538415),\n"," (9, 0.3257794921247188),\n"," (14, 0.2288244766505636),\n"," (15, 0.3770073331476839),\n"," (15, 0.2528985507246377),\n"," (15, 0.41781321659370446),\n"," (1, 0.3490044247787611),\n"," (15, 0.39183694215716175),\n"," (15, 0.4478204217010187),\n"," (3, 0.3140206812652068),\n"," (1, 0.38927528765241287),\n"," (2, 0.4248560370055698),\n"," (13, 0.18581495098039222),\n"," (13, 0.1994810744810745),\n"," (2, 0.5126300772071164),\n"," (13, 0.44606256742179073),\n"," (19, 0.3810599217103282),\n"," (6, 0.30388507063764797),\n"," (3, 0.25953159041394336),\n"," (8, 0.31652205350686907),\n"," (6, 0.3062193627450981),\n"," (4, 0.4021133890154392),\n"," (12, 0.2753585622947987),\n"," (16, 0.6211167086481948),\n"," (6, 0.22646812582227005),\n"," (8, 0.4050387596899225),\n"," (2, 0.2727758352758353),\n"," (1, 0.36262471220260933),\n"," (2, 0.37877030162412983),\n"," (5, 0.3599515074847143),\n"," (1, 0.47403169014084506),\n"," (2, 0.3111559139784947),\n"," (8, 0.45737836751105737),\n"," (8, 0.3534423994546694),\n"," (8, 0.4959016393442622),\n"," (8, 0.3233183856502242),\n"," (5, 0.589540314650934),\n"," (3, 0.29133053673227927),\n"," (5, 0.6432554325543255),\n"," (5, 0.47548870657348324),\n"," (5, 0.28370370370370374),\n"," (5, 0.5645090845562543),\n"," (2, 0.274428684003152),\n"," (8, 0.26880507199656134),\n"," (5, 0.41466789667896686),\n"," (5, 0.32240634005763696),\n"," (5, 0.3272634602909832),\n"," (0, 0.4584332533972822),\n"," (12, 0.40837479270315097),\n"," (8, 0.36650276052449965),\n"," (16, 0.6644136718320695),\n"," (0, 0.6160203827989064),\n"," (7, 0.20374545910058875),\n"," (16, 0.4073468893730332),\n"," (6, 0.37611533765379906),\n"," (17, 0.3605746304244159),\n"," (16, 0.4661886440480355),\n"," (0, 0.3518518518518518),\n"," (14, 0.24354923644023166),\n"," (0, 0.6126415671870218),\n"," (16, 0.6206543967280164),\n"," (7, 0.33436944937833035),\n"," (16, 0.49314481576692365),\n"," (7, 0.30690447641886487),\n"," (5, 0.28288089225589225),\n"," (0, 0.27131915866483763),\n"," (0, 0.26327985739750454),\n"," (4, 0.5497284453496266),\n"," (4, 0.48547906132503554),\n"," (4, 0.47784294558488105),\n"," (4, 0.5387608353033885),\n"," (4, 0.5257442348008385),\n"," (4, 0.6450292397660817),\n"," (4, 0.5776066350710899),\n"," (4, 0.6266795865633075),\n"," (7, 0.43350446155726685),\n"," (7, 0.38448615455914725),\n"," (14, 0.26065560435985036),\n"," (9, 0.3016564483166613),\n"," (7, 0.2549785019235121),\n"," (11, 0.22982253086419754),\n"," (17, 0.3003154253154253),\n"," (17, 0.29909819639278556),\n"," (5, 0.21659533999048972),\n"," (14, 0.25591296121097445),\n"," (7, 0.26562371684322905),\n"," (17, 0.25423194669547994),\n"," (2, 0.31341021660382934),\n"," (14, 0.1899330900243309),\n"," (13, 0.22409217034624196),\n"," (13, 0.2292181069958848),\n"," (13, 0.5148405126722561),\n"," (9, 0.46609379163336),\n"," (14, 0.34144331685315293),\n"," (2, 0.19658516455173838),\n"," (12, 0.42239535594256034),\n"," (17, 0.20976834768347682),\n"," (12, 0.3652214891611687),\n"," (7, 0.30934570117693994),\n"," (13, 0.34572619874913135),\n"," (2, 0.4391946992864423),\n"," (9, 0.486528917249063),\n"," (19, 0.2769221366581103),\n"," (9, 0.3235897435897435),\n"," (9, 0.39192818772971433),\n"," (16, 0.33460056772100566),\n"," (2, 0.2949783973261596),\n"," (17, 0.25258456288266584),\n"," (13, 0.20411025734845717),\n"," (6, 0.27004297114794346),\n"," (9, 0.30401234567901236),\n"," (14, 0.2683605780620706),\n"," (7, 0.5123020547048815),\n"," (11, 0.23946183752813593),\n"," (4, 0.2904474610356964),\n"," (18, 0.3864847914706805),\n"," (17, 0.19369212962962964),\n"," (18, 0.5262118736383443),\n"," (18, 0.6630029309885425),\n"," (18, 0.21991725768321516),\n"," (18, 0.5136346829358477),\n"," (18, 0.2474681712962963),\n"," (18, 0.3013035950707811),\n"," (12, 0.37400348779272546),\n"," (18, 0.5528218694885363),\n"," (18, 0.694497053872054),\n"," (3, 0.5412784679089028),\n"," (7, 0.3817942556356563),\n"," (15, 0.2876730564430245),\n"," (15, 0.26784576163161256),\n"," (15, 0.3974606857437484),\n"," (15, 0.5210584649487393),\n"," (15, 0.44196239966201933),\n"," (12, 0.3517438721520354),\n"," (1, 0.2290794120062413),\n"," (11, 0.25753546099290775),\n"," (11, 0.3909884941630974),\n"," (5, 0.26197715128063387),\n"," (5, 0.502700940672441),\n"," (13, 0.24075814536340853),\n"," (11, 0.3162694416470852),\n"," (11, 0.3426653569089718),\n"," (11, 0.3737657792775903),\n"," (11, 0.2527680652680652),\n"," (11, 0.33666885389326334),\n"," (11, 0.4373060211049038),\n"," (17, 0.24707300275482094),\n"," (17, 0.3828261629940539),\n"," (18, 0.1884647107340639),\n"," (17, 0.27623196552370244),\n"," (17, 0.2166529152915291),\n"," (10, 0.6185562152483273),\n"," (2, 0.3610446040613638),\n"," (15, 0.1974606457543376),\n"," (6, 0.34456264775413714),\n"," (6, 0.33618455158113014),\n"," (4, 0.4486413984077535),\n"," (9, 0.36943209876543215),\n"," (9, 0.2093062605752961),\n"," (15, 0.6222967934377331),\n"," (12, 0.19925925925925927),\n"," (15, 0.29074074074074074),\n"," (1, 0.4640401027425636),\n"," (1, 0.3888888888888889),\n"," (18, 0.41362805205980424),\n"," (9, 0.2219223849910049),\n"," (5, 0.4669605612438377),\n"," (15, 0.3703220191470844),\n"," (6, 0.30334155026020265),\n"," (12, 0.3296475003350757),\n"," (14, 0.21376289896808257),\n"," (6, 0.4553474484256246),\n"," (3, 0.31297411869701025),\n"," (1, 0.26694306246545046),\n"," (8, 0.3733626919602529),\n"," (8, 0.3710596452531936),\n"," (5, 0.3435327635327635),\n"," (1, 0.39182596575069695),\n"," (8, 0.39837526205450735),\n"," (8, 0.4420479302832244),\n"," (3, 0.4120908699397072),\n"," ...]"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"PRHWMXHuZvdc","colab_type":"code","outputId":"5763805d-dd46-4f0f-8d80-94fd39272840","executionInfo":{"status":"ok","timestamp":1590837055282,"user_tz":-180,"elapsed":660,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(corpus_topics)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1740"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"2vD9zPOMZS4B","colab_type":"text"},"source":["This provides a plethora of options that can be leveraged to extract useful insights from our corpus of research papers. To enable this, we construct a master dataframe that will hold the base statistics, which we use soon to depict different useful insights."]},{"cell_type":"code","metadata":{"id":"5PlLwOd4ZKyT","colab_type":"code","colab":{}},"source":["corpus_topic_df = pd.DataFrame()\n","\n","corpus_topic_df['Document'] = range(0, len(papers))\n","\n","corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n","\n","corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n","\n","corpus_topic_df['Topic Desc'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n","\n","corpus_topic_df['Paper'] = papers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ps6MKY5mclfb","colab_type":"code","outputId":"ab3c18ef-f507-43b0-c3b1-993083534dd4","executionInfo":{"status":"ok","timestamp":1590837059051,"user_tz":-180,"elapsed":1141,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["corpus_topic_df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Document</th>\n","      <th>Dominant Topic</th>\n","      <th>Contribution %</th>\n","      <th>Topic Desc</th>\n","      <th>Paper</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>38.04</td>\n","      <td>distribution, probability, prior, variable, gaussian, mixture, estimate, density, bayesian, approximation, likelihood, sample, log, expert, em, estimation, posterior, step, component, probabilistic</td>\n","      <td>1 \\nCONNECTIVITY VERSUS ENTROPY \\nYaser S. Abu-Mostafa \\nCalifornia Institute of Technology \\nPasadena, CA 91125 \\nABSTRACT \\nHow does the connectivity of a neural network (number of synapses per \\nneuron) relate to the complexity of the problems it can handle (measured by \\nthe entropy)? Switching theory would suggest no relation at all, since all Boolean \\nfunctions can be implemented using a circuit with very low connectivity (e.g., \\nusing two-input NAND gates). However, for a network that learns a problem \\nfrom examples using a local learning rule, we prove that the entropy of the \\nproblem becomes a lower bound for the connectivity of the network. \\nINTRODUCTION \\nThe most distinguishing feature of neural networks is their ability to spon- \\ntaneously learn the desired function from 'training' samples, i.e., their ability \\nto program themselves. Clearly, a given neural network cannot just learn any \\nfunction, there must be some restrictions on which networks can learn which \\nfunctions. One obvious restriction, which is independent of the learning aspect, \\nis that the network must be big enough to accommodate the circuit complex- \\nity of the function it will eventually simulate. Are there restrictions that arise \\nmerely from the fact that the network is expected to learn the function, rather \\nthan being purposely designed for the function? This paper reports a restriction \\nof this kind. \\nThe result imposes a lower bound on the connectivity of the network (num- \\nber of synapses per neuron). This lower bound can only be a consequence of \\nthe learning aspect, since switching theory provides purposely designed circuits \\nof low connectivity (e.g., using only two-input NAND gates) capable of imple- \\nmenting any Boolean function [1,2]. It also follows that the learning mechanism \\nmust be restricted for this lower bound to hold; a powerful mechanism can be \\n@ American Institute of Physics 1988 \\n2 \\ndesigned that will find one of the low-connectivity circuits (perhaps by exhaus- \\ntive search), and hence the lower bound on connectivity cannot hold in general. \\nIndeed, we restrict the learning mechanism to be local; when a training sample \\nis loaded into the network, each neuron has access only to those bits carried by \\nitself and the neurons it is directly connected to. This is a strong assumption \\nthat excludes sophisticated learning mechanisms used in neural-network models, \\nbut may be more plausible from a biological point of view. \\nThe lower bound on the connectivity of the network is given in terms of \\nthe entropy of the environment that provides the training samples. Entropy is a \\nquantitative measure of the disorder or randomness in an environment or, equiv- \\nalently, the amount of information needed to specify the environment. There \\nare many different ways to define entropy, and many technical variations of this \\nconcept [3]. In the next section, we shall introduce the formal definitions and \\nresults, but we start here with an informal exposition of the ideas involved. \\nThe environment in our model produces patterns represented by N bits \\nx = x... xN (pixels in the picture of a visual scene if you will). Only h different \\npatterns can be generated by a given environment, where h &lt; 2 v (the entropy \\nis essentially log 2 h). No knowledge is assumed about which patterns the en- \\nvironment is likely to generate, only that there are h of them. In the learning \\nprocess, a huge number of sample patterns are generated at random from the \\nenvironment and input to the network, one bit per neuron. The network uses \\nthis information to set its internal parameters and gradually tune itself to this \\nparticular environment. Because of the network architecture, each neuron knows \\nonly its own bit and (at best) the bits of the neurons it is directly connected to \\nby a synapse. Hence, the learning rules are local: a neuron does not have the \\nbenefit of the entire global pattern that is being learned. \\nAfter the learning process has taken place, each neuron is ready to perform \\na function derned by vhat it has larned. The collective interaction of the \\nfunctions of the neurons is what defines the overall function of the network. The \\nmain result of this paper is that (roughly speaking) if the connectivity of the \\nnetwork is less than the entropy of the environment, the network cannot learn \\nabout the environment. The idea of the proof is to show that if the connectivity \\nis small, the final function of each neuron is independent of the environment, \\nand hence to conclude that the overall network has accumulated no information \\nabout the environment it is supposed to learn about. \\nFORMAL RESULT \\nA neural network is an undirected graph (the vertices are the neurons and the \\nedges are the synapses). Label the neurons 1,-.-, N and define K, _c 1,.. -, N \\nto be the set of neurons connected by a synapse to neuron n, together with \\nneuron n itself. An environment is a subset e __C C0, 1 v (each x  e is a sample \\n3 \\nfrom the environment). During learning, Zl,-\", zv (the bits of x) are loaded \\ninto the neurons 1,...,N, respectively. Consider an arbitrary neuron n and \\ntelabel everything to make K become {1,...,K}. Thus the neuron sees the \\nfirst K coordinates of each x. \\nSince our result is asymptotic in N, we will specify K as a function of N; \\nK = aN where a = a(N) satifies limN-.,o a(N) = ao (0 &lt; ao &lt; 1). Since the \\nresult is also statistical, we will consider the ensemble of environments  \\ne = {, c I I,I = \\nwhere h = 2 oN and/ =/(N) satifies limN_.oo/(N) = /o (0 &lt; /o &lt; 1). The \\nprobability distribution on  is uniform; any environmen e G    likely \\noccur  any oher. \\nThe neuron sees only he firs K coordinates of each x generated by he \\nenvironment e. For each e, we define the function : {0,1}   {0,1,2,--.} \\nwhere \\nn(a...a) =l{x6e [ z,=a, fork=l,--.,K}l \\nand the normalized version \\nThe function v describes the relative frequency of occurrence for each of the 2 r \\nbinary vectors a:l -\" zr as x = zl '\" z Jr runs through all h vectors in e. In other \\nwords, /specifies the projection of e as seen by the neuron. Clearly, v(a) _&gt; 0 \\nfor all a  {0, 1} r and Z&amp;E{O,1}K v(a) = 1. \\nCorresponding to two environments el and es, we will have two functions vl \\nand bt 2. If//1 is not distinguishable from t/z, the neuron cannot tell the difference \\nbetween ea and es. The distinguishability between btl and t/: can be measured \\nby \\n1 \\nd(l/l'l/2) --  Z \\nThe range of d(t/1,) is 0 &lt;_ d(t/1,) &lt;_ 1, where '0' corresponds to complete \\nindistinguishability while '1' corresponds to maximum distinguishability. We \\naxe now in a position to state the main result. \\nLet e and es be independently selected environments from  according to the \\nuniform probability distribution. d(vl, v) is now a random variable, and we are \\ninterested in the expected value E(d(vl,v2)). The case where E(d(vl,v2)) -- 0 \\ncorresponds to the neuron getting no information about the environment, while \\nthe case where E(d(Vl,V2)) = I corresponds to the neuron getting maximum \\ninformation. The theorem predicts, in the limit, one of these extremes depending \\non how the connectivity (ao) compares to the entropy (/o). \\n4 \\nTheorem. \\n1. If ao &gt; f/o, then limv-.o E (d(Vl, v2)) = 1. \\n2. If co &lt; o, then limN._.ooS(d(,,,v2)) =0. \\nThe proof is given in the appendix, but the idea is easy to illustrate infor- \\nmally. Suppose h = 2 K+ (corresponding to part 2 of the theorem). For most \\nenvironments e 6 , the first K bits of x 6 e go through all 2 K possible val- \\nues approximately 2  times each as x goes through all h possible values once. \\nTherefore, the patterns seen by the neuron are drawn from the fixed ensemble of \\nall binary vectors of length K with essentially uniform probability distribution, \\ni.e., v is the same for most environments. This means that, statistically, the \\nneuron will end up doing the same function regardless of the environment at \\nhand. \\nWhat about the opposite case, where h = 2 K- (corresponding to part 1 of \\nthe theorem)? Now, with only 2 - patterns available from the environment, \\nthe first K bits of x can assume at most 2 K- values out of the possible 2 g \\nvalues a binary vector of length K can assume in principle. Furthermore, which \\nvalues can be assumed depends on the particular environment at hand, i.e., \\n, does depend on the environment. Therefore, although the neuron still does \\nnot have the global picture, the information it has says something about the \\nenvironment. \\nACKNOWLEDGEMENT \\nThis work was supported by the Air Force Office of Scientific Research under \\nGrant AFOSR-86-0296. \\nAPPENDIX \\nIn this appendix we prove the main theorem. We start by discussing some \\nbasic properties about the ensemble of environments . Since the probability \\ndistribution on  is uniform and since [1-- (2h), we have \\nwhich is equivalent to generating e by choosing h elements x 6 {0, 1} v with \\nuniform probability (without replacement). It follows that \\nh \\nPr(x6e)= 2 v \\n5 \\nwhile for x 1  x2, \\nh h-1 \\nPr(xle, xie) = 2 v x 2 v_l \\nand so on. \\nThe functions  and  are defined on K-bit vectors. \\n(a random variable for fixed a) is independent of a \\nThe statistics of r(a) \\nPr(r(sx) = m) = Pr(r(s) = m) \\nwhich follows from the symmetry with respect to each bit of a. The same holds \\nfor the statistics of (a). The expected value E(r(a)) = h2 -K (h objects going \\ninto 2 K cells), hence E((a)) = 2 -. We now restate and prove the theorem. \\nTheorem. \\n1. If co &gt; o, then limr-.o E (d(l, 2)) = 1. \\n2. If ao &lt; o, then limN-o E (d(l, 2)) =0. \\nProof. \\nWe expand E (d(l, 2)) as follows \\nwhere n and n2 denote nl(0.--0) and n2(0..-0), respectively, and the last step \\nfollows from the fact that the statistics of nl(a) and n2(a) is independent of a. \\nTherefore, to prove the theorem, we evaluate E(Irh- r21) for large N. \\n1. Assume ao &gt; fo. Let n denote n(0...0), and consider Pr(n - 0). For r to \\nbe zero, all 2 N-K strings x of N bits starting with K O's must not be in the \\nenvironment e. Hence \\nPr(r = O) = (1- -- \\nh \\n2)(1 \\nh h \\n2 r - 1 )'\" (1 - 2 r _ 2r_: + 1 ) \\nwhere the first term is the probability that 0... 00  e, the second term is the \\n6 \\nprobability that 0.--O1  e given that 0-.. O0  e, and so on. \\n: (1 - h2-m'(1 -- 2-x) -') \\n&gt;_ (1 - \\n&gt; 1 - 2h2-r2 v-K \\n= 1 - 2h2 -K \\n2N--K \\nHence, Pr(n, = 0): Pr(n2: 0): Pr(n: 0) _&gt; i - 2h2 -K. However, E(n,) = \\nE(n2) = h2 -. Therefore, \\nh h \\nZ Z Pr(rtl: i, rt2 '-- j)li -- Jl \\ni=0 j=O \\nh h \\n= Z Y] Pr(nl = i)Pr(n2: j)l i - Jl \\ni=0 j=0 \\nh \\n_  Pt(hi = 0)Pr(n2 = j)j \\nh \\n+ Z Pt(hi =/)Pr(n2 = 0)i \\ni=0 \\nwhich follows by throwing away all the terms where neither i nor j is zero (the \\nterm where both i an j are zero appears twice for convenience, but this term is \\nzero anyway). \\n= Pr(nl = 0)E(n2) + Pr(n2 = 0)E(rh) \\n&gt; 2(1- 2h2-)h2 - \\nSubstituting this estimate in the expression for E(d(tl, t2)), we get \\n2 K \\nE(d(//1,//2)) = 2hE(lnl -- \\n_ - x 2(1- 2h2-K)h2 -t \\n= 1 - 2h2 -K \\n= 1 - 2 x 2 (f-\")v \\nSince ao &gt;/o by assumption, this lower bound goes to 1 as N goes to infinity. \\nSince 1 is also an upper bound for d(l, 2) (and hence an upper bound for the \\nexpected value E(d(/l,/2))), limv-.oo E(d(/,/2)) must be 1. \\n7 \\n2. Assume ao &lt; o. Consider \\nTo evaluate E([n - h2-K[), we estimate the variance of n and use the fact \\nthat E([n- h2-KI) &lt;_ va, (recall that h2 - = E(n)). Since vat(n) = \\nE(n 2) - (E(n)) 2, we need an estimate for E(n2). We write n = Eaei0.1)N- a, \\nwhere \\n1, if 0.-.0a 6 e; \\n5 = 0, otherwise. \\nIn this notation, E(n 2) can be written as \\n: &gt;- \\n&amp;{0,1} N-I be{0,1} \\nFor the 'diagonal' terms (a = b), \\n= h2 -/v \\nThere are 2 N-K such diagonal terms, hence a total contribution of 2 N-K X \\nh2 -r = h2 -K to the sum. For the 'off-diagonal' terms (a  b), \\nE(5. Sb) = Pr(5. = 1, Sb = 1) \\n= Pr(5. = 1)Pr(Sb: l[a = 1) \\nh h-1 \\n-- -- X \\n2 r 2 r - I \\nThere are 2r-(2 v-K - 1) such off-diagonal terms, hence a total contribution of \\n2V-(2N-K  h(h-i) &lt; {h,_K2 2 v \\nk --'1 ^ 2N(2N--1) -- ,  ] --i to the sum. Putting the contributions \\n8 \\nfrom the diagonal and off-diagonal terms together, we get \\nwr(n) = E(n 2) - (E(n)) 2 \\n&lt; h2-K + (h2-K)a2v - 1 \\n1 \\n= h2-K + (h2-)a 2 N - 1 \\nh2_  \\n= h2 -K 1 + \\n&lt; 2h2 - \\nThe last step follows since h2 -K is much smaller than 2 r - 1. Therefore, E(In - \\n1 \\nh2-]) &lt; v &lt; (2h2-)  Substituting this estimate in the expression for \\nwe get \\nSince Cto &lt; f/o by assumption, this upper bound goes to 0 as N goes to infinity. \\nSince 0 is also a lower bound for d(yx,ya) (and hence a lower bound for the \\nexpected value E(d(,x,,a))), limo E(d(,,,a)) must be 0. I \\nREFERENCES \\n[1] Y. Abu-Mostafa, \"Neural networks for computing?,\"AIP Conference Pro- \\nceedings  151, Neural Networks for Computing, J. Denker (ed.), pp. 1-6, 1986. \\n[2] Z. Kohavi, Switching and Finite Automata Theory, McGraw-Hill, 1978. \\n[3] Y. Abu-Mostafa, \"The complexity of information extraction,\"IEEE Trans. \\non Information Theory, vol. IT-32, pp. 513-525, July 1986. \\n[4] Y. Abu-Mostafa, \"Complexity in neural systems,\"in Analog VLSiand Neural \\nSystems by C. Mead, Addison-Wesley, 1988. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>13</td>\n","      <td>38.55</td>\n","      <td>rule, representation, structure, sequence, symbol, connectionist, language, level, unit, string, activation, context, pattern, role, similarity, represented, task, learned, note, part</td>\n","      <td>22 \\nLEARNING ON A GENERAL NETWORK \\nAmir F. Atiya \\nDepartment of Electrical Engineering \\nCalifornia Institute of Technology \\nCa 91125 \\nAbstract \\nThis paper generalizes the backpropagation method to a general network containing feed- \\nback connections. The network model considered consists of interconnected groups of neurons, \\nwhere each group could be fully interconnected (it could have feedback connections, with pos- \\nsibly asymmetric weights), but no loops between the groups are allowed. A stochastic descent \\nalgorithm is applied, under a certain inequality constraint on each intra-group weight matrix \\nwhich ensures for the network to possess a unique equilibrium state for every input. \\nIntroduction \\nIt has been shown in the last few years that large networks of interconnected \"neuron\"-like \\nelements re quite suitable for performing a variety of computational and pattern recognition \\ntasks. One of the well-known neural network models is the backpropagation model [1]-[4]. It \\nis an elegant way for teaching a layered feedforward network by a set of given input/output \\nexamples. Neural network models having feedback connections, on the other hand, have lso \\nbeen devised (for example the Hopfield network [5]), and are shown to be quite successful in \\nperforming some computational tasks. It is important, though, to have a method for learning \\nby examples for a feedback network, since this is a general way of design, and thus one can \\navoid using an ad hoc design method for each different computational task. The existence \\nof feedback is expected to improve the computational abilities of a given network. This is \\nbecause in feedback networks the state iterates until a stable state is reached. Thus processing \\nis performed on several steps or recursions. This, in general allows more processing abilities \\nthan the \"single step\" feedforward case (note also the fact that a feedforward network is \\na special case of a feedback network). Therefore, in this work we consider the problem of \\ndeveloping a general learning algorithm for feedback networks. \\nIn developing a learning algorithm for feedback networks, one has to pay attention to the \\nfollowing (see Fig. I for an example of a configuration of a feedback network). The state of \\nthe network evolves in time until it goes to equilibrium, or possibly other types of behavior \\nsuch as periodic or chaotic motion could occur. However, we are interested in having a steady \\nand and fixed output for every input applied to the network. Therefore, we have the following \\ntwo important requirements for the network. Beginning in any initial condition, the state \\nshould ultimately go to equilibrium. The other requirement is that we have to have a unique \\nAmerican Institute of Physics 1988 \\n23 \\nequilibrium state. It is in fact that equilibrium state that determines the final output. The \\nobjective of the learning algorithm is to adjust the parameters (weights) of the network in small \\nsteps, so as to move the unique equilibrium state in a way that will result finally in an output \\nas close as possible to the required one (for each given input). The existence of more than one \\nequilibrium state for a given input causes the following problems. In some iterations one might \\nbe updating the weights so as to move one of the equilibrium states in a sought direction, while \\nin other iterations (especially with different input examples) a different equilibrium state is \\nmoved. Another important point is that when implementing the network (after the completion \\nof learning), for a fixed input there can be more than one possible output. Independently, other \\nwork appeared recently on training a feedback network [6],[7],[8]. Learning algorithms were \\ndeveloped, but solving the problem of ensuring a unique equilibrium was not considered, This \\nproblem is addressed in this paper and an appropriate network and a learning algorithm are \\nproposed. \\noutputs \\nFig. 1 \\nA recurrent network \\nThe Feedback Network \\nConsider a group of n neurons which could be fully inter-connected (see Fig. ! for an \\nexample). The weight matrix W can be asymmetric (as opposed to the Hopfield network). \\nThe inputs are also weighted before entering into the network (let V be the weight matrix). \\nLet x and y be the input and output vectors respectively. In our model y is governed by the \\nfollowing set of differential equations, proposed by Hopfield [5]: \\ndu \\n d--[ = Wf(u) - u + Vx, y = f(u) (1) \\n24 \\nwhere f(u) = (f(ux),...,f(ur)) T, W denotes the transpose operator, f is a bounded and \\ndifferentiable function, and r is a positive constant. \\nFor a given input, we would like the network after a short transient period to give a steady \\nand fixed output, no matter what the initial network state was. This means that beginning \\nany initial condition, the state is to be attracted towards a unique equilibrium. This leads to \\nlooking for a condition on the matrix W. \\nTheorem: A network (not necessarily symmetric) satisfying \\n2 1/max(?)2, \\ni j \\nexhibits no other behavior except going to a unique equilibrium for a given input. \\nProof: Let u(t) and u2(t) be two solutions of (1). Let \\nJ(t) = Ilu(t) - u2(t)ll 2 \\nwhere [I II is the [wo-norm. Differentiating J with respect to time, one obtains \\ndJ(t) _ 2(u(t)-u2(t))r(du(t) du2(t)  \\ndt dt  1' \\nUsing (1) , the expression becomes \\ndJ(t) 2 2 \\ndt - r []ul(t) -u2(t))[12 + ;(u(t) -u2(t))\"W[f(u(t)) - f(u2(t))]. \\nUsing Schwarz's Inequality, we obtain \\n2 \\ndJ(t) &lt; _211u(t ) _ u(t)ll  + _11u (t)_ u(t)11. iiW[f(u (t)) _ f(u2(t))] ii. \\ndt - r r \\nAgain, by Schwarz's Inequality, \\nw/ [f(u (t)) - f(u(t))] &lt;_ Ilwgll - Ilf(u(t)) - f(u2(t))11 , i = 1,...,n \\nwhere wi denotes the i th row of W. Using the mean value theorem, we get \\n[If(u (t)) - f(u2(t)) II &lt; (maxlf'l)llu (t) - u2(t)l I. (3) \\nUsing (2),(3), and the expression for J(t), we get \\ndJ(t) &lt;-aJ(t) (4) \\ndt - \\nwhere \\n2 2 (maxl f,i)/- jwj.. \\nT T i \\n(2) \\n25 \\nBy hypothesis of the Theorem, a is strictly positive. Multiplying both sides of (4) by exp(at), \\nthe inequality \\nd 0 \\n- \\nresults, from which we obtain \\nJ(t) _&lt; J(0)e \\nFrom that and from the fact that J is non-negative, it follows that J(t) goes to zero as t \\nTherefore, any two solutions corresponding to any two initial conditions ultimately approach \\neach other. To show that this asymptotic solution is in fact an equilibrium, one simply takes \\nu2(t): u(t + T), where T is a constant, and applies the above argument (that J(t) - 0 as \\nt --* c), and hence u (t + T) -- u (t) as t --* cw for any T, and this completes the proof. \\nFor example, if the function f is of the following widely used sigmoid-shaped form, \\n1 \\nf (u) - 1 + e- \\nthen the sum of the square of the weights should be less than 16. Note that for any function \\nf, scaling does not have an effect on the overall results. We have to work in our updating \\nscheme subject to the constraint given in the Theorem. In many cases where a large network \\nis necessary, this constraint might be too restrictive. Therefore we propose a general network, \\nwhich is explained in the next Section. \\nThe General Network \\nWe propose the following network (for an example refer to Fig. 2). The neurons are \\npartitioned into several groups. Within each group there are no restrictions on the connections \\nand therefore the group could be fully interconnected (i.e. it could have feedback connections). \\nThe groups are connected to each other, but in a way that there are no loops. The inputs to \\nthe whole network can be connected to the inputs of any of the groups (each input can have \\nseveral connections to several groups). The outputs of the whole network are taken to be the \\noutputs (or part of the outputs) of a certain group, say group f. The constraint given in the \\nTheorem is applied on each intra-group weight matrix separately. Let (qa, s,), a = 1, ..., N be \\nthe input/output vector pairs of the function to be implemented. We would like to minimize \\nthe sum of the square error, given by \\nN \\na----1 \\nwhere \\nM \\n---- --$i) , \\ni----1 \\nand f is the output vector of group f upon giving input q, and M is the dimension of vector \\ns a. The learning process is performed by feeding the input examples q sequentially to the \\nnetwork, each time updatg the weights in an attempt to minimize the eor. \\n26 \\ninputa outputs \\nFig. 2 \\nAn example of a general network \\n(each group represents a recurrent network) \\nNow, consider a single group l. Let W t be the intra-group weight matrix of group l, V 'q \\nbe the matrix of weights between the outputs of group r and the inputs of group l, and yt be \\nI rl \\nthe output vector of group l. Let the respective elements be wo. , vO. , and Yi- Furthermore, \\nlet nt be the number of neurons of group l. Assume that the time constant  is sufficiently \\nsmall so s to allow the network to settle quickly to the equilibrium state, which is given by \\nthe solution of the equation \\nyt= f(Wtyt + E VtYr)' (5) \\nrAl \\nwhere At is the set of the indices of the groups whose outputs are connected to the inputs of \\ngroup l. We would like each iteration to update the weight matrices W t and V rt so as to move \\nthe equilibrium in a direction to decrease the error. We need therefore to know the change in \\nthe error produced by a small change in the weight matrices. Let Oe \\n3-Wr, and ov-r, denote the \\nOe Oe Oea, \\nmatrices whose (i,j) th element are -,, and respectively. Let 3-fy be the column vector \\nwhose i th element is . We obtain the following relations: \\naea \\nav,,t \\n-1 3a t T \\n = [,_ (w') T] y,(y ) , \\n-1 aCa r T \\n- ['- (w')q y,(y ) , \\nwheee A t is the diagonal matrix whose ita diagonal element is 1/f'( k t t ,q ,- \\nfor a derivation refer to Appends). The vector  sociaed with group I can be obtaed \\nin erms of the vectors  jeBt where Bt is the se of he indices of the groups whose puts \\ne connected to the outputs of group l..We get (refer to Appends} \\n = (v')[a -(wq] - \\n., aye' () \\nThe matrix A t - (Wt? ' for any group l can never be singular, so we will not face any \\nproblem in the updating process. To prove that, let z be a vector satisfying \\n[' - (w')],. = 0. \\n27 \\nWe can write \\n,/maxl.f'l _&lt; -?:,, i= ],..., , \\nwhere z  the i  element of z. Usg Schwarz's Inequity, we obtain \\n_ ..., \\nSquing both sides and dg the inequalities for i = 1, ..., n, we get \\n,,)- (7) \\nk i k \\nSince the condition \\nw' 2 1/max(),)2) \\n, \\ni k \\nis enforced, it follows that (7) cannot be satisfied unless z is the zero vector. Thus, the matrix \\nAi - (Wl) T cannot be singular. \\nFor each iteration we begin by updating the weights of group f (the group containing \\n0e equals simply 2(y{ - s,...,y - sM 0, 0)T). Then \\nthe final outputs). For that group yy , ..., \\nwe move backwards to the groups connected to that group and obtain their corresponding \\n0e, vectors using (6) update the weights, and proceed in the same manner until we complete \\n0y ' \\nupdating all the groups. Updating the weights is performed using the following stochastic \\ndescent algorithm for each group, \\nO ea \\nc9 ea \\nAV ----- --aa + adeaR , \\nwhere 1 is a noise matrix whose elements axe characterized by independent zero-mean unity- \\nvariance Gaussian densities, and the a's are parameters. The purpose of adding noise is to \\nallow escaping local minima if one gets stuck in any of them. Note that the control parameter \\nis taken to be ea. Hence the variance of the added noise tends to decrease the more we \\napproach the ideal zero-error solution. This makes sense because for a large error, i.e. for an \\nunsatisfactory solution, it pays more to add noise to the weight matrices in order to escape \\nlocal minima. On the other hand, if the error is small, then we are possibly near the global \\nminimum or to an acceptable solution, and hence we do not want too much noise in order \\nnot to be thrown out of that basin. Note that once we reach the ideal zero-error solution the \\nadded noise as well as the gradient of e, become zero for all a and hence the increments of the \\nweight matrices become zero. If a/ter a certain iteration W happens to violate the constraint \\ni.w. _ constant &lt; 1/rnax(f') , then its elements are scaled so as to project it back onto \\nthe surface of the hypershere. \\nImplementation Example \\nA pattern recognition example is considered. Fig. $ shows a set of two-dimensional \\ntraining patterns from three classes. It is required to design a neural network recognizer with \\n28 \\nthree output neurons. Each of the neurons should be on if a sample of the corresponding class is \\npresented, and off otherwise, i.e. we would like to design a \"winner-take-all\" network. A single- \\nlayer three neuron feedback network is implemented. We obtained 3.3% error. Performing the \\nsame experiment on a feedforward single-layer network with three neurons, we obtained 20% \\nerror. For satisfactory results, a feedforward network should be two-layer. With one neuron \\nin the first layer and three in the second layer, we got 36.7% error. Finally, with two neurons \\nin the first layer and three in the second layer, we got a match with the feedback case, with \\n3.3% error. \\n2 \\n3 33 \\n3 3 \\n2 \\n2 \\n2 \\n2 \\n2 2 1 \\n2 2 1 \\n2 \\n2 2 1 \\n2 \\n3 \\n3 \\nI 1 \\n1 \\n33333 3 \\n3 \\n3 \\n3 \\n1 \\nI I \\n1 \\n1 \\nI I \\nFig. 3 \\nA pattern recognition example \\nConclusion \\nA way to extend the backpropagation method to feedback networks has been proposed. \\nA condition on the weight matrix is obtained, to insure having only one fixed point, so as \\nto prevent having more than one possible output for a fixed input. A general structure for \\nnetworks is presented, in which the network consists of a number of feedback groups connected \\nto each other in a feedforward manner. A stochastic descent rule is used to update the weights. \\nThe method is applied to a pattern recognition example. With a single-layer feedback network \\ni obtained good results. On the other hand, the feedforward backpropagation method achieved \\ngood resuls only for the case of more than one layer, hence also with a larger number of neurons \\nthan the feedback case. \\n29 \\nAcknowledgement \\nThe author would like to gratefully acknowledge Dr. Y. Abu-Mostafa for the useful \\ndiscussions. This work is supported by Air Force Office of Scientific Research under Grant \\nAFOSR-86-0296. \\nAppendix \\nDifferentiating (5), one obtains \\nm tOjm OtOp q- ypSjk), \\nOwl, - r(4.)( ' Oy, , \\nk, p = 1, ..., nt \\nwhere \\n1 ifj=k \\n8Y= 0 otherwise, \\nand \\nl \\nZj = \\nI I rl r \\nm reA rr \\nWe can write \\nc9yt -- (A t- Wt)-b \" (A- 1) \\nwhere b kv is the nt-dimensional vector whose i tn component is given by \\nt ifi=k \\nb . p= Yp \\n 0 otherwise. \\nBy the chain rule, \\nwhich, upon substituting from (A 1), can be put in the form  \\n- Yp o--fy, where g is the ktn \\ncolumn of (A t - Wt) -. Finally, we obtain the required expression, which is \\nOea Oea I T \\naw, - ['-(w')]-' W' (Y) ' \\nRegarding  it is obtained by differentiating (5) with respect to v t We get similarly \\nOVr ' kp' \\nOy  _ (A t - W)-c, \\nwitere c kp is the nt-dimensional vector whose i tn component is given by \\ncp { y[, ff i = k \\ni = 0 otherwise. \\n3O \\nA derivation very similar to the case of  results in the following required expression: \\naea - 1 aea r T \\naW, - ) ' \\nNow, finally consider oe o j ay \\n-. Let -', jeBt be the matrix whose (k,p) ta element is b'y' The \\nelements of  can be obtained by differentiating the equation for the fixed point for group \\nj, as follows, \\nag .  \\n,,, )' \\nHence, \\nay' - (A'- W')-IV'\" (A- 2) \\nUsing the chain rule, one can write \\nay, -  (---) ayy' \\nWe substitute from (A - 2) into the previous equation to complete the derivation by obtaining \\naea \\nae,, _ \\nay  \\n'B \\nReferences \\n[1] P. Werbos, \"Beyond regression: New tools for prediction and analysis in behavioral sci- \\nences\", Harvard University dissertation, 1974. \\n[2] D. Parker, \"Learning logic\", MIT Tech Report TR-47, Center for Computational Research \\nin Economics and Management Science, 1985. \\n[3] Y. Le Cun, \"A learning scheme for asymmetric threshold network\" Proceedings of Cog- \\nnitiva, Paris, June 1985. \\n[4] D. Rumelhart, G.Hinton, and R. Wilhams, \"earning internal representations by error \\npropagation\" in D. Rumelhart, J. McLelland and the PDP research group (Eds.), Parallel \\ndistributed processing: Explorations in the microstructure of co9nition, Vol. 1, MIT Press, \\nCambridge, MA, 1986. \\n[5] J. Hopfield, \"Neurons with graded response have collective computational properties hke \\nthose of two-state neurons\", Proc. Natl. Acad. Sci. USA, May 1984. \\n[6] L. Almeida, \"A learning rule for asynchronous perceptrons with feedback in a combinato- \\nrim environment\" Proc. of the First Int. Annual Conf. on Neural Networks, San Diego, \\nJune 1987. \\n[7] R. Rohwer, and B. Forrest, \"Training time-dependence in neural networks\", Proc. of the \\nFirst Int. Annual Conf. on Neural Networks, San Diego, June 1987. \\n[8] F. Pineda, \"Generahzation of back-propagation to recurrent neural networks\", Phys. Rev. \\nLett., vol. 59, no. 19, 9 Nov. 1987. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>13</td>\n","      <td>13.46</td>\n","      <td>rule, representation, structure, sequence, symbol, connectionist, language, level, unit, string, activation, context, pattern, role, similarity, represented, task, learned, note, part</td>\n","      <td>31 \\nAN ARTIFICIAL NEURAL NETWORK FOR SPATIO- \\nTEMPORAL BIPOLAR PATTERNS: APPLICATION TO \\nPHONEME CLASSIFICATION \\nToshiteru Homma \\nLes E. Atlas \\nRobert J. Marks H \\nInteractive Systems Design Laboratory \\nDepartment of Electrical Engineering, FT-10 \\nUniversity of Washington \\nSeattle, Washington 98195 \\nABSTRACT \\nAn artificial neural network is developed to recognize spatioqemporal \\nbipolar patterns associatively. The function of a formal neuron is generalized by \\nreplacing multiplication with convolution, weights with transfer functions, and \\nthresholding with nonlinear transform following adaptation. The Hebbian learn- \\ning rule and the delta learning rule are generalized accordingly, resulting in the \\nlearning of weights and delays. The neural network which was first developed \\nfor spatial patterns was thus generalized for spario-temporal patterns. It was \\ntested using a set of bipolar input patterns derived from speech signals, showing \\nrobust classification of 30 model phonemes. \\n1. INTRODUCTION \\nLearning spatio-temporal (or dynamic) patterns is of prominent importance in biological \\nsystems and in artificial neural network systems as well. In biological systems, it relates to such \\nissues as classical and operant conditioning, temporal coordination of sensorimotor systems and \\ntemporal reasoning. In artificial systems, it addresses such real-world tasks as robot control, \\nspeech recognition, dynamic image processing, moving target detection by sonars or radars, EEG \\ndiagnosis, and seismic signal processing. \\nMost of the processing elements used in neural network models for practical applications \\nhave been the formal neuron 1 or its variations. These elements lack a memory flexible to tem- \\nporal patterns, thus limiting most of the neural network models previously proposed to problems \\nof spatial (or static) patterns. Some past solutions have been to convert the dynamic problems to \\nstatic ones using buffer (or storage) neurons, or using a layered network with/without feedback. \\nWe propose in this paper to use a \"dynamic formal neuron\" as a processing element for \\nlearning dynamic patterns. The operation of the dynamic neuron is a temporal generalization of \\nthe formal neuron. As shown in the paper, the generalization is straightforward when the activa- \\ntion part of neuron operation is expressed in the frequency domain. Many of the existing learn- \\ning rules for static patterns can be easily generalized for dynamic patterns accordingly. We show \\nsome examples of applying these neural networks to classifying 30 model phonemes. \\nAmerican Institute of Physics 1988 \\n32 \\n2. FORMAL NEURON AND DYNAMIC FORMAL NEURON \\nThe formal neuron is schematically drawn in Fig. l(a), where \\nInput \\nActivation \\nOutput \\nTransmittance \\nNode operator \\nNeuron operation \\n= [x x2    xt.] r \\nYi, i = 1,2 ..... N \\nzi, i = 1,2 ..... N \\n= [wi wi2''' w.] r \\nwhere 1(') is a nonlinear memoryless transform \\n(2.1) \\nNote that a threshold can be implicitly included as a transmittance from a constant input. \\nIn its original form of formal neuron, xi  {0,1} and '1(') is a unit step function u (-). A \\nvariation of it is a bipolar formal neuron where xi  {-1,1} and 11(. ) is the sign function sgn('). \\nWhen the inputs and output are converted to frequency of spikes, it may be expressed as \\nxi  R and 11(') is a rectifying function r('). Other node operators such as a sigmoidal function \\nmay be used. \\nWe generalize the notion of formal neuron so that the input and output are functions of \\ntime. In doing so, weights are replaced with transfer functions, multiplication with convolution, \\nand the node operator with a nonlinear transform following adaptation as often observed in bio- \\nlogical systems. \\nFig. l(b) shows a schematic diagram of a dynamic formal neuron where \\nInput \\nActivation \\nOutput \\nTransfer function \\nAdaptation \\nNode operator \\nNeuron operation \\n(t) = [Xl(t ) x2(t )    xr(t)] ? \\nyi(t), i = 1,2 ..... N \\nzi (t), i = 1,2 ..... N \\nv(t) = [Wil(t ) Wi2(t ) ''' w.(t)] r \\nai (t) \\n11 where '1(') is a nonlinear memoryless transform \\nz i (t) = 11 (a i (-t), q (t)T, 2(t )) \\n(2.2) \\nFor convenience, we denote \\nwith b(0 is equivalent to correlating a(-0 with b(t). \\nIf the Fourier transforms (f ) = F {(t)}, \\nai Or ) = F {ai (t )} exist, then \\nyi O c ) = ai O c) [qOc) cr \\nwhere q (f)cr is the conjugate transpose of /(t). \\n X Wi L Yl = W ZI \\n (a) \\n/(f) = F {/(t)}, Yi(f) = F{yi(t)}, \\n. as correlation instead of convolution. Note that convolving a(t) \\nand \\nx,(I) \\nX2(I) W (I) w.(t) \\n.,,,,,'\"''WL() \\nx,lt) \\nFig. 1. Formal Neuron and Dynamic Formal Neuron. \\n(2.3) \\nz(i) \\n33 \\n3. LEARNING FOR FORMAL NEURON AND DYNAMIC FORMAL NEURON \\nA number of learning rules for formal neurons has been proposed in the past. In the fol- \\nlowing paragraphs, we formulate a learning problem and describe two of the existing learning \\nrules, namely, Hebbian learning and delta learning, as examples. \\nPresent to the neural network M pairs of input and desired output samples \\n{k), k)}, k = 1,2 ..... M , in order. Let W () = [t ) 2 ()  .. )]r where ?) is the \\ntransmittance vector at the k-th step of learning. Likewise, let \\n_Z () = [z 40 z 4-)  ' ' z4)], and D () = [0) (2) ... ()], \\nwhere \\nk) = W( ), z ) = 1'()), and q(.) = [q(Y0 q(Y2) ' ' ' al(Yv)] r. \\nThe Hebbian learning rule 2 is described as follows*: \\nW (k) = W(-0 + eu() \\nThe delta learning (or LMS learning) rule 3, 4 is described as follows: \\n_w() = _w(k-o _ a{_w(-) _ \\n(3.1) \\n(3.2) \\nThe leaming rules described in the previous section are generalized for the dynamic formal \\nneuron by replacing multiplication with correlation. First, the problem is reformulated and then \\nthe generalized rules are described as follows. \\nPresent to the neural network M pairs of time-varing input and output samples \\n{k)(t), )(t)}, k = 1,2 ..... M , in order. Let W()(t) = [g(t)()(t) ?)(t).   k)(t)] r \\nwhere ,Vi()(t) is the vector whose elements w?)(t) are transfer functions connecting the input j \\nto the neuron i at the k-th step of learning. The Hebbian learning rule for the dynamic neuron is \\nthen \\nW()(t ) = W(-O(t ) + a(-t ), k)(t ), )(t ) r . (3.3) \\nThe delta learning rule for dynamic neuron is then \\nW()(t ) = W(-O(t ) - a(-t ), {W(-0(t ), k)(t ) - )(t )}, )(t )r . (3.4) \\nThis generalization procedure can be applied to other learning rules in some linear discrim- \\ninant systems 5 , the serf-organizing mapping system by Kohonen 6 , the perceptton 7, the back- \\npropagation model 3 , etc. When a system includes a nonlinear operation, more careful analysis \\nis necesssay as pointed out in the Discussion section. \\n4. DELTA LEARNING, PSEUDO INVERSE AND REGULARIZATION \\nThis section reviews the relation of the delta learning rule to the pseudo-inverse and the \\ntechnique known as regularization. 4, 6, 8, 9, lo \\nConsider a minimization problem as described below: Find __W which minimizes \\nk \\nsubject to y(k) = \\nA solution by the delta rule is, using a gradient descent method, \\n_w() = __w( - ) _ a-R () \\n* This interpretation assumes a strong supervising signal at the output while learning. \\n(4.1) \\n(4.2) \\n34 \\nwhere R (t') = II t') - ')l[  The minimum norm solution to the problem, _W*, is unique and \\ncan Ie expressed as \\nIV* = DX t (4.3) \\nwhere/t is the Moore-Penrose pseudo-inverse of/ , i.e., \\nX t = lim(XrX + o'2I)-lX T = IinlXT(XX T + o21) -1. (4.4) \\n2 \\nOn the condition that 0 &lt; ct &lt;  where  is the maximum eigenvalue of XrX, ') and \\nS t') am independent, and IVO,) is uncorrelated with '), \\nE {_w* 3 =  {w(')3 (4.5) \\nwhere  {x } denotes the expected value of x. One way to make use of this relation is to calcu- \\nlate W* for known standard data and refine it by (4.2), thereby saving time in the early stage of \\nlearning, \\nHowever, this solution results in an ill-conditioned IV often in practice. When the prob- \\nlem is ill-posed as such, the technique known as regularization can alleviate the ill-conditioning \\nof IV. The problem is reformulated by finding IV which minimizes \\nR (o) = llY '') - ')ll  + o2y'.ll ,ll  (4.6) \\nsubject to ') = __WX ') where IV = [2 ' \"t] r  \\nThis reformulation regularizes (4.3) to \\nIv(o) = D_Xr(X_X_ r + o2/) - (4.7) \\nwhich is statistically equivalent to W(') when the input has an additive noise of variance o a \\nuncorrelated with '). Interestingly, the leaky LMS algorithm II leads to a statistically \\nequivalent solution \\nIvtk) = _ _ (4.8) \\n2 \\nwhere 0 &lt; 15 &lt; 1 and 0 &lt; ct &lt;  These solutions am related as \\nif o a = 1-- when W (t') is uncorrelated with ') .li \\n(4.9) \\nEquation (4.8) can be generalized for a network using dynamic formal neurons, resulting in \\na equation similar to (3.4). Making use of (4.9), (4.7) can be generalized for a dynamic neuron \\nnetwork as \\nIv(t ;(X) = F- {D (f )X_ (f )Cr (x (f )X (f ) cr + (x2/) 43 (4.10) \\nwhere F - denotes the inverse Fourier transform. \\n5. SYNTHESIS OF BIPOLAR PHONEME PATTERNS \\nThis section illustrates the scheme used to synthesize bipolar phoneme patterns and to \\nform prototype and test patterns. \\nThe fundamental and first three formant frequencies, along with their bandwidths, of \\nphoneroes provided by Klat0 2 were taken as parameters to synthesize 30 prototype phoneme pat- \\nterns. The phoneroes were labeled as shown in Table 1. An array of L (=100) input neurons \\nCovered the range of 100 to 4000 Hz. Each neuron had a bipolar state which was +1 only when \\none of the frequency bands in the phoneme presented to the network was within the critical band \\n35 \\nof the neuron and -1 otherwise. The center frequencies (fc) of critical bands were obtained by \\ndividing the 100 to 4000 Hz range into a log scale by L. The critical bandwidth was a constant \\n100 Hz up to the center frequency fc = 500 Hz and 0.2fc Hz when f &gt;500 Hz. 13 \\nThe parameters shown in Table 1 were used to construct Table 1. Labels of Phoneroes \\n30 prototype phoneme patterns. For O, it was constructed as a \\ncombination of t and 0. F, F 2 ,F s were the first, second, and \\nthird formants, and B, B 2, and B3. were corresponding \\nbandwidths. The fundamental frequency Fo = 130 Hz with B0 = \\n10 Hz was added when the phoneme was voiced. For plosives, \\nthere was a stop before formant traces start. The resulting bipo- \\nlar paRems are shown in Fig.2. Each pattern had length of 5 \\ntime units, composed by linearly interpolating the frequencies \\nwhen the formant frequency was gliding. \\nA sequence of phonemes converted from a continuous \\npronunciation of digits, {o, zero, one, two, three, four, five, six, \\nseven, eight, nine }, was translated into a bipolar pattern, adding \\ntwo time units of transition between two consequtive phonemes \\nby interpolating the frequency and bandwidth parameters \\nlinearly. A flip noise was added to the test pattern and created a \\nnoisy test pattern. The sign at every point in the original clean \\ntest pattern was flipped with the probability 0.2. These test pat- \\nterns are shown in Fig. 3. \\nLabel Phoneme \\n1 [i y] \\n2 [I ] \\n3 [eY] \\n4 \\n6 [a] \\n7 [o] \\n8 \\n9 [o w] \\nlO [u ] \\n11 [u TM] \\n12 \\n13 [a y] \\n14 [a TM] \\n15 [o y] \\n16 [w] \\n17 [y] \\n18 [r] \\n19 [1] \\n20 (q \\n21 [v] \\n22 [0] \\n23 [] \\n24 [s] \\n25 [z] \\n26 [p] \\n27 [t] \\n28 (d] \\n29 [k] \\n30 [n] \\nFig. 2. Prototype Phoneme Patterns. (Thirty phoneme patterns are shown \\nin sequence with intervals of two time units.) \\n6. SIMULATION OF SPATIO-TEMPORAL FILTERS FOR PHONEME CLASSIFICATION \\nThe network system described below was simulated and used to classify the prototype \\nphoneme patterns in the test patterns shown in the previoius section. It is an example of gen- \\neralizing a scheme developed for static patterns 13 to that for dynamic patterns. Its operation is \\nin two stages. The first stage operation is a spatio-temporal filter bank: \\n36 \\n(a) \\nFig. 3. Test Patterns. (a) Clean Test Pattern. (b) Noisy Test Pattern. \\ny'(t ) = __W (t ),:(t ) , and '(t ) = _r(a(-t )St(t )) . \\nThe second stage operation is the \"winner-take-all\" lateral inhibition: \\n0'(t) = '(t), and o'(t+a) = r_(A_(-t),(t) - h), \\nand \\n(6.1) \\n(6.2) \\nfor all f . (6.6) \\nThis minimizes \\nR (o,f \\nA_(t) = (1 + )/_.(t) -  (t-nA). (6.3) \\nwhere h  is a constant threshold vector with elements h i = h and (') is the Kronecker delta \\nfunction. This operation is repeated a sufficient number of times, No .13,1n The output is \\nO'(t + No-a). \\nTwo models based on different learning rules were simulated with parameters shown \\nbelow. \\nModel 1 (Spario-temporal Matched Filter Bank) \\nLet (x(t) = (t), f() =  in (3.3) where F is a unit vector with its elements e/ = 5(k-i). \\n___W (t) = X (t)T. (6.4) \\n4 1 \\nh=200, and a(t ) = -(t-n,x). \\nn=O  \\nModel 2 (Spario-temporal Pseudo-inverse Filter) \\nLet D =/ in (4.10). Using the alternative expression in (4.4), \\nW(t ) = F-I{(x0 e )CTxoe ) + (2I_)-lxCT}. (6.5) \\nh =0.05,o 2= 1000.0,and a(t)=(t). \\n37 \\nBecause the time and frequency were finite and discrete in simulation, the result of the \\ninverse discrete Fourier transform in (6.5) may be aliased. To alleviate the aliasing, the transfer \\nfunctions in the prototype matrix X(t) were padded with zeros, thereby doubling the lengths. \\nFurther zero-padding the transfer functions did not seem to change teh result significantly. \\nThe results are shown in Fig. 4(a)-(d). The arrows indicate the ideal response positions at \\nthe end of a phoneme. When the program was run with different thresholds and adaptation func- \\ntion a (t), the result was not very sensitive to the threshold value, but was, nevertheless affected \\nby the choice of the adaptation function. The maximum number of iterations for the lateral inhi- \\nbition network to converge was observed: for the experiments shown in Fig. 4(a) - (d), the \\nnumbers were 44, 69, 29, and 47, respectively. Model 1 missed one phoneme and falsely \\nresponded once in the clean test pattern. It missed three and had one false response in the noisy \\ntest pattern. Model 2 correctly recognized all phonemes in the clean test pattern, and false- \\nalarmed once in the noisy test pattern. \\n7. DISCUSSION \\nThe notion of convolution or correlation used in the models presented is popular in \\nengineering disciplines and has been applied extensively to designing filters, control systems, etc. \\nSuch operations also occur in biological systems and have been applied to modeling neural net- \\nworks.15,16 Thus the concept of dynamic formal neuron may be helpful for the improvement of \\nartificial neural network models as well as the understanding of biological systems. A portion of \\nthe system described by Tank and Hopfield 17 is similar to the matched filter bank model simu- \\nlated in this paper. \\nThe matched filter bank model (Model 1) performs well when all phonemes (as above) are \\nof the same duration. Otherwise, it would perform poorly unless the lengths were forced to a \\nmaximum length by padding the input and transfer functions with -l's during calculation. The \\npseudo-inverse filter model, on the other hand, should not suffer from this problem. However, \\nthis aspect of the model (Model 2) has not yet been explicitly simulated. \\nGiven a spatio-temporal pattern of size L x K, i.e., L spatial elements and K temporal ele- \\nments, the number of calculations required to process the first stage of filtering by both models is \\nthe same as that by a static formal neuron network in which each neuron is connected to the L x \\nK input elements. In both cases, L x K multiplications and additions are necessary to calculate \\none output value. In the case of bipolar patterns, the mutiplication used for calculation of activa- \\ntion can be replaced by sign-bit check and addition. A future investigation is to use recursive \\nfilters or analog filters as transfer functions for faster and more efficient calculation. There are \\nvarious schemes to obtain optimal recursive or analog filters. is, 19 Besides the lateral inhibition \\nscheme used in the models, there are a number of alternative procedures to realize a \"winner- \\ntake-all\" network in analog or digital fashion. 15,2,21 \\nAs pointed out in the previous section, the Fourier transform in (6.5) requires a precaution \\nconcerning the resulting length of transfer functions. Calculating the recursive correlation equa- \\ntion (3.4) also needs such preprocessing as windowing or truncation. 22 \\nThe generalization of static neural networks to dynamic ones along with their learning \\nrules is strainghfforward as shown if the neuron operation and the learning rule are linear. Gen- \\neralizing a system whose neuron operation and/or leaming rule are nonlinear requires more care- \\nful analysis and remains for future work. The system described by Watrous and Shastri 16 is an \\nexample of generalizing a backpropagation model. Their result showed a good potential of the \\nmodel and a need for more rigorous analysis of the model. Generalizing a system with recurrent \\nconnections is another task to be pursued. In a system with a certain analytical nonlinearity, the \\nsignals are expressed by Volterra functionals, for example. A practical learning system can then \\nbe constructed if higher kernels are neglected. For example, a cubic function can be used instead \\nof a sigmoidal function. \\n38 \\n(a) \\n3 \\nCo) \\nFig. 4. Performance of Models. (a) Model 1 with Clean Test Pattern. (b) \\nModel 2 with Clean Test Pattern. (c) Model 1 with Noisy Test Pattern. \\n(d) Model 2 with Noisy Test Pattern. Arrows indicate the ideal response \\npositions at the end of phoneme. \\n8. CONCLUSION \\nThe formal neuron was generalized to the dynamic formal neuron to recognize spario- \\ntemporal patterns. It is shown that existing learning rules can be generalized for dynamic formal \\nneurons. \\nAn artificial neural network using dynamic formal neurons was applied to classifying 30 \\nmodel phonemes with bipolar patterns created by using parameters of formant frequencies and \\ntheir bandwidths. The model operates in two stages: in the first stage, it calculates the correla- \\ntion between the input and prototype paRems stored in the transfer function matrix, and, in the \\nsecond stage, a lateral inhibition network selects the output of the phoneme pattern dose to the \\ninput pattern. \\n39 \\n(c) \\n(d) \\nFig. 4 (continued.) \\nTwo models with different transfer functions were tested. Model 1 was a matched filter \\nbank model and Model 2 was a pseudo-inverse filter model. A sequence of phoneme patterns \\ncorresponding to continuous pronunciation of digits was used as a test pattern. For the test pat- \\ntern, Model 1 missed to recognize one phoneme and responded falsely once while Model 2 \\ncorrectly recognized all the 32 phonemes in the test pattern. When the flip noise which flips the \\nsign of the pattern with the probability 0.2, Model 1 missed three phonemes and falsely \\nresponded once while Model 2 recognized all the phonemes and false-alarmed once. Both \\nmodels detected the phonems at the correct position within the continuous stream. \\nReferences \\nW. S. McCulloch and W. Pitts, \"A logical calculus of the ideas imminent in nervous \\nactivity,\" Bulletin of Mathematical Biophysics, vol. 5, pp. 115-133, 1943. \\nD. O. Hebb, The Organization of Behavior, Wiley, New York, 1949. \\n40 \\n3. D.E. Rumelhart, G. E. Hinton, and R. J. Williams, \"Learning internal representations by \\nerror propagation,\" in Parallel Distributed Processing, Vol. 1, MIT, Cambridge, 1986. \\n4. B. Widrow and M. E. Hoff, \"Adaptive switching circuits,\" Institute of Radio Engineers, \\nWestern Electronics Show and Convention, vol. Convention Record Part 4, pp. 96-104, \\n1960. \\n5. R.O. Duda and P. E. Hart, Pattern Classification and Scene Analysis, Chapter 5, Wiley, \\nNew York, 1973. \\n6. T. Kohonen, Self-organization and Associative Memory, Springer-Verlag, Berlin, 1984. \\n7. F. Rosenblatt, Principles of Neurodynamics, Spartan Books, Washington, 1962. \\n8. J.M. Varah, \"A practical examination of some numerical methods for linear discrete ill- \\nposed problems,\" SIAM Review, vol. 21, no. 1, pp. 100-111, 1979. \\n9. C. Koch, J. Marroquin, and A. Yuille, \"Analog neural networks in early vision,\" Proceed- \\nings of the National Academy of Sciences, USA, vol. 83, pp. 4263-4267, 1986. \\n10. G.O. Stone, \"An analysis of the delta rule and the learning of statistical associations,\" in \\nParallel Distributed Processing., Vol. 1, MIT, Cambridge, 1986. \\n11. B. Widrow and S. D. Steams, Adaptive Signal Processing, Prentice-Hall, Englewood \\nCliffs, 1985. \\n12. D.H. Klatt, \"Software for a cascade/parallel formant synthesizer,\" Journal of Acoustical \\nSociety of America, vol. 67, no. 3, pp. 971-995, 1980. \\n13. L.E. Arias, T. Homma, and R. J. Marks II, \"A neural network for vowel classification,\" \\nProceedings International Conference on Acoustics, Speech, and Signal Processing, 1987. \\n14. R.P. Lippman, \"An introduction to computing with neural nets,\" IEEE ASSP Magazine, \\nApril, 1987. \\n15. S. Amari and M. A. Arbib, \"Competition and cooperation in neural nets,\" in Systems Neu- \\nroscience, ed. J. Metzler, pp. 119-165, Academic Press, New York, 1977. \\n16. R.L. Watrous and L. Shastri, \"Learning acoustic features from speech data using connec- \\nfionist networks,\" Proceedings of The Ninth Annual Conference of The Cognitive Science \\nSociety, pp. 518-530, 1987. \\n17. D. Tank and J. J. Hopfield, \"Concentrating information in time: analog neural networks \\nwith applications to speech recognition problems,\" Proceedings of International Confer- \\nence on Neural Netoworks, San Diego, 1987. \\n18. J.R. Treichler, C. R. Johnson, Jr., and M. G. Larimore, Theory and Design of Adaptive \\nFilters, Chapter 5, Wiley, New York, 1987. \\n19. M Schetzen, The Volterra and Wiener Theories of Nonlinear Systems, Chapter 16, Wiley, \\nNew York, 1980. \\n20. S. Grossberg, \"Associative and competitive principles of learning,\" in Competition and \\nCooperation in Neural Nets, ed. M. A. Arbib, pp. 295-341, Springer-Verlag, New York, \\n1982. \\n21. R.J. Marks II, L. E. Atlas, J. J. Choi, S. Oh, K. F. Cheung, and D.C. Park, \"A perfor- \\nmance analysis of associative memories with nonlinearities in the correlation domain,\" \\n(submitted to Applied Optics), 1987. \\n22. D.E. Dudgeon and R. M. Mersereau, Multidimensional Digital Signal Processing, pp. \\n230-234, Prentice-Hall, Englewood Cliffs, 1984. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>37.82</td>\n","      <td>distribution, probability, prior, variable, gaussian, mixture, estimate, density, bayesian, approximation, likelihood, sample, log, expert, em, estimation, posterior, step, component, probabilistic</td>\n","      <td>41 \\nON PROPERTIES OF NETWORKS \\nOF NEURON-LIKE ELEMENTS \\nPierre Baldi* and Santosh S. Venlmtesh I \\n15 December 1987 \\nAbstract \\nThe complexity and computational capacity of multi-layered, feedforward \\nneural networks is examined. Neural networks for special purpose (structured) \\nfunctions are examined from the perspective of circuit complexity. Known re- \\nsults in complexity theory are applied to the special instance of neural network \\ncircuits, and in particular, classes of functions that can be implemented in \\nshallow circuits characterised. Some conclusions are also drawn about learning \\ncomplexity, and some open problems raised. The dual problem of determining \\nthe computational capacity of a class of multi-layered networks with dynamics \\nregulated by an algebraic Hamiltoninn is considered. Formal results are pre- \\nsented on the storage capacities of programmed higher-order structures, and \\na tradeoff between ease of programming and capacity is shown. A precise de- \\ntermination is made of the static fixed point structure of random higher-order \\nconstructs, and phase-transitions (0-1 laws) are shown. \\n1 INTRODUCTION \\nIn this article we consider two aspects of computation with neural networks. Firstly \\nwe consider the problem of the complexity of the network required to compute classes \\nof specified (structured) functions. We give a brief overview of basic known com- \\nplexity theorems for readers familiar with neural network models but less familiar \\nwith circuit complexity theories. We argue that there is considerable computational \\nand physiological justification for the thesis that shallow circuits (i.e., networks with \\nrelatively few layers) are computationally more efficient. We hence concentrate on \\nstructured (as opposed to random) problems that can be computed in shallow (con- \\nstant depth) circuits with a relatively few number (polynomial) of elements, and \\ndemonstrate classes of structured problems that are amenable to such low cost so- \\nlutions. We discuss an allied problem--the complexity of learning--and close with \\nsome open problems and a discussion of the observed limitations of the theoretical \\napproach. \\nWe next turn to a rigourous classification of how much a network of given \\nstructure can do; i.e., the computational capacity of a given construct. (This is, in \\n*Department of Mathematics, University of California (San Diego), La Jolla, CA 92093 \\ntMoore School of Electrical Engineering, University of Pennsylvania, Philadelphia, PA 19104 \\nAmerican Institute of Physics 1988 \\n42 \\na sense, the mirror image of the problem considered above, where we were seeking \\nto design a minimal structure to perform a given task.) In this article we restrict \\nourselves to the analysis of higher-order neural structures obtained from polynomial \\nthreshold rules. We demonstrate that these higher-order networks are a special class \\nof layered neural network, and present formal results on storage capacities for these \\nconstructs. Specifically, for the case of prograznmed interactions we demonstrate \\nthat the storage capacity is of the order of n d where d is the interaction order. \\nFor the case of random interactions, a type of phase trartsition is observed in the \\ndistribution of fixed points as a function of attraction depth. \\n2 COMPLEXITY \\nThere exist two broad classes of constraints on computations. \\n1. Physical constraints. These are related to the hardware in which the computa- \\ntion is embedded, and include among others time constants, energy limitations, \\nvolumes and geometrical relations in 3D space, and bandwidth capadties. \\n2. Logical constraints: These can be further subdivided into \\n Computability constraints--for instance, there exist unsolvable problems, \\ni.e., functions such as the halting problem which are not computable in \\nan absolute sense. \\n Complexity constraints--usually giving upper and/or lower bounds on \\nthe amount of resources such as the time, or the number of gates re- \\nquired to compute a given function. As an instance, the assertion \"There \\nexists an exponential time algorithm for the Traveling Salesman Prob- \\nlem,\" provides a computational upper bound. \\nIf we view brains as computational devices, it is not unreasonable to think \\nthat in the course of the evolutionary process, nature may have been faced several \\ntimes by problems related to physical and perhaps to a minor degree logical con- \\nstraints on computations. If this is the case, then complexity theory in a broad \\nsense could contribute in the future to our understanding of parallel computations \\nand architectural issues both in natural and synthetic neural systems. \\nA simple theory of parallel processing at the macro level (where the elements \\nare processors) can be developed based on the ratio of the time spent on com- \\nmunications between processors [7] for different classes of problems and different \\nprocessor architecture and interconnections. However, this approach does not seem \\nto work for parallel processing at the level of circuits, especially if calculations and \\ncommunications are intricately entangled. \\nRecent neural or connectionist models are based on a common structure, that \\nof highly interconnected networks of linear (or polynomial) threshold (or with sig- \\nmold input-output function) units with adjustable interconnection weights. We shall \\ntherefore review the complexity theory of such circuits. In doing so, it will be some- \\ntimes helpful to contrast it with the similar theory based on Boolean (AND, OR, \\nNOT) gates. The presentation will be rather informal and technical complements \\ncan easily be found in the references. \\n43 \\nConsider a circuit as being on a cyclic oriented graph connecting n Boolean \\ninputs to one Boolean output. The nodes of the graph correspond to the gates \\n(the n input units, the \"hidden\" units, and the output unit) of the circuit. The \\nsize of the circuit is the total number of gates and the depth is the length of the \\nlongest path connecting one input to the output. For a layered, feed-forward circuit, \\nthe width is the average number of computational units in the hidden (or interior) \\nlayers of elements. The first obvious thing when comparing Boolean and threshold \\nlogic is that they are equivalent in the sense that any Boolean function can be \\nimplemented using either logic. In fact, any such function can be computed in a \\ncircuit of depth two and exponential size. Simple counting arguments show that \\nthe fraction of functions requiring a circuit of exponential size approaches one as \\nn  oo in both cases, i.e., a random function will in general require an exponential \\nsize circuit. (Paradoxically, it is very difficult to construct a family of functions \\nfor which we can prove that an exponential circuit is necessary.) Yet, threshold \\nlogic is more powerful than Boolean logic. A Boolean gate can compute only one \\nfunction whereas a threshold gate can compute to the order of 2 an2 functions by \\nvarying the weights with 1/2 _&lt; a _&lt; 1 (see [19] for the lower bound; the upper \\nbound is a classical hyperplane counting argument, see for instance [20,30]). It \\nwould hence appear plausible that there exist wide classes of problems which can be \\ncomputed by threshold logic with circuits substantially smaller than those required \\nby Boolean logic. An important result which separates threshold and Boolean logic \\nfrom this point of view has been demonstrated by Yo [31] (see [10,24] for an elegant \\nproof). The result is that in order to compute a function such as parity in a circuit \\nof constant depth k, at least exp(cn /2k) Boolean gates with unbounded fanin are \\nrequired. As we shall demonstrate shortly, a circuit of depth two and linear size is \\nsufficient for the computation of such functions using threshold logic. \\nIt is not unusual to hear discussions about the tradeoffs between the depth \\nand the width of a circuit. We believe that one of the main constributions of \\ncomplexity analysis is to show that this tradeoff is in some sense minimal and that \\nin fact there exists a very strong bias in favor of shallow (i.e., constant depth) \\ncircuits. There are multiple reasons for this. In general, for a fixed size, the number \\nof different functions computable by a circuit of small depth exceeds the number \\nof those computable by a deeper circuit. That is, if one had no a priori knowledge \\nregarding the function to be computed and was given hidden units, then the optimal \\nstrategy would be to choose a circuit of depth two with the rn units in a single \\nlayer. In addition, if we view computations as propagating in a feedforward mode \\nfrom the inputs to the output unit, then shallow circuits compute faster. And the \\ndeeper a circuit, the more difficult become the issues of time delays, synchronisation, \\nand precision on the computations. Finally, it should be noticed that given overall \\nresponses of a few hundred milliseconds and given the known time scales for synaptic \\nintegration, biological circuitry must be shallow, at least within a \"module\" and \\nthis is corroborated by anatomical data. The relative slowness of neurons and their \\nshallow circuit architecture are to be taken together with the \"analog factor\" and \\n\"entropy factor\" [1] to understand the necessary high-connectivity requirements of \\nneural systems. \\n44 \\nFrom the previous analysis emerges an important class of circuits in threshold \\nlogic characterised by polynomial size and shallow depth. We have seen that, in \\ngeneral, a random function cannot be computed by such circuits. However, many \\ninteresting functions--the structured problems--are far from random, and it is then \\nnatural to ask what is the class of functions computable by such circuits? While \\na complete characterisation is probably difficult, there are several sub-classes of \\nstructural functions which are known to be computable in shallow poly-size circuits. \\nThe symmetric functions, i.e., functions which are invaxiant under any per- \\nmutation of the n input variables, are an important class of structured problems \\nthat can be implemented in shallow polynomial size circuits. In fact, any symmet- \\nric function can be computed by a threshold circuit of depth two and linear size; \\n(n hidden units and one output unit are always sufficient). We demonstrate the \\nvalidity of this assertion by the following instructive construction. We consider n \\nbinary inputs, each taking on values -1 and i only, and threshold gates as units. \\nNow axray the 2 n possible inputs in n -{- 1 rows with the elements in each row being \\npermuted versions of each other (i.e., n-tuples in a row all have the same number \\nof -{-1's) and with the rows going monotonically from zero +1's to n +1's. Any \\ngiven symmetric Boolean function clearly assumes the same value for all elements \\n(Boolean n-tuples) in a row, so that contiguous rows where the function assumes \\nthe value +1 form bands. (There axe at most n/2 bands--the worst case occuring \\nfor the parity function.) The symmetric function can now be computed with 2B \\nthreshold gates in a single hidden layer with the topmost \"neuron\" being activated \\nonly if the number of -{-1's in the input exceeds the number of -{-1's in the lower \\nedge of the lowest band, and proceeding systematically, the lowest \"neuron\" being \\nactivated only if the number of -{-1's in the input exceeds the number of -{- 1's in the \\nupper edge of the highest band. An input string will be within a band if and only if \\nan odd number of hidden neurons are activated starti:g contiguously from the top \\nof the hidden layer, and conversely. Hence, a single output unit can compute the \\ngiven symmetric function. \\nIt is easy to see that arithmetic operations on binary strings can be performed \\nwith polysize small depth circuits. Reif [23] has shown that for a fixed degree of pre- \\ncision, any analytic function such as polynomials, exponentials, and trigonometric \\nfunctions can be approximated with small and shallow threshold circuits. Finally, \\nin many situations one is interested in the value of a function only for a vanishingly \\nsmall (i.e., polynomial) fraction of the total number of possible inputs 2 '. These \\nfunctions can be implemented by polysize shallow circuits and one can relate the \\nsize and depths of the circuit to the cardinal of the interesting inputs. \\nSo fax we only have been concerned with the complexity of threshold circuits. \\nWe now turn to the complexity of leaxning, i.e., the problem of finding the weights \\nrequired to implement a given function. Consider the problem of repeating m points \\nin ]R t coloured in two colours, using k hyperplanes so that any region contains only \\nmonochromatic points. If t and k axe fixed the problem can be solved in polynomial \\ntime. If either t or k goes to infinity, the problem becomes NP-complete [?]. As \\na result, it is not difficult to see that the general learning problem is NP-complete \\n(see also [12] for a different proof and [21] for a proof of the fact it is already \\nNP-complete in the case of one single threshold gate). \\n45 \\nSome remarks on the limitations of the complexity approach are a propos at \\nthis juncture: \\nWhile a variety of structured Boolean functions can be implemented at rela- \\ntively low cost with networks of linear threshold gates (McCulloch-Pitts neu- \\nrons), the extension to different input-output functions and the continuous \\ndomain is not always straightforward. \\nEven restricting ourselves to networks of relatively simple Boolean devices such \\nas the linear threshold gate, in many instances, only relatively weak bounds \\naxe available for computational cost and complexity. \\no \\no \\nTime is probably the single most important ingredient which is completely \\nabsent from these threshold units and their interconnections [17,14]; there \\naxe, in addition, non-biological aspects of connectionist models [8]. \\nFinally, complexity results (where available) are often asymptotic in nature \\nand may not be meaningful in the range corresponding to a particular appli- \\ncation. \\nWe shall end this section with a few open questions and speculations. One \\nproblem has to do with the time it takes to learn. Leaxning is often seen as a \\nvery slow process both in artificial models (cf. back propagation, for instance) and \\nbiological systems (cf. human acquisition of complex skills). However, if we follow \\nthe standards of complexity theory, in order to be effective over a wide variety of \\nscales, a single learning algorithm should be polynomial time. We can therefore \\nask what is learnable by examples in polynomial time by polynomial size shallow \\nthreshold circuits? The status of back propagation type of algorithms with respect \\nto this question is not very clear. \\nThe existence of many tasks which are easily executed by biological organisms \\nand for which no satisfactory computer program has been found so far leads to the \\nquestion of the specificity of learning algorithms, i.e., whether there exists a com- \\nplexity class of problems or functions for which a \"program\" can be found only by \\nlearning from examples as opposed to by traditional programming. There is some \\ncircumstantial evidence against such conjecture. As pointed out by Valiant [25], \\ncryptography can be seen in some sense as the opposite of learning. The conjectures \\nexistence of one way function, i.e., functions which can be constructed in polyno- \\nmial time but cannot be invested (from examples) in polynomial time suggests that \\nlearning algorithms may have strict limitations. In addition, for most of the artificial \\napplications seen so far, the programs obtained through learning do not outperform \\nthe best already known software, though there may be many other reasons for that. \\nHowever, even if such a complexity class does not exist, learning algorithm may \\nstill be very important because of their inexpensiveness and generality. The work of \\nValiant [26,13] on polynomial time learning of Boolean formulas in his \"distribution \\nfree model\" explores some additional limitations of what can be learned by examples \\nwithout including any additional knowledge. \\nLearning may therefore turn out to be a powerful, inexpensive but limited \\nfamily of algorithms that need to be incorporated as \"sub-routines\" of more global \\n46 \\nprograms, the structure of which may be 'harder to find. Should evolution be re- \\ngarded as an \"exponential\" time learning process complemented by the \"polynomial\" time type of learning occurring in the lifetime of organisms? \\n3 CAPACITY \\nIn the previous section the focus of our investigation was on the structure and cost of \\nminimal networks that would compute specified Boolean functions. We now consider \\nthe dual question: What is the computational capacity of a threshold network of \\ngiven structure? As with the issues on complexity, it turns out that for fMrly general \\nnetworks, the capacity results favour shallow (but perhaps broad) circuits [29]. In \\nthis discourse, however, we shall restrict ourselves to a specified class of higher-order \\nnetworks, and to problems of associative memory. We will just quote the principal \\nrigourous results here, and present the involved proofs elsewhere [4]. \\nWe consider systems of n densely interacting threshold units each of which \\nyields an instantaneous state -1 or +1. (This corresponds in the literature to a \\nsystem of n Ising spins, or alternatively, a system of n neural states.) The state \\nspace is hence the set of vertices of the hypercube. We will in this discussion \\nalso restrict our attention throughout to symmetric interaction systems wherein the \\ninterconnections between threshold elements is bidirectional. \\nLet Zd be the family of all subsets of cardinality d+ 1 of the set {1, 2,..., n). \\nClearly ]Zd] = ( d + 1 )' For any subset I of {1,2,...,n), and for every state \\nu = (-1,1)\", set uz = rl6z \\nDefinition 1 A homogeneous algebraic threshold network of degree d is a network of \\nn \\nn threshold elements with interactions specified by a set of ( d + 1 ) real coefficients \\nwz indexed by I in Za, and the evolution rule \\n16 61 \\n(1) \\nThese systems can be readily seen to be natural generalisations to higher- \\norder of the familiar case d = i of linear threshold networks. The added degrees of \\nfreedom in the interaction coefficients can potentially result in enhanced flexibility \\nand programming capability over the linear case as has been noted independently \\nby several authors recently [2,3,4,5,22,27]. Note that each d-wise product Uz\\i is just \\nthe parity of the corresponding d inputs, and by our earlier discussion, this can be \\ncomputed with d hidden units in one layer followed by a single threshold unit. Thus \\nthe higher-order network can be realised by a network of depth three, where the first \\nhidden layer has d( n n \\nd ) units, the second hidden layer has ( d ) units, and there are \\nn output units which feedback into the n input units. Note that the weights from \\nthe input to the first hidden layer, and the first hidden layer to the second are fixed \\n47 \\n(computing the various d-wise products), and the weights from the second hidden \\nlayer to the output are the coefficients wx which are free parameters. \\nThese systems can be identified either with long range interactions for higher- \\norder spin glasses at zero temperature, or higher-order neural networks. Starting \\nfrom an arbitrary configuration or state, the system evolves asynchronously by a \\nsequence of single \"spin\" flips involving spins which are misaligned with the instan- \\ntaneous \"molecular field.\" The dynamics of these symmetric higher-order systems \\nare regulated analogous to the linear system by higher-order extensions of the clas- \\nsical quadratic Hamiltonian. We define the homogeneous algebraic Hamiltonian of \\ndegree d by \\nHd(u) =- wxm. \\nIEIa \\nThe algebraic Hamiltonians are functionals akin in behaviour to the classical \\nquadratic Hamiltonian as has been previously demonstrated [5]. \\nProposition 1 The functional Hd is non-increasing under the evolution rule 1. \\nIn the terminology of spin glasses, the state trajectories of these higher-order \\nnetworks can be seen to be following essentially a zero-temperature Monte Carlo \\n(or Glauber) dynamics. Because of the monotonicity of the algebraic Hamiltonians \\ngiven by equation 2 under the asynchronous evolution rule 1, the system always \\nreaches a stable state (fixed point) where the relation I is satisfied for each of the n \\nspins or neural states. The fixed points are hence the arbiters of system dynamics, \\nand determine the computational capacity of the system. \\nSystem behaviour and applications are somewhat different depending on \\nwhether the interactions are random or programmed. The case of random interac- \\ntions lends itself to natural extensions of spin glass formulations, while programmed \\ninteractions yield applications of higher-order extensions of neural network models. \\nWe consider the two cases in turn. \\n3.1 PROGRAMMED INTERACTIONS \\nHere we query whether given sets of binary n-vectors can be stored as fixed points \\nby a suitable selection of interaction coefficients. If such sets of prescribed vectors \\ncan be stored as stable states for some suitable choice of interaction coefficients, \\nthen proposition 1 will ensure that the chosen vectors are at the bottom of \"energy \\nwells\" in the state space with each vector exercising a region of attraction around \\nit--all characterestics of a physical associative memory. In such a situation the \\ndynamical evolution of the network can be interpreted in terms of computations: \\nerror-correction, nearest neighbour search and associative memory. Of importance \\nhere is the maximum number of states that can be stored as fixed points for an \\nappropriate choice of algebraic threshold network. This represents the maximal \\ninformation storage capacity of such higher-order neural networks. \\nLet d represent the degree of the algebraic threshold network. Let u(),..., u (m) \\nbe the m-set of vectors which we require to store as fixed points in a suitable al- \\ngebraic threshold network. We will henceforth refer to these prescribed vectors as \\n48 \\nmemories. We define the storage capacity of an algebraic threshold network of de- \\ngree d to be the maximal number m of arbitraxily chosen memories which can be \\nstored with high probability for appropriate choices of coefficients in the network. \\nTheorem 1 The maximal (algorithm independent) storage capacity of a homoge- \\nneous algebraic threshold network of degree d is less than or equal to 2 ( n \\nd)' \\nGeneralised Sum of Outer-Products Rule: The classical Hebbiazt rule for the \\nlineax case d = i (cf. [11] and quoted references) cazt be naturally extended to \\nnetworks of higher-order. The coefficients w.r, I  Zd axe constructed as the sum of \\ngeneralised Kronecker outer-products, \\nm \\na----1 \\nTheorem 2 The storage capacity of the outer-product algorithm applied to a ho- \\nmogeneous algebraic threshold network of degree d is less thazt or equal to nd/2(d + \\n1)logn (also cf. [15,27]). \\nGeneralised Spectral Rule: For d = 1 the spectral rule amounts to iteratively \\nprojecting states orthogonally onto the lineax space generated by u(),..., u(\"0, and \\nthen taking the dosest point on the hypercube to this projection (cf. [27,28]). This \\napproach can be extended to higher-orders as we now describe. \\nLet W denote the n x N(,,d) matrix of coefficients wt arranged lexicograph- \\nically; i.e., \\nW __ \\nWl,l,2,...,d-1,d Wl,2,3,...,d,d+ l ' ' ' \\nW2,1,2,...,d- l,d W2,2,3,...,d,d+ l  . \\nWn,l,2,...,d-l,d Wn,2,3,...,d,d+l  . . \\nWl,n-d+ l,n-d+ 2,...,n- l,n \\nW2,n-d+ l,n-d+ 2,...,n-l,n \\nWn,n-d + l ,n-d+ 2,...,n- l ,n \\nNote that the symmetry and the \"zero-diagonal\" nature of the interactions have \\nbeen relaxed to increase capacity. Let U be the n x m matrix of memories. Form \\nthe extended N(n,a ) x m binary matrix U = [lu() ... u(ra)], where \\n.() \\n1,2,...,d-1,d \\nu () \\nlU( ) ._ 1,2,...,d-l,d+l \\nu() \\nn-dc l,n-d+ 2,...,n- l,n \\nLet A'= dg[A0) ... A(m)] be a m x m diagonal matrix with positive diagonal terms. \\nA generalisation of the spectral algorithm for choosing coefficients yields \\nW = UAU t \\nwhere U t is the pseudo-inverse of U. \\n49 \\nTheorem 3 The storage capacity of the generalised spectral algorithm is at best \\n(d). \\n3.2 RANDOM INTERACTIONS \\nWe consider homogeneous algebraic threshold networks whose weights w.r are i.i.d., \\nfir(0, 1) random vaxiables. This is a natural generalisation to higher-order of Ising \\nspin glasses with Gaussian interactions. We will show an asymptotic estimate for \\nthe number of fixed points of the structure. Asymptotic results for the usual case \\nd = i of linear threshold networks with Gaussian interactions have been reported \\nin the literature [6,9,16]. \\nFor i = 1,...,n set \\n$ = ui  w.ru\\i . \\nI d: i6I \\nFor each n the random vaxiables S}, i = 1,..., n are identically distributed, jointly \\n2 n-1 \\nGaussian vaxiables with zero mean, and vaxiance a, = ( \\nd )' \\nDefinition 2 For any given  &gt;_ 0, a state u 6 lB n is S-strongly stable iff S &gt;_ firrs, \\nfor each i = 1,...,n. \\nThe case fl = 0 reverts to the usual case of fixed points. The parameter fl is \\nessentially a measure of how deep the well of attraction surrounding the fixed point \\nis. The following proposition asserts that a 0-1 law (\"phase transition\") governs \\nthe expected number of fixed points which have wells of attraction above a certain \\ndepth. Let Fd(/) be the expected number of/%strongly stable states. \\nTheorem 4 Corresponding to each fixed interaction order d there exists a positive \\nconstant  such that as n --, oo, \\nd \\nFff kd(/) 2 'cd() if/ &lt; \\n kd(/) if/ = ! \\n0 if/&gt; , \\nwhere ka(/) &gt; O, and 0 &lt; ca(/) &lt; 1 axe parameters depending solely on/ and the \\ninteraction order d. \\n4 CONCLUSION \\nIn fine, it appears possible to design shallow, polynomial size threshold circuits \\nto compute a wide class of structured problems. The thesis that shallow circuits \\ncompute more efficiently than deep circuits is borne out. For the paxticulax case of \\n5O \\nhigher-order networks, all the garnered results appear to point in the same direction: \\nFor neural networks of fixed degree d, the maximal number of programmable states is \\nessentially of the order of nd. The total number of fixed points, however, appear to \\nbe exponential in number (at least for the random interaction case) though almost \\nall of them have constant attraction depths. \\nReferences \\n[1] Y. S. Abu-Mostafa, \"Number of synapses per neuron,\" in Analog VLSI and \\nNeural Systems, ed. C. Mead, Addison Wesley, 1987. \\n[2] P. Baldi, II. Some Contributions to the Theory of Neural Networks. Ph.D. The- \\nsis, California Insitute of Technology, June 1986. \\n[3] P. Baldi and S. S. Venkatesh, \"Number of stable points for spin glasses and \\nneural networks of higher orders,\" Phys. Rev. Lett., vol. 58, pp. 913-916, 1987. \\n[4] P. Baldi and S.S. Venkatesh, \"Fixed points of algebraic threshold networks,\" \\nin prepaxation. \\n[5] H. H. Chen, et al, \"Higher order correlation model of associative memory,\" in \\nNeural Networks for Computing. New York: AIP Conf. Proc., vol. 151, 1986. \\n[6] S. F. Edwards and F. Tanaka, \"Analytical theory of the ground state properties \\nof a spin glass: I. ising spin glass,\" Jnl. Phys. F, vol. 10, pp. 2769--2778, 1980. \\n[7] G. C. Fox and S. W. Otto, \"Concurrent Computations and the Theory of \\nComplex Systems,\" Caltech Concurrent Computation Program, March 1986. \\n[8] F. H. Grick and C. Asanuma, \"Certain aspects of the anatomy and physiology \\nof the cerebral cortex,\" in Parallel Distributed Processing, vol. 2, eds. D. E. \\nRumelhart and J. L. McCelland, pp. 333-371, MIT Press, 1986. \\n[9] D. J. Gross and M. Mezard, \"The simplest spin glass,\" Nucl. Phys., vol. B240, \\npp. 431-452, 1984. \\n[10] J. Hasted, \"Almost optimal lower bounds for small depth circuits,\" Proc. 18-th \\nACM STOC, pp. 6-20, 1986. \\n[11] J. J. Hopfield, \"Neural networks and physical sytems with emergent collective \\ncomputational abilities,\" Proc. Natl. Acad. Sci. USA, vol. 79, pp. 2554-2558, \\n1982. \\n[12] J. S. Judd, \"Complexity of connectionist learning with various node functions,\" \\nDept. of Computer and Information Science Technical Report, vol. 87-60, Univ. \\nof Massachussetts, Amherst, 1987. \\n[13] M. Kearns, M. Li, L. Pitt, and L. Valiant, \"On the learnability of Boolean \\nformulae,\" Proc. 19-th ACM STOC, 1987. \\n[14] C. Koch, T. Poggio, and V. Torre, \"Retinal ganglion cells: A functional inter- \\npretation of dendritic morphology,\" Phil. Trans. R. Soc. London, vol. B 288, \\npp. 227-264, 1982. \\n51 \\n[15] R. J. McEliece, E. C. Posner, E. R. Rodemich, and $. $. Venkatesh, \"The \\ncapacity of the Hopfield associative memory,\" IEEE Trans. Inform. Theory, \\nvol. IT-33, pp. 461-482, 1987. \\n[16] R. J. McEliece and E. C. Posner, \"The number of stable points of an infinite- \\nrange spin glass memory,\" JPL Telecomre. and Data Acquisition Progress Re- \\nport, vol. 42-83, pp. 209-215, 1985. \\n[17] C. A. Mead (ed.), Analog VLSI and Neural Systems, Addison Wesley, 1987. \\n[18] N. Megiddo, \"On the complexity of polyhedral separability,\" to appear in Jnl. \\nDiscrete and Computational Geometr7, 1987. \\n[19] S. Muroga, \"Lower bounds on the number of threshold functions,\" IEEE Trans. \\nElec. Comp., vol. 15, pp. 805-806, 1966. \\n[20] $. Muroga, Threshold Logic and its Applications, Wiley Interscience, 1971. \\n[21] V. N. Peled and B. $imeone, \"Polynomial-time algorithms for regular set- \\ncovering and threshold synthesis,\" Discr. Appl. Math., vol. 12, pp. 57-69, 1985. \\n[22] D. Psaltis and C. H. Park, \"Nonlinear discriminant functions and associative \\nmemories,\" in Neural Networks for Computing. New York: AIP Conf. Proc., \\nvol. 151, 1986. \\n[23] J. Reif, \"On threshold circuits and polynomial computation,\" preprint. \\n[24] R. $molenski, \"Algebraic methods in the theory of lower bounds for Boolean \\ncircuit complexity,\" Proc. 19-th ACM STOC, 1987. \\n[25] L. G. Valiant, \"A theory of the learnable,\" Comm. ACM, vol. 27, pp. 1134-1142, \\n1984. \\n[26] L. G. Valiant, \"Deductive learning,\" Phil. Trans. R. Soc. London, vol. A 312, \\npp. 441-446, 1984. \\n[27] S. S. Venkatesh, Linear Maps with Point Rules: Applications to Pattern Clas- \\nsiftcation and Associative Memory. Ph.D. Thesis, California Institute of Tech- \\nnology, Aug. 1986. \\n[28] S. S. Venkatesh and D. Psaltis, \"Linear and logarithmic capacities in associative \\nneural networks,\" to appear IEEE Trans. Inform. Theory. \\n[29] S. S. Venkatesh, D. Psaltis, and J. Yu, private communication. \\n[30] R. O. Winder, \"Bounds on threshold gate realisability,\" IRE Trans. Elec. \\nComp., vol. EC-12, pp. 561-564, 1963. \\n[31] A. C. C. Yao, \"Separating the poly-time hierarchy by oracles,\" Proc. 26-th \\nIEEE FOCS, pp. 1-10, 1985. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>14</td>\n","      <td>21.81</td>\n","      <td>node, class, classification, classifier, pattern, tree, vector, code, probability, feature, bit, sample, label, binary, decision, stage, labeled, decision_tree, technique, error_rate</td>\n","      <td>52 \\nSupervised Learning of Probability Distributions \\nby Neural Networks \\nEric B. Baum \\nJet Propulsion Laboratory, Pasadena CA 91109 \\nFrank Wilczek \\nDepartment of Physics,Harvard University, Cambridge MA 02138 \\nAbstract: \\nWe propose that the back propagation algorithm for super- \\nvised learning can be generalized, put on a satisfactory conceptual \\nfooting, and very likely made more efficient by defining the val- \\nues of the output and input neurons as probabilities and varying \\nthe synaptic weights in the gradient direction of the log likelihood, \\nrather than the 'error'. \\nIn the past thirty years many researchers have studied the \\nquestion of supervised learning in 'neural'-like networks. Recently \\na learning algorithm called 'back propagation '-4 or the 'general- \\nized delta-rule' has been applied to numerous problems including \\nthe mapping of text to phonemes 5, the diagnosis of illnesses 6 and \\nthe classification of sonar targets 7. In these applications, it would \\noften be natural to consider imperfect, or probabilistic informa- \\ntion. We believe that by considering supervised learning from this \\nslightly larger perspective, one can not only place back propaga- \\n Permanent address: Institute for Theoretical Physics, Univer- \\nsity of California, Santa Barbara CA 93106 \\nAmerican Institute of Physics 1988 \\n53 \\ntion on a more rigorous and general basis, relating it to other well \\nstudied pattern recognition algorithms, but very likely improve its \\nperformance as well. \\nThe problem of supervised learning is to model some mapping \\nbetween input vectors and output vectors presented to us by some \\nreal world phenomena. To be specific, consider the question of \\nmedical diagnosis. The input vector corresponds to the symptoms \\nof the patient; the i-th component is defined to be 1 if symptom i \\nis present and 0 if symptom i is absent. The output vector corre- \\nsponds to the illnesses, so that its j-th component is 1 if the j-th \\nillness is present and 0 otherwise. Given a data base consisting \\nof a number of diagnosed cases, the goal is to construct (learn) a \\nmapping which accounts for these examples and can be applied to \\ndiagnose new patients in a reliable way. One could hope, for in- \\nstance, that such a learning algorithm might yield an expert system \\nto simulate the performance of doctors. Little expert advice would \\nbe required for its design, which is advantageous both because ex- \\nperts' time is valuable and because experts often have extraodinary \\ndifficulty in describing how they make decisions. \\nA feedforward neural network implements such a mapping be- \\ntween input vectors and output vectors. Such a network has a set \\nof input nodes, one or several layers of intermediate nodes, and a \\nlayer of output nodes. The nodes are connected in a forward di- \\nrected manner, so that the output of a node may be connected to \\nthe inputs of nodes in subsequent layers, but closed loops do not \\noccur. See figure 1. The output of each node is assumed to be a \\nbounded semilinear function of its inputs. That is, if vj denotes \\nthe output of the j-th node and wij denotes the weight associated \\nwith the connection of the output of the j-th node to the input of \\n54 \\nthe i-th, then the i-th neuron takes value vi = g(]]j. wijvj), where \\ng is a bounded, differentiable function called the activation func- \\ntion. g(x) = 1/(1 + e-x), called the logistic function, is frequently \\nused. Given a fixed set of weights {wij}, we set the input node \\nvalues to equal some input vector, compute the value of the nodes \\nlayer by layer until we compute the output nodes, and so generate \\nan output vector. \\nFigure 1: A 5 layer network. Note bottleneck at layer 3. \\n55 \\nSuch networks have been studied because of analogies to neu- \\nrobiology, because it may be easy to fabricate them in hardware, \\nand because learning algorithms such as the Perceptron learning \\nalgorithm s, Widrow- Hoff , and backpropagation have been able \\nto choose weights wij that solve interesting problems. \\nGiven a set of input vectors s, together with associated target \\nvalues tq back propagation attempts to adjust the weights so as \\n$' \\nto minimize the error E in achieving these target values, defined as \\n(1) \\n' is the output of the j-th node when s ' is presented as \\nwhere oa. \\ninput. Back propagation starts with randomly chosen wid and \\nthen varies in the gradient direction of E until a local minimum \\nis obtained. Although only a locally optimal set of weights is ob- \\ntained, in a number of experiments the neural net so generated \\nhas performed surprisingly well not only on the training set but on \\nsubsequent data. 4- This performance is probably the main reason \\nfor widespread interest in backpropagation. \\nIt seems to us natural, in the context of the medical diagnosis \\nproblem, the other real world problems to which backpropagation \\nhas been applied, and indeed in any mapping problem where one \\ndesires to generalize from a limited and noisy set of examples, to \\ninterpret the output vector in probabilistic terms. Such an inter- \\npretation is standard in the literature on pattern classification. m \\nIndeed, the examples might even be probabilistic themselves. That \\nis to say it might not be certain whether symptom i was present \\nin case/ or not. \\nLet s represent the probability symptom i is present in case \\n' represent the probability disease j ocurred in case \\n/% and let tj \\n56 \\n/a. Consider for the moment the case where the t. are 1 or 0, \\nso that the cases are in fact fully diagnosed. Let fj(,$) be our \\nprediction of the probability of disease j given input vector , where \\n is some set of parameters determined by our learning algorithm. \\nIn the neural network case, the  are the connection weights and \\nNow lacking a priori knowledge of good 0, the best one can do \\nis to choose the parameters  to maximize the likelihood that the \\ngiven set of examples should have occurred. i The formula for this \\nlikelihood, p, is immediate: \\nor \\nlog(p) --  [  log( f j ( ', ) ) +  \\nlog(1- fj(u,))] \\nThe extension of equation (2), and thus equation (3) to the \\ncase where the  are probabilities, taking values in [0, 1], is straight- \\n57 \\nforward '1 and yields \\n(4) \\nExpressions of this sort often arise in physics and information the- \\nory and are generally interpreted as an entropy. TM \\nWe may now vary the {} in the gradient direction of the en- \\ntropy. The back propagation algorithm generalizes immediately \\nfrom minimizing 'Error' or 'Energy' to maximizing entropy or log \\nlikelihood, or indeed any other function of the outputs and the \\ninputs TM. Of course it remains true that the gradient can be com- \\nputed by back propagation with essentially the same number of \\ncomputations as are required to compute the output of the net- \\nwork. \\nA backpropagation algorithm based on log-likelihood is not \\nonly more intuitively appealing than one based on an ad-hoc def- \\ninition of error, but will make quite different and more accurate \\npredictions as well. Consider e.g. training the net on an exam- \\nple which it already understands fairly well. Say t] = 0, and \\nfj(s ) = e. Now, from eqn(1) OE/Ofj = 2e, so using 'Error' as a \\n* We may see this by constructing an equivalent larger set of \\nexamples with the  taking only values 0 or i with the appropriate \\nfrequency. Thus assume the tj are rational numbers with denomi- \\n and let p IIt,,j  What we mean by \\nt, and numerator nj \\nnator dj -- dj. \\nthe set of examples {t  / -- 1, ..., M} can be represented by con- \\nru=0 \\nsidering a set of N = Mp examples {} where for each/, tj \\n'), and r, = 1 \\nfor p(/- 1) &lt; y &lt;_ p/ and I &lt;_ ymod(d) &lt;_ (d - nj tj \\notherwise. Now applying equation (3) gives equation (4), up to an \\noverall normalization. \\n58 \\ncriterion the net learns very little from this example, whereas, us- \\ning eqn(3), Olog(p)/Ofj = 1/(1 -), so the net continues to learn \\nand can in fact converge to predict probabilities near 1. Indeed \\nbecause backpropagation using the standard 'Error' measure can \\nnot converge to generate outputs of 1 or 0, it has been custom- \\nary in the literature 4 to round the target values so that a target \\nof i would be presented in the learning algorithm as some ad hoc \\nnumber such as .8, whereas a target of 0 would be presented as .2. \\nIn the context of our general discussion it is natural to ask \\nwhether using a feedforward network and varying the weights is in \\nfact the most effective alternative. Anderson and Abrahams 3 have \\ndiscussed this issue from a Bayesian viewpoint. From this point of \\nview, fitting output to input using normal distributions and varying \\nthe means and covariance matrix may seem to be more logical. \\nFeedforward networks do however have several advantages for \\ncomplex problems. Experience with neural networks has shown the \\nimportance of including hidden units wherein the network can form \\nan internal representation of the world. If one simply uses normal \\ndistributions, any hidden variables included will simply integrate \\nout in calculating an output. It will thus be necessary to include at \\nleast third order correlations to implement useful hidden variables. \\nUnfortunately, the number of possible third order correlations is \\nvery large, so that there may be practical obstacles to such an \\napproach. Indeed it is well known folklore in curve fitting and \\npattern classification that the number of parameters must be small \\ncompared to the size of the data set if any generalization to future \\ncases is expected.  \\nIn feedforward nets the question takes a different form. There \\ncan be bottlenecks to information flow. Specifically, if the net is \\n59 \\nconstructed with an intermediate layer which is not bypassed by \\nany connections (i.e. there are no connections from layers preceding \\nto layers subsequent), and if furthermore the activation functions \\nare chosen so that the values of each of the intermediate nodes \\ntend towards either i or 0 '2, then this layer serves as a bottleneck \\nto information flow. No matter how many input nodes, output \\nnodes, or free parameters there are in the net, the output will be \\nconstrained to take on no more than 2  different patterns, where \\nI is the number of nodes in the bottleneck layer. Thus if I is \\nsmall, some sort of 'generalization' must occur even if the number \\nof weights is large. One plausible reason for the success of back \\npropagation in adequately solving tasks, in spite of the fact that \\nit finds only local minima, is its ability to vary a large number of \\nparameters. This freedom may allow back propagation to escape \\nfrom many putative traps and to find an acceptable solution. \\nA good expert system, say for medical diagnosis, should not \\nonly give a diagnosis based on the available information, but should \\nbe able to suggest, in questionable cases, which lab tests might be \\nperformed to clarify matters. Actually back propagation inher- \\nently has such a capability. Back propagation involves calculation \\nof Olog(p)/Owij. This information allows one to compute immedi- \\nately Olog(p)/Osj. Those input nodes for which this partial deriva- \\ntive is large correspond to important experiments. \\nIn conclusion, we propose that back propagation can be gen- \\neralized, put on a satisfactory conceptual footing, and very likely \\nmade more efficient, by defining the values of the output and in- \\n 2 Alternatively when necessary this can be enforced by adding \\nan energy term to the log-likelihood to constrain the parameter \\nvariation so that the neuronal values are near either I or 0. \\n6O \\nput neurons as probabilities, and replacing the 'Error' by the log- \\nlikelihood. \\nAcknowledgement: E. B. Baum was supported in part by DARPA \\nthrough arrangement with NASA and by NSF grant DMB-840649, \\n802. F. Wilczek was supported in part by NSF grant PHY82-17853 \\nReferences \\n(1)Werbos,P,\"Beyond Regression: New Tools for Prediction and \\nAnalysis in the Behavioral Sciences\", Harvard University Disserta- \\ntion (1974) \\n(2)Parker D. B.,\"Learning Logic\",MIT Tech Report TR-47, Center \\nfor Computationl Research in Economics and Management Science, \\nMIT, 1985 \\n(3)Le Cun, Y., Proceedings of Cognitiva 85,p599-604, Paris (X9S5) \\n(4)Rumelhart, D. E., Hinton, G. E., Williams, G. E., \"Learning \\nInternal Representations by Error Propagation\", in \"Parallel Dis- \\ntributed Processing\", vol 1, eds. Rumelhart, D. E., McClelland, J. \\nL., MIT Press, Cambridge MA,(1986) \\n(5)Sejnowski, T. J., Rosenberg, C. R., Complex Systems, v 1, pp \\n145-168 (1987) \\n(6)LeCun, Y., Address at 1987 Snowbird Conference on Neural \\nNetworks \\n(7)Gorman, P., Sejnowski, T. J.,\"Learned Classification of Sonar \\nTargets Using a Massively Parallel Network\", in \"Workshop on \\nNeural Network Devices and Applications\", JPLD-4406, (1987) \\npp224-237 \\n(8)Rosenblatt, F.,\"Principles of Neurodynamics: Perceptrons and \\n61 \\nthe theory of brain mechanisms\", Spartan Books, Washington DC \\n(1962) \\n(9)Widrow, B., HolT, M. E., 1960 IRE WESCON Cony. Record, \\nPart 4, 96-104 (1960) \\n(10)Duda, R. O., Hart, P. E., \"Pattern Classification and Scene \\nAnalysis\", John Wiley and Sons, N.Y., (1973) \\n(11)Guiasu, S., \"Information Theory with Applications\", McGraw \\nHill, NY, (1977) \\n(12)Baum,E.B.,\"Generalizing Back Propagation to Computation\", \\nin \"Neural Networks for Computing\", AIP Conf. Proc. 151, Snow- \\nbird UT (1986)pp47-53 \\n(13)Anderson, C.H., Abrahams, E.,\"The Bayes Connection\", Pro- \\nceedings of the IEEE International Conference on Neural Networks, \\nSan Diego,(1987) \\n</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1735</th>\n","      <td>1735</td>\n","      <td>5</td>\n","      <td>35.56</td>\n","      <td>cell, response, activity, stimulus, neuron, pattern, cortical, layer, receptive_field, cortex, connection, orientation, unit, visual, spatial, contrast, simulation, mechanism, population, synaptic</td>\n","      <td>Coastal Navigation with Mobile Robots \\nNicholas Roy and Sebastian Thrun \\nSchool of Computer Science \\nCarnegie Mellon University \\nPittsburgh, PA 15213 \\n{ nicholas. roy I sebastian. thrun } @ cs. cmu. edu \\nAbstract \\nThe problem that we address in this paper is how a mobile robot can plan in order \\nto arrive at its goal with minimum uncertainty. Traditional motion planning algo- \\nrithms often assume that a mobile robot can track its position reliably, however, in real \\nworld situations, reliable localization may not always be feasible. Partially Observable \\nMarkov Decision Processes (POMDPs) provide one way to maximize the certainty of \\nreaching the goal state, but at the cost of computational intractability for large state \\nspaces. \\nThe method we propose explicitly models the uncertainty of the robot's position as \\na state variable, and generates trajectories through the augmented pose-uncertainty \\nspace. By minimizing the positional uncertainty at the goal, the robot reduces the \\nlikelihood it becomes lost. We demonstrate experimentally that coastal navigation \\nreduces the uncertainty at the goal, especially with degraded localization. \\n1 Introduction \\nFor an operational mobile robot, it is essential to prevent becoming lost. Early motion \\nplanners assumed that a robot would never be lost - that a robot could always know its \\nposition via dead reckoning without error [7]. This assumption proved to be untenable due \\nto the small and inevitable inconsistencies in actual robot motion; robots that rely solely on \\ndead reckoning for their position estimates lose their position quickly. Mobile robots now \\nperform position tracking using a combination of sensor data and odometry [2, 10, 5]. \\nHowever, the robot's ability to track its position can vary considerably with the robot's \\nposition in the environment. Some parts of the environment may lack good features for lo- \\ncalization [ 11 ]. Other parts of the environment can have a large number of dynamic features \\n(for example, people) that can mislead the localization system. Motion planners rarely, if \\never, take the robot's position tracking ability into consideration. As the robot's localiza- \\ntion suffers, the likelihood that the robot becomes lost increases, and as a consequence, the \\nrobot is less likely to complete the given trajectory. \\nMost localization systems therefore compensate by adding environment-specific knowl- \\nedge to the localization system, or by adding additional sensing capabilities to the robot, \\nto guarantee that the robot can complete every possible path. In general, however, such \\nalterations to the position tracking abilities of the robot have limitations, and an alternative \\nscheme must be used to ensure that the robot can navigate with maximum reliability. The \\nconventional planners represent one end of a spectrum of approaches (figure 1), in that a \\nplan can be computed easily, but at the cost of not modelling localization performance. \\nAt opposite end of the spectrum is the Partially Observable Markov Decision Process \\n1044 N. Roy and S. Thrun \\nConventional \\nPath Planner POMDP \\nTractable Intractable \\nNot Robust Robust \\nFigure 1: The continuum of possible approaches to the motion planning, from the robust but in- \\ntractable POMDP, to the potentially failure-prone but real-time conventional planners. Coastal navi- \\ngation lies in the middle of this spectrum. \\n(POMDP). POMDPs in a sense are the brass ring of planning with uncertainty; a POMDP \\npolicy will make exactly the right kind of compromise between conventional optimality \\nconsiderations and certainty of achieving the goal state. Many people have examined the \\nuse of POMDPs for mobile robot navigation [5, 6, 8]. However, computing a POMDP \\nsolution is computationally intractable (PSPACE-hard) for large state systems - a mobile \\nrobot operating in the real world often has millions of possible states. As a result, many \\nof the mobile robot POMDP solutions have made simplifying assumptions about the world \\nin order to reduce the state space size. Many of these assumptions do not scale to larger \\nenvironments or robots. In contrast, our hypothesis is that only a small number of the \\ndimensions of the uncertainty matter, and that we can augment the state with these dimen- \\nsions to approximate a solution to the POMDP. \\nThe coastal navigation model developed in this paper represents a tradeoff between robust \\ntrajectories and computational tractability, and is inspired by traditional navigation of ships. \\nShips often use the coasts of continents for navigation in the absence of better tools such \\nas GPS, since being close to the land allows sailors to determine with high accuracy where \\nthey are. The success of this method results from coast lines containing enough information \\nin their structure for accurate localization. By navigating sufficiently close to areas of the \\nmap that have high information content, the likelihood of getting lost can be minimized. \\n2 Modelling Uncertainty \\nThe problem that we address in this paper is how a mobile robot can plan in order to arrive \\nat its goal with minimum uncertainty. Throughout this discussion, we will be assuming a \\nknown map of the environment [9]. The position, x, of the robot is given as the location \\n(z, y) and direction 0, defined over a space X = (X, Y, O). Our localization method is \\na grid-based implementation of Markov localization [3, 5]. This method represents the \\nrobot's belief in its current position using a 3-dimensional grid over X - (X, Y, ), which \\nallows for a discrete approximation of arbitrary probability distributions. The probability \\nthat the robot has a particular pose x is given by the probability p(x). \\nState Augmentation We can extend the state of the robot from the 3-dimensional pose \\nspace to an augmented pose-uncertainty space. We can represent the uncertainty of the \\nrobot's positional distribution as the entropy, \\nH(px) = - fp(x)log(p(x)) \\nx \\n(1) \\nWe therefore represent the state space of the robot as the tuple \\nS = \\n: (x, (x)) \\nState Transitions In order to construct a plan between two points in the environment, \\nwe need to be able to represent the effect of the robot's sensing and moving actions. The \\nimplementation of Markov localization provides the following equations for the tracking \\nCoastal Navigation with Mobile Robots 1045 \\nthe robot's pose from x to x': \\n= f,(x'lx, (2) \\nx \\n= ap(z[x)p(x) (3) \\nThese equations are taken from [3, 12], where equation (2) gives the prediction phase of \\nlocalization (after motion u), and equation (3) gives the update phase of localization (after \\nreceiving observation z). c is a normalizing constant. We extend these equations to the \\nfourth dimension as follows: \\n3 Planning \\np(slu) : (4) \\np(sl,.) : \\nEquations (4) and (5) provide a mechanism for tracking the robot's state, and in fact contain \\nredundant information, since the extra state variable H (x) is also contained in the probabil- \\nity distribution p(x). However, in order to make the planning problem tractable, we cannot \\nin fact maintain the probabilistic sensing model. To do so would put the planning problem \\nfirmly in the domain of POMDPs, with the associated computational intractability. Instead, \\nwe make a simplifying assumption, that is, that the positional probability distribution of \\nthe robot can be represented at all times by a Gaussian centered at the mean x. This allows \\nus to approximate the positional distribution with a single statistic, the entropy. In POMDP \\nterms, we using the assumption of Gaussian distributions to compress the belief space to a \\nsingle dimension. We can now represent the positional probability distribution completely \\nwith the vector s, since the width of the Gaussian is represented by the entropy H (x). \\nMore importantly, the simplifying assumption allows us to track the state of the robot de- \\nterministically. Although the state transitions are stochastic (as in equation (4)), the obser- \\nvations are not. At any point in time, the sensors identify the true state of the system, with \\nsome certainty given by H(p(xlz)). This allows us to compress the state transitions into a \\nsingle rule: \\np(sl)- (6) \\nThe final position of the robot depends only on the motion command t and can be identified \\nby sensing z. However, the uncertainty of the pose, H(p(xlt , z)), is a function not only \\nof the motion command but also the sensing. The simplifying assumption of Gaussian \\nmodels is in general untenable for localization; however, we shall see that this assumption \\nis sufficient for the purpose of motion planning. \\nOne final modification must be made to the state transition rule. In a perfect world, it \\nwould be possible to predict exactly what observation would be made. However, it is \\nexactly the stochastic and noisy nature of real sensors that generates planning difficulty, \\nyet the update rule (6) assumes that it is possible to predict measurement z at pose x. \\nDeterministic prediction is not possible; however, it is possible to compute probabilities \\nfor sensor measurements, and thus generate an expected value for the entropy based on the \\nprobability distribution of observations Z, which leads to the final state transition rule: \\npl,) = (p(xl,),Ez[r(p(xl,,,.))]) (7) \\nwhere Ez [H(p(xl u, z))] represents the expected value of the entropy of the pose distribu- \\ntion over the space of possible sensor measurements. \\nWith the transition rule in equation (7), we can now compute the transition probabilities \\nfor any particular state using a model of the robot's motion, a model of the robot's sensor \\nand a map of the environment. The probability p(xlu) is given by a model of the robot's \\nmotion, and can be easily precomputed for each action u. The expectation term Ez [HI \\n1046 N. Roy and S. Thrun \\ncan also be precomputed for each possible state s. The precomputation of these transition \\nprobabilities is very time-intensive, because it requires simulating sensing at each state in \\nthe environment, and then computing the posterior distribution. However, as the precom- \\nputation is a one-time operation for the environment and robot, planning itself can be an \\nonline operation and is (in the limit) unaffected by the speed of computing the transition \\nprobabilities. \\n3.1 Computing Trajectories \\nWith the state update rule given in equation (7), we can now compute the optimal trajectory \\nto a particular goal. We would in fact like to compute not just the optimal trajectory from \\nthe current robot position, but the optimal action from any position in the world. If the robot \\nshould deviate from the expected trajectory for any reason (such as error in the motion, or \\ndue to low-level control constraints), interests of efficiency suggest precomputing actions \\nfor continuing to the goal, rather than continually replanning as these contingencies arise. \\nNote that the motion planning problem as we have now phrased it can be viewed as the \\nproblem of computing the optimal policy for a given problem. The Markovian, stochastic \\nnature of the transitions, coupled with the need to compute the optimal policy for all states, \\nsuggests a value iteration approach. \\nValue iteration attempts to find the policy that maximizes the long-term reward [ 1, 4]. The \\nproblem becomes one of finding the value function, J(s) which assigns a value to each \\nstate. The optimal action at each state can then be easily computed by determining the \\nexpected value of each action at each state, from the neighboring values. We use a modified \\nform of Bellman's equations to give the value of state J(s) and policy as \\nN \\nJ(si) - max[R(si)+ C(s,u)+ J(s:)] (8) \\nj=l \\nN \\nrr(si) = argmax[R(si) + C(s, u) + p(sj[s/, u). J(sj)] (9) \\nu j=l \\nBy iterating equation (8), the value function iteratively settles to a converged value over all \\nstates. Iteration stops when no state value changes above some threshold value. \\nIn the above equations, R(si) is the immediate reward at state si, p(sj [si, u) is the transition \\nprobability from state si to state sj, and C(s, u) is the cost of taking action u at state s. Note \\nthat the form of the equations is undiscounted in the traditional sense, however, the additive \\ncost term plays a similar role in that the system is penalized for policies that take longer \\ntrajectories. The cost in general is simply the distance of one step in the given direction u, \\nalthough the cost of travel close to obstacles is higher, in order to create a safety margin \\naround obstacles. The cost of an action that would cause a collision is infinite, preventing \\nsuch actions from being used. \\nThe immediate reward is localized only at the goal pose. However, the goal pose has a \\nrange of possible values for the uncertainty, creating a set of goal states, 6. In order to \\nreward policies that arrive at a goal state with a lower uncertainty, the reward is scaled \\nlinearly with goal state uncertainty. \\n/r(Xi ) __ {-- /r/(S) S( (10) \\notherwise \\nBy implementing the value iteration given in the equations (8) and (9) in a dynamic pro- \\ngram, we can compute the value function in O(nkcrit) where n is the number of states in \\nthe environment (number of positions x number of entropy levels) and kcrit is the num- \\nber of iterations to convergence. With the value function computed, we can generate the \\noptimal action for any state in O(a) time, where a is the number of actions out of each \\nstate. \\nCoastal Navigation with Mobile Robots 1047 \\n4 Experimental Results \\nFigure 2 shows the mobile robot, Minerva, used for this research. Minerva is a RWI B-18, \\nand senses using a 360  field of view laser range finder at 1  increments. \\nFigure 2: Minerva, the B-18 mobile robot used for this research, and an example environment map, \\nthe Smithsonian National Museum of American History. The black areas are the walls and obstacles. \\nNote the large sparse areas in the center of the environment. \\nAlso shown in figure 2 is an example environment,the Smithsonian National Museum of \\nAmerican History. Minerva was used to generate this map, and operated as a tour-guide in \\nthe museum for two weeks in the summer of 1998. This museum has many of the features \\nthat make localization difficult- large open spaces, and many dynamic obstacles (people) \\nthat can mislead the sensors. \\nGoal \\narti t'ositio. \\nStartingSPosition \\n.... GalaQ i S a \\n(a) Conventional \\n(b) Coastal \\n(c) Sensor Map \\nFigure 3: Two examples in the museum environment. The left trajectory is given by a conventional, \\nshortest-path planner. The middle trajectory is given by the coastal navigation planner. The black \\nareas correspond to obstacles, the dark grey areas correspond to regions where sensor information is \\navailable, the light grey areas to regions where no sensor information is available. \\nFigure 3 shows the effect of different planners in the sample environment. Panel (a) shows \\nthe trajectory of a conventional, shortest distance planner. Note that the robot moves di- \\n1048 N. Roy and S. Thrun \\nrectly towards the goal. Panel (b) shows the trajectory given by the coastal planner. In both \\nexamples, the robot moves towards an obstacle, and relocalizes once it is in sensor range of \\nthe obstacle, before moving towards the goal. These periodic relocalizations are essential \\nfor the robot to arrive at the goal with minimum positional uncertainty, and maximum reli- \\nability. Panel (c) shows the sensor map of the environment. The black areas show obstacles \\nand walls, and the light grey areas are where no information is available to the sensors, be- \\ncause all environmental features are outside the range of the sensors. The dark grey areas \\nindicate areas where the information gain from the sensors is not zero; the darker grey the \\narea, the better the information gain from the sensors. \\n20 \\n18 \\n16 \\n14 \\n12 \\n10 \\n8 \\n6 \\n4 \\n2 \\n0 \\n-2 \\n0 \\nPositional Uncertainty at Goal \\nConventional Navigation \\nCoastal Navigation \\n0'.5 i 1'.5  2.5  3'.5 ' 4'.5  \\nMaximum Range of Laser Range Sensor in Meters \\n5.5 \\nFigure 4: The performance of the coastal navigation algorithm compared to the coastal motion plan- \\nner. The graph depicts the entropy of the position probability distribution against the range of the \\nlaser sensor. Note that the coastal navigation dramatically improves the certainty of the goal position \\nwith shorter range laser sensing. \\nFigure 4 is a comparison of the average positional certainty (computed as entropy of the \\npositional probability) of the robot at its goal position, compared to the range of the laser \\nrange sensor. As the range of the laser range gets shorter, the robot can see fewer and \\nfewer environmental features - this is essentially a way of reducing the ability of the robot \\nto localize itself. The upper line is the performance of a conventional shortest-distance \\npath planner, and the lower line is the coastal planner. The coastal planner has a lower \\nuncertainty for all ranges of the laser sensor, and is substantially lower at shorter ranges, \\nconfirming that the coastal navigation has the most effect when the localization is worst. \\n5 Conclusion \\nIn this paper, we have described a particular problem of motion planning- how to guarantee \\nthat a mobile robot can reach its goal with maximum reliability. Conventional motion \\nplanners do not typically plan according to the ability of the localization unit in different \\nareas of the environment, and thus make no claims about the robustness of the generated \\ntrajectory. In contrast, POMDPs provide the correct solution to the problem of robust \\ntrajectories, however, computing the solution to a POMDP is intractable for the size of the \\nstate space for typical mobile robot environments. \\nWe propose a motion planner with an augmented state space that represents positional \\nuncertainty explicitly as an extra dimension. The motion planner then plans through pose- \\nuncertainty space, to arrive at the goal pose with the lowest possible uncertainty. This can \\nbe seen to be an approximation to a POMDP where the multi-dimensional belief space is \\nrepresented as a subset of statistics, in this case the entropy of the belief space. \\nWe have shown some experimental comparisons with a conventional motion planner. Not \\nonly did the coastal navigation generated trajectories that provided substantial improve- \\nment of the positional certainty at the goal compared to the conventional planner, but the \\nimprovement became more pronounced as the localization was degraded. \\nCoastal Navigation with Mobile Robots 1049 \\nThe model presented here, however, is not complete. The entire methodology hinges upon \\nthe assumption that the robot's probability distribution can be adequately represented by \\nthe entropy of the distribution. This assumption is valid if the distribution is restricted \\nto a uni-modal Gaussian, however, most Markov localization methods that are based on \\nthis assumption fail, because multi-modal, non-Gaussian positional distributions are quite \\ncommon for moving robots. Nonetheless, it may be that multiple uncertainty statistics \\nalong multiple dimensions (e.g., z and g/) may do a better job of capturing the uncertainty \\nsufficiently. It is an question for future work as to how many statistics can capture the \\nuncertainty of a mobile robot, and under what environmental conditions. \\nAcknowledgments \\nThe authors gratefully acknowledge the advice and collaboration of Tom Mitchell throughout the \\ndevelopment of this work. Wolfram Burgard and Dieter Fox played an instrumental role in the de- \\nvelopment of earlier versions of this work, and their involvement and discussion of this new model is \\nmuch appreciated. This work was partially funded by the Fonds pour la Formation de Chercheurs et \\nl'Aide h la Recherche (FCAR). \\nReferences \\n[1] R. Bellman. Dynamic Programming. Princeton University Press, NJ, 1957. \\n[2] W. Burgard, D. Fox, D. Hennig, and T. Schmidt. Estimating the absolute position of a mobile \\nrobot using position probability grids. In AAAI, 1996. \\n[3] D. Fox, W. Burgard, and S. Thrun. Active Markov localization for mobile robots. Robotics and \\nAutonomous Systems, 25(3-4), 1998. \\n[4] R. A. Howard. Dynamic Programming and Markov Processes. MIT, 1960. \\n[5] L. Kaelbling, A. R. Cassandra, and J. A. Kurien. Acting under uncertainty: Discrete Bayesian \\nmodels for mobile-robot navigation. In IROS, 1996. \\n[6] S. Koenig and R. Simmons. The effect of representation and knowledge on goal-directed explo- \\nration with reinforcement learning algorithms. Machine Learning Journal, 22:227-250, 1996. \\n[7] J.-C. Latombe. Robot Motion Planning. Kluwer Academic Publishers, 1991. \\n[8] S. Mahadevan and N. Khaleeli. Robust mobile robot navigation using partially-observable \\nsemi-Markov decision processes. 1999. \\n[9] H. P. Moravec and A. Elfes. High resolution maps from wide angle sonar. In ICRA, 1985. \\n[10] R. Sim and G. Dudek. Mobile robot localization from learned landmarks. In IROS, 1998. \\n[11] H. Takeda, C. Facchinetti, and J.-C. Latombe. Planning the motions of mobile robot in a sensory \\nuncertainty field. IEEE Trans. on Pattern Analysis and Machine Intelligence, 16(10), 1994. \\n[12] S. Thrun, D. Fox, and W. Burgard. A probabilistic approach to concurrent mapping and local- \\nization for mobile robots. Machine Learning, 431, 1998. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>1736</th>\n","      <td>1736</td>\n","      <td>5</td>\n","      <td>33.06</td>\n","      <td>cell, response, activity, stimulus, neuron, pattern, cortical, layer, receptive_field, cortex, connection, orientation, unit, visual, spatial, contrast, simulation, mechanism, population, synaptic</td>\n","      <td>Learning Factored Representations for Partially \\nObservable Markov Decision Processes \\nBrian Sallans \\nDepartment of Computer Science \\nUniversity of Toronto \\nToronto M5S 2Z9 Canada \\nGatsby Computational Neuroscience Unit* \\nUniversity College London \\nLondon WC1N 3AR U.K. \\nsallans @ cs. toronto. edu \\nAbstract \\nThe problem of reinforcement learning in a non-Markov environment is \\nexplored using a dynamic Bayesian network, where conditional indepen- \\ndence assumptions between random variables are compactly represented \\nby network parameters. The parameters are learned on-line, and approx- \\nimations are used to perform inference and to compute the optimal value \\nfunction. The relative effects of inference and value function approxi- \\nmations on the quality of the final policy are investigated, by learning to \\nsolve a moderately difficult driving task. The two value function approx- \\nimations, linear and quadratic, were found to perform similarly, but the \\nquadratic model was more sensitive to initialization. Both performed be- \\nlow the level of human performance on the task. The dynamic Bayesian \\nnetwork performed comparably to a model using a localist hidden state \\nrepresentation, while requiring exponentially fewer parameters. \\n1 Introduction \\nReinforcement learning (RL) addresses the problem of learning to act so as to maximize \\na reward signal provided by the environment. Online RL algorithms try to find a policy \\nwhich maximizes the expected time-discounted reward. They do this through experience \\nby performing sample backups to learn a value function over states or state-action pairs. \\nIf the decision problem is Markov in the observable states, then the optimal value function \\nover state-action pairs yields all of the information required to find the optimal policy for \\nthe decision problem. When complete knowledge of the environment is not available, states \\nwhich are different may look the same; this uncertainty is called perceptual aliasing [1], \\nand causes decision problems to have dynamics which are non-Markov in the perceived \\nstate. \\n* Correspondence address \\nLearning Factored Representations for POMDPs 1051 \\n1.1 Partially observable Markov decision processes \\nMany interesting decision problems are not Markov in the inputs. A partially observable \\nMarkov decision process (POMDP) is a formalism in which it is assumed that a process is \\nMarkov, but with respect to some unobserved (i.e. \"hidden\") random variable. The state of \\nthe variable at time t, denoted s t, is dependent only on the state at the previous time step and \\non the action performed. The currently-observed evidence is assumed to be independent of \\nprevious states and observations given the current state. \\nThe state of the hidden variable is not known with certainty, so a belief state is maintained \\ninstead. At each time step, the beliefs are updated by using Bayes' theorem to combine the \\nbelief state at the previous time step (passed through a model of the system dynamics) with \\nnewly observed evidence. In the case of discrete time and finite discrete state and actions, a \\nPOMDP is typically represented by conditional probability tables (CPTs) specifying emis- \\nsion probabilities for each state, and transition probabilities and expected rewards for states \\nand actions. This corresponds to a hidden Markov model (HMM) with a distinct transition \\nmatrix for each action. The hidden state is represented by a single random variable that can \\ntake on one of K values. Exact belief updates can be computed using Bayes' rule. \\nThe value function is not over the discrete state, but over the real-valued belief state. It has \\nbeen shown that the value function is piecewise linear and convex [2]. In the worst case, \\nthe number of linear pieces grows exponentially with the problem horizon, making exact \\ncomputation of the optimal value function intractable. \\nNotice that the localist representation, in which the state is encoded in a single random \\nvariable, is exponentially inefficient: Encoding r bits of information about the state of the \\nprocess requires 2 n possible hidden states. This does not bode well for the abilities of \\nmodels which use this representation to scale up to problems with high-dimensional inputs \\nand complex non-Markov structure. \\n1.2 Factored representations \\nA Bayesian network can compactly represent the state of the system in a set of random \\nvariables [3]. A two time-slice dynamic Bayesian network (DBN) represents the system at \\ntwo time steps [4]. The conditional dependencies between random variables from time t to \\ntime t + 1, and within time step t, are represented by edges in a directed acyclic graph. The \\nconditional probabilities can be stored explicitly, or parameterized by weights on edges in \\nthe graph. \\nIf the network is densely-connected then inference is intractable [5]. Approximate infer- \\nence methods include Markov chain Monte Carlo [6], variational methods [7], and belief \\nstate simplification [8]. \\nIn applying a DBN to a large problem there are three distinct issues to disentangle: How \\nwell does a parameterized DBN capture the underlying POMDP; how much is the DBN \\nhurt by approximate inference; and how good must the approximation of the value function \\nbe to achieve reasonable performance? We try to tease these issues apart by looking at the \\nperformance of a DBN on a problem with a moderately large state-space and non-Markov \\nstructure. \\n2 The algorithm \\nWe use a fully-connected dynamic sigmoid belief network (DSBN) [9], with K units at \\neach time slice (see figure 1). The random variables si are binary, and conditional proba- \\n1052 B. Sallans \\ntime t t+l \\nFigure 1: Architecture of the \\ndynamic sigmoid belief network. \\nCircles indicate random variables, \\nwhere a filled circle is observed \\nand an empty circle is unobserved. \\nSquares are action nodes, and dia- \\nmonds are rewards. \\nbilities relating variables at adjacent time-steps are encoded in action-specific weights: \\nk=l \\na t \\nwhere wik is the weight from the i th unit at time step t to the k th unit at time step t + 1, \\nassuming action a t is taken at time t. The nonlinearity is the usual sigmoid function: \\na(z) = 1/1 + exp{-z}. Note that a bias can be incorporated into the weights by clamping \\none of the binary units to 1. \\nThe observed variables are assumed to be discrete; the conditional distribution of an output \\ngiven the hidden state is multinomial and parameterized by output weights. The probability \\nof observing an output with value I is given by: \\nP(o t l[{s K exp {rkK__l \\n= = (2) \\n__1  exp { \\nwhere o t E 0 and ukl denotes the output weight from hidden unit k to output value I. \\n2.1 Approximate inference \\nInference in the fully-connected Bayesian network is intractable. Instead we use a varia- \\ntional method with a fully-factored approximating distribution: \\nK \\nP(stls*-X,a*-,o*)  Ps'  1-I/(1-/) 1-L (3) \\nk=l \\nwhere the/,t are variational parameters to be optimized. This is the standard mean-field \\napproximation for a sigmoid belief network [ 10]. The parameters/,t are optimized by iterat- \\ning the mean-field equations, and converge in a few iterations. The values of the variational \\nparameters at time t are held fixed while computing the values for step t + 1. This is \\nanalogous to running only the forward portion of the HMM forward-backward algorithm \\n[11]. \\nThe parameters of the DSBN are optimized online using stochastic gradient ascent in the \\n2 exp] } \\nlog-likelihood: \\n(4) \\nLearning Factored Representations for POMDPs 1053 \\nwhere W and U are the transition and emission matrices respectively, aw and au are \\nlearning rates, the vector/.t contains the fully-factored approximate belief state, and v is a \\nvector of zeros with a one in the o tth place. The notation [']k denotes the k th element of a \\nvector (or k  column of a matrix). \\n2.2 Approximating the value function \\nComputing the optimal value function is also intractable. If a factored state-space represen- \\ntation is appropriate, it is natural (if extreme) to assume that the state-action value function \\ncan be decomposed in the same way: \\nK \\nq(vst,a t) qF(u, *) \\nk=l \\nThis simplifying assumption is still not enough to make finding the optimal value func- \\ntion tractable. Even if the states were completely independent, each Qk would still be \\npiecewise-linear and convex, with the number of pieces scaling exponentially with the hori- \\nzon. We test two approximate value functions, a linear approximation: \\nK \\nand a quadratic approximation: \\nK \\nat) = E q3k,at tX + qk,t k + ba t \\n= + [q]t + \\n(6) \\n(7) \\nWhere (I,, Q and b are parameters of the approximations. The notation [.]i denotes the \\ni t column of a matrix, [-]- denotes matrix transpose and  denotes element-wise vector \\nmultiplication. \\nWe update each term of the factored approximation with a modified Q-learning rule [12], \\nwhich corresponds to a delta-rule where the target for input/x is r t + ? maxa Qv(lu t+x , a)' \\nqk,a t 3-- qk,a  + O k EB \\nba t t- bat + a EB \\n(8) \\nHere oz is a learning rate, ? is the temporal discount factor, and EB is the Bellman residual: \\nEB = r t + ?maxQr(/xt+, a) - Q'(txt, a t) (9) \\na \\n3 Experimental results \\nThe \"New York Driving\" task [ 13] involves navigating through slower and faster one-way \\ntraffic on a multi-lane highway. The speed of the agent is fixed, and it must change lanes to \\navoid slower cars and move out of the way of faster cars. If the agent remains in front of a \\nfaster car, the driver of the fast car will honk its horn, resulting in a reward of -1.0. Instead \\nof colliding with a slower car, the agent can squeeze past in the same lane, resulting in a \\nreward of -10.0. A time step with no horns or lane-squeezes constitutes clear progress, \\nand is rewarded with +0.1. See [13] for a detailed description of this task. \\n1054 B. Sallans \\nTable 1: Sensory input for the New York driving task \\nDimension ]Size[Values \\nHear horn 2 yes, no \\nGaze object 3 truck, shoulder, road \\nGaze speed 2 looming, receding \\nGaze distance 3 far, near, nose \\nGaze refined distance 2 far-half, near-half \\nGaze colour 6 red, blue, yellow, white, gray, tan \\nA modified version of the New York Driving task was used to test our algorithm. The \\ntask was essentially the same as described in [13], except that the \"gaze side\" and \"gaze \\ndirection\" inputs were removed. See table 1 for a list of the modified sensory inputs. \\nThe performance of a number of algorithms and approximations were measured on the task: \\na random policy; Q-learning on the sensory inputs; a model with a localist representation \\n(i.e. the hidden state consisted of a single multinomial random variable) with linear and \\nquadratic approximate value functions; the DSBN with mean-field inference and linear and \\nquadratic approximations; and a human driver. The localist representation used the linear \\nQ-learning approximation of [14], and the corresponding quadratic approximation. The \\nquadratic approximations were trained both from random initialization, and from initial- \\nization with the corresponding learned linear models (and random quadratic portion). The \\nnon-human algorithms were each trained for 100000 iterations, and in each case a constant \\nlearning rate of 0.01 and temporal decay rate of 0.9 were used. The human driver (the au- \\nthor) was trained for 1000 iterations using a simple character-based graphical display, with \\neach iteration lasting 0.5 seconds. \\nStochastic policies were used for all RL algorithms, with actions being chosen from a \\nBoltzmann distribution with temperature decreasing over time: \\n1 \\nP(atlt) - Zs exp{qv(t'at)/T (10) \\nThe DSBN had 4 hidden units per time slice, and the localist model used a multinomial \\nwith 16 states. The Q-learner had a table representation with 2160 entries. After training, \\neach non-human algorithm was tested for 20 trials of 5000 time steps each. The human was \\ntested for 2000 time steps, and the results were renormalized for comparison with the other \\nmethods. The results are shown in figure 2. All results were negative, so lower numbers \\nindicate better performance in the graph. The error bars show one standard deviation across \\nthe 20 trials. \\nThere was little performance difference between the localist representation and the DSBN \\nbut, as expected, the DSBN was exponentially more efficient in its hidden-state represen- \\ntation. The linear and quadratic approximations performed comparably, but well below \\nhuman performance. However, the DSBN with quadratic approximation was more sensi- \\ntive to initialization. When initialized with random parameter settings, it failed to find a \\ngood policy. However, it did converge to a reasonable policy when the linear portion of the \\nquadratic model was initialized with a previously learned linear model. \\nThe hidden units in the DSBN encode useful features of the input, such as whether a car \\nwas at the \"near\" or \"nose\" position. They also encode some history, such as current gaze \\ndirection. This has advantages over a simple stochastic policy learned via Q-learning: If the \\nQ-learner knows that there is an oncoming car, it can randomly select to look left or right. \\nThe DSBN systematically looks to the left, and then to the right, wasting fewer actions. \\nLearning Factored Representations for POMDPs 1055 \\n4000 \\n3500 \\n3000 \\n- 2500 \\n2000 \\n1500 \\n1000 \\n500 \\n0 \\nQCL \\nAlgorithm \\nQDR \\nFigure 2: Results on the New York \\nDriving task for nine algorithms: \\nR=random; Q=Q-learning; LC=linear \\nmultinomial; QCR=quadratic multi- \\nnomial, random init.; QCL=quadratic \\nmultinomial, linear init; LD=linear \\nDSBN; QDR=quadratic DSBN, ran- \\ndom init.; QDL=quadratic DSBN, \\nlinear init.; H=human \\nH \\n4 Discussion \\nThe DSBN performed better than a standard Q-learner, and comparably to a model with \\na localist representation, despite using approximate inference and exponentially fewer pa- \\nrmeters. This is encouraging, since an efficient encoding of the state is a prerequisite \\nfor tackling larger decision problems. Less encouraging was the value-function approxi- \\nmation: When compared to human performance, it is clear that all methods are far from \\noptimal, although again the factored approximation of the DSBN did not hurt performance \\nrelative to the localist multinomial representation. The sensitivity to initialization of the \\nquadratic approximation is worrisome, but the success of initializing from a simpler model \\nsuggests that staged learning may be appropriate, where simple models are learned and \\nused to initialize more complex models. These findings echo those of [ 14] in the context of \\nlearning a non-factored approximate value function. \\nThere are a number of related works, both in the fields of reinforcement learning and \\nBayesian networks. We use the sigmoid belief network mean-field approximation given \\nin [10], and discussed in the context of time-series models (the \"fully factored\" approxi- \\nmation) in [ 15]. Approximate inference in dynamic Bayesian networks has been discussed \\nin [15] and [8]. The additive factored value function was used in the context of factored \\nMDPs (with no hidden state) in [16], and the linear Q-learning approximation was given \\nin [ 14]. Approximate inference was combined with more sophisticated value function ap- \\nproximation in [17]. To our knowledge, this is the first attempt to explore the practicality \\nof combining all of these techniques in order to solve a single problem. \\nThere are several possible extensions. As described above, the representation learned by \\nthe DSBN is not tuned to the task at hand. The reinforcement information could be used \\nto guide the learning of the DSBN parameters[18, 13]. Also, if this were done, then the \\nreinforcement signals would provide additional evidence as to what state the POMDP is in, \\nand could be used to aid inference. More sophisticated function approximation could be \\nused [ 17]. Finally, although this method appears to work in practice, there is no guarantee \\nthat the reinforcement learning will converge. We view this work as an encouraging first \\nstep, with much further study required. \\n5 Conclusions \\nWe have shown that a dynamic Bayesian network can be used to construct a compact rep- \\nresentation useful for solving a decision problem with hidden state. The parameters of the \\nDBN can be learned from experience. Learning occurs despite the use of simple value- \\n1056 B. Sallans \\nfunction approximations and mean-field inference. Approximations of the value function \\nresult in good performance, but are clearly far from optimal. The fully-factored assump- \\ntions made for the belief state and the value function do not appear to impact performance, \\nas compared to the non-factored model. The algorithm as presented runs entirely on-line \\nby performing \"forward\" inference only. There is much room for future work, including \\nimproving the utility of the factored representation learned, and the quality of approximate \\ninference and the value function approximation. \\nAcknowledgments \\nWe thank Geoffrey Hinton, Zoubin Ghahramani and Andy Brown for helpful discussions, \\nthe anonymous referees for valuable comments and criticism, and particularly Peter Dayan \\nfor helpful discussions and comments on an early draft of this paper. This research was \\nfunded by NSERC Canada and the Gatsby Charitable Foundation. \\nReferences \\n[11] \\n[12] \\n[13] \\n[14] \\n[15] \\n[16] \\n[17] \\n[18] \\n[1] S.D. Whitehead and D.H. Ballard. Learning to perceive and act by trial and error. Machine \\nLearning, 7, 1991. \\n[2] E.J. Sondik. The optimal control of partially observable Markov processes over the infinite \\nhorizon: Discounted costs. Operations Research, 26:282-304, 1973. \\n[3] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Mor- \\ngan Kaufmann, San Mateo, CA, 1988. \\n[4] T Dean and K. Kanazawa. A model for reasoning about persistence and causation. Computa- \\ntional Intelligence, 5, 1989. \\n[5] Gregory F. Cooper. The computational complexity of probabilistic inference using Bayesian \\nbelief networks. Artificial Intelligence, 42:393-405, 1990. \\n[6] R.M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report \\nCRG-TR-93-1, Department of Computer Science, University of Toronto, 1993. \\n[7] M.I. Jordan, Z. Ghahramani, T.S. laakkola, and L.K. Saul. An introduction to variational meth- \\nods for graphical models. Machine Learning, 1999. in press. \\n[8] X. Boyen and D. Koller. Tractable inference for complex stochastic processes. In Proc. UAI'98, \\n1998. \\n[9] R.M. Neal. Connectionist learning of belief networks. Artificial Intelligence, 56:71-113, 1992. \\n[10] L. K. Saul, T Jaakkola, and M. I. Jordan. Mean field theory for sigmoid belief networks. \\nJournal of Artificial Intelligence Research, 4:61-76, 1996. \\nLawrence R. Rabiner and Biing-Hwang Juang. An introduction to hidden Markov models. \\nIEEEASSAP Magazine, 3:4-16, January 1986. \\nC.J.C.H. Watkins and P. Dayan. Q-learning. Machine Learning, 8:279-292, 1992. \\nA.K. McCallurn. Reinforcement learning with selective perception and hidden state. Dept. of \\nComputer Science, Universiy of Rochester, Rochester NY, 1995. Ph.D. thesis. \\nM.L. Littman, A.R. Cassandra, and L.P. Kaelbling. Learning policies for partially observable \\nenvironments: Scaling up. In Proc. International Conference on Machine Learning, 1995. \\nZ. Ghahramani and M. I. Jordan. Factorial hidden Markov models. Machine Learning, 1997. \\nD. Koller and R. Parr. Computing factored value functions for policies in structured MDPs. In \\nProc. IJCAI'99, 1999. \\nA. Rodriguez, R. Parr, and D. Koller. Reinforcement learning using approximate belief states. In \\nS. A. Solla, T. K. Leen, and K.-R. Mtiller, editors, Advances in Neural Information Processing \\nSystems, volume 12. The MIT Press, Cambridge, 2000. \\nL. Chrisman. Reinforcement learning with perceptual aliasing: The perceptual distinctions \\napproach. In Tenth National Conference on AI, 1992. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>1737</th>\n","      <td>1737</td>\n","      <td>5</td>\n","      <td>48.96</td>\n","      <td>cell, response, activity, stimulus, neuron, pattern, cortical, layer, receptive_field, cortex, connection, orientation, unit, visual, spatial, contrast, simulation, mechanism, population, synaptic</td>\n","      <td>Policy Gradient Methods for \\nReinforcement Learning with Function \\nApproximation \\nRichard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour \\nAT&amp;T Labs - Research, 180 Park Avenue, Florham Park, NJ 07932 \\nAbstract \\nFunction approximation is essential to reinforcement learning, but \\nthe standard approach of approximating a value function and deter- \\nmining a policy from it has so far proven theoretically intractable. \\nIn this paper we explore an alternative approach in which the policy \\nis explicitly represented by its own function approximator, indepen- \\ndent of the value function, and is updated according to the gradient \\nof expected reward with respect to the policy parameters. Williams's \\nREINFORCE method and actor-critic methods are examples of this \\napproach. Our main new result is to show that the gradient can \\nbe written in a form suitable for estimation from experience aided \\nby an approximate action-value or advantage function. Using this \\nresult, we prove for the first time that a version of policy iteration \\nwith arbitrary differentiable function approximation is convergent to \\na locally optimal policy. \\nLarge applications of reinforcement learning (RL) require the use of generalizing func- \\ntion approximators such neural networks, decision-trees, or instance-based methods. \\nThe dominant approach for the last decade has been the value-function approach, in \\nwhich all function approximation effort goes into estimating a value function, with \\nthe action-selection policy represented implicitly as the \"greedy\" policy with respect \\nto the estimated values (e.g., as the policy that selects in each state the action with \\nhighest estimated value). The value-function approach has worked well in many appli- \\ncations, but has several limitations. First, it is oriented toward finding deterministic \\npolicies, whereas the optimal policy is often stochastic, selecting different actions with \\nspecific probabilities (e.g., see Singh, Jaakkola, and Jordan, 1994). Second, an arbi- \\ntrarily small change in the estimated value of an action can cause it to be, or not be, \\nselected. Such discontinuous changes have been identified as a key obstacle to estab- \\nlishing convergence assurances for algorithms following the value-function approach \\n(Bertsekas and Tsitsiklis, 1996). For example, Q-learning, Sarsa, and dynamic pro- \\ngramming methods have all been shown unable to converge to any policy for simple \\nMDPs and simple function approximators (Gordon, 1995, 1996; Baird, 1995; Tsit- \\nsiklis and van Roy, 1996; Bertsekas and Tsitsiklis, 1996). This can occur even if the \\nbest approximation is found at each step before changing the policy, and whether the \\nnotion of \"best\" is in the mean-squared-error sense or the slightly different senses of \\nresidual-gradient, temporal-difference, and dynamic-programming methods. \\nIn this paper we explore an alternative approach to function approximation in RL. \\n1058 R. S. Sutton, D. McAllester, S. Singh and Y. Mansour \\nRather than approximating a value function and using that to compute a determinis- \\ntic policy, we approximate a stochastic policy directly using an independent function \\napproximator with its own parameters. For example, the policy might be represented \\nby a neural network whose input is a representation of the state, whose output is \\naction selection probabilities, and whose weights are the policy parameters. Let 0 \\ndenote the vector of policy parameters and p the performance of the corresponding \\npolicy (e.g., the average reward per step). Then, in the policy gradient approach, the \\npolicy parameters are updated approximately proportional to the gradient: \\nOp \\nA  a, (1) \\nwhere a is a positive-definite step size. If the above can be achieved, then 0 can \\nusually be assured to converge to a locally optimal policy in the performance measure \\np. Unlike the value-function approach, here small changes in 0 can cause only small \\nchanges in the policy and in the state-visitation distribution. \\nIn this paper we prove that an unbiased estimate of the gradient (1) can be obtained \\nfrom experience using an approximate value function satisfying certain properties. \\nWilliams's (1988, 1992) REINFORCE algorithm also finds an unbiased estimate of \\nthe gradient, but without the assistance of a learned value function. REINFORCE \\nlearns much more slowly than RL methods using value functions and has received \\nrelatively little attention. Learning a value function and using it to reduce'the variance \\nof the gradient estimate appears to be essential for rapid learning. Jaakkola, Singh \\nand Jordan (1995) proved a result very similar to ours for the special case of function \\napproximation corresponding to tabular POMDPs. Our result strengthens theirs and \\ngeneralizes it to arbitrary differentiable function approximators. Konda and Tsitsiklis \\n(in prep.) independently developed a very simialr result to ours. See also Baxter and \\nBartlett (in prep.) and Marbach and Tsitsiklis (1998). \\nOur result also suggests a way of proving the convergence of a wide variety of algo- \\nrithms based on \"actor-critic\" or policy-iteration architectures (e.g., Barto, Sutton, \\nand Anderson, 1983; Sutton, 1984; Kimura and Kobayashi, 1998). In this paper we \\ntake the first step in this direction by proving for the first time that a version of \\npolicy iteration with general differentiable function approximation is convergent to \\na locally optimal policy. Baird and Moore (1999) obtained a weaker but superfi- \\ncially similar result for their VAPS family of methods. Like policy-gradient methods, \\nVAPS includes separately parameterized policy and value functions updated by gra- \\ndient methods. However, VAPS methods do not climb the gradient of performance \\n(expected long-term reward), but of a measure combining performance and value- \\nfunction accuracy. As a result, VAPS does not converge to a locally optimal policy, \\nexcept in the case that no weight is put upon value-function accuracy, in which case \\nVAPS degenerates to REINFORCE. Similarly, Gordon's (1995) fitted value iteration \\nis also convergent and value-based, but does not find a locally optimal policy. \\nI Policy Gradient Theorem \\nWe consider the standard reinforcement learning framework (see, e.g., Sutton and \\nBarto, 1998), in which a learning agent interacts with a Markov decision process \\n(MDP). The state, action, and reward at each time t 6 {0, 1, 2,...} are denoted st  \\n$, at  .A, and rt   respectively. The environment's dynamics are characterized by \\nstate transition probabilities, Pa s, = Pr {St+l = s' I st = s, at = a}, and expected re- \\nwards 7Z] = E {rt+l I st = s, at = a}, Vs, s   $, a  .A. The agent's decision making \\nprocedure at each time is characterized by a policy, r(s, a, 0) = Pr {at = alst = s, 0}, \\nVs  $, a  jI, where 0  t, for I &lt;&lt; I$1, is a parameter vector. We assume that r \\nis diffentiable with respect to its parameter, i.e., that o exists. We also usually \\nwrite just r(s,a) for r(s,a,O). \\nPolicy Gradient Methods for RL with Function Approximation 1059 \\nWith function approximation, two ways of formulating the agent's objective are use- \\nful. One is the average reward formulation, in which policies are ranked according to \\ntheir long-term expected reward per step, p(r): \\np(r) = lim 1-E{rl + r2 +... + rn [r} = Z d(s) Z r(s,a)7, \\nwhere d  (s) = limt_ Pr (st = s[so, r) is the stationary distribution of states under \\nr, which we assume exists and is independent of so for all policies. In the average \\nreward formulation, the value of a state-action pair given a policy is defined as \\nao=a,r ), Vse$,aeA. \\nt----1 \\nThe second formulation we cover is that in which there is a designated start state \\nso, and we care only about the long-term reward obtained from it. We will give our \\nresults only once, but they will apply to this formulation as well under the definitions \\np(r) = E 7t-lrt SO,r and Qr(s,a) = E 7k-lrt+k st = s, at = a,r . \\nt=l Xk=l \\nwhere 7 6 [0, 1] is a discount rate (7 = I is allowed only in episodic tasks). In this \\nformulation, we define dr(s) as a discounted weighting of states encountered starting \\nat so and then following r: dr(s) = --o 7 tPr (st- S[So,'). \\nOur first result concerns the gradient of the performance metric with respect to the \\npolicy parameter: \\nTheorem I (Policy Gradient). For any MDP, in either the average-reward or \\nstart-state formulations, \\nOp Or( s, a) \\noo = a's) oo (2) \\nProof: See the appendix. \\nThis way of expressing the gradient was first discussed for the average-reward formu- \\nlation by Marbach and Tsitsiklis (1998), based on a related expression in terms of the \\nstate-value function due to Jaakkola, Singh, and Jordan (1995) and Cao and Chen \\n(1997). We extend their results to the start-state formulation and provide simpler \\nand more direct proofs. Williams's (1988, 1992) theory of REINFORCE algorithms \\ncan also be viewed as implying (2). In any event, the key aspect of both expressions \\nfor the gradient is that their are no terms of the form oa._s: the effect of policy \\nchanges on the distribution of states does not appear. This is convenient for approxi- \\nmating the gradient by sampling. For example, if s was sampled from the distribution \\n(8'a)O(s,a) would be an unbiased estimate of \\nobtained by following r, then -]-a oe ' \\n-e Of course, Q (s, a) is also not normally known and must be estimated. One ap- \\n00' \\nproach is to use the actual returns, Rt oo oo \\n-- Ek=i l't+k -- p(w) (or Re = Ek=l vk-iwt+k \\nin the start-state formulation) as an approximation for each Qr(st, at). This leads to \\nWilliams's episodic REINFORCE algorithm, A0t oc o(,,)Re  (the 1 \\ncorrects for the oversampling of actions preferred by r), which is known to follow  \\nin expected value (Williams, 1988, 1992). \\n2 Policy Gradient with Approximation \\nNow consider the case in which Q is approximated by a learned function approxima- \\ntor. If the approximation is sufficiently good, we might hope to use it in place of Q \\n1060 R. S. Sutton, D. Mc,41lester, S. Singh and Y. Mansour \\nin (2) and still point roughly in the direction of the gradient. For example, Jaakkola, \\nSingh, and Jordan (1995) proved that for the special case of function approximation \\narising in a tabular POMDP one could assure positive inner product with the gra- \\ndient, which is sufficient to ensure improvement for moving in that direction. Here \\nwe extend their result to general function approximation and prove equality with the \\ngradient. \\nLet fw: $ x j[ - R be our approximation to Qx, with parameter w. It is natural \\na rtr / s \\nto learn fw by following rr and updating w by a rule such as Awt o&lt;  [  t, at) - \\n 'fw(8\"aO where Or(st, at) is some unbiased \\nf(st,at)]   [O(st,at)- f(st,-tj o , \\nestimator of Qr(st, at), perhaps Pu. When such a process has converged to a local \\noptimum, then \\n d'r(s) E '(s'a)[ Q'r(s'a) - fu,(s,a)] Ofu,(s,a) \\nOw - O. \\n(s) \\nTheorem 2 (Policy Gradient with Function Approximation). If fw satisfies \\n(3) and is compatible with the policy parameterization in the sense that I \\nOfu,(s,a) O'(s,a) I \\nOw O0 r(s,a) ' \\nthen \\nOp &amp;r( s, a) \\n00 = d(s)  S(s,a). \\n(4) \\n(5) \\nProof: Combining (3) and (4) gives \\n&amp;r(s,a) \\noo - = 0 \\n(6) \\nwhich tells us that the error in fu(s, a) is orthogonal to the gradient of the policy \\nparameterization. Because the expression above is zero, we can subtract it from the \\npolicy gradient theorem (2) to yield \\nOp &amp;r( s, a) \\nO0 = y\" dr(s) y\" O0 qr(s'a) - E dr(s) Y O'(s,a) \\n00 [(s,)- w(s,)] \\n&amp;r(s,a) [Q=(s,a) - Q=(s,a) + f,,(s,a)] \\n&amp;r( s, a) \\n= d(s) oo /(s,a). q..V. \\n3 Application to Deriving Algorithms and Advantages \\nGiven a policy parameterization, Theorem 2 can be used to derive an appropriate \\nform for the value-function parameterization. For example, consider a policy that is \\na Gibbs distribution in a linear combination of features: \\nEb eOTq ' \\nVs 6 $, s  A, \\nTsitsiklis (personal communication) points out that f being linear in the features given \\non the righthand side may be the only way to satisfy this condition. \\nPolicy Gradient Methods for RL with Function Approximation 1061 \\nwhere each qbsa is an/-dimensional feature vector characterizing state-action pair s, a. \\nMeeting the compatibility condition (4) requires that \\nOfw(s,a) Or(s,a) 1 y.r(s,b)qbsb, \\n0w = b \\nso that the natural parameterization of fw is \\nIn other words, fw must be linear in the same features as the policy, except normalized \\nto be mean zero for each state. Other algorithms can easily be derived for a variety \\nof nonlinear policy parameterizations, such as multi-layer backpropagation networks. \\nThe careful reader will have noticed that the form given above for f requires \\nthat it have zero mean for each state: -.ar(s,a)fw(s,a) = O, s  $. In this \\nsense it is better to think of fw as an approximation of the advantage function, \\nAr(s,a) = Qr(s,a) - Vr(S) (much as in Baird, 1993), rather than of Qx. Our \\nconvergence requirement (3) is really that fw get the relative value of the ac- \\ntions correct in each state, not the absolute value, nor the variation from state to \\nstate. Our results can be viewed as a justification for the special status of advan- \\ntages as the target for value function approximation in RL. In fact, our (2), (3), \\nand (5), can all be generalized to include an arbitrary function of state added to \\nthe value function or its approximation. For example, (5) can be generalized to \\no_ _ sdr(s) --a O [fw(s,a) + v(s)] ,where v' $ - R is an arbitrary function. \\no0-- \\n(This follows immediately because a -0'a -- 0, s  $.) The choice of v does not \\naffect any of our theorems, but can substantially affect the variance of the gradient \\nestimators. The issues here are entirely analogous to those in the use of reinforce- \\nment baselines in earlier work (e.g., Williams, 1992; Dayan, 1991; Sutton, 1984). In \\npractice, v should presumably be set to the best available approximation of V . Our \\nresults establish that that approximation process can proceed without affecting the \\nexpected evolution of f and r. \\n4 Convergence of Policy Iteration with Function Approximation \\nGiven Theorem 2, we can prove for the first time that a form of policy iteration with \\nfunction approximation is convergent to a locally optimal policy. \\nTheorem 3 (Policy Iteration with Function Approximation). Let rr \\nand fw be any differentiable function approximators for the policy and value \\nfunction respectively that satisfy the compatibility condition (4) and for which \\nmax0,,.,i,j I ooaoj I &lt; B &lt; oo. Let {rk}=o be any step-size sequence such that \\nlimkm ak = 0 and }-'--k ak = oo. Then, for any MDP with bounded rewards, the \\nsequence {p(rrk)}k=o , defined by any 0o, rk = r(.,., Ok), and \\nwk = w such that y.d(s) y.rk(s,a)[Q'*(s,a) - f(s,a)]Of- 'a) = 0 \\nOk+ 1 ---- 0 k q-Otk ydW($) y o7rk($'a) \\nOO \\nconverges such that limk_m o__ _- 0. \\nProof: Our Theorem 2 assures that the 0k update is in the direction of the gradient. \\n(8'a) and on the MDP's rewards together assure us that 0____ \\nThe bounds on oo, ooj ao,ao \\n1062 R. S. Sutton, D. McAllester, S. Singh and Y. Mansour \\nis also bounded. These, together with the step-size requirements, are the necessary \\nconditions to apply Proposition 3.5 from page 96 of Bertsekas and Tsitsildis (1996), \\nwhich assures convergence to a local optimum. Q.E.D. \\nAcknowledgements \\nThe authors wish to thank Martha Steenstrup and Doina Precup for comments, and Michael \\nKearns for insights into the notion of optimal policy under function approximation. \\nReferences \\nBaird, L. C. (1993). Advantage Updating. Wright Lab. Technical Report WL-TR-93-1146. \\nBaird, L. C. (1995). Residual algorithms: Reinforcement learning with function approxima- \\ntion. Proc. of the Twelfth Int. Conf. on Machine Learning, pp. 30-37. Morgan Kaufmann. \\nBaird, L. C., Moore, A. W. (1999). Gradient descent for general reinforcement learning. \\nNIPS 11. MIT Press. \\nBarto, A. G., Sutton, R. S., Anderson, C. W. (1983). Neuronlike elements that can solve \\ndifficult learning control problems. IEEE Trans. on Systems, Man, and Cybernetics 3:835. \\nBaxter, J., Bartlett, P. (in prep.) Direct gradient-based reinforcement learning: I. Gradient \\nestimation algorithms. \\nBertsekas, D. P., Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific. \\nCao, X.-R., Chen, H.-F. (1997). Perturbation realization, potentials, and sensitivity analysis \\nof Markov Processes, IEEE Trans. on Automatic Control J(10):1382-1393. \\nDayan, P. (1991). Reinforcement comparison. In D. S. Touretzky, J. L. Elman, T. J. Se- \\njnowski, and G. E. Hinton (eds.), Connectionist Models: Proceedings of the 1990 Summer \\nSchool, pp. 45-51. Morgan Kaufmann. \\nGordon, G. J. (1995). Stable function approximation in dynamic programming. Proceedings \\nof the Twelfth Int. Conf. on Machine Learning, pp. 261-268. Morgan Kaufmann. \\nGordon, G. J. (1996). Chattering in SARSA(A). CMU Learning Lab Technical Report. \\nJaakkola, T., Singh, S. P., Jordan, M. I. (1995) Reinforcement learning algorithms for par- \\ntially observable Markov decision problems, NIPS 7, pp. 345-352. Morgan Kaufman. \\nKimura, H., Kobayashi, S. (1998). An analysis of actor/critic algorithms using eligibility \\ntraces: Reinforcement learning with imperfect value functions. Proc. ICML-98, pp. 278-286. \\nKonda, V. R., Tsitsiklis, J. N. (in prep.) Actor-critic algorithms. \\nMarbach, P., Tsitsiklis, J. N. (1998) Simulation-based optimization of Markov reward pro- \\ncesses, technical report LIDS-P-2411, Massachusetts Institute of Technology. \\nSingh, S. P., Jaakkola, T., Jordan, M. I. (1994). Learning without state-estimation in \\npartially observable Markovian decision problems. Proc. ICML-9J, pp. 284-292. \\nSutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. Ph.D. thesis, \\nUniversity of Massachusetts, Amherst. \\nSutton, R. S., Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press. \\nTsitsiklis, J. N. Van Roy, B. (1996). Feature-based methods for large scale dynamic pro- \\ngramming. Machine Learning 22:59-94. \\nWilliams, R. J. (1988). Toward a theory of reinforcement-learning connectionist systems. \\nTechnical Report NU-CCS-88-3, Northeastern University, College of Computer Science. \\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist \\nreinforcement learning. Machine Learning 8:229-256. \\nAppendix: Proof of Theorem 1 \\nWe prove the theorem first for the average-reward formulation and then for the start- \\nstate formulation. \\nOV' (s) 0 \\nO0 -- OOZW(s,a)Q'(s,a) Vse$ \\na \\n: 'Ols, a'ls, \\nO0 \\nPolicy Gradient Methods for RL with Function Approximation 1063 \\nTherefore, \\n0-7 = [ oo q'(,,a) + (,,a)  \\nS t \\nSumming both sides over the stationary distribution d , \\nbut since d  is stationary, \\nOp \\n$ \\nO(s,a)  ,, ou(s ') \\n= d(s) oo q(s,a) + d (s   \\n$ a S t \\n$ \\n0__ 0(s, \\no0 = d(s) o0 )(s,). q.s.. \\nFor the sta-state formulation: \\nOVa(s) df 0 \\noo = oo(s,)(s,) \\n=  'O(s,)(s,) + (s,) \\no0 (s,) + (s,) ,p2,, U(s') (7) \\n O(,a) \\n= (3,,) oo \\nx k=0 a \\naer several steps of unrolling (7), where Pr(s  x, k, ) is the probability of going \\nkom state s to state x in k steps under policy . It is then immiate that \\nt=l \\n O(s,a) -(s,a) \\ns k=O a \\n0(s, a) \\n= :(s) oo q(s,a). \\nQ.E.D. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>1738</th>\n","      <td>1738</td>\n","      <td>5</td>\n","      <td>48.52</td>\n","      <td>cell, response, activity, stimulus, neuron, pattern, cortical, layer, receptive_field, cortex, connection, orientation, unit, visual, spatial, contrast, simulation, mechanism, population, synaptic</td>\n","      <td>Monte Carlo POMDPs \\nSebastian Thrun \\nSchool of Computer Science \\nCarnegie Mellon University \\nPittsburgh, PA 15213 \\nAbstract \\nWe present a Monte Carlo algorithm for learning to act in partially observable \\nMarkov decision processes (POMDPs) with real-valued state and action spaces. \\nOur approach uses importance sampling for representing beliefs, and Monte Carlo \\napproximation for belief propagation. A reinforcement learning algorithm, value \\niteration, is employed to learn value functions over belief states. Finally, a sample- \\nbased version of nearest neighbor is used to generalize across states. Initial \\nempirical results suggest that our approach works well in practical applications. \\n1 Introduction \\nPOMDPs address the problem of acting optimally in partially observable dynamic environ- \\nment [6]. In POMDPs, a learner interacts with a stochastic environment whose state is only \\npartially observable. Actions change the state of the environment and lead to numerical \\npenalties/rewards, which may be observed with an unknown temporal delay. The learner's \\ngoal is to devise a policy for action selection that maximizes the reward. Obviously, the \\nPOMDP framework embraces a large range of practical problems. \\nPast work has predominately studied POMDPs in discrete worlds [ 1 ]. Discrete worlds have \\nthe advantage that distributions over states (so-called \"belief states\") can be represented \\nexactly, using one parameter per state. The optimal value function (for finite planning \\nhorizon) has been shown to be convex and piecewise linear [10, 14], which makes it \\npossible to derive exact solutions for discrete POMDPs. \\nHere we are interested in POMDPs with continuous state and action spaces, paying tribute \\nto the fact that a large number of real-world problems are continuous in nature. In general, \\nsuch POMDPs are not solvable exactly, and little is known about special cases that can be \\nsolved. This paper proposes an approximate approach, the MC-POMDP algorithm, which \\ncan accommodate real-valued spaces and models. The central idea is to use Monte Carlo \\nsampling for belief representation and propagation. Reinforcement learning in belief space \\nis employed to learn value functions, using a sample-based version of nearest neighbor \\nfor generalization. Empirical results illustrate that our approach finds to close-to-optimal \\nsolutions efficiently. \\n2 Monte Carlo POMDPs \\n2.1 Preliminaries \\nPOMDPs address the problem of selection actions in stationary, partially observable, con- \\ntrollable Markov chains. To establish the basic vocabulary, let us define: \\n State. At any point in time, the world is in a specific state, denoted by :c. \\nMonte Carlo POMDPs 1065 \\n Action. The agent can execute actions, denoted a. \\n Observation. Through its sensors, the agent can observe a (noisy) projection of the \\nworld's state. We use o to denote observations. \\n Reward. Additionally, the agent receives rewards/penalties, denoted R  . To \\nsimplify the notation, we assume that the reward is part of the observation. More \\nspecifically, we will use R(o) to denote the function that \"extracts\" the reward from \\nthe observation. \\nThroughout this paper, we use the subscript t to refer to a specific point in time (e.g., st \\nrefers to the state at time t). \\nPOMDPs are characterized by three probability distributions: \\n1. The initial distribution, '(x) :-- Pr(zo), specifies the initial distribution of states at \\ntime t -- 0. \\n2. The next state distribution,/_t(z I a,) := Pt(oct - vc [ at_l '- a, vct_l -- \\ndescribes the likelihood that action a, when executed at state , leads to state . \\n3. The perceptual distribution, v(o [ ) := Pt(or = o[ct - ), describes the likeli- \\nhood of observing o when the world is in state \\nA history is a sequence of states and observations. For simplicity, we assume that actions \\nand observations are alternated. We use dt to denote the history leading up to time t: \\ndt :- {ot, at-l,Ot-l, at-2,...,ao, oo} (1) \\nThe fundamental problem in POMDPs is to devise a policy for action selection that maxi- \\nmizes reward. A policy, denoted \\ncr  d &gt; a (2) \\nis a mapping from histories to actions. Assuming that actions are chosen by a policy or, \\neach policy induces an expected cumulative (and possibly discounted by a discount factor \\n? _&lt; 1) reward, defined as \\n: s (3) \\nHere E[ ] denotes the mathematical expectation. The POMDP problem is, thus, to find a \\npolicy or* that maximizes J\", i.e., \\nor* -- argmax J\" (4) \\no' \\n2.2 Belief States \\nTo avoid the difficulty of learning a function with unbounded input (the history can be \\narbitrarily long), it is common practice to map histories into belief states, and learn a \\nmapping from belief states to actions instead [10]. \\nFormally, a belief state (denoted 0) is a probability distribution over states conditioned on \\npast actions and observations: \\nOt- Pr(xt I dt) - Pr(xt I ot,at_l,...,oo) (5) \\nBelief are computed incrementally, using knowledge of the POMDP's defining distributions \\n', p, and ,. Initially \\n00 : r \\nFor t &gt; O, we obtain \\nOt+l = Pr(xt+l l ot+l,at,...,oo) \\n= o Pr(ot+l \\n= o Pr(ot+l \\n= o Pr(Ot+l \\n(6) \\n(7) \\n(8) \\no0) -r(xt+l I at,..., o0) \\nzt+) f Pr(zt+l l at,...,oo, zt) Pr(zt l at,...,oo) dzt (9) \\nct+) j Pr(oet+l l at,ct) Ot dzt (10) \\n1066 $. Thrun \\n0.2 \\nlllllllllNIllllllllllllllllllllll IIIIlll II  II I I \\n2 4 6 8 10 12 \\n2 4 $ 8 10 12 \\nFigure 1: Sampling: (a) Likelihood-weighted sampling and (b) importance sampling. At the bottom \\nof each graph, samples are shown that approximate the function f shown at the top. The height of \\nthe samples illustrates their importance factors. \\nHere a denotes a constant normalizer. The derivations of (8) and (10) follow directly from \\nthe fact that the environment is a stationary Markov chain, for which future states and \\nobservations are conditionally independent from past ones given knowledge of the state. \\nEquation (9) is obtained using the theorem of total probability. \\nArmed with the notion of belief states, the policy is now a mapping from belief states \\n(instead of histories) to actions: \\no':0 &gt;a (11) \\nThe legitimacy of conditioning a on 0, instead of d, follows directly from the fact that the \\nenvironment is Markov, which implies that 0 is all one needs to know about the past to \\nmake optimal decisions. \\n2.3 Sample Representations \\nThus far, we intentionally left open how belief states 0 are represented. In prior work, state \\nspaces have been discrete. In discrete worlds, beliefs can be represented by a collection \\nof probabilities (one for each state), hence, beliefs can be represented exactly. Here were \\nare interested in real-valued state spaces. In general, probability distributions over real- \\nvalued spaces possess infinitely many dimensions, hence cannot be represented on a digital \\ncomputer. \\nThe key idea is to represent belief states by sets of (weighted) samples drawn from the \\nbelief distribution. Figure 1 illustrates two popular schemes for sample-based approxima- \\ntion: likelihood-weighted sampling, in which samples (shown at the bottom of Figure la) \\nare drawn directly from the target distribution (labeled f in Figure la), and importance \\nsampling, where samples are drawn from some other distribution, such as the curve labeled \\n# in Figure lb. In the latter case, samples z are annotated by a numerical importance factor \\nf(z) (12) \\np(x) = g(x) \\nto account for the difference in the sampling distribution, g, and the target distribution f \\n(the height of the bars in Figure 1 b illustrates the importance factors). Importance sampling \\nrequires that f &gt; 0 - g &gt; 0, which will be the case throughout this paper. Obviously, both \\nsampling methods generate approximations only. Under mild assumptions, they converge \\n with N denoting the sample set size [16]. \\nto the target distribution at a rate of , \\nIn the context of POMDPs, the use of sample-based representations gives rise to the \\nfollowing algorithm for approximate belief propagation (c.f., Equation (10)): \\nAlgorithm particle_filter(Or, at, Or+  ): \\nOt+ = 0 \\ndo N times: \\ndraw random state zt from Ot \\nMonte Carlo POMDPs 1067 \\nsample xt+, according to(xt+, [ at, xt) \\nset importance factor p( z t + l ) -- v ( ot + , l z t + , ) \\nadd (zt+,,p(zt+,)) toot+, \\nnormalize all p(zt+,)  Or+, so that -p(zt+,) = 1 \\nreturn Ot + , \\nThis algorithm converges to (10) for arbitrary models /, ,, and r and arbitrary belief \\ndistributions 0, defined over discrete, continuous, or mixed continuous-discrete state and \\naction spaces. It has, with minor modifications, been proposed under names like particle \\nfilters [13], condensation algorithm [5], survival of the fittest [8], and, in the context of \\nrobotics, Monte Carlo localization [4]. \\n2.4 Projection \\nIn conventional planning, the result of applying an action at at a state zt is a distribution \\nPr(zt+,, Rt+, I at, zt) over states zt+, and rewards Rt+, at the next time step. This \\noperation is called projection. In POMDPs, the state zt is unknown. Instead, one has to \\ncompute the result of applying action at to a belief state Or. The result is a distribution \\nPt(Or+,, Rt+, I at, Or) over belief states Or+, and rewards Rt+,. Since belief states them- \\nselves are distributions, the result of a projection in POMDPs is, technically, a distribution \\nover distributions. \\nThe projection algorithm is derived as follows. Using total probability, we obtain: \\nPr(0t+,,/t+, l at,Or) - Pr(Ot+,,Rt+, l at,dr) (13) \\n,Pr(Ot+,,tt+, I ot+,,at,dt) Pt(or+, l at,dr) dot+, (14) \\n(,) (**) \\nThe term (,) has already been derived in the previous section (c.f., Equation (10)), under \\nthe observation that the reward/t+, is trivially computed from the observation \\nThe second term, (**), is obtained by integrating out the unknown variables, zt+, and zt, \\nand by once again exploiting the Markov property: \\nPr(ot+t l at,dt) -' / Pr(ot+, I Xt+l) Pr(xt+, [ at,dt) dzt+, (15) \\n= / Pt(or+, I zt+,) / Pr(zt+, I zt,at) Pr(zt Idt)dzt dzt+(i16) \\nThis leads to the following approximate algorithm for projecting belief state. In the spirit \\nof this paper, our approach uses Monte Carlo integration instead of exact integration. It \\nrepresents distributions (and distributions over distributions) by samples drawn from such \\ndistributions. \\nAlgorithm partide_projecfion(0t, at): \\nOt = 0 \\ndo N times: \\ndraw random state zt from Ot \\nsample a next state zt+ 1 according to p(zt+, I at, \\nsample an observation or+, according to '(Ot+l I \\ncompute Ot + , =particle_filter(Or, at, Ot + l) \\nadd {Ot+,,(ot+,)&gt; toot \\nreturn Ot \\nThe result of this algorithm, Or, is a sample set of belief states Or+, and rewards \\ndrawn from the desired distribution Pv(Ot+l, Rt+l [ Or, at). As N - c, Ot converges \\nwith probability 1 to the true posterior [16]. \\n1068 S. Thrun \\n2.5 Learning Value Functions \\nFollowing the rich literature on reinforcement learning [7, 15], our approach solves the \\nPOMDP problem by value iteration in belief space. More specifically, our approach \\nrecursively learns a value function Q over belief states and action, by backing up values \\nfrom subsequent belief states: \\nQ(Ot,at) &lt; E [/l:(ot_l_l) q- 7maaxQ(0t+l, 5)] (18) \\nLeaving open (for a moment) how Q is represented, it is easy to be seen how the algorithm \\nparticle_projection can be applied to compute a Monte Carlo approximation of the right \\nhand-side expression: Given a belief state Ot and an action at, particle_projection computes \\na sample of ](ot+l ) and Or+l, from which the expected value on the right hand side of (18) \\ncan be approximated. \\nIt has been shown [2] that if both sides of (18) are equal, the greedy policy \\no'Q(0) = argmaxQ(0, o,) (19) \\na \\nis optimal, i.e., or* -- crq?. Furthermore, it has been shown (for the discrete case!) that \\nrepetitive application of (18) leads to an optimal value function and, thus, to the optimal \\npolicy [17, 3]. \\nOur approach essentially performs model-based reinforcement learning in belief space \\nusing approximate sample-based representations. This makes it possible to apply a rich \\nbag of tricks found in the literature on MDPs. In our experiments below, we use on- \\nline reinforcement learning with counter-based exploration and experience replay [9] to \\ndetermine the order in which belief states are updated. \\n2.6 Nearest Neighbor \\nWe now return to the issue how to represent Q. Since we are operating in real-valued \\nspaces, some sort of function approximation method is called for. However, recall that \\nQ accepts a probability distribution (a sample set) as an input. This makes most existing \\nfunction approximators (e.g., neural networks) inapplicable. \\nIn our current implementation, nearest neighbor [11] is applied to represent Q. More \\nspecifically, our algorithm maintains a set of sample sets 0 (belief states) annotated by an \\naction a and a Q-value Q(O, a). When a new belief state 0 r is encountered, its Q-value is \\nobtained by finding the k nearest neighbors in the database, and linearly averaging their \\nQ-values. If there aren't sufficiently many neighbors (within a pre-specified maximum \\ndistance), 0' is added to the database; hence, the database grows over time. \\nOur approach uses KL divergence (relative entropy) as a distance function . Technically, \\nthe KL-divergence between two continuous distributions is well-defined. When applied \\nto sample sets, however, it cannot be computed. Hence, when evaluating the distance be- \\ntween two different sample sets, our approach maps them into continuous-valued densities \\nusing Gaussian kernels, and uses Monte Carlo sampling to approximate the KL divergence \\nbetween them. This algorithm is fairly generic an extension of nearest neighbors to func- \\ntion approximation in density space, where densities are represented by samples. Space \\nlimitations preclude us from providing further detail (see [11, 12]). \\n3 Experimental Results \\nPreliminary results have been obtained in a world shown in two domains, one synthetic and \\none using a simulator of a RWI B21 robot. \\nIn the synthetic environment (Figure 2a), the agents starts at the lower left corner. Its \\nobjective is to reach \"heaven\" which is either at the upper left corner or the lower right \\nStdctly speaking, KL divergence is not a distance metric, but this is ignored here. \\nMonte Carlo POMDPs 1069 \\n(a) (b![ \\n:I \\nFigure 2: (a) The environment, schematically. (b) Average performance (reward) as a function of \\ntraining episodes. The black graph corresponds to the smaller environment (25 steps min), the grey \\ngraph to the larger environment (50 steps min). (c) Same results, plotted as a function of number of \\nbackups (in thousands). \\ncomer. The opposite location is \"hell.\" The agent does not know the location of heaven, \\nbut it can ask a \"priest\" who is located in the upper right comer. Thus, an optimal solution \\nrequires the agent to go first to the priest, and then head to heaven. The state space contains \\na real-valued (coordinates of the agent) and discrete (location of heaven) component. Both \\nare unobservable: In addition to not knowing the location of heaven, the agent also cannot \\nsense its (real-valued) coordinates. 5% random motion noise is injected at each move. \\nWhen an agent hits a boundary, it is penalized, but it is also told which boundary it hit \\n(which makes it possible to infer its coordinates along one axis). However, notice that the \\ninitial coordinates of the agent are known. \\nThe optimal solution takes approximately 25 steps; thus, a successful POMDP planner must \\nbe capable of looking 25 steps ahead. We will use the term \"successful policy\" to refer \\nto a policy that always leads to heaven, even if the path is suboptimal. For a policy to be \\nsuccessful, the agent must have learned to first move to the priest (information gathering), \\nand then proceed to the right target location. \\nFigures 2b&amp;c show performance results, averaged over 13 experiments. The solid (black) \\ncurve in both diagrams plots the average cumulative reward J as a function of the number \\nof training episodes (Figure 2b), and as a function of the number of backups (Figure 2c). \\nA successful policy was consistently found after 17 episodes (or 6,150 backups), in all \\n13 experiments. In our current implementation, 6,150 backups require approximately 29 \\nminutes on a Pentium PC. In some experiments, a successful policy was identified in 6 \\nepisodes (less than 1,500 backups or 7 minutes). After a successful policy is found, further \\nlearning gradually optimizes the path. To investigate scaling, we doubled the size of the \\nenvironment (quadrupling the size of the state space), making the optimal solution 50 steps \\nlong. The results are depicted by the gray curves in Figures 2b&amp;c. Here a successful \\npolicy is consistently found after 33 episodes (10,250 backups, 58 minutes). In some runs, \\na successful policy is identified after only 14 episodes. \\nWe also applied MC-POMDPs to a robotic locate-and-retrieve task. Here a robot (Figure 3a) \\nis to find and grasp an object somewhere in its vicinity (at floor or table height). The robot's \\ntask is to grasp the object using its gripper. It is rewarded for successfully grasping the \\nobject, and penalized for unsuccessful grasps or for moving too far away from the object. \\nThe state space is continuous in a: and y coordinates, and discrete in the object's height. \\nThe robot uses a mono-camera system for object detection; hence, viewing the object from \\na single location is insufficient for its 3D localization. Moreover, initially the object might \\nnot be in sight of the robot's camera, so that the robot must look around first. In our \\nsimulation, we assume 30% general detection error (false-positive and false-negative), with \\nadditional Gaussian noise if the object is detected correctly. The robot's actions include \\ntums (by a variable angle), translations (by a variable distance), and grasps (at one of two \\nlegal heights). Robot control is erroneous with a variance of 20% (in :c-y-space) and 5% (in \\nrotational space). Typical belief states range from uniformly distributed sample sets (initial \\nbelief) to samples narrowly focused on a specific a:-t-z location. \\n1070 S. Thrun \\n(a) \\nFigure 3: Find and fetch task: \\n(c) \\n% success \\niteration \\n(a) The mobile robot with gripper and camera, holding the target \\nobject (experiments are carded out in simulation!), (b) three successful runs (trajectory projected into \\n2D), and (c) success rate as a function of number of planning steps. \\nFigure 3c shows the rate of successful grasps as a function of iterations (actions). While \\ninitially, the robot fails to grasp the object, after approximately 4,000 iterations its perfor- \\nmance surpasses 80%. Here the planning time is in the order of 2 hours. However, the robot \\nfails to reach 100%. This is in part because certain initial configurations make it impossible \\nto succeed (e.g., when the object is too close to the maximum allowed distance), in part \\nbecause the robot occasionally misses the object by a few centimeters. Figure 3b depicts \\nthree successful example trajectories. In all three, the robot initially searches the object, \\nthen moves towards it and grasps it successfully. \\n4 Discussion \\nWe have presented a Monte Carlo approach for learning how to act in partially observable \\nMarkov decision processes (POMDPs). Our approach represents all belief distributions \\nusing samples drawn from these distributions. Reinforcement learning in belief space is \\napplied to learn optimal policies, using a sample-based version of nearest neighbor for \\ngeneralization. Backups are performed using Monte Carlo sampling. Initial experimental \\nresults demonstrate that our approach is applicable to real-valued domains, and that it yields \\ngood performance results in environments that are--by POMDP standards--relatively large. \\nReferences \\n[1] AAAI Fall symposium on POMDPs. 1998. See http://www.cs.duke.edu/mlittman/talks/ \\npomdp-symposium. html \\n[2] R.E. Bellman. Dynamic Programming. Princeton University Press, 1957. \\n[3] P. Dayan and T. J. Sejnowski. TD(,X) converges with probability 1. 1993. \\n[4] D. Fox, W. Burgard, E Dellaert, and S. Thrun. Monte carlo localization: Efficient position estimation for mobile robots. \\nAAAI-99. \\n[5] M. Isard and A. Bake. Cndensatin: cnditina density prpagatinfr visua tracking. nternatinalJurnal f Crnputer \\nVision, 1998. \\n[6] L.P. Kaelbling, M.L. Littman, and A.R. Cassandra. Planning and acting in partially observable stochastic domains. Submitted \\nfor publication, 1997. \\n[7] L.P. Kaelbling, M.L. Littman, and A.W. Moore. Reinforcement learning: A survey. JAIR, 4, 1996. \\n[8] K. Kanazawa, D. Koller, and S.J. Russell. Stochastic simulation algorithms for dynamic probabilistic networks. UAI-95. \\n[9] L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8, \\n1992. \\n[10] M.L. Littman, A.R. Cassandra, and L.P. Kaelbling. Learning policies for partially observable environments: Scaling up. \\nICML-95. \\n[11] A.W. Moore, C.G. Atkeson, and S.A. Schaal. Locally weighted learning for control. AIReview, 11, 1997. \\n[12] D. Ormoneit and S. Sen. Kernel-based reinforcementlearning. TR 1999-8, Statistics, Stanford University, 1999. \\n[ 13] M. Pitt and N. Shephard. Filtering via simulation: auxiliary particle filter. Journal of the American Statistical Association, \\n1999. \\n[14] E. Sondik. The Optimal Control of Partially Observable Markov Processes. PhD thesis, Stanford, 1971. \\n[15] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998. \\n[16] M.A. Tanner. Tools for Statistical Inference. Springer Verlag, 1993. \\n[17] C.J.C.H. Watkins. Learning from Delayed Rewards. PhD thesis, King's College, Cambridge, 1989. \\n</td>\n","    </tr>\n","    <tr>\n","      <th>1739</th>\n","      <td>1739</td>\n","      <td>10</td>\n","      <td>42.87</td>\n","      <td>map, region, subject, location, effect, change, study, condition, light, experiment, et_al, brain, pair, normal, correlation, trial, eeg, site, left, theory</td>\n","      <td>Gaussian Fields for Approximate Inference \\nin Layered Sigmoid Belief Networks \\nDavid Barber* \\nStichting Neurale Netwerken \\nMedical Physics and Biophysics \\nNijmegen University, The Netherlands \\nbarberdaston. ac. uk \\nPeter Sollich \\nDepartment of Mathematics \\nKing's College, University of London \\nLondon WC2R 2LS, U.K. \\npeter. sollichkcl. ac. uk \\nAbstract \\nLayered Sigmoid Belief Networks are directed graphical models \\nin which the local conditional probabilities are parameterised by \\nweighted sums of parental states. Learning and inference in such \\nnetworks are generally intractable, and approximations need to be \\nconsidered. Progress in learning these networks has been made by \\nusing variational procedures. We demonstrate, however, that vari- \\national procedures can be inappropriate for the equally important \\nissue of inference - that is, calculating marginMs of the network. \\nWe introduce an alternative procedure, based on assuming that the \\nweighted input to a node is approximately Gaussian distributed. \\nOur approach goes beyond previous Gaussian field assumptions in \\nthat we take into account correlations between parents of nodes. \\nThis procedure is specialized for calculating marginals and is sig- \\nnificantly faster and simpler than the variational procedure. \\nI Introduction \\nLayered Sigmoid Belief Networks [1] are directed graphical models [2] in which \\nthe local conditional probabilities are parameterised by weighted sums of parental \\nstates, see fig(l). This is a graphical representation of a distribution over a set of \\nbinary variables si 6 {0, 1}. Typically, one supposes that the states of the nodes \\nat the bottom of the network are generated by states in previous layers. Whilst, in \\nprinciple, there is no restriction on the number of nodes in any layer, typically, one \\nconsiders structures similar to the \"fan out\" in fig(l) in which higher level layers \\nprovide an \"explanation\" for patterns generated in lower layers. Such graphical \\nmodels are attractive since they correspond to layers of information processors, of \\npotentially increasing complexity. Unfortunately, learning and inference in such net- \\nworks is generally intractable, and approximations need to be considered. Progress \\nin learning has been made by using variational procedures [3, 4, 5]. However, an- \\nother crucial aspect remains inference [2]. That is, given some evidence (or none), \\ncalculate the marginal of a variable, conditional on this evidence. This assumes \\nthat we have found a suitable network from some learning procedure, and now wish \\n*Present Address: NCRG, Aston University, Birmingham B4 7ET, U.K. \\n394 D. Barber and P. Sollich \\nto query this network. Whilst the variational procedure is attractive for learning, \\nsince it generally provides a bound on the likelihood of the visible units, we demon- \\nstrate that it may not always be equally appropriate for the inference problem. \\nA directed graphical model defines a distribution over \\na set of variables s = (sx ...sn) that factorises into \\nthe local conditional distributions, \\np(sx...s,) = l'Ip(silri) (1) \\ni:1 \\nwhere ri denotes the parent nodes of node i. In a \\nlayered network, these are the nodes in the proceed- \\ning layer that feed into node i. In a sigmoid belief \\nnetwork the local probabilities are defined as \\np(si--ll7ri)--r (Ewijsj-Oi)-o'(hi) \\nJ \\nFigure 1: A Layered Sig- \\nmoid Belief Network \\n(2) \\nwhere the \"field\" at node/is defined as hi = Y'.j w,jsj +Oi and (r(h) = 1/(1 +e-n). \\nwij is the strength of the connection between node i and its parent node j; if j is \\nnot a parent of i we set wlj = O. Oi is a bias term that gives a parent-independent \\nbias to the state of node i. \\nWe are interested in inference - in particular, calculating marginals of the network \\nfor cases with and without evidential nodes. In section (2) we describe how to \\napproximate the quantities p(si -- 1) and discuss in section (2.1) why our method \\ncan improve on the standard variational mean field theory. Conditional marginals, \\nsuch as p(si = 11s j = 1, sk = 0) are considered in section (3). \\n2 Gaussian Field Distributions \\nUnder the 0/1 coding for the variables si, the mean of a variable, mi is given by the \\nprobability that it is in state 1. Using the fact from (2) that the local conditional \\ndistribution of node i is dependent on its parents only through its field hi, we have \\nrni =p(si: 1)= f p, = l[hi)p(hi)dhi _= (o'(hi))p(h,) (3) \\nwhere we use the notation ((.))p to denote an average with respect to the distri- \\nbution p. If there are many parents of node i, a reasonable assumption is that the \\ndistribution of the field hi will be Gaussian, p(hi) -, N (lui, efT). Under this Gaus- \\nsian Field (GF) assumption, we need to work out the mean and variance, which are \\ngiven by \\n= = + o, = + O, (4) \\nJ J \\n(5) \\nj,k \\nwhere Rjk = (AsjAs). We use the notation A (.) -- (.) - ((.)). \\nThe diagonal terms of the node covariance matrix are Rii - mi (1 - mi). In contrast \\nto previous studies, we include off diagonal terms in the calculation of R [4]. From \\nGaussian Fields for Approximate Inference 395 \\n(5) we only need to find correlations between parents i and j of a node. These are \\neasy to calculate in the layered networks that we are considering, because neither i \\nnor j is a descendant of the other: \\nRid --p(si -- 1, sj -- 1) - mimj \\n= fp(, = = 11hj)p(hi,hj)dh-mimj \\n= {(r (hi) (r (hj))p(n,,ni) - mimj \\nAssuming that the joint distribution p(hi, hj) is Gaussian, \\nand covariance, given by \\nl \\n(6) \\n(7) \\n(8) \\nwe again need its mean \\nE,j = {AhiAhj) = Z w,kwj, {AskAs,) = Z wiwjtR, (10) \\nkl kl \\nUnder this scheme, we have a closed set of equations, (4,5,8,10) for the means \\nmi and covariance matrix tij which can be solved by forward propagation of the \\nequations. That is, we start from nodes without parents, and then consider the \\nnext layer of nodes, repeating the procedure until a full sweep through the network \\nhas been completed. The one and two dimensional field averages, equations (3) \\nand (8), are computed using Gaussian Quadrature. This results in an extremely \\nfast procedure for approximating the marginals mi, requiring only a single sweep \\nthrough the network. \\nOur approach is related to that of [6] by the common motivating assumption that \\neach node has a large number of parents. This is used in [6] to obtain actual \\nbounds on quantities of interest such as joint marginals. Our approach does not \\ngive bounds. Its advantage, however, is that it allows fluctuations in the fields hi, \\nwhich are effectively excluded in [6] by the assumed scaling of the weights wij with \\nthe number of parents per node. \\n2.1 Relation to Variational Mean Field Theory \\nIn the variational approach, one fits a tractable approximating distribution Q to \\nthe SBN. Taking Q factorised, Q(s) - 1-Ii rn'(1 - rni) x-' we have the bound \\nlnp ( sx . . . sn ) _&gt; E {-mi lnmi - (1 - mi ) In (1 - mi ) } \\ni \\n+Z{Zmiwijmj+Oimi_(ln(l+eh'))Q} (11) \\ni j \\nThe final term in (11) causes some difficulty even in the case in which Q is a fac- \\ntorised model. Formally, this is because this term does not have the same graphical \\nstructure as the tractable model Q. One way around around this difficulty is to em- \\nploy a further bound, with associated variational parameters [7]. Another approach \\nis to make the Gaussian assumption for the field hi as in section (2). Because Q is \\nfactorised, corresponding to a diagonal correlation matrix R, this gives [4] \\n(ln (1 + e n') )Q 0 (ln (1 + e'))N(,,a) (12) \\n396 D. Barber and P. Sollich \\nwhere/ui - -.j wijmj q- Oi and eri 2 -- -]j wi2jmj(1 - mj). Note that this is a one \\ndimensional integral of a smooth function. In contrast to [4] we therefore evaluate \\nthis quantity using Gaussian Quadrature. This has the advantage that no extra \\nvariational parameters need to be introduced. Technically, the assumption of a \\nGaussian field distribution means that (11) is no longer a bound. Nevertheless, in \\npractice it is found that this has little effect on the quality of the resulting solution. \\nIn our implementation of the variational approach, we find the optimal parameters \\nmi by maximising the above equation for each component mi separately, cycling \\nthrough the nodes until the parameters mi do not change by more than 10 -. \\nThis is repeated 5 times, and the solution with the highest bound score is chosen. \\nNote that these equations cannot be solved by forward propagation alone since the \\nfinal term contains contributions from all the nodes in the network. This is in \\ncontrast to the GF approach of section (2). Finding appropriate parameters mi by \\nthe variational approach is therefore rather slower than using the GF method. \\nIn arriving at the above equations, we have made two assumptions. The first is \\nthat the intractable distribution is well approximated by a factorised model. The \\nsecond is that the field distribution is Gaussian. The first step is necessary in \\norder to obtain a bound on the likelihood of the model (although this is slightly \\ncompromised by the Gaussian fielc[ assumption). In the GF approach we dispense \\nwith this assumption of an effectively factorised network (partially because if we \\nare only interested in inference, a bound on the model likelihood is less relevant). \\nThe GF method may therefore prove useful for a broader class of networks than the \\nvariational approach. \\n2.2 Results for unconditional marginals \\nWe compared three procedures for estimating the conditional values p(si = 1) for \\nall the nodes in the network, namely the variational theory, as described in section \\n(2.1), the diagonal Gaussian field theory, and the non-diagonal Gaussian field theory \\nwhich includes correlation effects between parents. Results for small weight values \\nwij are shown in fig(2). In this case, all three methods perform reasonably well, \\nalthough there is a significant improvement in using the GF methods over the \\nvariational procedure; parental correlations are not important (compare figs(2b) \\nand (2c)). In fig(3) the weights and biases are chosen such that the exact mean \\nvariables mi are roughly 0.5 with non-trivial correlation effects between parents. \\nNote that the variational mean field theory now provides a poor solution, whereas \\nthe GF methods are relatively accurate. The effect of using the non-diagonal R \\nterms is beneficial, although not dramatically so. \\n3 Calculating Conditional Marginals \\nWe consider now how to calculate conditional marginals, given some evidential \\nnodes. (In contrast to [6], any set of nodes in the network, not just output nodes, \\ncan be considered evidential.) We write the evidence in the following manner \\nThe quantities that we are interested in are conditional marginals which, from Bayes \\nrule are related to the joint distribution by \\np(s = lIE)= p(s = 1, E) \\np(si = O,E) +p(s = 1, E) (13) \\nThat is, provided that we have a procedure for estimating joint marginals, we can \\nobtain conditional marginals too. Without loss of generality, we therefore consider \\nGaussian Fields for Approximate Inference 397 \\n2O \\n15 \\n10 \\nErro using factodeed model fit \\n0 002 004 006 \\nError using Gaussian Field. Dagonal covariance \\n0.002 0 004 0 006 0.008 0.01 \\nError using Gaussian Field, Non Diagonal covariance \\n(a) Mean error = 0.0377 \\n(b) Mean error = 0.0018 \\n(c) Mean error = 0.0017 \\nFigure 2: Error in approximating p(si = 1) for the network in fig(l), averaged over \\nall the nodes in the network. In each of 100 trials, weights were drawn from a \\nzero mean, unit variance Gaussian; biases were set to 0. Note the different scale \\nin (b) and (c). In (a) we use the variational procedure with a factorised Q, as \\nin section (2.1). In (b) we use the Gaussian field equations, assuming a diagonal \\ncovariance matrix R. This procedure was repeated in (c) including correlations \\nbetween parents. \\nE + = E U {si = 1}, which then contains n + 1 \"evidential\" variables. That is, the \\ndesired marginal variable is absorbed into the evidence set. For convenience, we \\nthen split the nodes into two sets, those containing the evidential or \"clamped\" \\nnodes, C, and the remaining \"free\" nodes F. The joint evidence is then given by \\np(E +) = Ep(Ec,,...Ec+,,sf,,...sf.) (14) \\n 7r* 71'* * \\n= \\n, (15) \\nv]ues  specified [ +.  he sgmo[d belief ewo \\n=  (2S - 1) ws + Ok ' si = otherwise \\nis therefore determined by the distribution of the field h; =  wks +0. \\nExamining (15), we see that the product over the \"free\" nodes defines a SBN in \\nwhich the local probability distributions are given by those of the original network, \\nbut with any evidential parental nodes clamped to their evidence values. Therefore, \\nConsistent with our previous assumptions, we assume that the distribution of the \\n( ') \\nfields h* - h...hc+ is jointly Gaussian. We can then find the mean and \\ncovariance matrix for the distribution of h* by repeating the calculation of section \\n(2) in which evidential nodes have been clamped to their evidence values. Once this \\nGaussian has been determined, it can be used in (17) to determine p(E +). Gaussian \\naverages of products of sigmoids are calculated by drawing 1000 samples from the \\nGaussian over which we wish to integrate x. Note that if there are evidential nodes \\nIn one and two dimensions (n = 0, 1), or n = 1, we use Gaussian Quadrature. \\n398 D. Barber and P Sollich \\nError using factorised model fit Error using Gaussian Field, Diagonal covariance Error using Gaussian Field, Non Diagonal covariance \\n30 \\n20 10 1011_ \\no o., o o o. o o. o o o. o. o. o. o o o, o' o. o. o. . \\n(a) Mean error = 0.4188 \\n(b) Mean error = 0.0253 \\n(c) Mean error = 0.0198 \\nFigure 3: All weights are set to uniformly from 0 to 50. Biases are set to -0.5 of \\nthe summed parental weights plus a uniform random number from -2.5 to 2.5. The \\nroot node is set to be 1 with probability 0.5. This has the effect of making all the \\nnodes in the exact network roughly 0.5 in mean, with non-negligible correlations \\nbetween parental nodes. 160 simulations were made. \\nin different layers, we require the correlations between their fields h to evaluate (17). \\nSuch 'inter-layer' correlations were not required in section (2), and to be able to use \\nthe same calculational scheme we simply neglect them. (We leave a study of the \\neffects of this assumption for future work.) The average in (17) then factors into \\ngroups, where each group contains evidential terms in a particular layer. \\nThe conditional marginal for node i is obtained from repeating the above procedure \\nin which the desired marginal node is clamped to its opposite value, and then using \\nthese results in (13). The above procedure is repeated for each conditional marginal \\nthat we are interested in. Although this may seem computationally expensive, the \\nmarginal for each node is computed quickly, since the equations are solved by one \\nforward propagation sweep only. \\n6O \\n50 \\n40 \\n30 \\n2O \\n10 \\n0 \\n0 \\nError using factorised model fit \\nError using Gaussian Field, Diagonal covariance \\n0.1 0.2 03 0.4 0.5 0.6 0 0.1 0.2 0.3 0.4 0.5 06 \\nError using Gaussian Field, Non Diagonal covariance \\n) 01 02 0.3 04 0.5 06 \\n(a) Mean error - 0.1534 \\n(b) Mean error = 0.0931 \\n(c) Mean error = 0.0865 \\nFigure 4: Estimating the conditional marginal of the top node being in state 1, \\ngiven that the four bottom nodes are in state 1. Weights were drawn from a zero \\nmean Gaussian with variance 5, with biases set to -0.5 the summed parental weights \\nplus a uniform random number from -2.5 to 2.5. Results of 160 simulations. \\n3.1 Results for conditional marginals \\nWe used the same structure as in the previous experiments, as shown in fig(l). We \\nare interested here in calculating the probability that the top node is in state 1, \\nGaussian Fields for Approximate Inference 399 \\ngiven that the four bottom nodes are in state 1. Weights were chosen from a zero \\nmean Gaussian with variance 5. Biases were set to negative half Of the summed \\nparent weights, plus a uniform random value from -2.5 to 2.5. Correlation effects \\nin these networks are not as.strong as in the experiments in section (2.2), although \\nthe improvement of the GF theory over the variational theory seen in fig(4) remains \\nclear. The improvement from the off diagonal terms in R is minimal. \\n4 Conclusion \\nDespite their appropriateness for learning, variational methods may not be equally \\nsuited to inference, making more tailored methods attractive. We have considered \\nan approximation procedure that is based on assuming that the distribution of the \\nweighted input to a node is approximately Gaussian. Correlation effects between \\nparents of a node were taken into account to improve the Gaussian theory, although \\nin our examples this gave only relatively modest improvements. \\nThe variational mean field theory performs poorly in networks with strong cor- \\nrelation effects between nodes. On the other hand, one may conjecture that the \\nGaussian Field approach will not generally perform catastrophically worse than the \\nfactorised variational mean field theory. One advantage of the variational theory \\nis the presence of an objective function against which competing solutions can be \\ncompared. However, finding an optimum solution for the mean parameters mi from \\nthis function is numerically complex. Since the Gaussian Field theory is extremely \\nfast to solve, an interesting compromise might be to prime the variational solution \\nwith the results from the Gaussian Field theory. \\nAcknowledgments \\nDB would like to thank Bert Kappen and Wim Wiegerinck for stimulating and \\nhelpful discussions. PS thanks the Royal Society for financial support. \\nReferences\\n[1] R. Neal. Connectionist learning of Belief Networks. Artificial Intelligence, 56:71-113, \\n1992. \\n[2] E. Castillo, J. M. Gutierrez, and A. S. Hadi. Expert Systems and Probabilistic Network \\nModels. Springer, 1997. \\n[3] M. I. Jordan, Z. Gharamani, T. S. Jaakola, and L. K. Saul. An Introduction to Vari- \\national Methods for Graphical Models. In M. I. Jordan, editor, Learning in Graphical \\nModels, pages 105-161. Kluwer, 1998. \\n[4] L. Saul and M. I. Jordan. A mean field learning algorithm for unsupervised neural \\nnetworks. In M. I. Jordan, editor, Learning in Graphical Models, 1998. \\n[5] D. Barber and W Wiegerinck. Tractable variational structures for approximating \\ngraphical models. In M.S. Kearns, S.A. Solla, and D.A. Cohn, editors, Advances in \\nNeural Information Processing Systems NIPS 11. MIT Press, 1999. \\n[6] M. Kearns and L. Saul. Inference in Multilayer Networks via Large Deviation Bounds. \\nIn Advances in Neural Information Processing Systems NIPS 11, 1999. \\n[7] L. K. Saul, T. Jaakkola, and M. I. Jordan. Mean Field Theory for Sigmoid Belief \\nNetworks. Journal of Artificial Intelligence Research, 4:61-76, 1996. \\n</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1740 rows × 5 columns</p>\n","</div>"],"text/plain":["      Document  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Paper\n","0     0         ...  1 \\nCONNECTIVITY VERSUS ENTROPY \\nYaser S. Abu-Mostafa \\nCalifornia Institute of Technology \\nPasadena, CA 91125 \\nABSTRACT \\nHow does the connectivity of a neural network (number of synapses per \\nneuron) relate to the complexity of the problems it can handle (measured by \\nthe entropy)? Switching theory would suggest no relation at all, since all Boolean \\nfunctions can be implemented using a circuit with very low connectivity (e.g., \\nusing two-input NAND gates). However, for a network that learns a problem \\nfrom examples using a local learning rule, we prove that the entropy of the \\nproblem becomes a lower bound for the connectivity of the network. \\nINTRODUCTION \\nThe most distinguishing feature of neural networks is their ability to spon- \\ntaneously learn the desired function from 'training' samples, i.e., their ability \\nto program themselves. Clearly, a given neural network cannot just learn any \\nfunction, there must be some restrictions on which networks can learn which \\nfunctions. One obvious restriction, which is independent of the learning aspect, \\nis that the network must be big enough to accommodate the circuit complex- \\nity of the function it will eventually simulate. Are there restrictions that arise \\nmerely from the fact that the network is expected to learn the function, rather \\nthan being purposely designed for the function? This paper reports a restriction \\nof this kind. \\nThe result imposes a lower bound on the connectivity of the network (num- \\nber of synapses per neuron). This lower bound can only be a consequence of \\nthe learning aspect, since switching theory provides purposely designed circuits \\nof low connectivity (e.g., using only two-input NAND gates) capable of imple- \\nmenting any Boolean function [1,2]. It also follows that the learning mechanism \\nmust be restricted for this lower bound to hold; a powerful mechanism can be \\n@ American Institute of Physics 1988 \\n2 \\ndesigned that will find one of the low-connectivity circuits (perhaps by exhaus- \\ntive search), and hence the lower bound on connectivity cannot hold in general. \\nIndeed, we restrict the learning mechanism to be local; when a training sample \\nis loaded into the network, each neuron has access only to those bits carried by \\nitself and the neurons it is directly connected to. This is a strong assumption \\nthat excludes sophisticated learning mechanisms used in neural-network models, \\nbut may be more plausible from a biological point of view. \\nThe lower bound on the connectivity of the network is given in terms of \\nthe entropy of the environment that provides the training samples. Entropy is a \\nquantitative measure of the disorder or randomness in an environment or, equiv- \\nalently, the amount of information needed to specify the environment. There \\nare many different ways to define entropy, and many technical variations of this \\nconcept [3]. In the next section, we shall introduce the formal definitions and \\nresults, but we start here with an informal exposition of the ideas involved. \\nThe environment in our model produces patterns represented by N bits \\nx = x... xN (pixels in the picture of a visual scene if you will). Only h different \\npatterns can be generated by a given environment, where h < 2 v (the entropy \\nis essentially log 2 h). No knowledge is assumed about which patterns the en- \\nvironment is likely to generate, only that there are h of them. In the learning \\nprocess, a huge number of sample patterns are generated at random from the \\nenvironment and input to the network, one bit per neuron. The network uses \\nthis information to set its internal parameters and gradually tune itself to this \\nparticular environment. Because of the network architecture, each neuron knows \\nonly its own bit and (at best) the bits of the neurons it is directly connected to \\nby a synapse. Hence, the learning rules are local: a neuron does not have the \\nbenefit of the entire global pattern that is being learned. \\nAfter the learning process has taken place, each neuron is ready to perform \\na function derned by vhat it has larned. The collective interaction of the \\nfunctions of the neurons is what defines the overall function of the network. The \\nmain result of this paper is that (roughly speaking) if the connectivity of the \\nnetwork is less than the entropy of the environment, the network cannot learn \\nabout the environment. The idea of the proof is to show that if the connectivity \\nis small, the final function of each neuron is independent of the environment, \\nand hence to conclude that the overall network has accumulated no information \\nabout the environment it is supposed to learn about. \\nFORMAL RESULT \\nA neural network is an undirected graph (the vertices are the neurons and the \\nedges are the synapses). Label the neurons 1,-.-, N and define K, _c 1,.. -, N \\nto be the set of neurons connected by a synapse to neuron n, together with \\nneuron n itself. An environment is a subset e __C C0, 1 v (each x  e is a sample \\n3 \\nfrom the environment). During learning, Zl,-\", zv (the bits of x) are loaded \\ninto the neurons 1,...,N, respectively. Consider an arbitrary neuron n and \\ntelabel everything to make K become {1,...,K}. Thus the neuron sees the \\nfirst K coordinates of each x. \\nSince our result is asymptotic in N, we will specify K as a function of N; \\nK = aN where a = a(N) satifies limN-.,o a(N) = ao (0 < ao < 1). Since the \\nresult is also statistical, we will consider the ensemble of environments  \\ne = {, c I I,I = \\nwhere h = 2 oN and/ =/(N) satifies limN_.oo/(N) = /o (0 < /o < 1). The \\nprobability distribution on  is uniform; any environmen e G    likely \\noccur  any oher. \\nThe neuron sees only he firs K coordinates of each x generated by he \\nenvironment e. For each e, we define the function : {0,1}   {0,1,2,--.} \\nwhere \\nn(a...a) =l{x6e [ z,=a, fork=l,--.,K}l \\nand the normalized version \\nThe function v describes the relative frequency of occurrence for each of the 2 r \\nbinary vectors a:l -\" zr as x = zl '\" z Jr runs through all h vectors in e. In other \\nwords, /specifies the projection of e as seen by the neuron. Clearly, v(a) _> 0 \\nfor all a  {0, 1} r and Z&E{O,1}K v(a) = 1. \\nCorresponding to two environments el and es, we will have two functions vl \\nand bt 2. If//1 is not distinguishable from t/z, the neuron cannot tell the difference \\nbetween ea and es. The distinguishability between btl and t/: can be measured \\nby \\n1 \\nd(l/l'l/2) --  Z \\nThe range of d(t/1,) is 0 <_ d(t/1,) <_ 1, where '0' corresponds to complete \\nindistinguishability while '1' corresponds to maximum distinguishability. We \\naxe now in a position to state the main result. \\nLet e and es be independently selected environments from  according to the \\nuniform probability distribution. d(vl, v) is now a random variable, and we are \\ninterested in the expected value E(d(vl,v2)). The case where E(d(vl,v2)) -- 0 \\ncorresponds to the neuron getting no information about the environment, while \\nthe case where E(d(Vl,V2)) = I corresponds to the neuron getting maximum \\ninformation. The theorem predicts, in the limit, one of these extremes depending \\non how the connectivity (ao) compares to the entropy (/o). \\n4 \\nTheorem. \\n1. If ao > f/o, then limv-.o E (d(Vl, v2)) = 1. \\n2. If co < o, then limN._.ooS(d(,,,v2)) =0. \\nThe proof is given in the appendix, but the idea is easy to illustrate infor- \\nmally. Suppose h = 2 K+ (corresponding to part 2 of the theorem). For most \\nenvironments e 6 , the first K bits of x 6 e go through all 2 K possible val- \\nues approximately 2  times each as x goes through all h possible values once. \\nTherefore, the patterns seen by the neuron are drawn from the fixed ensemble of \\nall binary vectors of length K with essentially uniform probability distribution, \\ni.e., v is the same for most environments. This means that, statistically, the \\nneuron will end up doing the same function regardless of the environment at \\nhand. \\nWhat about the opposite case, where h = 2 K- (corresponding to part 1 of \\nthe theorem)? Now, with only 2 - patterns available from the environment, \\nthe first K bits of x can assume at most 2 K- values out of the possible 2 g \\nvalues a binary vector of length K can assume in principle. Furthermore, which \\nvalues can be assumed depends on the particular environment at hand, i.e., \\n, does depend on the environment. Therefore, although the neuron still does \\nnot have the global picture, the information it has says something about the \\nenvironment. \\nACKNOWLEDGEMENT \\nThis work was supported by the Air Force Office of Scientific Research under \\nGrant AFOSR-86-0296. \\nAPPENDIX \\nIn this appendix we prove the main theorem. We start by discussing some \\nbasic properties about the ensemble of environments . Since the probability \\ndistribution on  is uniform and since [1-- (2h), we have \\nwhich is equivalent to generating e by choosing h elements x 6 {0, 1} v with \\nuniform probability (without replacement). It follows that \\nh \\nPr(x6e)= 2 v \\n5 \\nwhile for x 1  x2, \\nh h-1 \\nPr(xle, xie) = 2 v x 2 v_l \\nand so on. \\nThe functions  and  are defined on K-bit vectors. \\n(a random variable for fixed a) is independent of a \\nThe statistics of r(a) \\nPr(r(sx) = m) = Pr(r(s) = m) \\nwhich follows from the symmetry with respect to each bit of a. The same holds \\nfor the statistics of (a). The expected value E(r(a)) = h2 -K (h objects going \\ninto 2 K cells), hence E((a)) = 2 -. We now restate and prove the theorem. \\nTheorem. \\n1. If co > o, then limr-.o E (d(l, 2)) = 1. \\n2. If ao < o, then limN-o E (d(l, 2)) =0. \\nProof. \\nWe expand E (d(l, 2)) as follows \\nwhere n and n2 denote nl(0.--0) and n2(0..-0), respectively, and the last step \\nfollows from the fact that the statistics of nl(a) and n2(a) is independent of a. \\nTherefore, to prove the theorem, we evaluate E(Irh- r21) for large N. \\n1. Assume ao > fo. Let n denote n(0...0), and consider Pr(n - 0). For r to \\nbe zero, all 2 N-K strings x of N bits starting with K O's must not be in the \\nenvironment e. Hence \\nPr(r = O) = (1- -- \\nh \\n2)(1 \\nh h \\n2 r - 1 )'\" (1 - 2 r _ 2r_: + 1 ) \\nwhere the first term is the probability that 0... 00  e, the second term is the \\n6 \\nprobability that 0.--O1  e given that 0-.. O0  e, and so on. \\n: (1 - h2-m'(1 -- 2-x) -') \\n>_ (1 - \\n> 1 - 2h2-r2 v-K \\n= 1 - 2h2 -K \\n2N--K \\nHence, Pr(n, = 0): Pr(n2: 0): Pr(n: 0) _> i - 2h2 -K. However, E(n,) = \\nE(n2) = h2 -. Therefore, \\nh h \\nZ Z Pr(rtl: i, rt2 '-- j)li -- Jl \\ni=0 j=O \\nh h \\n= Z Y] Pr(nl = i)Pr(n2: j)l i - Jl \\ni=0 j=0 \\nh \\n_  Pt(hi = 0)Pr(n2 = j)j \\nh \\n+ Z Pt(hi =/)Pr(n2 = 0)i \\ni=0 \\nwhich follows by throwing away all the terms where neither i nor j is zero (the \\nterm where both i an j are zero appears twice for convenience, but this term is \\nzero anyway). \\n= Pr(nl = 0)E(n2) + Pr(n2 = 0)E(rh) \\n> 2(1- 2h2-)h2 - \\nSubstituting this estimate in the expression for E(d(tl, t2)), we get \\n2 K \\nE(d(//1,//2)) = 2hE(lnl -- \\n_ - x 2(1- 2h2-K)h2 -t \\n= 1 - 2h2 -K \\n= 1 - 2 x 2 (f-\")v \\nSince ao >/o by assumption, this lower bound goes to 1 as N goes to infinity. \\nSince 1 is also an upper bound for d(l, 2) (and hence an upper bound for the \\nexpected value E(d(/l,/2))), limv-.oo E(d(/,/2)) must be 1. \\n7 \\n2. Assume ao < o. Consider \\nTo evaluate E([n - h2-K[), we estimate the variance of n and use the fact \\nthat E([n- h2-KI) <_ va, (recall that h2 - = E(n)). Since vat(n) = \\nE(n 2) - (E(n)) 2, we need an estimate for E(n2). We write n = Eaei0.1)N- a, \\nwhere \\n1, if 0.-.0a 6 e; \\n5 = 0, otherwise. \\nIn this notation, E(n 2) can be written as \\n: >- \\n&{0,1} N-I be{0,1} \\nFor the 'diagonal' terms (a = b), \\n= h2 -/v \\nThere are 2 N-K such diagonal terms, hence a total contribution of 2 N-K X \\nh2 -r = h2 -K to the sum. For the 'off-diagonal' terms (a  b), \\nE(5. Sb) = Pr(5. = 1, Sb = 1) \\n= Pr(5. = 1)Pr(Sb: l[a = 1) \\nh h-1 \\n-- -- X \\n2 r 2 r - I \\nThere are 2r-(2 v-K - 1) such off-diagonal terms, hence a total contribution of \\n2V-(2N-K  h(h-i) < {h,_K2 2 v \\nk --'1 ^ 2N(2N--1) -- ,  ] --i to the sum. Putting the contributions \\n8 \\nfrom the diagonal and off-diagonal terms together, we get \\nwr(n) = E(n 2) - (E(n)) 2 \\n< h2-K + (h2-K)a2v - 1 \\n1 \\n= h2-K + (h2-)a 2 N - 1 \\nh2_  \\n= h2 -K 1 + \\n< 2h2 - \\nThe last step follows since h2 -K is much smaller than 2 r - 1. Therefore, E(In - \\n1 \\nh2-]) < v < (2h2-)  Substituting this estimate in the expression for \\nwe get \\nSince Cto < f/o by assumption, this upper bound goes to 0 as N goes to infinity. \\nSince 0 is also a lower bound for d(yx,ya) (and hence a lower bound for the \\nexpected value E(d(,x,,a))), limo E(d(,,,a)) must be 0. I \\nREFERENCES \\n[1] Y. Abu-Mostafa, \"Neural networks for computing?,\"AIP Conference Pro- \\nceedings  151, Neural Networks for Computing, J. Denker (ed.), pp. 1-6, 1986. \\n[2] Z. Kohavi, Switching and Finite Automata Theory, McGraw-Hill, 1978. \\n[3] Y. Abu-Mostafa, \"The complexity of information extraction,\"IEEE Trans. \\non Information Theory, vol. IT-32, pp. 513-525, July 1986. \\n[4] Y. Abu-Mostafa, \"Complexity in neural systems,\"in Analog VLSiand Neural \\nSystems by C. Mead, Addison-Wesley, 1988. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n","1     1         ...  22 \\nLEARNING ON A GENERAL NETWORK \\nAmir F. Atiya \\nDepartment of Electrical Engineering \\nCalifornia Institute of Technology \\nCa 91125 \\nAbstract \\nThis paper generalizes the backpropagation method to a general network containing feed- \\nback connections. The network model considered consists of interconnected groups of neurons, \\nwhere each group could be fully interconnected (it could have feedback connections, with pos- \\nsibly asymmetric weights), but no loops between the groups are allowed. A stochastic descent \\nalgorithm is applied, under a certain inequality constraint on each intra-group weight matrix \\nwhich ensures for the network to possess a unique equilibrium state for every input. \\nIntroduction \\nIt has been shown in the last few years that large networks of interconnected \"neuron\"-like \\nelements re quite suitable for performing a variety of computational and pattern recognition \\ntasks. One of the well-known neural network models is the backpropagation model [1]-[4]. It \\nis an elegant way for teaching a layered feedforward network by a set of given input/output \\nexamples. Neural network models having feedback connections, on the other hand, have lso \\nbeen devised (for example the Hopfield network [5]), and are shown to be quite successful in \\nperforming some computational tasks. It is important, though, to have a method for learning \\nby examples for a feedback network, since this is a general way of design, and thus one can \\navoid using an ad hoc design method for each different computational task. The existence \\nof feedback is expected to improve the computational abilities of a given network. This is \\nbecause in feedback networks the state iterates until a stable state is reached. Thus processing \\nis performed on several steps or recursions. This, in general allows more processing abilities \\nthan the \"single step\" feedforward case (note also the fact that a feedforward network is \\na special case of a feedback network). Therefore, in this work we consider the problem of \\ndeveloping a general learning algorithm for feedback networks. \\nIn developing a learning algorithm for feedback networks, one has to pay attention to the \\nfollowing (see Fig. I for an example of a configuration of a feedback network). The state of \\nthe network evolves in time until it goes to equilibrium, or possibly other types of behavior \\nsuch as periodic or chaotic motion could occur. However, we are interested in having a steady \\nand and fixed output for every input applied to the network. Therefore, we have the following \\ntwo important requirements for the network. Beginning in any initial condition, the state \\nshould ultimately go to equilibrium. The other requirement is that we have to have a unique \\nAmerican Institute of Physics 1988 \\n23 \\nequilibrium state. It is in fact that equilibrium state that determines the final output. The \\nobjective of the learning algorithm is to adjust the parameters (weights) of the network in small \\nsteps, so as to move the unique equilibrium state in a way that will result finally in an output \\nas close as possible to the required one (for each given input). The existence of more than one \\nequilibrium state for a given input causes the following problems. In some iterations one might \\nbe updating the weights so as to move one of the equilibrium states in a sought direction, while \\nin other iterations (especially with different input examples) a different equilibrium state is \\nmoved. Another important point is that when implementing the network (after the completion \\nof learning), for a fixed input there can be more than one possible output. Independently, other \\nwork appeared recently on training a feedback network [6],[7],[8]. Learning algorithms were \\ndeveloped, but solving the problem of ensuring a unique equilibrium was not considered, This \\nproblem is addressed in this paper and an appropriate network and a learning algorithm are \\nproposed. \\noutputs \\nFig. 1 \\nA recurrent network \\nThe Feedback Network \\nConsider a group of n neurons which could be fully inter-connected (see Fig. ! for an \\nexample). The weight matrix W can be asymmetric (as opposed to the Hopfield network). \\nThe inputs are also weighted before entering into the network (let V be the weight matrix). \\nLet x and y be the input and output vectors respectively. In our model y is governed by the \\nfollowing set of differential equations, proposed by Hopfield [5]: \\ndu \\n d--[ = Wf(u) - u + Vx, y = f(u) (1) \\n24 \\nwhere f(u) = (f(ux),...,f(ur)) T, W denotes the transpose operator, f is a bounded and \\ndifferentiable function, and r is a positive constant. \\nFor a given input, we would like the network after a short transient period to give a steady \\nand fixed output, no matter what the initial network state was. This means that beginning \\nany initial condition, the state is to be attracted towards a unique equilibrium. This leads to \\nlooking for a condition on the matrix W. \\nTheorem: A network (not necessarily symmetric) satisfying \\n2 1/max(?)2, \\ni j \\nexhibits no other behavior except going to a unique equilibrium for a given input. \\nProof: Let u(t) and u2(t) be two solutions of (1). Let \\nJ(t) = Ilu(t) - u2(t)ll 2 \\nwhere [I II is the [wo-norm. Differentiating J with respect to time, one obtains \\ndJ(t) _ 2(u(t)-u2(t))r(du(t) du2(t)  \\ndt dt  1' \\nUsing (1) , the expression becomes \\ndJ(t) 2 2 \\ndt - r []ul(t) -u2(t))[12 + ;(u(t) -u2(t))\"W[f(u(t)) - f(u2(t))]. \\nUsing Schwarz's Inequality, we obtain \\n2 \\ndJ(t) < _211u(t ) _ u(t)ll  + _11u (t)_ u(t)11. iiW[f(u (t)) _ f(u2(t))] ii. \\ndt - r r \\nAgain, by Schwarz's Inequality, \\nw/ [f(u (t)) - f(u(t))] <_ Ilwgll - Ilf(u(t)) - f(u2(t))11 , i = 1,...,n \\nwhere wi denotes the i th row of W. Using the mean value theorem, we get \\n[If(u (t)) - f(u2(t)) II < (maxlf'l)llu (t) - u2(t)l I. (3) \\nUsing (2),(3), and the expression for J(t), we get \\ndJ(t) <-aJ(t) (4) \\ndt - \\nwhere \\n2 2 (maxl f,i)/- jwj.. \\nT T i \\n(2) \\n25 \\nBy hypothesis of the Theorem, a is strictly positive. Multiplying both sides of (4) by exp(at), \\nthe inequality \\nd 0 \\n- \\nresults, from which we obtain \\nJ(t) _< J(0)e \\nFrom that and from the fact that J is non-negative, it follows that J(t) goes to zero as t \\nTherefore, any two solutions corresponding to any two initial conditions ultimately approach \\neach other. To show that this asymptotic solution is in fact an equilibrium, one simply takes \\nu2(t): u(t + T), where T is a constant, and applies the above argument (that J(t) - 0 as \\nt --* c), and hence u (t + T) -- u (t) as t --* cw for any T, and this completes the proof. \\nFor example, if the function f is of the following widely used sigmoid-shaped form, \\n1 \\nf (u) - 1 + e- \\nthen the sum of the square of the weights should be less than 16. Note that for any function \\nf, scaling does not have an effect on the overall results. We have to work in our updating \\nscheme subject to the constraint given in the Theorem. In many cases where a large network \\nis necessary, this constraint might be too restrictive. Therefore we propose a general network, \\nwhich is explained in the next Section. \\nThe General Network \\nWe propose the following network (for an example refer to Fig. 2). The neurons are \\npartitioned into several groups. Within each group there are no restrictions on the connections \\nand therefore the group could be fully interconnected (i.e. it could have feedback connections). \\nThe groups are connected to each other, but in a way that there are no loops. The inputs to \\nthe whole network can be connected to the inputs of any of the groups (each input can have \\nseveral connections to several groups). The outputs of the whole network are taken to be the \\noutputs (or part of the outputs) of a certain group, say group f. The constraint given in the \\nTheorem is applied on each intra-group weight matrix separately. Let (qa, s,), a = 1, ..., N be \\nthe input/output vector pairs of the function to be implemented. We would like to minimize \\nthe sum of the square error, given by \\nN \\na----1 \\nwhere \\nM \\n---- --$i) , \\ni----1 \\nand f is the output vector of group f upon giving input q, and M is the dimension of vector \\ns a. The learning process is performed by feeding the input examples q sequentially to the \\nnetwork, each time updatg the weights in an attempt to minimize the eor. \\n26 \\ninputa outputs \\nFig. 2 \\nAn example of a general network \\n(each group represents a recurrent network) \\nNow, consider a single group l. Let W t be the intra-group weight matrix of group l, V 'q \\nbe the matrix of weights between the outputs of group r and the inputs of group l, and yt be \\nI rl \\nthe output vector of group l. Let the respective elements be wo. , vO. , and Yi- Furthermore, \\nlet nt be the number of neurons of group l. Assume that the time constant  is sufficiently \\nsmall so s to allow the network to settle quickly to the equilibrium state, which is given by \\nthe solution of the equation \\nyt= f(Wtyt + E VtYr)' (5) \\nrAl \\nwhere At is the set of the indices of the groups whose outputs are connected to the inputs of \\ngroup l. We would like each iteration to update the weight matrices W t and V rt so as to move \\nthe equilibrium in a direction to decrease the error. We need therefore to know the change in \\nthe error produced by a small change in the weight matrices. Let Oe \\n3-Wr, and ov-r, denote the \\nOe Oe Oea, \\nmatrices whose (i,j) th element are -,, and respectively. Let 3-fy be the column vector \\nwhose i th element is . We obtain the following relations: \\naea \\nav,,t \\n-1 3a t T \\n = [,_ (w') T] y,(y ) , \\n-1 aCa r T \\n- ['- (w')q y,(y ) , \\nwheee A t is the diagonal matrix whose ita diagonal element is 1/f'( k t t ,q ,- \\nfor a derivation refer to Appends). The vector  sociaed with group I can be obtaed \\nin erms of the vectors  jeBt where Bt is the se of he indices of the groups whose puts \\ne connected to the outputs of group l..We get (refer to Appends} \\n = (v')[a -(wq] - \\n., aye' () \\nThe matrix A t - (Wt? ' for any group l can never be singular, so we will not face any \\nproblem in the updating process. To prove that, let z be a vector satisfying \\n[' - (w')],. = 0. \\n27 \\nWe can write \\n,/maxl.f'l _< -?:,, i= ],..., , \\nwhere z  the i  element of z. Usg Schwarz's Inequity, we obtain \\n_ ..., \\nSquing both sides and dg the inequalities for i = 1, ..., n, we get \\n,,)- (7) \\nk i k \\nSince the condition \\nw' 2 1/max(),)2) \\n, \\ni k \\nis enforced, it follows that (7) cannot be satisfied unless z is the zero vector. Thus, the matrix \\nAi - (Wl) T cannot be singular. \\nFor each iteration we begin by updating the weights of group f (the group containing \\n0e equals simply 2(y{ - s,...,y - sM 0, 0)T). Then \\nthe final outputs). For that group yy , ..., \\nwe move backwards to the groups connected to that group and obtain their corresponding \\n0e, vectors using (6) update the weights, and proceed in the same manner until we complete \\n0y ' \\nupdating all the groups. Updating the weights is performed using the following stochastic \\ndescent algorithm for each group, \\nO ea \\nc9 ea \\nAV ----- --aa + adeaR , \\nwhere 1 is a noise matrix whose elements axe characterized by independent zero-mean unity- \\nvariance Gaussian densities, and the a's are parameters. The purpose of adding noise is to \\nallow escaping local minima if one gets stuck in any of them. Note that the control parameter \\nis taken to be ea. Hence the variance of the added noise tends to decrease the more we \\napproach the ideal zero-error solution. This makes sense because for a large error, i.e. for an \\nunsatisfactory solution, it pays more to add noise to the weight matrices in order to escape \\nlocal minima. On the other hand, if the error is small, then we are possibly near the global \\nminimum or to an acceptable solution, and hence we do not want too much noise in order \\nnot to be thrown out of that basin. Note that once we reach the ideal zero-error solution the \\nadded noise as well as the gradient of e, become zero for all a and hence the increments of the \\nweight matrices become zero. If a/ter a certain iteration W happens to violate the constraint \\ni.w. _ constant < 1/rnax(f') , then its elements are scaled so as to project it back onto \\nthe surface of the hypershere. \\nImplementation Example \\nA pattern recognition example is considered. Fig. $ shows a set of two-dimensional \\ntraining patterns from three classes. It is required to design a neural network recognizer with \\n28 \\nthree output neurons. Each of the neurons should be on if a sample of the corresponding class is \\npresented, and off otherwise, i.e. we would like to design a \"winner-take-all\" network. A single- \\nlayer three neuron feedback network is implemented. We obtained 3.3% error. Performing the \\nsame experiment on a feedforward single-layer network with three neurons, we obtained 20% \\nerror. For satisfactory results, a feedforward network should be two-layer. With one neuron \\nin the first layer and three in the second layer, we got 36.7% error. Finally, with two neurons \\nin the first layer and three in the second layer, we got a match with the feedback case, with \\n3.3% error. \\n2 \\n3 33 \\n3 3 \\n2 \\n2 \\n2 \\n2 \\n2 2 1 \\n2 2 1 \\n2 \\n2 2 1 \\n2 \\n3 \\n3 \\nI 1 \\n1 \\n33333 3 \\n3 \\n3 \\n3 \\n1 \\nI I \\n1 \\n1 \\nI I \\nFig. 3 \\nA pattern recognition example \\nConclusion \\nA way to extend the backpropagation method to feedback networks has been proposed. \\nA condition on the weight matrix is obtained, to insure having only one fixed point, so as \\nto prevent having more than one possible output for a fixed input. A general structure for \\nnetworks is presented, in which the network consists of a number of feedback groups connected \\nto each other in a feedforward manner. A stochastic descent rule is used to update the weights. \\nThe method is applied to a pattern recognition example. With a single-layer feedback network \\ni obtained good results. On the other hand, the feedforward backpropagation method achieved \\ngood resuls only for the case of more than one layer, hence also with a larger number of neurons \\nthan the feedback case. \\n29 \\nAcknowledgement \\nThe author would like to gratefully acknowledge Dr. Y. Abu-Mostafa for the useful \\ndiscussions. This work is supported by Air Force Office of Scientific Research under Grant \\nAFOSR-86-0296. \\nAppendix \\nDifferentiating (5), one obtains \\nm tOjm OtOp q- ypSjk), \\nOwl, - r(4.)( ' Oy, , \\nk, p = 1, ..., nt \\nwhere \\n1 ifj=k \\n8Y= 0 otherwise, \\nand \\nl \\nZj = \\nI I rl r \\nm reA rr \\nWe can write \\nc9yt -- (A t- Wt)-b \" (A- 1) \\nwhere b kv is the nt-dimensional vector whose i tn component is given by \\nt ifi=k \\nb . p= Yp \\n 0 otherwise. \\nBy the chain rule, \\nwhich, upon substituting from (A 1), can be put in the form  \\n- Yp o--fy, where g is the ktn \\ncolumn of (A t - Wt) -. Finally, we obtain the required expression, which is \\nOea Oea I T \\naw, - ['-(w')]-' W' (Y) ' \\nRegarding  it is obtained by differentiating (5) with respect to v t We get similarly \\nOVr ' kp' \\nOy  _ (A t - W)-c, \\nwitere c kp is the nt-dimensional vector whose i tn component is given by \\ncp { y[, ff i = k \\ni = 0 otherwise. \\n3O \\nA derivation very similar to the case of  results in the following required expression: \\naea - 1 aea r T \\naW, - ) ' \\nNow, finally consider oe o j ay \\n-. Let -', jeBt be the matrix whose (k,p) ta element is b'y' The \\nelements of  can be obtained by differentiating the equation for the fixed point for group \\nj, as follows, \\nag .  \\n,,, )' \\nHence, \\nay' - (A'- W')-IV'\" (A- 2) \\nUsing the chain rule, one can write \\nay, -  (---) ayy' \\nWe substitute from (A - 2) into the previous equation to complete the derivation by obtaining \\naea \\nae,, _ \\nay  \\n'B \\nReferences \\n[1] P. Werbos, \"Beyond regression: New tools for prediction and analysis in behavioral sci- \\nences\", Harvard University dissertation, 1974. \\n[2] D. Parker, \"Learning logic\", MIT Tech Report TR-47, Center for Computational Research \\nin Economics and Management Science, 1985. \\n[3] Y. Le Cun, \"A learning scheme for asymmetric threshold network\" Proceedings of Cog- \\nnitiva, Paris, June 1985. \\n[4] D. Rumelhart, G.Hinton, and R. Wilhams, \"earning internal representations by error \\npropagation\" in D. Rumelhart, J. McLelland and the PDP research group (Eds.), Parallel \\ndistributed processing: Explorations in the microstructure of co9nition, Vol. 1, MIT Press, \\nCambridge, MA, 1986. \\n[5] J. Hopfield, \"Neurons with graded response have collective computational properties hke \\nthose of two-state neurons\", Proc. Natl. Acad. Sci. USA, May 1984. \\n[6] L. Almeida, \"A learning rule for asynchronous perceptrons with feedback in a combinato- \\nrim environment\" Proc. of the First Int. Annual Conf. on Neural Networks, San Diego, \\nJune 1987. \\n[7] R. Rohwer, and B. Forrest, \"Training time-dependence in neural networks\", Proc. of the \\nFirst Int. Annual Conf. on Neural Networks, San Diego, June 1987. \\n[8] F. Pineda, \"Generahzation of back-propagation to recurrent neural networks\", Phys. Rev. \\nLett., vol. 59, no. 19, 9 Nov. 1987. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n","2     2         ...  31 \\nAN ARTIFICIAL NEURAL NETWORK FOR SPATIO- \\nTEMPORAL BIPOLAR PATTERNS: APPLICATION TO \\nPHONEME CLASSIFICATION \\nToshiteru Homma \\nLes E. Atlas \\nRobert J. Marks H \\nInteractive Systems Design Laboratory \\nDepartment of Electrical Engineering, FT-10 \\nUniversity of Washington \\nSeattle, Washington 98195 \\nABSTRACT \\nAn artificial neural network is developed to recognize spatioqemporal \\nbipolar patterns associatively. The function of a formal neuron is generalized by \\nreplacing multiplication with convolution, weights with transfer functions, and \\nthresholding with nonlinear transform following adaptation. The Hebbian learn- \\ning rule and the delta learning rule are generalized accordingly, resulting in the \\nlearning of weights and delays. The neural network which was first developed \\nfor spatial patterns was thus generalized for spario-temporal patterns. It was \\ntested using a set of bipolar input patterns derived from speech signals, showing \\nrobust classification of 30 model phonemes. \\n1. INTRODUCTION \\nLearning spatio-temporal (or dynamic) patterns is of prominent importance in biological \\nsystems and in artificial neural network systems as well. In biological systems, it relates to such \\nissues as classical and operant conditioning, temporal coordination of sensorimotor systems and \\ntemporal reasoning. In artificial systems, it addresses such real-world tasks as robot control, \\nspeech recognition, dynamic image processing, moving target detection by sonars or radars, EEG \\ndiagnosis, and seismic signal processing. \\nMost of the processing elements used in neural network models for practical applications \\nhave been the formal neuron 1 or its variations. These elements lack a memory flexible to tem- \\nporal patterns, thus limiting most of the neural network models previously proposed to problems \\nof spatial (or static) patterns. Some past solutions have been to convert the dynamic problems to \\nstatic ones using buffer (or storage) neurons, or using a layered network with/without feedback. \\nWe propose in this paper to use a \"dynamic formal neuron\" as a processing element for \\nlearning dynamic patterns. The operation of the dynamic neuron is a temporal generalization of \\nthe formal neuron. As shown in the paper, the generalization is straightforward when the activa- \\ntion part of neuron operation is expressed in the frequency domain. Many of the existing learn- \\ning rules for static patterns can be easily generalized for dynamic patterns accordingly. We show \\nsome examples of applying these neural networks to classifying 30 model phonemes. \\nAmerican Institute of Physics 1988 \\n32 \\n2. FORMAL NEURON AND DYNAMIC FORMAL NEURON \\nThe formal neuron is schematically drawn in Fig. l(a), where \\nInput \\nActivation \\nOutput \\nTransmittance \\nNode operator \\nNeuron operation \\n= [x x2    xt.] r \\nYi, i = 1,2 ..... N \\nzi, i = 1,2 ..... N \\n= [wi wi2''' w.] r \\nwhere 1(') is a nonlinear memoryless transform \\n(2.1) \\nNote that a threshold can be implicitly included as a transmittance from a constant input. \\nIn its original form of formal neuron, xi  {0,1} and '1(') is a unit step function u (-). A \\nvariation of it is a bipolar formal neuron where xi  {-1,1} and 11(. ) is the sign function sgn('). \\nWhen the inputs and output are converted to frequency of spikes, it may be expressed as \\nxi  R and 11(') is a rectifying function r('). Other node operators such as a sigmoidal function \\nmay be used. \\nWe generalize the notion of formal neuron so that the input and output are functions of \\ntime. In doing so, weights are replaced with transfer functions, multiplication with convolution, \\nand the node operator with a nonlinear transform following adaptation as often observed in bio- \\nlogical systems. \\nFig. l(b) shows a schematic diagram of a dynamic formal neuron where \\nInput \\nActivation \\nOutput \\nTransfer function \\nAdaptation \\nNode operator \\nNeuron operation \\n(t) = [Xl(t ) x2(t )    xr(t)] ? \\nyi(t), i = 1,2 ..... N \\nzi (t), i = 1,2 ..... N \\nv(t) = [Wil(t ) Wi2(t ) ''' w.(t)] r \\nai (t) \\n11 where '1(') is a nonlinear memoryless transform \\nz i (t) = 11 (a i (-t), q (t)T, 2(t )) \\n(2.2) \\nFor convenience, we denote \\nwith b(0 is equivalent to correlating a(-0 with b(t). \\nIf the Fourier transforms (f ) = F {(t)}, \\nai Or ) = F {ai (t )} exist, then \\nyi O c ) = ai O c) [qOc) cr \\nwhere q (f)cr is the conjugate transpose of /(t). \\n X Wi L Yl = W ZI \\n (a) \\n/(f) = F {/(t)}, Yi(f) = F{yi(t)}, \\n. as correlation instead of convolution. Note that convolving a(t) \\nand \\nx,(I) \\nX2(I) W (I) w.(t) \\n.,,,,,'\"''WL() \\nx,lt) \\nFig. 1. Formal Neuron and Dynamic Formal Neuron. \\n(2.3) \\nz(i) \\n33 \\n3. LEARNING FOR FORMAL NEURON AND DYNAMIC FORMAL NEURON \\nA number of learning rules for formal neurons has been proposed in the past. In the fol- \\nlowing paragraphs, we formulate a learning problem and describe two of the existing learning \\nrules, namely, Hebbian learning and delta learning, as examples. \\nPresent to the neural network M pairs of input and desired output samples \\n{k), k)}, k = 1,2 ..... M , in order. Let W () = [t ) 2 ()  .. )]r where ?) is the \\ntransmittance vector at the k-th step of learning. Likewise, let \\n_Z () = [z 40 z 4-)  ' ' z4)], and D () = [0) (2) ... ()], \\nwhere \\nk) = W( ), z ) = 1'()), and q(.) = [q(Y0 q(Y2) ' ' ' al(Yv)] r. \\nThe Hebbian learning rule 2 is described as follows*: \\nW (k) = W(-0 + eu() \\nThe delta learning (or LMS learning) rule 3, 4 is described as follows: \\n_w() = _w(k-o _ a{_w(-) _ \\n(3.1) \\n(3.2) \\nThe leaming rules described in the previous section are generalized for the dynamic formal \\nneuron by replacing multiplication with correlation. First, the problem is reformulated and then \\nthe generalized rules are described as follows. \\nPresent to the neural network M pairs of time-varing input and output samples \\n{k)(t), )(t)}, k = 1,2 ..... M , in order. Let W()(t) = [g(t)()(t) ?)(t).   k)(t)] r \\nwhere ,Vi()(t) is the vector whose elements w?)(t) are transfer functions connecting the input j \\nto the neuron i at the k-th step of learning. The Hebbian learning rule for the dynamic neuron is \\nthen \\nW()(t ) = W(-O(t ) + a(-t ), k)(t ), )(t ) r . (3.3) \\nThe delta learning rule for dynamic neuron is then \\nW()(t ) = W(-O(t ) - a(-t ), {W(-0(t ), k)(t ) - )(t )}, )(t )r . (3.4) \\nThis generalization procedure can be applied to other learning rules in some linear discrim- \\ninant systems 5 , the serf-organizing mapping system by Kohonen 6 , the perceptton 7, the back- \\npropagation model 3 , etc. When a system includes a nonlinear operation, more careful analysis \\nis necesssay as pointed out in the Discussion section. \\n4. DELTA LEARNING, PSEUDO INVERSE AND REGULARIZATION \\nThis section reviews the relation of the delta learning rule to the pseudo-inverse and the \\ntechnique known as regularization. 4, 6, 8, 9, lo \\nConsider a minimization problem as described below: Find __W which minimizes \\nk \\nsubject to y(k) = \\nA solution by the delta rule is, using a gradient descent method, \\n_w() = __w( - ) _ a-R () \\n* This interpretation assumes a strong supervising signal at the output while learning. \\n(4.1) \\n(4.2) \\n34 \\nwhere R (t') = II t') - ')l[  The minimum norm solution to the problem, _W*, is unique and \\ncan Ie expressed as \\nIV* = DX t (4.3) \\nwhere/t is the Moore-Penrose pseudo-inverse of/ , i.e., \\nX t = lim(XrX + o'2I)-lX T = IinlXT(XX T + o21) -1. (4.4) \\n2 \\nOn the condition that 0 < ct <  where  is the maximum eigenvalue of XrX, ') and \\nS t') am independent, and IVO,) is uncorrelated with '), \\nE {_w* 3 =  {w(')3 (4.5) \\nwhere  {x } denotes the expected value of x. One way to make use of this relation is to calcu- \\nlate W* for known standard data and refine it by (4.2), thereby saving time in the early stage of \\nlearning, \\nHowever, this solution results in an ill-conditioned IV often in practice. When the prob- \\nlem is ill-posed as such, the technique known as regularization can alleviate the ill-conditioning \\nof IV. The problem is reformulated by finding IV which minimizes \\nR (o) = llY '') - ')ll  + o2y'.ll ,ll  (4.6) \\nsubject to ') = __WX ') where IV = [2 ' \"t] r  \\nThis reformulation regularizes (4.3) to \\nIv(o) = D_Xr(X_X_ r + o2/) - (4.7) \\nwhich is statistically equivalent to W(') when the input has an additive noise of variance o a \\nuncorrelated with '). Interestingly, the leaky LMS algorithm II leads to a statistically \\nequivalent solution \\nIvtk) = _ _ (4.8) \\n2 \\nwhere 0 < 15 < 1 and 0 < ct <  These solutions am related as \\nif o a = 1-- when W (t') is uncorrelated with ') .li \\n(4.9) \\nEquation (4.8) can be generalized for a network using dynamic formal neurons, resulting in \\na equation similar to (3.4). Making use of (4.9), (4.7) can be generalized for a dynamic neuron \\nnetwork as \\nIv(t ;(X) = F- {D (f )X_ (f )Cr (x (f )X (f ) cr + (x2/) 43 (4.10) \\nwhere F - denotes the inverse Fourier transform. \\n5. SYNTHESIS OF BIPOLAR PHONEME PATTERNS \\nThis section illustrates the scheme used to synthesize bipolar phoneme patterns and to \\nform prototype and test patterns. \\nThe fundamental and first three formant frequencies, along with their bandwidths, of \\nphoneroes provided by Klat0 2 were taken as parameters to synthesize 30 prototype phoneme pat- \\nterns. The phoneroes were labeled as shown in Table 1. An array of L (=100) input neurons \\nCovered the range of 100 to 4000 Hz. Each neuron had a bipolar state which was +1 only when \\none of the frequency bands in the phoneme presented to the network was within the critical band \\n35 \\nof the neuron and -1 otherwise. The center frequencies (fc) of critical bands were obtained by \\ndividing the 100 to 4000 Hz range into a log scale by L. The critical bandwidth was a constant \\n100 Hz up to the center frequency fc = 500 Hz and 0.2fc Hz when f >500 Hz. 13 \\nThe parameters shown in Table 1 were used to construct Table 1. Labels of Phoneroes \\n30 prototype phoneme patterns. For O, it was constructed as a \\ncombination of t and 0. F, F 2 ,F s were the first, second, and \\nthird formants, and B, B 2, and B3. were corresponding \\nbandwidths. The fundamental frequency Fo = 130 Hz with B0 = \\n10 Hz was added when the phoneme was voiced. For plosives, \\nthere was a stop before formant traces start. The resulting bipo- \\nlar paRems are shown in Fig.2. Each pattern had length of 5 \\ntime units, composed by linearly interpolating the frequencies \\nwhen the formant frequency was gliding. \\nA sequence of phonemes converted from a continuous \\npronunciation of digits, {o, zero, one, two, three, four, five, six, \\nseven, eight, nine }, was translated into a bipolar pattern, adding \\ntwo time units of transition between two consequtive phonemes \\nby interpolating the frequency and bandwidth parameters \\nlinearly. A flip noise was added to the test pattern and created a \\nnoisy test pattern. The sign at every point in the original clean \\ntest pattern was flipped with the probability 0.2. These test pat- \\nterns are shown in Fig. 3. \\nLabel Phoneme \\n1 [i y] \\n2 [I ] \\n3 [eY] \\n4 \\n6 [a] \\n7 [o] \\n8 \\n9 [o w] \\nlO [u ] \\n11 [u TM] \\n12 \\n13 [a y] \\n14 [a TM] \\n15 [o y] \\n16 [w] \\n17 [y] \\n18 [r] \\n19 [1] \\n20 (q \\n21 [v] \\n22 [0] \\n23 [] \\n24 [s] \\n25 [z] \\n26 [p] \\n27 [t] \\n28 (d] \\n29 [k] \\n30 [n] \\nFig. 2. Prototype Phoneme Patterns. (Thirty phoneme patterns are shown \\nin sequence with intervals of two time units.) \\n6. SIMULATION OF SPATIO-TEMPORAL FILTERS FOR PHONEME CLASSIFICATION \\nThe network system described below was simulated and used to classify the prototype \\nphoneme patterns in the test patterns shown in the previoius section. It is an example of gen- \\neralizing a scheme developed for static patterns 13 to that for dynamic patterns. Its operation is \\nin two stages. The first stage operation is a spatio-temporal filter bank: \\n36 \\n(a) \\nFig. 3. Test Patterns. (a) Clean Test Pattern. (b) Noisy Test Pattern. \\ny'(t ) = __W (t ),:(t ) , and '(t ) = _r(a(-t )St(t )) . \\nThe second stage operation is the \"winner-take-all\" lateral inhibition: \\n0'(t) = '(t), and o'(t+a) = r_(A_(-t),(t) - h), \\nand \\n(6.1) \\n(6.2) \\nfor all f . (6.6) \\nThis minimizes \\nR (o,f \\nA_(t) = (1 + )/_.(t) -  (t-nA). (6.3) \\nwhere h  is a constant threshold vector with elements h i = h and (') is the Kronecker delta \\nfunction. This operation is repeated a sufficient number of times, No .13,1n The output is \\nO'(t + No-a). \\nTwo models based on different learning rules were simulated with parameters shown \\nbelow. \\nModel 1 (Spario-temporal Matched Filter Bank) \\nLet (x(t) = (t), f() =  in (3.3) where F is a unit vector with its elements e/ = 5(k-i). \\n___W (t) = X (t)T. (6.4) \\n4 1 \\nh=200, and a(t ) = -(t-n,x). \\nn=O  \\nModel 2 (Spario-temporal Pseudo-inverse Filter) \\nLet D =/ in (4.10). Using the alternative expression in (4.4), \\nW(t ) = F-I{(x0 e )CTxoe ) + (2I_)-lxCT}. (6.5) \\nh =0.05,o 2= 1000.0,and a(t)=(t). \\n37 \\nBecause the time and frequency were finite and discrete in simulation, the result of the \\ninverse discrete Fourier transform in (6.5) may be aliased. To alleviate the aliasing, the transfer \\nfunctions in the prototype matrix X(t) were padded with zeros, thereby doubling the lengths. \\nFurther zero-padding the transfer functions did not seem to change teh result significantly. \\nThe results are shown in Fig. 4(a)-(d). The arrows indicate the ideal response positions at \\nthe end of a phoneme. When the program was run with different thresholds and adaptation func- \\ntion a (t), the result was not very sensitive to the threshold value, but was, nevertheless affected \\nby the choice of the adaptation function. The maximum number of iterations for the lateral inhi- \\nbition network to converge was observed: for the experiments shown in Fig. 4(a) - (d), the \\nnumbers were 44, 69, 29, and 47, respectively. Model 1 missed one phoneme and falsely \\nresponded once in the clean test pattern. It missed three and had one false response in the noisy \\ntest pattern. Model 2 correctly recognized all phonemes in the clean test pattern, and false- \\nalarmed once in the noisy test pattern. \\n7. DISCUSSION \\nThe notion of convolution or correlation used in the models presented is popular in \\nengineering disciplines and has been applied extensively to designing filters, control systems, etc. \\nSuch operations also occur in biological systems and have been applied to modeling neural net- \\nworks.15,16 Thus the concept of dynamic formal neuron may be helpful for the improvement of \\nartificial neural network models as well as the understanding of biological systems. A portion of \\nthe system described by Tank and Hopfield 17 is similar to the matched filter bank model simu- \\nlated in this paper. \\nThe matched filter bank model (Model 1) performs well when all phonemes (as above) are \\nof the same duration. Otherwise, it would perform poorly unless the lengths were forced to a \\nmaximum length by padding the input and transfer functions with -l's during calculation. The \\npseudo-inverse filter model, on the other hand, should not suffer from this problem. However, \\nthis aspect of the model (Model 2) has not yet been explicitly simulated. \\nGiven a spatio-temporal pattern of size L x K, i.e., L spatial elements and K temporal ele- \\nments, the number of calculations required to process the first stage of filtering by both models is \\nthe same as that by a static formal neuron network in which each neuron is connected to the L x \\nK input elements. In both cases, L x K multiplications and additions are necessary to calculate \\none output value. In the case of bipolar patterns, the mutiplication used for calculation of activa- \\ntion can be replaced by sign-bit check and addition. A future investigation is to use recursive \\nfilters or analog filters as transfer functions for faster and more efficient calculation. There are \\nvarious schemes to obtain optimal recursive or analog filters. is, 19 Besides the lateral inhibition \\nscheme used in the models, there are a number of alternative procedures to realize a \"winner- \\ntake-all\" network in analog or digital fashion. 15,2,21 \\nAs pointed out in the previous section, the Fourier transform in (6.5) requires a precaution \\nconcerning the resulting length of transfer functions. Calculating the recursive correlation equa- \\ntion (3.4) also needs such preprocessing as windowing or truncation. 22 \\nThe generalization of static neural networks to dynamic ones along with their learning \\nrules is strainghfforward as shown if the neuron operation and the learning rule are linear. Gen- \\neralizing a system whose neuron operation and/or leaming rule are nonlinear requires more care- \\nful analysis and remains for future work. The system described by Watrous and Shastri 16 is an \\nexample of generalizing a backpropagation model. Their result showed a good potential of the \\nmodel and a need for more rigorous analysis of the model. Generalizing a system with recurrent \\nconnections is another task to be pursued. In a system with a certain analytical nonlinearity, the \\nsignals are expressed by Volterra functionals, for example. A practical learning system can then \\nbe constructed if higher kernels are neglected. For example, a cubic function can be used instead \\nof a sigmoidal function. \\n38 \\n(a) \\n3 \\nCo) \\nFig. 4. Performance of Models. (a) Model 1 with Clean Test Pattern. (b) \\nModel 2 with Clean Test Pattern. (c) Model 1 with Noisy Test Pattern. \\n(d) Model 2 with Noisy Test Pattern. Arrows indicate the ideal response \\npositions at the end of phoneme. \\n8. CONCLUSION \\nThe formal neuron was generalized to the dynamic formal neuron to recognize spario- \\ntemporal patterns. It is shown that existing learning rules can be generalized for dynamic formal \\nneurons. \\nAn artificial neural network using dynamic formal neurons was applied to classifying 30 \\nmodel phonemes with bipolar patterns created by using parameters of formant frequencies and \\ntheir bandwidths. The model operates in two stages: in the first stage, it calculates the correla- \\ntion between the input and prototype paRems stored in the transfer function matrix, and, in the \\nsecond stage, a lateral inhibition network selects the output of the phoneme pattern dose to the \\ninput pattern. \\n39 \\n(c) \\n(d) \\nFig. 4 (continued.) \\nTwo models with different transfer functions were tested. Model 1 was a matched filter \\nbank model and Model 2 was a pseudo-inverse filter model. A sequence of phoneme patterns \\ncorresponding to continuous pronunciation of digits was used as a test pattern. For the test pat- \\ntern, Model 1 missed to recognize one phoneme and responded falsely once while Model 2 \\ncorrectly recognized all the 32 phonemes in the test pattern. When the flip noise which flips the \\nsign of the pattern with the probability 0.2, Model 1 missed three phonemes and falsely \\nresponded once while Model 2 recognized all the phonemes and false-alarmed once. Both \\nmodels detected the phonems at the correct position within the continuous stream. \\nReferences \\nW. S. McCulloch and W. Pitts, \"A logical calculus of the ideas imminent in nervous \\nactivity,\" Bulletin of Mathematical Biophysics, vol. 5, pp. 115-133, 1943. \\nD. O. Hebb, The Organization of Behavior, Wiley, New York, 1949. \\n40 \\n3. D.E. Rumelhart, G. E. Hinton, and R. J. Williams, \"Learning internal representations by \\nerror propagation,\" in Parallel Distributed Processing, Vol. 1, MIT, Cambridge, 1986. \\n4. B. Widrow and M. E. Hoff, \"Adaptive switching circuits,\" Institute of Radio Engineers, \\nWestern Electronics Show and Convention, vol. Convention Record Part 4, pp. 96-104, \\n1960. \\n5. R.O. Duda and P. E. Hart, Pattern Classification and Scene Analysis, Chapter 5, Wiley, \\nNew York, 1973. \\n6. T. Kohonen, Self-organization and Associative Memory, Springer-Verlag, Berlin, 1984. \\n7. F. Rosenblatt, Principles of Neurodynamics, Spartan Books, Washington, 1962. \\n8. J.M. Varah, \"A practical examination of some numerical methods for linear discrete ill- \\nposed problems,\" SIAM Review, vol. 21, no. 1, pp. 100-111, 1979. \\n9. C. Koch, J. Marroquin, and A. Yuille, \"Analog neural networks in early vision,\" Proceed- \\nings of the National Academy of Sciences, USA, vol. 83, pp. 4263-4267, 1986. \\n10. G.O. Stone, \"An analysis of the delta rule and the learning of statistical associations,\" in \\nParallel Distributed Processing., Vol. 1, MIT, Cambridge, 1986. \\n11. B. Widrow and S. D. Steams, Adaptive Signal Processing, Prentice-Hall, Englewood \\nCliffs, 1985. \\n12. D.H. Klatt, \"Software for a cascade/parallel formant synthesizer,\" Journal of Acoustical \\nSociety of America, vol. 67, no. 3, pp. 971-995, 1980. \\n13. L.E. Arias, T. Homma, and R. J. Marks II, \"A neural network for vowel classification,\" \\nProceedings International Conference on Acoustics, Speech, and Signal Processing, 1987. \\n14. R.P. Lippman, \"An introduction to computing with neural nets,\" IEEE ASSP Magazine, \\nApril, 1987. \\n15. S. Amari and M. A. Arbib, \"Competition and cooperation in neural nets,\" in Systems Neu- \\nroscience, ed. J. Metzler, pp. 119-165, Academic Press, New York, 1977. \\n16. R.L. Watrous and L. Shastri, \"Learning acoustic features from speech data using connec- \\nfionist networks,\" Proceedings of The Ninth Annual Conference of The Cognitive Science \\nSociety, pp. 518-530, 1987. \\n17. D. Tank and J. J. Hopfield, \"Concentrating information in time: analog neural networks \\nwith applications to speech recognition problems,\" Proceedings of International Confer- \\nence on Neural Netoworks, San Diego, 1987. \\n18. J.R. Treichler, C. R. Johnson, Jr., and M. G. Larimore, Theory and Design of Adaptive \\nFilters, Chapter 5, Wiley, New York, 1987. \\n19. M Schetzen, The Volterra and Wiener Theories of Nonlinear Systems, Chapter 16, Wiley, \\nNew York, 1980. \\n20. S. Grossberg, \"Associative and competitive principles of learning,\" in Competition and \\nCooperation in Neural Nets, ed. M. A. Arbib, pp. 295-341, Springer-Verlag, New York, \\n1982. \\n21. R.J. Marks II, L. E. Atlas, J. J. Choi, S. Oh, K. F. Cheung, and D.C. Park, \"A perfor- \\nmance analysis of associative memories with nonlinearities in the correlation domain,\" \\n(submitted to Applied Optics), 1987. \\n22. D.E. Dudgeon and R. M. Mersereau, Multidimensional Digital Signal Processing, pp. \\n230-234, Prentice-Hall, Englewood Cliffs, 1984. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n","3     3         ...  41 \\nON PROPERTIES OF NETWORKS \\nOF NEURON-LIKE ELEMENTS \\nPierre Baldi* and Santosh S. Venlmtesh I \\n15 December 1987 \\nAbstract \\nThe complexity and computational capacity of multi-layered, feedforward \\nneural networks is examined. Neural networks for special purpose (structured) \\nfunctions are examined from the perspective of circuit complexity. Known re- \\nsults in complexity theory are applied to the special instance of neural network \\ncircuits, and in particular, classes of functions that can be implemented in \\nshallow circuits characterised. Some conclusions are also drawn about learning \\ncomplexity, and some open problems raised. The dual problem of determining \\nthe computational capacity of a class of multi-layered networks with dynamics \\nregulated by an algebraic Hamiltoninn is considered. Formal results are pre- \\nsented on the storage capacities of programmed higher-order structures, and \\na tradeoff between ease of programming and capacity is shown. A precise de- \\ntermination is made of the static fixed point structure of random higher-order \\nconstructs, and phase-transitions (0-1 laws) are shown. \\n1 INTRODUCTION \\nIn this article we consider two aspects of computation with neural networks. Firstly \\nwe consider the problem of the complexity of the network required to compute classes \\nof specified (structured) functions. We give a brief overview of basic known com- \\nplexity theorems for readers familiar with neural network models but less familiar \\nwith circuit complexity theories. We argue that there is considerable computational \\nand physiological justification for the thesis that shallow circuits (i.e., networks with \\nrelatively few layers) are computationally more efficient. We hence concentrate on \\nstructured (as opposed to random) problems that can be computed in shallow (con- \\nstant depth) circuits with a relatively few number (polynomial) of elements, and \\ndemonstrate classes of structured problems that are amenable to such low cost so- \\nlutions. We discuss an allied problem--the complexity of learning--and close with \\nsome open problems and a discussion of the observed limitations of the theoretical \\napproach. \\nWe next turn to a rigourous classification of how much a network of given \\nstructure can do; i.e., the computational capacity of a given construct. (This is, in \\n*Department of Mathematics, University of California (San Diego), La Jolla, CA 92093 \\ntMoore School of Electrical Engineering, University of Pennsylvania, Philadelphia, PA 19104 \\nAmerican Institute of Physics 1988 \\n42 \\na sense, the mirror image of the problem considered above, where we were seeking \\nto design a minimal structure to perform a given task.) In this article we restrict \\nourselves to the analysis of higher-order neural structures obtained from polynomial \\nthreshold rules. We demonstrate that these higher-order networks are a special class \\nof layered neural network, and present formal results on storage capacities for these \\nconstructs. Specifically, for the case of prograznmed interactions we demonstrate \\nthat the storage capacity is of the order of n d where d is the interaction order. \\nFor the case of random interactions, a type of phase trartsition is observed in the \\ndistribution of fixed points as a function of attraction depth. \\n2 COMPLEXITY \\nThere exist two broad classes of constraints on computations. \\n1. Physical constraints. These are related to the hardware in which the computa- \\ntion is embedded, and include among others time constants, energy limitations, \\nvolumes and geometrical relations in 3D space, and bandwidth capadties. \\n2. Logical constraints: These can be further subdivided into \\n Computability constraints--for instance, there exist unsolvable problems, \\ni.e., functions such as the halting problem which are not computable in \\nan absolute sense. \\n Complexity constraints--usually giving upper and/or lower bounds on \\nthe amount of resources such as the time, or the number of gates re- \\nquired to compute a given function. As an instance, the assertion \"There \\nexists an exponential time algorithm for the Traveling Salesman Prob- \\nlem,\" provides a computational upper bound. \\nIf we view brains as computational devices, it is not unreasonable to think \\nthat in the course of the evolutionary process, nature may have been faced several \\ntimes by problems related to physical and perhaps to a minor degree logical con- \\nstraints on computations. If this is the case, then complexity theory in a broad \\nsense could contribute in the future to our understanding of parallel computations \\nand architectural issues both in natural and synthetic neural systems. \\nA simple theory of parallel processing at the macro level (where the elements \\nare processors) can be developed based on the ratio of the time spent on com- \\nmunications between processors [7] for different classes of problems and different \\nprocessor architecture and interconnections. However, this approach does not seem \\nto work for parallel processing at the level of circuits, especially if calculations and \\ncommunications are intricately entangled. \\nRecent neural or connectionist models are based on a common structure, that \\nof highly interconnected networks of linear (or polynomial) threshold (or with sig- \\nmold input-output function) units with adjustable interconnection weights. We shall \\ntherefore review the complexity theory of such circuits. In doing so, it will be some- \\ntimes helpful to contrast it with the similar theory based on Boolean (AND, OR, \\nNOT) gates. The presentation will be rather informal and technical complements \\ncan easily be found in the references. \\n43 \\nConsider a circuit as being on a cyclic oriented graph connecting n Boolean \\ninputs to one Boolean output. The nodes of the graph correspond to the gates \\n(the n input units, the \"hidden\" units, and the output unit) of the circuit. The \\nsize of the circuit is the total number of gates and the depth is the length of the \\nlongest path connecting one input to the output. For a layered, feed-forward circuit, \\nthe width is the average number of computational units in the hidden (or interior) \\nlayers of elements. The first obvious thing when comparing Boolean and threshold \\nlogic is that they are equivalent in the sense that any Boolean function can be \\nimplemented using either logic. In fact, any such function can be computed in a \\ncircuit of depth two and exponential size. Simple counting arguments show that \\nthe fraction of functions requiring a circuit of exponential size approaches one as \\nn  oo in both cases, i.e., a random function will in general require an exponential \\nsize circuit. (Paradoxically, it is very difficult to construct a family of functions \\nfor which we can prove that an exponential circuit is necessary.) Yet, threshold \\nlogic is more powerful than Boolean logic. A Boolean gate can compute only one \\nfunction whereas a threshold gate can compute to the order of 2 an2 functions by \\nvarying the weights with 1/2 _< a _< 1 (see [19] for the lower bound; the upper \\nbound is a classical hyperplane counting argument, see for instance [20,30]). It \\nwould hence appear plausible that there exist wide classes of problems which can be \\ncomputed by threshold logic with circuits substantially smaller than those required \\nby Boolean logic. An important result which separates threshold and Boolean logic \\nfrom this point of view has been demonstrated by Yo [31] (see [10,24] for an elegant \\nproof). The result is that in order to compute a function such as parity in a circuit \\nof constant depth k, at least exp(cn /2k) Boolean gates with unbounded fanin are \\nrequired. As we shall demonstrate shortly, a circuit of depth two and linear size is \\nsufficient for the computation of such functions using threshold logic. \\nIt is not unusual to hear discussions about the tradeoffs between the depth \\nand the width of a circuit. We believe that one of the main constributions of \\ncomplexity analysis is to show that this tradeoff is in some sense minimal and that \\nin fact there exists a very strong bias in favor of shallow (i.e., constant depth) \\ncircuits. There are multiple reasons for this. In general, for a fixed size, the number \\nof different functions computable by a circuit of small depth exceeds the number \\nof those computable by a deeper circuit. That is, if one had no a priori knowledge \\nregarding the function to be computed and was given hidden units, then the optimal \\nstrategy would be to choose a circuit of depth two with the rn units in a single \\nlayer. In addition, if we view computations as propagating in a feedforward mode \\nfrom the inputs to the output unit, then shallow circuits compute faster. And the \\ndeeper a circuit, the more difficult become the issues of time delays, synchronisation, \\nand precision on the computations. Finally, it should be noticed that given overall \\nresponses of a few hundred milliseconds and given the known time scales for synaptic \\nintegration, biological circuitry must be shallow, at least within a \"module\" and \\nthis is corroborated by anatomical data. The relative slowness of neurons and their \\nshallow circuit architecture are to be taken together with the \"analog factor\" and \\n\"entropy factor\" [1] to understand the necessary high-connectivity requirements of \\nneural systems. \\n44 \\nFrom the previous analysis emerges an important class of circuits in threshold \\nlogic characterised by polynomial size and shallow depth. We have seen that, in \\ngeneral, a random function cannot be computed by such circuits. However, many \\ninteresting functions--the structured problems--are far from random, and it is then \\nnatural to ask what is the class of functions computable by such circuits? While \\na complete characterisation is probably difficult, there are several sub-classes of \\nstructural functions which are known to be computable in shallow poly-size circuits. \\nThe symmetric functions, i.e., functions which are invaxiant under any per- \\nmutation of the n input variables, are an important class of structured problems \\nthat can be implemented in shallow polynomial size circuits. In fact, any symmet- \\nric function can be computed by a threshold circuit of depth two and linear size; \\n(n hidden units and one output unit are always sufficient). We demonstrate the \\nvalidity of this assertion by the following instructive construction. We consider n \\nbinary inputs, each taking on values -1 and i only, and threshold gates as units. \\nNow axray the 2 n possible inputs in n -{- 1 rows with the elements in each row being \\npermuted versions of each other (i.e., n-tuples in a row all have the same number \\nof -{-1's) and with the rows going monotonically from zero +1's to n +1's. Any \\ngiven symmetric Boolean function clearly assumes the same value for all elements \\n(Boolean n-tuples) in a row, so that contiguous rows where the function assumes \\nthe value +1 form bands. (There axe at most n/2 bands--the worst case occuring \\nfor the parity function.) The symmetric function can now be computed with 2B \\nthreshold gates in a single hidden layer with the topmost \"neuron\" being activated \\nonly if the number of -{-1's in the input exceeds the number of -{-1's in the lower \\nedge of the lowest band, and proceeding systematically, the lowest \"neuron\" being \\nactivated only if the number of -{-1's in the input exceeds the number of -{- 1's in the \\nupper edge of the highest band. An input string will be within a band if and only if \\nan odd number of hidden neurons are activated starti:g contiguously from the top \\nof the hidden layer, and conversely. Hence, a single output unit can compute the \\ngiven symmetric function. \\nIt is easy to see that arithmetic operations on binary strings can be performed \\nwith polysize small depth circuits. Reif [23] has shown that for a fixed degree of pre- \\ncision, any analytic function such as polynomials, exponentials, and trigonometric \\nfunctions can be approximated with small and shallow threshold circuits. Finally, \\nin many situations one is interested in the value of a function only for a vanishingly \\nsmall (i.e., polynomial) fraction of the total number of possible inputs 2 '. These \\nfunctions can be implemented by polysize shallow circuits and one can relate the \\nsize and depths of the circuit to the cardinal of the interesting inputs. \\nSo fax we only have been concerned with the complexity of threshold circuits. \\nWe now turn to the complexity of leaxning, i.e., the problem of finding the weights \\nrequired to implement a given function. Consider the problem of repeating m points \\nin ]R t coloured in two colours, using k hyperplanes so that any region contains only \\nmonochromatic points. If t and k axe fixed the problem can be solved in polynomial \\ntime. If either t or k goes to infinity, the problem becomes NP-complete [?]. As \\na result, it is not difficult to see that the general learning problem is NP-complete \\n(see also [12] for a different proof and [21] for a proof of the fact it is already \\nNP-complete in the case of one single threshold gate). \\n45 \\nSome remarks on the limitations of the complexity approach are a propos at \\nthis juncture: \\nWhile a variety of structured Boolean functions can be implemented at rela- \\ntively low cost with networks of linear threshold gates (McCulloch-Pitts neu- \\nrons), the extension to different input-output functions and the continuous \\ndomain is not always straightforward. \\nEven restricting ourselves to networks of relatively simple Boolean devices such \\nas the linear threshold gate, in many instances, only relatively weak bounds \\naxe available for computational cost and complexity. \\no \\no \\nTime is probably the single most important ingredient which is completely \\nabsent from these threshold units and their interconnections [17,14]; there \\naxe, in addition, non-biological aspects of connectionist models [8]. \\nFinally, complexity results (where available) are often asymptotic in nature \\nand may not be meaningful in the range corresponding to a particular appli- \\ncation. \\nWe shall end this section with a few open questions and speculations. One \\nproblem has to do with the time it takes to learn. Leaxning is often seen as a \\nvery slow process both in artificial models (cf. back propagation, for instance) and \\nbiological systems (cf. human acquisition of complex skills). However, if we follow \\nthe standards of complexity theory, in order to be effective over a wide variety of \\nscales, a single learning algorithm should be polynomial time. We can therefore \\nask what is learnable by examples in polynomial time by polynomial size shallow \\nthreshold circuits? The status of back propagation type of algorithms with respect \\nto this question is not very clear. \\nThe existence of many tasks which are easily executed by biological organisms \\nand for which no satisfactory computer program has been found so far leads to the \\nquestion of the specificity of learning algorithms, i.e., whether there exists a com- \\nplexity class of problems or functions for which a \"program\" can be found only by \\nlearning from examples as opposed to by traditional programming. There is some \\ncircumstantial evidence against such conjecture. As pointed out by Valiant [25], \\ncryptography can be seen in some sense as the opposite of learning. The conjectures \\nexistence of one way function, i.e., functions which can be constructed in polyno- \\nmial time but cannot be invested (from examples) in polynomial time suggests that \\nlearning algorithms may have strict limitations. In addition, for most of the artificial \\napplications seen so far, the programs obtained through learning do not outperform \\nthe best already known software, though there may be many other reasons for that. \\nHowever, even if such a complexity class does not exist, learning algorithm may \\nstill be very important because of their inexpensiveness and generality. The work of \\nValiant [26,13] on polynomial time learning of Boolean formulas in his \"distribution \\nfree model\" explores some additional limitations of what can be learned by examples \\nwithout including any additional knowledge. \\nLearning may therefore turn out to be a powerful, inexpensive but limited \\nfamily of algorithms that need to be incorporated as \"sub-routines\" of more global \\n46 \\nprograms, the structure of which may be 'harder to find. Should evolution be re- \\ngarded as an \"exponential\" time learning process complemented by the \"polynomial\" time type of learning occurring in the lifetime of organisms? \\n3 CAPACITY \\nIn the previous section the focus of our investigation was on the structure and cost of \\nminimal networks that would compute specified Boolean functions. We now consider \\nthe dual question: What is the computational capacity of a threshold network of \\ngiven structure? As with the issues on complexity, it turns out that for fMrly general \\nnetworks, the capacity results favour shallow (but perhaps broad) circuits [29]. In \\nthis discourse, however, we shall restrict ourselves to a specified class of higher-order \\nnetworks, and to problems of associative memory. We will just quote the principal \\nrigourous results here, and present the involved proofs elsewhere [4]. \\nWe consider systems of n densely interacting threshold units each of which \\nyields an instantaneous state -1 or +1. (This corresponds in the literature to a \\nsystem of n Ising spins, or alternatively, a system of n neural states.) The state \\nspace is hence the set of vertices of the hypercube. We will in this discussion \\nalso restrict our attention throughout to symmetric interaction systems wherein the \\ninterconnections between threshold elements is bidirectional. \\nLet Zd be the family of all subsets of cardinality d+ 1 of the set {1, 2,..., n). \\nClearly ]Zd] = ( d + 1 )' For any subset I of {1,2,...,n), and for every state \\nu = (-1,1)\", set uz = rl6z \\nDefinition 1 A homogeneous algebraic threshold network of degree d is a network of \\nn \\nn threshold elements with interactions specified by a set of ( d + 1 ) real coefficients \\nwz indexed by I in Za, and the evolution rule \\n16 61 \\n(1) \\nThese systems can be readily seen to be natural generalisations to higher- \\norder of the familiar case d = i of linear threshold networks. The added degrees of \\nfreedom in the interaction coefficients can potentially result in enhanced flexibility \\nand programming capability over the linear case as has been noted independently \\nby several authors recently [2,3,4,5,22,27]. Note that each d-wise product Uz\\i is just \\nthe parity of the corresponding d inputs, and by our earlier discussion, this can be \\ncomputed with d hidden units in one layer followed by a single threshold unit. Thus \\nthe higher-order network can be realised by a network of depth three, where the first \\nhidden layer has d( n n \\nd ) units, the second hidden layer has ( d ) units, and there are \\nn output units which feedback into the n input units. Note that the weights from \\nthe input to the first hidden layer, and the first hidden layer to the second are fixed \\n47 \\n(computing the various d-wise products), and the weights from the second hidden \\nlayer to the output are the coefficients wx which are free parameters. \\nThese systems can be identified either with long range interactions for higher- \\norder spin glasses at zero temperature, or higher-order neural networks. Starting \\nfrom an arbitrary configuration or state, the system evolves asynchronously by a \\nsequence of single \"spin\" flips involving spins which are misaligned with the instan- \\ntaneous \"molecular field.\" The dynamics of these symmetric higher-order systems \\nare regulated analogous to the linear system by higher-order extensions of the clas- \\nsical quadratic Hamiltonian. We define the homogeneous algebraic Hamiltonian of \\ndegree d by \\nHd(u) =- wxm. \\nIEIa \\nThe algebraic Hamiltonians are functionals akin in behaviour to the classical \\nquadratic Hamiltonian as has been previously demonstrated [5]. \\nProposition 1 The functional Hd is non-increasing under the evolution rule 1. \\nIn the terminology of spin glasses, the state trajectories of these higher-order \\nnetworks can be seen to be following essentially a zero-temperature Monte Carlo \\n(or Glauber) dynamics. Because of the monotonicity of the algebraic Hamiltonians \\ngiven by equation 2 under the asynchronous evolution rule 1, the system always \\nreaches a stable state (fixed point) where the relation I is satisfied for each of the n \\nspins or neural states. The fixed points are hence the arbiters of system dynamics, \\nand determine the computational capacity of the system. \\nSystem behaviour and applications are somewhat different depending on \\nwhether the interactions are random or programmed. The case of random interac- \\ntions lends itself to natural extensions of spin glass formulations, while programmed \\ninteractions yield applications of higher-order extensions of neural network models. \\nWe consider the two cases in turn. \\n3.1 PROGRAMMED INTERACTIONS \\nHere we query whether given sets of binary n-vectors can be stored as fixed points \\nby a suitable selection of interaction coefficients. If such sets of prescribed vectors \\ncan be stored as stable states for some suitable choice of interaction coefficients, \\nthen proposition 1 will ensure that the chosen vectors are at the bottom of \"energy \\nwells\" in the state space with each vector exercising a region of attraction around \\nit--all characterestics of a physical associative memory. In such a situation the \\ndynamical evolution of the network can be interpreted in terms of computations: \\nerror-correction, nearest neighbour search and associative memory. Of importance \\nhere is the maximum number of states that can be stored as fixed points for an \\nappropriate choice of algebraic threshold network. This represents the maximal \\ninformation storage capacity of such higher-order neural networks. \\nLet d represent the degree of the algebraic threshold network. Let u(),..., u (m) \\nbe the m-set of vectors which we require to store as fixed points in a suitable al- \\ngebraic threshold network. We will henceforth refer to these prescribed vectors as \\n48 \\nmemories. We define the storage capacity of an algebraic threshold network of de- \\ngree d to be the maximal number m of arbitraxily chosen memories which can be \\nstored with high probability for appropriate choices of coefficients in the network. \\nTheorem 1 The maximal (algorithm independent) storage capacity of a homoge- \\nneous algebraic threshold network of degree d is less than or equal to 2 ( n \\nd)' \\nGeneralised Sum of Outer-Products Rule: The classical Hebbiazt rule for the \\nlineax case d = i (cf. [11] and quoted references) cazt be naturally extended to \\nnetworks of higher-order. The coefficients w.r, I  Zd axe constructed as the sum of \\ngeneralised Kronecker outer-products, \\nm \\na----1 \\nTheorem 2 The storage capacity of the outer-product algorithm applied to a ho- \\nmogeneous algebraic threshold network of degree d is less thazt or equal to nd/2(d + \\n1)logn (also cf. [15,27]). \\nGeneralised Spectral Rule: For d = 1 the spectral rule amounts to iteratively \\nprojecting states orthogonally onto the lineax space generated by u(),..., u(\"0, and \\nthen taking the dosest point on the hypercube to this projection (cf. [27,28]). This \\napproach can be extended to higher-orders as we now describe. \\nLet W denote the n x N(,,d) matrix of coefficients wt arranged lexicograph- \\nically; i.e., \\nW __ \\nWl,l,2,...,d-1,d Wl,2,3,...,d,d+ l ' ' ' \\nW2,1,2,...,d- l,d W2,2,3,...,d,d+ l  . \\nWn,l,2,...,d-l,d Wn,2,3,...,d,d+l  . . \\nWl,n-d+ l,n-d+ 2,...,n- l,n \\nW2,n-d+ l,n-d+ 2,...,n-l,n \\nWn,n-d + l ,n-d+ 2,...,n- l ,n \\nNote that the symmetry and the \"zero-diagonal\" nature of the interactions have \\nbeen relaxed to increase capacity. Let U be the n x m matrix of memories. Form \\nthe extended N(n,a ) x m binary matrix U = [lu() ... u(ra)], where \\n.() \\n1,2,...,d-1,d \\nu () \\nlU( ) ._ 1,2,...,d-l,d+l \\nu() \\nn-dc l,n-d+ 2,...,n- l,n \\nLet A'= dg[A0) ... A(m)] be a m x m diagonal matrix with positive diagonal terms. \\nA generalisation of the spectral algorithm for choosing coefficients yields \\nW = UAU t \\nwhere U t is the pseudo-inverse of U. \\n49 \\nTheorem 3 The storage capacity of the generalised spectral algorithm is at best \\n(d). \\n3.2 RANDOM INTERACTIONS \\nWe consider homogeneous algebraic threshold networks whose weights w.r are i.i.d., \\nfir(0, 1) random vaxiables. This is a natural generalisation to higher-order of Ising \\nspin glasses with Gaussian interactions. We will show an asymptotic estimate for \\nthe number of fixed points of the structure. Asymptotic results for the usual case \\nd = i of linear threshold networks with Gaussian interactions have been reported \\nin the literature [6,9,16]. \\nFor i = 1,...,n set \\n$ = ui  w.ru\\i . \\nI d: i6I \\nFor each n the random vaxiables S}, i = 1,..., n are identically distributed, jointly \\n2 n-1 \\nGaussian vaxiables with zero mean, and vaxiance a, = ( \\nd )' \\nDefinition 2 For any given  >_ 0, a state u 6 lB n is S-strongly stable iff S >_ firrs, \\nfor each i = 1,...,n. \\nThe case fl = 0 reverts to the usual case of fixed points. The parameter fl is \\nessentially a measure of how deep the well of attraction surrounding the fixed point \\nis. The following proposition asserts that a 0-1 law (\"phase transition\") governs \\nthe expected number of fixed points which have wells of attraction above a certain \\ndepth. Let Fd(/) be the expected number of/%strongly stable states. \\nTheorem 4 Corresponding to each fixed interaction order d there exists a positive \\nconstant  such that as n --, oo, \\nd \\nFff kd(/) 2 'cd() if/ < \\n kd(/) if/ = ! \\n0 if/> , \\nwhere ka(/) > O, and 0 < ca(/) < 1 axe parameters depending solely on/ and the \\ninteraction order d. \\n4 CONCLUSION \\nIn fine, it appears possible to design shallow, polynomial size threshold circuits \\nto compute a wide class of structured problems. The thesis that shallow circuits \\ncompute more efficiently than deep circuits is borne out. For the paxticulax case of \\n5O \\nhigher-order networks, all the garnered results appear to point in the same direction: \\nFor neural networks of fixed degree d, the maximal number of programmable states is \\nessentially of the order of nd. The total number of fixed points, however, appear to \\nbe exponential in number (at least for the random interaction case) though almost \\nall of them have constant attraction depths. \\nReferences \\n[1] Y. S. Abu-Mostafa, \"Number of synapses per neuron,\" in Analog VLSI and \\nNeural Systems, ed. C. Mead, Addison Wesley, 1987. \\n[2] P. Baldi, II. Some Contributions to the Theory of Neural Networks. Ph.D. The- \\nsis, California Insitute of Technology, June 1986. \\n[3] P. Baldi and S. S. Venkatesh, \"Number of stable points for spin glasses and \\nneural networks of higher orders,\" Phys. Rev. Lett., vol. 58, pp. 913-916, 1987. \\n[4] P. Baldi and S.S. Venkatesh, \"Fixed points of algebraic threshold networks,\" \\nin prepaxation. \\n[5] H. H. Chen, et al, \"Higher order correlation model of associative memory,\" in \\nNeural Networks for Computing. New York: AIP Conf. Proc., vol. 151, 1986. \\n[6] S. F. Edwards and F. Tanaka, \"Analytical theory of the ground state properties \\nof a spin glass: I. ising spin glass,\" Jnl. Phys. F, vol. 10, pp. 2769--2778, 1980. \\n[7] G. C. Fox and S. W. Otto, \"Concurrent Computations and the Theory of \\nComplex Systems,\" Caltech Concurrent Computation Program, March 1986. \\n[8] F. H. Grick and C. Asanuma, \"Certain aspects of the anatomy and physiology \\nof the cerebral cortex,\" in Parallel Distributed Processing, vol. 2, eds. D. E. \\nRumelhart and J. L. McCelland, pp. 333-371, MIT Press, 1986. \\n[9] D. J. Gross and M. Mezard, \"The simplest spin glass,\" Nucl. Phys., vol. B240, \\npp. 431-452, 1984. \\n[10] J. Hasted, \"Almost optimal lower bounds for small depth circuits,\" Proc. 18-th \\nACM STOC, pp. 6-20, 1986. \\n[11] J. J. Hopfield, \"Neural networks and physical sytems with emergent collective \\ncomputational abilities,\" Proc. Natl. Acad. Sci. USA, vol. 79, pp. 2554-2558, \\n1982. \\n[12] J. S. Judd, \"Complexity of connectionist learning with various node functions,\" \\nDept. of Computer and Information Science Technical Report, vol. 87-60, Univ. \\nof Massachussetts, Amherst, 1987. \\n[13] M. Kearns, M. Li, L. Pitt, and L. Valiant, \"On the learnability of Boolean \\nformulae,\" Proc. 19-th ACM STOC, 1987. \\n[14] C. Koch, T. Poggio, and V. Torre, \"Retinal ganglion cells: A functional inter- \\npretation of dendritic morphology,\" Phil. Trans. R. Soc. London, vol. B 288, \\npp. 227-264, 1982. \\n51 \\n[15] R. J. McEliece, E. C. Posner, E. R. Rodemich, and $. $. Venkatesh, \"The \\ncapacity of the Hopfield associative memory,\" IEEE Trans. Inform. Theory, \\nvol. IT-33, pp. 461-482, 1987. \\n[16] R. J. McEliece and E. C. Posner, \"The number of stable points of an infinite- \\nrange spin glass memory,\" JPL Telecomre. and Data Acquisition Progress Re- \\nport, vol. 42-83, pp. 209-215, 1985. \\n[17] C. A. Mead (ed.), Analog VLSI and Neural Systems, Addison Wesley, 1987. \\n[18] N. Megiddo, \"On the complexity of polyhedral separability,\" to appear in Jnl. \\nDiscrete and Computational Geometr7, 1987. \\n[19] S. Muroga, \"Lower bounds on the number of threshold functions,\" IEEE Trans. \\nElec. Comp., vol. 15, pp. 805-806, 1966. \\n[20] $. Muroga, Threshold Logic and its Applications, Wiley Interscience, 1971. \\n[21] V. N. Peled and B. $imeone, \"Polynomial-time algorithms for regular set- \\ncovering and threshold synthesis,\" Discr. Appl. Math., vol. 12, pp. 57-69, 1985. \\n[22] D. Psaltis and C. H. Park, \"Nonlinear discriminant functions and associative \\nmemories,\" in Neural Networks for Computing. New York: AIP Conf. Proc., \\nvol. 151, 1986. \\n[23] J. Reif, \"On threshold circuits and polynomial computation,\" preprint. \\n[24] R. $molenski, \"Algebraic methods in the theory of lower bounds for Boolean \\ncircuit complexity,\" Proc. 19-th ACM STOC, 1987. \\n[25] L. G. Valiant, \"A theory of the learnable,\" Comm. ACM, vol. 27, pp. 1134-1142, \\n1984. \\n[26] L. G. Valiant, \"Deductive learning,\" Phil. Trans. R. Soc. London, vol. A 312, \\npp. 441-446, 1984. \\n[27] S. S. Venkatesh, Linear Maps with Point Rules: Applications to Pattern Clas- \\nsiftcation and Associative Memory. Ph.D. Thesis, California Institute of Tech- \\nnology, Aug. 1986. \\n[28] S. S. Venkatesh and D. Psaltis, \"Linear and logarithmic capacities in associative \\nneural networks,\" to appear IEEE Trans. Inform. Theory. \\n[29] S. S. Venkatesh, D. Psaltis, and J. Yu, private communication. \\n[30] R. O. Winder, \"Bounds on threshold gate realisability,\" IRE Trans. Elec. \\nComp., vol. EC-12, pp. 561-564, 1963. \\n[31] A. C. C. Yao, \"Separating the poly-time hierarchy by oracles,\" Proc. 26-th \\nIEEE FOCS, pp. 1-10, 1985. \\n\n","4     4         ...  52 \\nSupervised Learning of Probability Distributions \\nby Neural Networks \\nEric B. Baum \\nJet Propulsion Laboratory, Pasadena CA 91109 \\nFrank Wilczek \\nDepartment of Physics,Harvard University, Cambridge MA 02138 \\nAbstract: \\nWe propose that the back propagation algorithm for super- \\nvised learning can be generalized, put on a satisfactory conceptual \\nfooting, and very likely made more efficient by defining the val- \\nues of the output and input neurons as probabilities and varying \\nthe synaptic weights in the gradient direction of the log likelihood, \\nrather than the 'error'. \\nIn the past thirty years many researchers have studied the \\nquestion of supervised learning in 'neural'-like networks. Recently \\na learning algorithm called 'back propagation '-4 or the 'general- \\nized delta-rule' has been applied to numerous problems including \\nthe mapping of text to phonemes 5, the diagnosis of illnesses 6 and \\nthe classification of sonar targets 7. In these applications, it would \\noften be natural to consider imperfect, or probabilistic informa- \\ntion. We believe that by considering supervised learning from this \\nslightly larger perspective, one can not only place back propaga- \\n Permanent address: Institute for Theoretical Physics, Univer- \\nsity of California, Santa Barbara CA 93106 \\nAmerican Institute of Physics 1988 \\n53 \\ntion on a more rigorous and general basis, relating it to other well \\nstudied pattern recognition algorithms, but very likely improve its \\nperformance as well. \\nThe problem of supervised learning is to model some mapping \\nbetween input vectors and output vectors presented to us by some \\nreal world phenomena. To be specific, consider the question of \\nmedical diagnosis. The input vector corresponds to the symptoms \\nof the patient; the i-th component is defined to be 1 if symptom i \\nis present and 0 if symptom i is absent. The output vector corre- \\nsponds to the illnesses, so that its j-th component is 1 if the j-th \\nillness is present and 0 otherwise. Given a data base consisting \\nof a number of diagnosed cases, the goal is to construct (learn) a \\nmapping which accounts for these examples and can be applied to \\ndiagnose new patients in a reliable way. One could hope, for in- \\nstance, that such a learning algorithm might yield an expert system \\nto simulate the performance of doctors. Little expert advice would \\nbe required for its design, which is advantageous both because ex- \\nperts' time is valuable and because experts often have extraodinary \\ndifficulty in describing how they make decisions. \\nA feedforward neural network implements such a mapping be- \\ntween input vectors and output vectors. Such a network has a set \\nof input nodes, one or several layers of intermediate nodes, and a \\nlayer of output nodes. The nodes are connected in a forward di- \\nrected manner, so that the output of a node may be connected to \\nthe inputs of nodes in subsequent layers, but closed loops do not \\noccur. See figure 1. The output of each node is assumed to be a \\nbounded semilinear function of its inputs. That is, if vj denotes \\nthe output of the j-th node and wij denotes the weight associated \\nwith the connection of the output of the j-th node to the input of \\n54 \\nthe i-th, then the i-th neuron takes value vi = g(]]j. wijvj), where \\ng is a bounded, differentiable function called the activation func- \\ntion. g(x) = 1/(1 + e-x), called the logistic function, is frequently \\nused. Given a fixed set of weights {wij}, we set the input node \\nvalues to equal some input vector, compute the value of the nodes \\nlayer by layer until we compute the output nodes, and so generate \\nan output vector. \\nFigure 1: A 5 layer network. Note bottleneck at layer 3. \\n55 \\nSuch networks have been studied because of analogies to neu- \\nrobiology, because it may be easy to fabricate them in hardware, \\nand because learning algorithms such as the Perceptron learning \\nalgorithm s, Widrow- Hoff , and backpropagation have been able \\nto choose weights wij that solve interesting problems. \\nGiven a set of input vectors s, together with associated target \\nvalues tq back propagation attempts to adjust the weights so as \\n$' \\nto minimize the error E in achieving these target values, defined as \\n(1) \\n' is the output of the j-th node when s ' is presented as \\nwhere oa. \\ninput. Back propagation starts with randomly chosen wid and \\nthen varies in the gradient direction of E until a local minimum \\nis obtained. Although only a locally optimal set of weights is ob- \\ntained, in a number of experiments the neural net so generated \\nhas performed surprisingly well not only on the training set but on \\nsubsequent data. 4- This performance is probably the main reason \\nfor widespread interest in backpropagation. \\nIt seems to us natural, in the context of the medical diagnosis \\nproblem, the other real world problems to which backpropagation \\nhas been applied, and indeed in any mapping problem where one \\ndesires to generalize from a limited and noisy set of examples, to \\ninterpret the output vector in probabilistic terms. Such an inter- \\npretation is standard in the literature on pattern classification. m \\nIndeed, the examples might even be probabilistic themselves. That \\nis to say it might not be certain whether symptom i was present \\nin case/ or not. \\nLet s represent the probability symptom i is present in case \\n' represent the probability disease j ocurred in case \\n/% and let tj \\n56 \\n/a. Consider for the moment the case where the t. are 1 or 0, \\nso that the cases are in fact fully diagnosed. Let fj(,$) be our \\nprediction of the probability of disease j given input vector , where \\n is some set of parameters determined by our learning algorithm. \\nIn the neural network case, the  are the connection weights and \\nNow lacking a priori knowledge of good 0, the best one can do \\nis to choose the parameters  to maximize the likelihood that the \\ngiven set of examples should have occurred. i The formula for this \\nlikelihood, p, is immediate: \\nor \\nlog(p) --  [  log( f j ( ', ) ) +  \\nlog(1- fj(u,))] \\nThe extension of equation (2), and thus equation (3) to the \\ncase where the  are probabilities, taking values in [0, 1], is straight- \\n57 \\nforward '1 and yields \\n(4) \\nExpressions of this sort often arise in physics and information the- \\nory and are generally interpreted as an entropy. TM \\nWe may now vary the {} in the gradient direction of the en- \\ntropy. The back propagation algorithm generalizes immediately \\nfrom minimizing 'Error' or 'Energy' to maximizing entropy or log \\nlikelihood, or indeed any other function of the outputs and the \\ninputs TM. Of course it remains true that the gradient can be com- \\nputed by back propagation with essentially the same number of \\ncomputations as are required to compute the output of the net- \\nwork. \\nA backpropagation algorithm based on log-likelihood is not \\nonly more intuitively appealing than one based on an ad-hoc def- \\ninition of error, but will make quite different and more accurate \\npredictions as well. Consider e.g. training the net on an exam- \\nple which it already understands fairly well. Say t] = 0, and \\nfj(s ) = e. Now, from eqn(1) OE/Ofj = 2e, so using 'Error' as a \\n* We may see this by constructing an equivalent larger set of \\nexamples with the  taking only values 0 or i with the appropriate \\nfrequency. Thus assume the tj are rational numbers with denomi- \\n and let p IIt,,j  What we mean by \\nt, and numerator nj \\nnator dj -- dj. \\nthe set of examples {t  / -- 1, ..., M} can be represented by con- \\nru=0 \\nsidering a set of N = Mp examples {} where for each/, tj \\n'), and r, = 1 \\nfor p(/- 1) < y <_ p/ and I <_ ymod(d) <_ (d - nj tj \\notherwise. Now applying equation (3) gives equation (4), up to an \\noverall normalization. \\n58 \\ncriterion the net learns very little from this example, whereas, us- \\ning eqn(3), Olog(p)/Ofj = 1/(1 -), so the net continues to learn \\nand can in fact converge to predict probabilities near 1. Indeed \\nbecause backpropagation using the standard 'Error' measure can \\nnot converge to generate outputs of 1 or 0, it has been custom- \\nary in the literature 4 to round the target values so that a target \\nof i would be presented in the learning algorithm as some ad hoc \\nnumber such as .8, whereas a target of 0 would be presented as .2. \\nIn the context of our general discussion it is natural to ask \\nwhether using a feedforward network and varying the weights is in \\nfact the most effective alternative. Anderson and Abrahams 3 have \\ndiscussed this issue from a Bayesian viewpoint. From this point of \\nview, fitting output to input using normal distributions and varying \\nthe means and covariance matrix may seem to be more logical. \\nFeedforward networks do however have several advantages for \\ncomplex problems. Experience with neural networks has shown the \\nimportance of including hidden units wherein the network can form \\nan internal representation of the world. If one simply uses normal \\ndistributions, any hidden variables included will simply integrate \\nout in calculating an output. It will thus be necessary to include at \\nleast third order correlations to implement useful hidden variables. \\nUnfortunately, the number of possible third order correlations is \\nvery large, so that there may be practical obstacles to such an \\napproach. Indeed it is well known folklore in curve fitting and \\npattern classification that the number of parameters must be small \\ncompared to the size of the data set if any generalization to future \\ncases is expected.  \\nIn feedforward nets the question takes a different form. There \\ncan be bottlenecks to information flow. Specifically, if the net is \\n59 \\nconstructed with an intermediate layer which is not bypassed by \\nany connections (i.e. there are no connections from layers preceding \\nto layers subsequent), and if furthermore the activation functions \\nare chosen so that the values of each of the intermediate nodes \\ntend towards either i or 0 '2, then this layer serves as a bottleneck \\nto information flow. No matter how many input nodes, output \\nnodes, or free parameters there are in the net, the output will be \\nconstrained to take on no more than 2  different patterns, where \\nI is the number of nodes in the bottleneck layer. Thus if I is \\nsmall, some sort of 'generalization' must occur even if the number \\nof weights is large. One plausible reason for the success of back \\npropagation in adequately solving tasks, in spite of the fact that \\nit finds only local minima, is its ability to vary a large number of \\nparameters. This freedom may allow back propagation to escape \\nfrom many putative traps and to find an acceptable solution. \\nA good expert system, say for medical diagnosis, should not \\nonly give a diagnosis based on the available information, but should \\nbe able to suggest, in questionable cases, which lab tests might be \\nperformed to clarify matters. Actually back propagation inher- \\nently has such a capability. Back propagation involves calculation \\nof Olog(p)/Owij. This information allows one to compute immedi- \\nately Olog(p)/Osj. Those input nodes for which this partial deriva- \\ntive is large correspond to important experiments. \\nIn conclusion, we propose that back propagation can be gen- \\neralized, put on a satisfactory conceptual footing, and very likely \\nmade more efficient, by defining the values of the output and in- \\n 2 Alternatively when necessary this can be enforced by adding \\nan energy term to the log-likelihood to constrain the parameter \\nvariation so that the neuronal values are near either I or 0. \\n6O \\nput neurons as probabilities, and replacing the 'Error' by the log- \\nlikelihood. \\nAcknowledgement: E. B. Baum was supported in part by DARPA \\nthrough arrangement with NASA and by NSF grant DMB-840649, \\n802. F. Wilczek was supported in part by NSF grant PHY82-17853 \\nReferences \\n(1)Werbos,P,\"Beyond Regression: New Tools for Prediction and \\nAnalysis in the Behavioral Sciences\", Harvard University Disserta- \\ntion (1974) \\n(2)Parker D. B.,\"Learning Logic\",MIT Tech Report TR-47, Center \\nfor Computationl Research in Economics and Management Science, \\nMIT, 1985 \\n(3)Le Cun, Y., Proceedings of Cognitiva 85,p599-604, Paris (X9S5) \\n(4)Rumelhart, D. E., Hinton, G. E., Williams, G. E., \"Learning \\nInternal Representations by Error Propagation\", in \"Parallel Dis- \\ntributed Processing\", vol 1, eds. Rumelhart, D. E., McClelland, J. \\nL., MIT Press, Cambridge MA,(1986) \\n(5)Sejnowski, T. J., Rosenberg, C. R., Complex Systems, v 1, pp \\n145-168 (1987) \\n(6)LeCun, Y., Address at 1987 Snowbird Conference on Neural \\nNetworks \\n(7)Gorman, P., Sejnowski, T. J.,\"Learned Classification of Sonar \\nTargets Using a Massively Parallel Network\", in \"Workshop on \\nNeural Network Devices and Applications\", JPLD-4406, (1987) \\npp224-237 \\n(8)Rosenblatt, F.,\"Principles of Neurodynamics: Perceptrons and \\n61 \\nthe theory of brain mechanisms\", Spartan Books, Washington DC \\n(1962) \\n(9)Widrow, B., HolT, M. E., 1960 IRE WESCON Cony. Record, \\nPart 4, 96-104 (1960) \\n(10)Duda, R. O., Hart, P. E., \"Pattern Classification and Scene \\nAnalysis\", John Wiley and Sons, N.Y., (1973) \\n(11)Guiasu, S., \"Information Theory with Applications\", McGraw \\nHill, NY, (1977) \\n(12)Baum,E.B.,\"Generalizing Back Propagation to Computation\", \\nin \"Neural Networks for Computing\", AIP Conf. Proc. 151, Snow- \\nbird UT (1986)pp47-53 \\n(13)Anderson, C.H., Abrahams, E.,\"The Bayes Connection\", Pro- \\nceedings of the IEEE International Conference on Neural Networks, \\nSan Diego,(1987) \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n","...  ..         ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n","1735  1735      ...  Coastal Navigation with Mobile Robots \\nNicholas Roy and Sebastian Thrun \\nSchool of Computer Science \\nCarnegie Mellon University \\nPittsburgh, PA 15213 \\n{ nicholas. roy I sebastian. thrun } @ cs. cmu. edu \\nAbstract \\nThe problem that we address in this paper is how a mobile robot can plan in order \\nto arrive at its goal with minimum uncertainty. Traditional motion planning algo- \\nrithms often assume that a mobile robot can track its position reliably, however, in real \\nworld situations, reliable localization may not always be feasible. Partially Observable \\nMarkov Decision Processes (POMDPs) provide one way to maximize the certainty of \\nreaching the goal state, but at the cost of computational intractability for large state \\nspaces. \\nThe method we propose explicitly models the uncertainty of the robot's position as \\na state variable, and generates trajectories through the augmented pose-uncertainty \\nspace. By minimizing the positional uncertainty at the goal, the robot reduces the \\nlikelihood it becomes lost. We demonstrate experimentally that coastal navigation \\nreduces the uncertainty at the goal, especially with degraded localization. \\n1 Introduction \\nFor an operational mobile robot, it is essential to prevent becoming lost. Early motion \\nplanners assumed that a robot would never be lost - that a robot could always know its \\nposition via dead reckoning without error [7]. This assumption proved to be untenable due \\nto the small and inevitable inconsistencies in actual robot motion; robots that rely solely on \\ndead reckoning for their position estimates lose their position quickly. Mobile robots now \\nperform position tracking using a combination of sensor data and odometry [2, 10, 5]. \\nHowever, the robot's ability to track its position can vary considerably with the robot's \\nposition in the environment. Some parts of the environment may lack good features for lo- \\ncalization [ 11 ]. Other parts of the environment can have a large number of dynamic features \\n(for example, people) that can mislead the localization system. Motion planners rarely, if \\never, take the robot's position tracking ability into consideration. As the robot's localiza- \\ntion suffers, the likelihood that the robot becomes lost increases, and as a consequence, the \\nrobot is less likely to complete the given trajectory. \\nMost localization systems therefore compensate by adding environment-specific knowl- \\nedge to the localization system, or by adding additional sensing capabilities to the robot, \\nto guarantee that the robot can complete every possible path. In general, however, such \\nalterations to the position tracking abilities of the robot have limitations, and an alternative \\nscheme must be used to ensure that the robot can navigate with maximum reliability. The \\nconventional planners represent one end of a spectrum of approaches (figure 1), in that a \\nplan can be computed easily, but at the cost of not modelling localization performance. \\nAt opposite end of the spectrum is the Partially Observable Markov Decision Process \\n1044 N. Roy and S. Thrun \\nConventional \\nPath Planner POMDP \\nTractable Intractable \\nNot Robust Robust \\nFigure 1: The continuum of possible approaches to the motion planning, from the robust but in- \\ntractable POMDP, to the potentially failure-prone but real-time conventional planners. Coastal navi- \\ngation lies in the middle of this spectrum. \\n(POMDP). POMDPs in a sense are the brass ring of planning with uncertainty; a POMDP \\npolicy will make exactly the right kind of compromise between conventional optimality \\nconsiderations and certainty of achieving the goal state. Many people have examined the \\nuse of POMDPs for mobile robot navigation [5, 6, 8]. However, computing a POMDP \\nsolution is computationally intractable (PSPACE-hard) for large state systems - a mobile \\nrobot operating in the real world often has millions of possible states. As a result, many \\nof the mobile robot POMDP solutions have made simplifying assumptions about the world \\nin order to reduce the state space size. Many of these assumptions do not scale to larger \\nenvironments or robots. In contrast, our hypothesis is that only a small number of the \\ndimensions of the uncertainty matter, and that we can augment the state with these dimen- \\nsions to approximate a solution to the POMDP. \\nThe coastal navigation model developed in this paper represents a tradeoff between robust \\ntrajectories and computational tractability, and is inspired by traditional navigation of ships. \\nShips often use the coasts of continents for navigation in the absence of better tools such \\nas GPS, since being close to the land allows sailors to determine with high accuracy where \\nthey are. The success of this method results from coast lines containing enough information \\nin their structure for accurate localization. By navigating sufficiently close to areas of the \\nmap that have high information content, the likelihood of getting lost can be minimized. \\n2 Modelling Uncertainty \\nThe problem that we address in this paper is how a mobile robot can plan in order to arrive \\nat its goal with minimum uncertainty. Throughout this discussion, we will be assuming a \\nknown map of the environment [9]. The position, x, of the robot is given as the location \\n(z, y) and direction 0, defined over a space X = (X, Y, O). Our localization method is \\na grid-based implementation of Markov localization [3, 5]. This method represents the \\nrobot's belief in its current position using a 3-dimensional grid over X - (X, Y, ), which \\nallows for a discrete approximation of arbitrary probability distributions. The probability \\nthat the robot has a particular pose x is given by the probability p(x). \\nState Augmentation We can extend the state of the robot from the 3-dimensional pose \\nspace to an augmented pose-uncertainty space. We can represent the uncertainty of the \\nrobot's positional distribution as the entropy, \\nH(px) = - fp(x)log(p(x)) \\nx \\n(1) \\nWe therefore represent the state space of the robot as the tuple \\nS = \\n: (x, (x)) \\nState Transitions In order to construct a plan between two points in the environment, \\nwe need to be able to represent the effect of the robot's sensing and moving actions. The \\nimplementation of Markov localization provides the following equations for the tracking \\nCoastal Navigation with Mobile Robots 1045 \\nthe robot's pose from x to x': \\n= f,(x'lx, (2) \\nx \\n= ap(z[x)p(x) (3) \\nThese equations are taken from [3, 12], where equation (2) gives the prediction phase of \\nlocalization (after motion u), and equation (3) gives the update phase of localization (after \\nreceiving observation z). c is a normalizing constant. We extend these equations to the \\nfourth dimension as follows: \\n3 Planning \\np(slu) : (4) \\np(sl,.) : \\nEquations (4) and (5) provide a mechanism for tracking the robot's state, and in fact contain \\nredundant information, since the extra state variable H (x) is also contained in the probabil- \\nity distribution p(x). However, in order to make the planning problem tractable, we cannot \\nin fact maintain the probabilistic sensing model. To do so would put the planning problem \\nfirmly in the domain of POMDPs, with the associated computational intractability. Instead, \\nwe make a simplifying assumption, that is, that the positional probability distribution of \\nthe robot can be represented at all times by a Gaussian centered at the mean x. This allows \\nus to approximate the positional distribution with a single statistic, the entropy. In POMDP \\nterms, we using the assumption of Gaussian distributions to compress the belief space to a \\nsingle dimension. We can now represent the positional probability distribution completely \\nwith the vector s, since the width of the Gaussian is represented by the entropy H (x). \\nMore importantly, the simplifying assumption allows us to track the state of the robot de- \\nterministically. Although the state transitions are stochastic (as in equation (4)), the obser- \\nvations are not. At any point in time, the sensors identify the true state of the system, with \\nsome certainty given by H(p(xlz)). This allows us to compress the state transitions into a \\nsingle rule: \\np(sl)- (6) \\nThe final position of the robot depends only on the motion command t and can be identified \\nby sensing z. However, the uncertainty of the pose, H(p(xlt , z)), is a function not only \\nof the motion command but also the sensing. The simplifying assumption of Gaussian \\nmodels is in general untenable for localization; however, we shall see that this assumption \\nis sufficient for the purpose of motion planning. \\nOne final modification must be made to the state transition rule. In a perfect world, it \\nwould be possible to predict exactly what observation would be made. However, it is \\nexactly the stochastic and noisy nature of real sensors that generates planning difficulty, \\nyet the update rule (6) assumes that it is possible to predict measurement z at pose x. \\nDeterministic prediction is not possible; however, it is possible to compute probabilities \\nfor sensor measurements, and thus generate an expected value for the entropy based on the \\nprobability distribution of observations Z, which leads to the final state transition rule: \\npl,) = (p(xl,),Ez[r(p(xl,,,.))]) (7) \\nwhere Ez [H(p(xl u, z))] represents the expected value of the entropy of the pose distribu- \\ntion over the space of possible sensor measurements. \\nWith the transition rule in equation (7), we can now compute the transition probabilities \\nfor any particular state using a model of the robot's motion, a model of the robot's sensor \\nand a map of the environment. The probability p(xlu) is given by a model of the robot's \\nmotion, and can be easily precomputed for each action u. The expectation term Ez [HI \\n1046 N. Roy and S. Thrun \\ncan also be precomputed for each possible state s. The precomputation of these transition \\nprobabilities is very time-intensive, because it requires simulating sensing at each state in \\nthe environment, and then computing the posterior distribution. However, as the precom- \\nputation is a one-time operation for the environment and robot, planning itself can be an \\nonline operation and is (in the limit) unaffected by the speed of computing the transition \\nprobabilities. \\n3.1 Computing Trajectories \\nWith the state update rule given in equation (7), we can now compute the optimal trajectory \\nto a particular goal. We would in fact like to compute not just the optimal trajectory from \\nthe current robot position, but the optimal action from any position in the world. If the robot \\nshould deviate from the expected trajectory for any reason (such as error in the motion, or \\ndue to low-level control constraints), interests of efficiency suggest precomputing actions \\nfor continuing to the goal, rather than continually replanning as these contingencies arise. \\nNote that the motion planning problem as we have now phrased it can be viewed as the \\nproblem of computing the optimal policy for a given problem. The Markovian, stochastic \\nnature of the transitions, coupled with the need to compute the optimal policy for all states, \\nsuggests a value iteration approach. \\nValue iteration attempts to find the policy that maximizes the long-term reward [ 1, 4]. The \\nproblem becomes one of finding the value function, J(s) which assigns a value to each \\nstate. The optimal action at each state can then be easily computed by determining the \\nexpected value of each action at each state, from the neighboring values. We use a modified \\nform of Bellman's equations to give the value of state J(s) and policy as \\nN \\nJ(si) - max[R(si)+ C(s,u)+ J(s:)] (8) \\nj=l \\nN \\nrr(si) = argmax[R(si) + C(s, u) + p(sj[s/, u). J(sj)] (9) \\nu j=l \\nBy iterating equation (8), the value function iteratively settles to a converged value over all \\nstates. Iteration stops when no state value changes above some threshold value. \\nIn the above equations, R(si) is the immediate reward at state si, p(sj [si, u) is the transition \\nprobability from state si to state sj, and C(s, u) is the cost of taking action u at state s. Note \\nthat the form of the equations is undiscounted in the traditional sense, however, the additive \\ncost term plays a similar role in that the system is penalized for policies that take longer \\ntrajectories. The cost in general is simply the distance of one step in the given direction u, \\nalthough the cost of travel close to obstacles is higher, in order to create a safety margin \\naround obstacles. The cost of an action that would cause a collision is infinite, preventing \\nsuch actions from being used. \\nThe immediate reward is localized only at the goal pose. However, the goal pose has a \\nrange of possible values for the uncertainty, creating a set of goal states, 6. In order to \\nreward policies that arrive at a goal state with a lower uncertainty, the reward is scaled \\nlinearly with goal state uncertainty. \\n/r(Xi ) __ {-- /r/(S) S( (10) \\notherwise \\nBy implementing the value iteration given in the equations (8) and (9) in a dynamic pro- \\ngram, we can compute the value function in O(nkcrit) where n is the number of states in \\nthe environment (number of positions x number of entropy levels) and kcrit is the num- \\nber of iterations to convergence. With the value function computed, we can generate the \\noptimal action for any state in O(a) time, where a is the number of actions out of each \\nstate. \\nCoastal Navigation with Mobile Robots 1047 \\n4 Experimental Results \\nFigure 2 shows the mobile robot, Minerva, used for this research. Minerva is a RWI B-18, \\nand senses using a 360  field of view laser range finder at 1  increments. \\nFigure 2: Minerva, the B-18 mobile robot used for this research, and an example environment map, \\nthe Smithsonian National Museum of American History. The black areas are the walls and obstacles. \\nNote the large sparse areas in the center of the environment. \\nAlso shown in figure 2 is an example environment,the Smithsonian National Museum of \\nAmerican History. Minerva was used to generate this map, and operated as a tour-guide in \\nthe museum for two weeks in the summer of 1998. This museum has many of the features \\nthat make localization difficult- large open spaces, and many dynamic obstacles (people) \\nthat can mislead the sensors. \\nGoal \\narti t'ositio. \\nStartingSPosition \\n.... GalaQ i S a \\n(a) Conventional \\n(b) Coastal \\n(c) Sensor Map \\nFigure 3: Two examples in the museum environment. The left trajectory is given by a conventional, \\nshortest-path planner. The middle trajectory is given by the coastal navigation planner. The black \\nareas correspond to obstacles, the dark grey areas correspond to regions where sensor information is \\navailable, the light grey areas to regions where no sensor information is available. \\nFigure 3 shows the effect of different planners in the sample environment. Panel (a) shows \\nthe trajectory of a conventional, shortest distance planner. Note that the robot moves di- \\n1048 N. Roy and S. Thrun \\nrectly towards the goal. Panel (b) shows the trajectory given by the coastal planner. In both \\nexamples, the robot moves towards an obstacle, and relocalizes once it is in sensor range of \\nthe obstacle, before moving towards the goal. These periodic relocalizations are essential \\nfor the robot to arrive at the goal with minimum positional uncertainty, and maximum reli- \\nability. Panel (c) shows the sensor map of the environment. The black areas show obstacles \\nand walls, and the light grey areas are where no information is available to the sensors, be- \\ncause all environmental features are outside the range of the sensors. The dark grey areas \\nindicate areas where the information gain from the sensors is not zero; the darker grey the \\narea, the better the information gain from the sensors. \\n20 \\n18 \\n16 \\n14 \\n12 \\n10 \\n8 \\n6 \\n4 \\n2 \\n0 \\n-2 \\n0 \\nPositional Uncertainty at Goal \\nConventional Navigation \\nCoastal Navigation \\n0'.5 i 1'.5  2.5  3'.5 ' 4'.5  \\nMaximum Range of Laser Range Sensor in Meters \\n5.5 \\nFigure 4: The performance of the coastal navigation algorithm compared to the coastal motion plan- \\nner. The graph depicts the entropy of the position probability distribution against the range of the \\nlaser sensor. Note that the coastal navigation dramatically improves the certainty of the goal position \\nwith shorter range laser sensing. \\nFigure 4 is a comparison of the average positional certainty (computed as entropy of the \\npositional probability) of the robot at its goal position, compared to the range of the laser \\nrange sensor. As the range of the laser range gets shorter, the robot can see fewer and \\nfewer environmental features - this is essentially a way of reducing the ability of the robot \\nto localize itself. The upper line is the performance of a conventional shortest-distance \\npath planner, and the lower line is the coastal planner. The coastal planner has a lower \\nuncertainty for all ranges of the laser sensor, and is substantially lower at shorter ranges, \\nconfirming that the coastal navigation has the most effect when the localization is worst. \\n5 Conclusion \\nIn this paper, we have described a particular problem of motion planning- how to guarantee \\nthat a mobile robot can reach its goal with maximum reliability. Conventional motion \\nplanners do not typically plan according to the ability of the localization unit in different \\nareas of the environment, and thus make no claims about the robustness of the generated \\ntrajectory. In contrast, POMDPs provide the correct solution to the problem of robust \\ntrajectories, however, computing the solution to a POMDP is intractable for the size of the \\nstate space for typical mobile robot environments. \\nWe propose a motion planner with an augmented state space that represents positional \\nuncertainty explicitly as an extra dimension. The motion planner then plans through pose- \\nuncertainty space, to arrive at the goal pose with the lowest possible uncertainty. This can \\nbe seen to be an approximation to a POMDP where the multi-dimensional belief space is \\nrepresented as a subset of statistics, in this case the entropy of the belief space. \\nWe have shown some experimental comparisons with a conventional motion planner. Not \\nonly did the coastal navigation generated trajectories that provided substantial improve- \\nment of the positional certainty at the goal compared to the conventional planner, but the \\nimprovement became more pronounced as the localization was degraded. \\nCoastal Navigation with Mobile Robots 1049 \\nThe model presented here, however, is not complete. The entire methodology hinges upon \\nthe assumption that the robot's probability distribution can be adequately represented by \\nthe entropy of the distribution. This assumption is valid if the distribution is restricted \\nto a uni-modal Gaussian, however, most Markov localization methods that are based on \\nthis assumption fail, because multi-modal, non-Gaussian positional distributions are quite \\ncommon for moving robots. Nonetheless, it may be that multiple uncertainty statistics \\nalong multiple dimensions (e.g., z and g/) may do a better job of capturing the uncertainty \\nsufficiently. It is an question for future work as to how many statistics can capture the \\nuncertainty of a mobile robot, and under what environmental conditions. \\nAcknowledgments \\nThe authors gratefully acknowledge the advice and collaboration of Tom Mitchell throughout the \\ndevelopment of this work. Wolfram Burgard and Dieter Fox played an instrumental role in the de- \\nvelopment of earlier versions of this work, and their involvement and discussion of this new model is \\nmuch appreciated. This work was partially funded by the Fonds pour la Formation de Chercheurs et \\nl'Aide h la Recherche (FCAR). \\nReferences \\n[1] R. Bellman. Dynamic Programming. Princeton University Press, NJ, 1957. \\n[2] W. Burgard, D. Fox, D. Hennig, and T. Schmidt. Estimating the absolute position of a mobile \\nrobot using position probability grids. In AAAI, 1996. \\n[3] D. Fox, W. Burgard, and S. Thrun. Active Markov localization for mobile robots. Robotics and \\nAutonomous Systems, 25(3-4), 1998. \\n[4] R. A. Howard. Dynamic Programming and Markov Processes. MIT, 1960. \\n[5] L. Kaelbling, A. R. Cassandra, and J. A. Kurien. Acting under uncertainty: Discrete Bayesian \\nmodels for mobile-robot navigation. In IROS, 1996. \\n[6] S. Koenig and R. Simmons. The effect of representation and knowledge on goal-directed explo- \\nration with reinforcement learning algorithms. Machine Learning Journal, 22:227-250, 1996. \\n[7] J.-C. Latombe. Robot Motion Planning. Kluwer Academic Publishers, 1991. \\n[8] S. Mahadevan and N. Khaleeli. Robust mobile robot navigation using partially-observable \\nsemi-Markov decision processes. 1999. \\n[9] H. P. Moravec and A. Elfes. High resolution maps from wide angle sonar. In ICRA, 1985. \\n[10] R. Sim and G. Dudek. Mobile robot localization from learned landmarks. In IROS, 1998. \\n[11] H. Takeda, C. Facchinetti, and J.-C. Latombe. Planning the motions of mobile robot in a sensory \\nuncertainty field. IEEE Trans. on Pattern Analysis and Machine Intelligence, 16(10), 1994. \\n[12] S. Thrun, D. Fox, and W. Burgard. A probabilistic approach to concurrent mapping and local- \\nization for mobile robots. Machine Learning, 431, 1998. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n","1736  1736      ...  Learning Factored Representations for Partially \\nObservable Markov Decision Processes \\nBrian Sallans \\nDepartment of Computer Science \\nUniversity of Toronto \\nToronto M5S 2Z9 Canada \\nGatsby Computational Neuroscience Unit* \\nUniversity College London \\nLondon WC1N 3AR U.K. \\nsallans @ cs. toronto. edu \\nAbstract \\nThe problem of reinforcement learning in a non-Markov environment is \\nexplored using a dynamic Bayesian network, where conditional indepen- \\ndence assumptions between random variables are compactly represented \\nby network parameters. The parameters are learned on-line, and approx- \\nimations are used to perform inference and to compute the optimal value \\nfunction. The relative effects of inference and value function approxi- \\nmations on the quality of the final policy are investigated, by learning to \\nsolve a moderately difficult driving task. The two value function approx- \\nimations, linear and quadratic, were found to perform similarly, but the \\nquadratic model was more sensitive to initialization. Both performed be- \\nlow the level of human performance on the task. The dynamic Bayesian \\nnetwork performed comparably to a model using a localist hidden state \\nrepresentation, while requiring exponentially fewer parameters. \\n1 Introduction \\nReinforcement learning (RL) addresses the problem of learning to act so as to maximize \\na reward signal provided by the environment. Online RL algorithms try to find a policy \\nwhich maximizes the expected time-discounted reward. They do this through experience \\nby performing sample backups to learn a value function over states or state-action pairs. \\nIf the decision problem is Markov in the observable states, then the optimal value function \\nover state-action pairs yields all of the information required to find the optimal policy for \\nthe decision problem. When complete knowledge of the environment is not available, states \\nwhich are different may look the same; this uncertainty is called perceptual aliasing [1], \\nand causes decision problems to have dynamics which are non-Markov in the perceived \\nstate. \\n* Correspondence address \\nLearning Factored Representations for POMDPs 1051 \\n1.1 Partially observable Markov decision processes \\nMany interesting decision problems are not Markov in the inputs. A partially observable \\nMarkov decision process (POMDP) is a formalism in which it is assumed that a process is \\nMarkov, but with respect to some unobserved (i.e. \"hidden\") random variable. The state of \\nthe variable at time t, denoted s t, is dependent only on the state at the previous time step and \\non the action performed. The currently-observed evidence is assumed to be independent of \\nprevious states and observations given the current state. \\nThe state of the hidden variable is not known with certainty, so a belief state is maintained \\ninstead. At each time step, the beliefs are updated by using Bayes' theorem to combine the \\nbelief state at the previous time step (passed through a model of the system dynamics) with \\nnewly observed evidence. In the case of discrete time and finite discrete state and actions, a \\nPOMDP is typically represented by conditional probability tables (CPTs) specifying emis- \\nsion probabilities for each state, and transition probabilities and expected rewards for states \\nand actions. This corresponds to a hidden Markov model (HMM) with a distinct transition \\nmatrix for each action. The hidden state is represented by a single random variable that can \\ntake on one of K values. Exact belief updates can be computed using Bayes' rule. \\nThe value function is not over the discrete state, but over the real-valued belief state. It has \\nbeen shown that the value function is piecewise linear and convex [2]. In the worst case, \\nthe number of linear pieces grows exponentially with the problem horizon, making exact \\ncomputation of the optimal value function intractable. \\nNotice that the localist representation, in which the state is encoded in a single random \\nvariable, is exponentially inefficient: Encoding r bits of information about the state of the \\nprocess requires 2 n possible hidden states. This does not bode well for the abilities of \\nmodels which use this representation to scale up to problems with high-dimensional inputs \\nand complex non-Markov structure. \\n1.2 Factored representations \\nA Bayesian network can compactly represent the state of the system in a set of random \\nvariables [3]. A two time-slice dynamic Bayesian network (DBN) represents the system at \\ntwo time steps [4]. The conditional dependencies between random variables from time t to \\ntime t + 1, and within time step t, are represented by edges in a directed acyclic graph. The \\nconditional probabilities can be stored explicitly, or parameterized by weights on edges in \\nthe graph. \\nIf the network is densely-connected then inference is intractable [5]. Approximate infer- \\nence methods include Markov chain Monte Carlo [6], variational methods [7], and belief \\nstate simplification [8]. \\nIn applying a DBN to a large problem there are three distinct issues to disentangle: How \\nwell does a parameterized DBN capture the underlying POMDP; how much is the DBN \\nhurt by approximate inference; and how good must the approximation of the value function \\nbe to achieve reasonable performance? We try to tease these issues apart by looking at the \\nperformance of a DBN on a problem with a moderately large state-space and non-Markov \\nstructure. \\n2 The algorithm \\nWe use a fully-connected dynamic sigmoid belief network (DSBN) [9], with K units at \\neach time slice (see figure 1). The random variables si are binary, and conditional proba- \\n1052 B. Sallans \\ntime t t+l \\nFigure 1: Architecture of the \\ndynamic sigmoid belief network. \\nCircles indicate random variables, \\nwhere a filled circle is observed \\nand an empty circle is unobserved. \\nSquares are action nodes, and dia- \\nmonds are rewards. \\nbilities relating variables at adjacent time-steps are encoded in action-specific weights: \\nk=l \\na t \\nwhere wik is the weight from the i th unit at time step t to the k th unit at time step t + 1, \\nassuming action a t is taken at time t. The nonlinearity is the usual sigmoid function: \\na(z) = 1/1 + exp{-z}. Note that a bias can be incorporated into the weights by clamping \\none of the binary units to 1. \\nThe observed variables are assumed to be discrete; the conditional distribution of an output \\ngiven the hidden state is multinomial and parameterized by output weights. The probability \\nof observing an output with value I is given by: \\nP(o t l[{s K exp {rkK__l \\n= = (2) \\n__1  exp { \\nwhere o t E 0 and ukl denotes the output weight from hidden unit k to output value I. \\n2.1 Approximate inference \\nInference in the fully-connected Bayesian network is intractable. Instead we use a varia- \\ntional method with a fully-factored approximating distribution: \\nK \\nP(stls*-X,a*-,o*)  Ps'  1-I/(1-/) 1-L (3) \\nk=l \\nwhere the/,t are variational parameters to be optimized. This is the standard mean-field \\napproximation for a sigmoid belief network [ 10]. The parameters/,t are optimized by iterat- \\ning the mean-field equations, and converge in a few iterations. The values of the variational \\nparameters at time t are held fixed while computing the values for step t + 1. This is \\nanalogous to running only the forward portion of the HMM forward-backward algorithm \\n[11]. \\nThe parameters of the DSBN are optimized online using stochastic gradient ascent in the \\n2 exp] } \\nlog-likelihood: \\n(4) \\nLearning Factored Representations for POMDPs 1053 \\nwhere W and U are the transition and emission matrices respectively, aw and au are \\nlearning rates, the vector/.t contains the fully-factored approximate belief state, and v is a \\nvector of zeros with a one in the o tth place. The notation [']k denotes the k th element of a \\nvector (or k  column of a matrix). \\n2.2 Approximating the value function \\nComputing the optimal value function is also intractable. If a factored state-space represen- \\ntation is appropriate, it is natural (if extreme) to assume that the state-action value function \\ncan be decomposed in the same way: \\nK \\nq(vst,a t) qF(u, *) \\nk=l \\nThis simplifying assumption is still not enough to make finding the optimal value func- \\ntion tractable. Even if the states were completely independent, each Qk would still be \\npiecewise-linear and convex, with the number of pieces scaling exponentially with the hori- \\nzon. We test two approximate value functions, a linear approximation: \\nK \\nand a quadratic approximation: \\nK \\nat) = E q3k,at tX + qk,t k + ba t \\n= + [q]t + \\n(6) \\n(7) \\nWhere (I,, Q and b are parameters of the approximations. The notation [.]i denotes the \\ni t column of a matrix, [-]- denotes matrix transpose and  denotes element-wise vector \\nmultiplication. \\nWe update each term of the factored approximation with a modified Q-learning rule [12], \\nwhich corresponds to a delta-rule where the target for input/x is r t + ? maxa Qv(lu t+x , a)' \\nqk,a t 3-- qk,a  + O k EB \\nba t t- bat + a EB \\n(8) \\nHere oz is a learning rate, ? is the temporal discount factor, and EB is the Bellman residual: \\nEB = r t + ?maxQr(/xt+, a) - Q'(txt, a t) (9) \\na \\n3 Experimental results \\nThe \"New York Driving\" task [ 13] involves navigating through slower and faster one-way \\ntraffic on a multi-lane highway. The speed of the agent is fixed, and it must change lanes to \\navoid slower cars and move out of the way of faster cars. If the agent remains in front of a \\nfaster car, the driver of the fast car will honk its horn, resulting in a reward of -1.0. Instead \\nof colliding with a slower car, the agent can squeeze past in the same lane, resulting in a \\nreward of -10.0. A time step with no horns or lane-squeezes constitutes clear progress, \\nand is rewarded with +0.1. See [13] for a detailed description of this task. \\n1054 B. Sallans \\nTable 1: Sensory input for the New York driving task \\nDimension ]Size[Values \\nHear horn 2 yes, no \\nGaze object 3 truck, shoulder, road \\nGaze speed 2 looming, receding \\nGaze distance 3 far, near, nose \\nGaze refined distance 2 far-half, near-half \\nGaze colour 6 red, blue, yellow, white, gray, tan \\nA modified version of the New York Driving task was used to test our algorithm. The \\ntask was essentially the same as described in [13], except that the \"gaze side\" and \"gaze \\ndirection\" inputs were removed. See table 1 for a list of the modified sensory inputs. \\nThe performance of a number of algorithms and approximations were measured on the task: \\na random policy; Q-learning on the sensory inputs; a model with a localist representation \\n(i.e. the hidden state consisted of a single multinomial random variable) with linear and \\nquadratic approximate value functions; the DSBN with mean-field inference and linear and \\nquadratic approximations; and a human driver. The localist representation used the linear \\nQ-learning approximation of [14], and the corresponding quadratic approximation. The \\nquadratic approximations were trained both from random initialization, and from initial- \\nization with the corresponding learned linear models (and random quadratic portion). The \\nnon-human algorithms were each trained for 100000 iterations, and in each case a constant \\nlearning rate of 0.01 and temporal decay rate of 0.9 were used. The human driver (the au- \\nthor) was trained for 1000 iterations using a simple character-based graphical display, with \\neach iteration lasting 0.5 seconds. \\nStochastic policies were used for all RL algorithms, with actions being chosen from a \\nBoltzmann distribution with temperature decreasing over time: \\n1 \\nP(atlt) - Zs exp{qv(t'at)/T (10) \\nThe DSBN had 4 hidden units per time slice, and the localist model used a multinomial \\nwith 16 states. The Q-learner had a table representation with 2160 entries. After training, \\neach non-human algorithm was tested for 20 trials of 5000 time steps each. The human was \\ntested for 2000 time steps, and the results were renormalized for comparison with the other \\nmethods. The results are shown in figure 2. All results were negative, so lower numbers \\nindicate better performance in the graph. The error bars show one standard deviation across \\nthe 20 trials. \\nThere was little performance difference between the localist representation and the DSBN \\nbut, as expected, the DSBN was exponentially more efficient in its hidden-state represen- \\ntation. The linear and quadratic approximations performed comparably, but well below \\nhuman performance. However, the DSBN with quadratic approximation was more sensi- \\ntive to initialization. When initialized with random parameter settings, it failed to find a \\ngood policy. However, it did converge to a reasonable policy when the linear portion of the \\nquadratic model was initialized with a previously learned linear model. \\nThe hidden units in the DSBN encode useful features of the input, such as whether a car \\nwas at the \"near\" or \"nose\" position. They also encode some history, such as current gaze \\ndirection. This has advantages over a simple stochastic policy learned via Q-learning: If the \\nQ-learner knows that there is an oncoming car, it can randomly select to look left or right. \\nThe DSBN systematically looks to the left, and then to the right, wasting fewer actions. \\nLearning Factored Representations for POMDPs 1055 \\n4000 \\n3500 \\n3000 \\n- 2500 \\n2000 \\n1500 \\n1000 \\n500 \\n0 \\nQCL \\nAlgorithm \\nQDR \\nFigure 2: Results on the New York \\nDriving task for nine algorithms: \\nR=random; Q=Q-learning; LC=linear \\nmultinomial; QCR=quadratic multi- \\nnomial, random init.; QCL=quadratic \\nmultinomial, linear init; LD=linear \\nDSBN; QDR=quadratic DSBN, ran- \\ndom init.; QDL=quadratic DSBN, \\nlinear init.; H=human \\nH \\n4 Discussion \\nThe DSBN performed better than a standard Q-learner, and comparably to a model with \\na localist representation, despite using approximate inference and exponentially fewer pa- \\nrmeters. This is encouraging, since an efficient encoding of the state is a prerequisite \\nfor tackling larger decision problems. Less encouraging was the value-function approxi- \\nmation: When compared to human performance, it is clear that all methods are far from \\noptimal, although again the factored approximation of the DSBN did not hurt performance \\nrelative to the localist multinomial representation. The sensitivity to initialization of the \\nquadratic approximation is worrisome, but the success of initializing from a simpler model \\nsuggests that staged learning may be appropriate, where simple models are learned and \\nused to initialize more complex models. These findings echo those of [ 14] in the context of \\nlearning a non-factored approximate value function. \\nThere are a number of related works, both in the fields of reinforcement learning and \\nBayesian networks. We use the sigmoid belief network mean-field approximation given \\nin [10], and discussed in the context of time-series models (the \"fully factored\" approxi- \\nmation) in [ 15]. Approximate inference in dynamic Bayesian networks has been discussed \\nin [15] and [8]. The additive factored value function was used in the context of factored \\nMDPs (with no hidden state) in [16], and the linear Q-learning approximation was given \\nin [ 14]. Approximate inference was combined with more sophisticated value function ap- \\nproximation in [17]. To our knowledge, this is the first attempt to explore the practicality \\nof combining all of these techniques in order to solve a single problem. \\nThere are several possible extensions. As described above, the representation learned by \\nthe DSBN is not tuned to the task at hand. The reinforcement information could be used \\nto guide the learning of the DSBN parameters[18, 13]. Also, if this were done, then the \\nreinforcement signals would provide additional evidence as to what state the POMDP is in, \\nand could be used to aid inference. More sophisticated function approximation could be \\nused [ 17]. Finally, although this method appears to work in practice, there is no guarantee \\nthat the reinforcement learning will converge. We view this work as an encouraging first \\nstep, with much further study required. \\n5 Conclusions \\nWe have shown that a dynamic Bayesian network can be used to construct a compact rep- \\nresentation useful for solving a decision problem with hidden state. The parameters of the \\nDBN can be learned from experience. Learning occurs despite the use of simple value- \\n1056 B. Sallans \\nfunction approximations and mean-field inference. Approximations of the value function \\nresult in good performance, but are clearly far from optimal. The fully-factored assump- \\ntions made for the belief state and the value function do not appear to impact performance, \\nas compared to the non-factored model. The algorithm as presented runs entirely on-line \\nby performing \"forward\" inference only. There is much room for future work, including \\nimproving the utility of the factored representation learned, and the quality of approximate \\ninference and the value function approximation. \\nAcknowledgments \\nWe thank Geoffrey Hinton, Zoubin Ghahramani and Andy Brown for helpful discussions, \\nthe anonymous referees for valuable comments and criticism, and particularly Peter Dayan \\nfor helpful discussions and comments on an early draft of this paper. This research was \\nfunded by NSERC Canada and the Gatsby Charitable Foundation. \\nReferences \\n[11] \\n[12] \\n[13] \\n[14] \\n[15] \\n[16] \\n[17] \\n[18] \\n[1] S.D. Whitehead and D.H. Ballard. Learning to perceive and act by trial and error. Machine \\nLearning, 7, 1991. \\n[2] E.J. Sondik. The optimal control of partially observable Markov processes over the infinite \\nhorizon: Discounted costs. Operations Research, 26:282-304, 1973. \\n[3] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Mor- \\ngan Kaufmann, San Mateo, CA, 1988. \\n[4] T Dean and K. Kanazawa. A model for reasoning about persistence and causation. Computa- \\ntional Intelligence, 5, 1989. \\n[5] Gregory F. Cooper. The computational complexity of probabilistic inference using Bayesian \\nbelief networks. Artificial Intelligence, 42:393-405, 1990. \\n[6] R.M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report \\nCRG-TR-93-1, Department of Computer Science, University of Toronto, 1993. \\n[7] M.I. Jordan, Z. Ghahramani, T.S. laakkola, and L.K. Saul. An introduction to variational meth- \\nods for graphical models. Machine Learning, 1999. in press. \\n[8] X. Boyen and D. Koller. Tractable inference for complex stochastic processes. In Proc. UAI'98, \\n1998. \\n[9] R.M. Neal. Connectionist learning of belief networks. Artificial Intelligence, 56:71-113, 1992. \\n[10] L. K. Saul, T Jaakkola, and M. I. Jordan. Mean field theory for sigmoid belief networks. \\nJournal of Artificial Intelligence Research, 4:61-76, 1996. \\nLawrence R. Rabiner and Biing-Hwang Juang. An introduction to hidden Markov models. \\nIEEEASSAP Magazine, 3:4-16, January 1986. \\nC.J.C.H. Watkins and P. Dayan. Q-learning. Machine Learning, 8:279-292, 1992. \\nA.K. McCallurn. Reinforcement learning with selective perception and hidden state. Dept. of \\nComputer Science, Universiy of Rochester, Rochester NY, 1995. Ph.D. thesis. \\nM.L. Littman, A.R. Cassandra, and L.P. Kaelbling. Learning policies for partially observable \\nenvironments: Scaling up. In Proc. International Conference on Machine Learning, 1995. \\nZ. Ghahramani and M. I. Jordan. Factorial hidden Markov models. Machine Learning, 1997. \\nD. Koller and R. Parr. Computing factored value functions for policies in structured MDPs. In \\nProc. IJCAI'99, 1999. \\nA. Rodriguez, R. Parr, and D. Koller. Reinforcement learning using approximate belief states. In \\nS. A. Solla, T. K. Leen, and K.-R. Mtiller, editors, Advances in Neural Information Processing \\nSystems, volume 12. The MIT Press, Cambridge, 2000. \\nL. Chrisman. Reinforcement learning with perceptual aliasing: The perceptual distinctions \\napproach. In Tenth National Conference on AI, 1992. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n","1737  1737      ...  Policy Gradient Methods for \\nReinforcement Learning with Function \\nApproximation \\nRichard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour \\nAT&T Labs - Research, 180 Park Avenue, Florham Park, NJ 07932 \\nAbstract \\nFunction approximation is essential to reinforcement learning, but \\nthe standard approach of approximating a value function and deter- \\nmining a policy from it has so far proven theoretically intractable. \\nIn this paper we explore an alternative approach in which the policy \\nis explicitly represented by its own function approximator, indepen- \\ndent of the value function, and is updated according to the gradient \\nof expected reward with respect to the policy parameters. Williams's \\nREINFORCE method and actor-critic methods are examples of this \\napproach. Our main new result is to show that the gradient can \\nbe written in a form suitable for estimation from experience aided \\nby an approximate action-value or advantage function. Using this \\nresult, we prove for the first time that a version of policy iteration \\nwith arbitrary differentiable function approximation is convergent to \\na locally optimal policy. \\nLarge applications of reinforcement learning (RL) require the use of generalizing func- \\ntion approximators such neural networks, decision-trees, or instance-based methods. \\nThe dominant approach for the last decade has been the value-function approach, in \\nwhich all function approximation effort goes into estimating a value function, with \\nthe action-selection policy represented implicitly as the \"greedy\" policy with respect \\nto the estimated values (e.g., as the policy that selects in each state the action with \\nhighest estimated value). The value-function approach has worked well in many appli- \\ncations, but has several limitations. First, it is oriented toward finding deterministic \\npolicies, whereas the optimal policy is often stochastic, selecting different actions with \\nspecific probabilities (e.g., see Singh, Jaakkola, and Jordan, 1994). Second, an arbi- \\ntrarily small change in the estimated value of an action can cause it to be, or not be, \\nselected. Such discontinuous changes have been identified as a key obstacle to estab- \\nlishing convergence assurances for algorithms following the value-function approach \\n(Bertsekas and Tsitsiklis, 1996). For example, Q-learning, Sarsa, and dynamic pro- \\ngramming methods have all been shown unable to converge to any policy for simple \\nMDPs and simple function approximators (Gordon, 1995, 1996; Baird, 1995; Tsit- \\nsiklis and van Roy, 1996; Bertsekas and Tsitsiklis, 1996). This can occur even if the \\nbest approximation is found at each step before changing the policy, and whether the \\nnotion of \"best\" is in the mean-squared-error sense or the slightly different senses of \\nresidual-gradient, temporal-difference, and dynamic-programming methods. \\nIn this paper we explore an alternative approach to function approximation in RL. \\n1058 R. S. Sutton, D. McAllester, S. Singh and Y. Mansour \\nRather than approximating a value function and using that to compute a determinis- \\ntic policy, we approximate a stochastic policy directly using an independent function \\napproximator with its own parameters. For example, the policy might be represented \\nby a neural network whose input is a representation of the state, whose output is \\naction selection probabilities, and whose weights are the policy parameters. Let 0 \\ndenote the vector of policy parameters and p the performance of the corresponding \\npolicy (e.g., the average reward per step). Then, in the policy gradient approach, the \\npolicy parameters are updated approximately proportional to the gradient: \\nOp \\nA  a, (1) \\nwhere a is a positive-definite step size. If the above can be achieved, then 0 can \\nusually be assured to converge to a locally optimal policy in the performance measure \\np. Unlike the value-function approach, here small changes in 0 can cause only small \\nchanges in the policy and in the state-visitation distribution. \\nIn this paper we prove that an unbiased estimate of the gradient (1) can be obtained \\nfrom experience using an approximate value function satisfying certain properties. \\nWilliams's (1988, 1992) REINFORCE algorithm also finds an unbiased estimate of \\nthe gradient, but without the assistance of a learned value function. REINFORCE \\nlearns much more slowly than RL methods using value functions and has received \\nrelatively little attention. Learning a value function and using it to reduce'the variance \\nof the gradient estimate appears to be essential for rapid learning. Jaakkola, Singh \\nand Jordan (1995) proved a result very similar to ours for the special case of function \\napproximation corresponding to tabular POMDPs. Our result strengthens theirs and \\ngeneralizes it to arbitrary differentiable function approximators. Konda and Tsitsiklis \\n(in prep.) independently developed a very simialr result to ours. See also Baxter and \\nBartlett (in prep.) and Marbach and Tsitsiklis (1998). \\nOur result also suggests a way of proving the convergence of a wide variety of algo- \\nrithms based on \"actor-critic\" or policy-iteration architectures (e.g., Barto, Sutton, \\nand Anderson, 1983; Sutton, 1984; Kimura and Kobayashi, 1998). In this paper we \\ntake the first step in this direction by proving for the first time that a version of \\npolicy iteration with general differentiable function approximation is convergent to \\na locally optimal policy. Baird and Moore (1999) obtained a weaker but superfi- \\ncially similar result for their VAPS family of methods. Like policy-gradient methods, \\nVAPS includes separately parameterized policy and value functions updated by gra- \\ndient methods. However, VAPS methods do not climb the gradient of performance \\n(expected long-term reward), but of a measure combining performance and value- \\nfunction accuracy. As a result, VAPS does not converge to a locally optimal policy, \\nexcept in the case that no weight is put upon value-function accuracy, in which case \\nVAPS degenerates to REINFORCE. Similarly, Gordon's (1995) fitted value iteration \\nis also convergent and value-based, but does not find a locally optimal policy. \\nI Policy Gradient Theorem \\nWe consider the standard reinforcement learning framework (see, e.g., Sutton and \\nBarto, 1998), in which a learning agent interacts with a Markov decision process \\n(MDP). The state, action, and reward at each time t 6 {0, 1, 2,...} are denoted st  \\n$, at  .A, and rt   respectively. The environment's dynamics are characterized by \\nstate transition probabilities, Pa s, = Pr {St+l = s' I st = s, at = a}, and expected re- \\nwards 7Z] = E {rt+l I st = s, at = a}, Vs, s   $, a  .A. The agent's decision making \\nprocedure at each time is characterized by a policy, r(s, a, 0) = Pr {at = alst = s, 0}, \\nVs  $, a  jI, where 0  t, for I << I$1, is a parameter vector. We assume that r \\nis diffentiable with respect to its parameter, i.e., that o exists. We also usually \\nwrite just r(s,a) for r(s,a,O). \\nPolicy Gradient Methods for RL with Function Approximation 1059 \\nWith function approximation, two ways of formulating the agent's objective are use- \\nful. One is the average reward formulation, in which policies are ranked according to \\ntheir long-term expected reward per step, p(r): \\np(r) = lim 1-E{rl + r2 +... + rn [r} = Z d(s) Z r(s,a)7, \\nwhere d  (s) = limt_ Pr (st = s[so, r) is the stationary distribution of states under \\nr, which we assume exists and is independent of so for all policies. In the average \\nreward formulation, the value of a state-action pair given a policy is defined as \\nao=a,r ), Vse$,aeA. \\nt----1 \\nThe second formulation we cover is that in which there is a designated start state \\nso, and we care only about the long-term reward obtained from it. We will give our \\nresults only once, but they will apply to this formulation as well under the definitions \\np(r) = E 7t-lrt SO,r and Qr(s,a) = E 7k-lrt+k st = s, at = a,r . \\nt=l Xk=l \\nwhere 7 6 [0, 1] is a discount rate (7 = I is allowed only in episodic tasks). In this \\nformulation, we define dr(s) as a discounted weighting of states encountered starting \\nat so and then following r: dr(s) = --o 7 tPr (st- S[So,'). \\nOur first result concerns the gradient of the performance metric with respect to the \\npolicy parameter: \\nTheorem I (Policy Gradient). For any MDP, in either the average-reward or \\nstart-state formulations, \\nOp Or( s, a) \\noo = a's) oo (2) \\nProof: See the appendix. \\nThis way of expressing the gradient was first discussed for the average-reward formu- \\nlation by Marbach and Tsitsiklis (1998), based on a related expression in terms of the \\nstate-value function due to Jaakkola, Singh, and Jordan (1995) and Cao and Chen \\n(1997). We extend their results to the start-state formulation and provide simpler \\nand more direct proofs. Williams's (1988, 1992) theory of REINFORCE algorithms \\ncan also be viewed as implying (2). In any event, the key aspect of both expressions \\nfor the gradient is that their are no terms of the form oa._s: the effect of policy \\nchanges on the distribution of states does not appear. This is convenient for approxi- \\nmating the gradient by sampling. For example, if s was sampled from the distribution \\n(8'a)O(s,a) would be an unbiased estimate of \\nobtained by following r, then -]-a oe ' \\n-e Of course, Q (s, a) is also not normally known and must be estimated. One ap- \\n00' \\nproach is to use the actual returns, Rt oo oo \\n-- Ek=i l't+k -- p(w) (or Re = Ek=l vk-iwt+k \\nin the start-state formulation) as an approximation for each Qr(st, at). This leads to \\nWilliams's episodic REINFORCE algorithm, A0t oc o(,,)Re  (the 1 \\ncorrects for the oversampling of actions preferred by r), which is known to follow  \\nin expected value (Williams, 1988, 1992). \\n2 Policy Gradient with Approximation \\nNow consider the case in which Q is approximated by a learned function approxima- \\ntor. If the approximation is sufficiently good, we might hope to use it in place of Q \\n1060 R. S. Sutton, D. Mc,41lester, S. Singh and Y. Mansour \\nin (2) and still point roughly in the direction of the gradient. For example, Jaakkola, \\nSingh, and Jordan (1995) proved that for the special case of function approximation \\narising in a tabular POMDP one could assure positive inner product with the gra- \\ndient, which is sufficient to ensure improvement for moving in that direction. Here \\nwe extend their result to general function approximation and prove equality with the \\ngradient. \\nLet fw: $ x j[ - R be our approximation to Qx, with parameter w. It is natural \\na rtr / s \\nto learn fw by following rr and updating w by a rule such as Awt o<  [  t, at) - \\n 'fw(8\"aO where Or(st, at) is some unbiased \\nf(st,at)]   [O(st,at)- f(st,-tj o , \\nestimator of Qr(st, at), perhaps Pu. When such a process has converged to a local \\noptimum, then \\n d'r(s) E '(s'a)[ Q'r(s'a) - fu,(s,a)] Ofu,(s,a) \\nOw - O. \\n(s) \\nTheorem 2 (Policy Gradient with Function Approximation). If fw satisfies \\n(3) and is compatible with the policy parameterization in the sense that I \\nOfu,(s,a) O'(s,a) I \\nOw O0 r(s,a) ' \\nthen \\nOp &r( s, a) \\n00 = d(s)  S(s,a). \\n(4) \\n(5) \\nProof: Combining (3) and (4) gives \\n&r(s,a) \\noo - = 0 \\n(6) \\nwhich tells us that the error in fu(s, a) is orthogonal to the gradient of the policy \\nparameterization. Because the expression above is zero, we can subtract it from the \\npolicy gradient theorem (2) to yield \\nOp &r( s, a) \\nO0 = y\" dr(s) y\" O0 qr(s'a) - E dr(s) Y O'(s,a) \\n00 [(s,)- w(s,)] \\n&r(s,a) [Q=(s,a) - Q=(s,a) + f,,(s,a)] \\n&r( s, a) \\n= d(s) oo /(s,a). q..V. \\n3 Application to Deriving Algorithms and Advantages \\nGiven a policy parameterization, Theorem 2 can be used to derive an appropriate \\nform for the value-function parameterization. For example, consider a policy that is \\na Gibbs distribution in a linear combination of features: \\nEb eOTq ' \\nVs 6 $, s  A, \\nTsitsiklis (personal communication) points out that f being linear in the features given \\non the righthand side may be the only way to satisfy this condition. \\nPolicy Gradient Methods for RL with Function Approximation 1061 \\nwhere each qbsa is an/-dimensional feature vector characterizing state-action pair s, a. \\nMeeting the compatibility condition (4) requires that \\nOfw(s,a) Or(s,a) 1 y.r(s,b)qbsb, \\n0w = b \\nso that the natural parameterization of fw is \\nIn other words, fw must be linear in the same features as the policy, except normalized \\nto be mean zero for each state. Other algorithms can easily be derived for a variety \\nof nonlinear policy parameterizations, such as multi-layer backpropagation networks. \\nThe careful reader will have noticed that the form given above for f requires \\nthat it have zero mean for each state: -.ar(s,a)fw(s,a) = O, s  $. In this \\nsense it is better to think of fw as an approximation of the advantage function, \\nAr(s,a) = Qr(s,a) - Vr(S) (much as in Baird, 1993), rather than of Qx. Our \\nconvergence requirement (3) is really that fw get the relative value of the ac- \\ntions correct in each state, not the absolute value, nor the variation from state to \\nstate. Our results can be viewed as a justification for the special status of advan- \\ntages as the target for value function approximation in RL. In fact, our (2), (3), \\nand (5), can all be generalized to include an arbitrary function of state added to \\nthe value function or its approximation. For example, (5) can be generalized to \\no_ _ sdr(s) --a O [fw(s,a) + v(s)] ,where v' $ - R is an arbitrary function. \\no0-- \\n(This follows immediately because a -0'a -- 0, s  $.) The choice of v does not \\naffect any of our theorems, but can substantially affect the variance of the gradient \\nestimators. The issues here are entirely analogous to those in the use of reinforce- \\nment baselines in earlier work (e.g., Williams, 1992; Dayan, 1991; Sutton, 1984). In \\npractice, v should presumably be set to the best available approximation of V . Our \\nresults establish that that approximation process can proceed without affecting the \\nexpected evolution of f and r. \\n4 Convergence of Policy Iteration with Function Approximation \\nGiven Theorem 2, we can prove for the first time that a form of policy iteration with \\nfunction approximation is convergent to a locally optimal policy. \\nTheorem 3 (Policy Iteration with Function Approximation). Let rr \\nand fw be any differentiable function approximators for the policy and value \\nfunction respectively that satisfy the compatibility condition (4) and for which \\nmax0,,.,i,j I ooaoj I < B < oo. Let {rk}=o be any step-size sequence such that \\nlimkm ak = 0 and }-'--k ak = oo. Then, for any MDP with bounded rewards, the \\nsequence {p(rrk)}k=o , defined by any 0o, rk = r(.,., Ok), and \\nwk = w such that y.d(s) y.rk(s,a)[Q'*(s,a) - f(s,a)]Of- 'a) = 0 \\nOk+ 1 ---- 0 k q-Otk ydW($) y o7rk($'a) \\nOO \\nconverges such that limk_m o__ _- 0. \\nProof: Our Theorem 2 assures that the 0k update is in the direction of the gradient. \\n(8'a) and on the MDP's rewards together assure us that 0____ \\nThe bounds on oo, ooj ao,ao \\n1062 R. S. Sutton, D. McAllester, S. Singh and Y. Mansour \\nis also bounded. These, together with the step-size requirements, are the necessary \\nconditions to apply Proposition 3.5 from page 96 of Bertsekas and Tsitsildis (1996), \\nwhich assures convergence to a local optimum. Q.E.D. \\nAcknowledgements \\nThe authors wish to thank Martha Steenstrup and Doina Precup for comments, and Michael \\nKearns for insights into the notion of optimal policy under function approximation. \\nReferences \\nBaird, L. C. (1993). Advantage Updating. Wright Lab. Technical Report WL-TR-93-1146. \\nBaird, L. C. (1995). Residual algorithms: Reinforcement learning with function approxima- \\ntion. Proc. of the Twelfth Int. Conf. on Machine Learning, pp. 30-37. Morgan Kaufmann. \\nBaird, L. C., Moore, A. W. (1999). Gradient descent for general reinforcement learning. \\nNIPS 11. MIT Press. \\nBarto, A. G., Sutton, R. S., Anderson, C. W. (1983). Neuronlike elements that can solve \\ndifficult learning control problems. IEEE Trans. on Systems, Man, and Cybernetics 3:835. \\nBaxter, J., Bartlett, P. (in prep.) Direct gradient-based reinforcement learning: I. Gradient \\nestimation algorithms. \\nBertsekas, D. P., Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific. \\nCao, X.-R., Chen, H.-F. (1997). Perturbation realization, potentials, and sensitivity analysis \\nof Markov Processes, IEEE Trans. on Automatic Control J(10):1382-1393. \\nDayan, P. (1991). Reinforcement comparison. In D. S. Touretzky, J. L. Elman, T. J. Se- \\njnowski, and G. E. Hinton (eds.), Connectionist Models: Proceedings of the 1990 Summer \\nSchool, pp. 45-51. Morgan Kaufmann. \\nGordon, G. J. (1995). Stable function approximation in dynamic programming. Proceedings \\nof the Twelfth Int. Conf. on Machine Learning, pp. 261-268. Morgan Kaufmann. \\nGordon, G. J. (1996). Chattering in SARSA(A). CMU Learning Lab Technical Report. \\nJaakkola, T., Singh, S. P., Jordan, M. I. (1995) Reinforcement learning algorithms for par- \\ntially observable Markov decision problems, NIPS 7, pp. 345-352. Morgan Kaufman. \\nKimura, H., Kobayashi, S. (1998). An analysis of actor/critic algorithms using eligibility \\ntraces: Reinforcement learning with imperfect value functions. Proc. ICML-98, pp. 278-286. \\nKonda, V. R., Tsitsiklis, J. N. (in prep.) Actor-critic algorithms. \\nMarbach, P., Tsitsiklis, J. N. (1998) Simulation-based optimization of Markov reward pro- \\ncesses, technical report LIDS-P-2411, Massachusetts Institute of Technology. \\nSingh, S. P., Jaakkola, T., Jordan, M. I. (1994). Learning without state-estimation in \\npartially observable Markovian decision problems. Proc. ICML-9J, pp. 284-292. \\nSutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. Ph.D. thesis, \\nUniversity of Massachusetts, Amherst. \\nSutton, R. S., Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press. \\nTsitsiklis, J. N. Van Roy, B. (1996). Feature-based methods for large scale dynamic pro- \\ngramming. Machine Learning 22:59-94. \\nWilliams, R. J. (1988). Toward a theory of reinforcement-learning connectionist systems. \\nTechnical Report NU-CCS-88-3, Northeastern University, College of Computer Science. \\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist \\nreinforcement learning. Machine Learning 8:229-256. \\nAppendix: Proof of Theorem 1 \\nWe prove the theorem first for the average-reward formulation and then for the start- \\nstate formulation. \\nOV' (s) 0 \\nO0 -- OOZW(s,a)Q'(s,a) Vse$ \\na \\n: 'Ols, a'ls, \\nO0 \\nPolicy Gradient Methods for RL with Function Approximation 1063 \\nTherefore, \\n0-7 = [ oo q'(,,a) + (,,a)  \\nS t \\nSumming both sides over the stationary distribution d , \\nbut since d  is stationary, \\nOp \\n$ \\nO(s,a)  ,, ou(s ') \\n= d(s) oo q(s,a) + d (s   \\n$ a S t \\n$ \\n0__ 0(s, \\no0 = d(s) o0 )(s,). q.s.. \\nFor the sta-state formulation: \\nOVa(s) df 0 \\noo = oo(s,)(s,) \\n=  'O(s,)(s,) + (s,) \\no0 (s,) + (s,) ,p2,, U(s') (7) \\n O(,a) \\n= (3,,) oo \\nx k=0 a \\naer several steps of unrolling (7), where Pr(s  x, k, ) is the probability of going \\nkom state s to state x in k steps under policy . It is then immiate that \\nt=l \\n O(s,a) -(s,a) \\ns k=O a \\n0(s, a) \\n= :(s) oo q(s,a). \\nQ.E.D. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n","1738  1738      ...  Monte Carlo POMDPs \\nSebastian Thrun \\nSchool of Computer Science \\nCarnegie Mellon University \\nPittsburgh, PA 15213 \\nAbstract \\nWe present a Monte Carlo algorithm for learning to act in partially observable \\nMarkov decision processes (POMDPs) with real-valued state and action spaces. \\nOur approach uses importance sampling for representing beliefs, and Monte Carlo \\napproximation for belief propagation. A reinforcement learning algorithm, value \\niteration, is employed to learn value functions over belief states. Finally, a sample- \\nbased version of nearest neighbor is used to generalize across states. Initial \\nempirical results suggest that our approach works well in practical applications. \\n1 Introduction \\nPOMDPs address the problem of acting optimally in partially observable dynamic environ- \\nment [6]. In POMDPs, a learner interacts with a stochastic environment whose state is only \\npartially observable. Actions change the state of the environment and lead to numerical \\npenalties/rewards, which may be observed with an unknown temporal delay. The learner's \\ngoal is to devise a policy for action selection that maximizes the reward. Obviously, the \\nPOMDP framework embraces a large range of practical problems. \\nPast work has predominately studied POMDPs in discrete worlds [ 1 ]. Discrete worlds have \\nthe advantage that distributions over states (so-called \"belief states\") can be represented \\nexactly, using one parameter per state. The optimal value function (for finite planning \\nhorizon) has been shown to be convex and piecewise linear [10, 14], which makes it \\npossible to derive exact solutions for discrete POMDPs. \\nHere we are interested in POMDPs with continuous state and action spaces, paying tribute \\nto the fact that a large number of real-world problems are continuous in nature. In general, \\nsuch POMDPs are not solvable exactly, and little is known about special cases that can be \\nsolved. This paper proposes an approximate approach, the MC-POMDP algorithm, which \\ncan accommodate real-valued spaces and models. The central idea is to use Monte Carlo \\nsampling for belief representation and propagation. Reinforcement learning in belief space \\nis employed to learn value functions, using a sample-based version of nearest neighbor \\nfor generalization. Empirical results illustrate that our approach finds to close-to-optimal \\nsolutions efficiently. \\n2 Monte Carlo POMDPs \\n2.1 Preliminaries \\nPOMDPs address the problem of selection actions in stationary, partially observable, con- \\ntrollable Markov chains. To establish the basic vocabulary, let us define: \\n State. At any point in time, the world is in a specific state, denoted by :c. \\nMonte Carlo POMDPs 1065 \\n Action. The agent can execute actions, denoted a. \\n Observation. Through its sensors, the agent can observe a (noisy) projection of the \\nworld's state. We use o to denote observations. \\n Reward. Additionally, the agent receives rewards/penalties, denoted R  . To \\nsimplify the notation, we assume that the reward is part of the observation. More \\nspecifically, we will use R(o) to denote the function that \"extracts\" the reward from \\nthe observation. \\nThroughout this paper, we use the subscript t to refer to a specific point in time (e.g., st \\nrefers to the state at time t). \\nPOMDPs are characterized by three probability distributions: \\n1. The initial distribution, '(x) :-- Pr(zo), specifies the initial distribution of states at \\ntime t -- 0. \\n2. The next state distribution,/_t(z I a,) := Pt(oct - vc [ at_l '- a, vct_l -- \\ndescribes the likelihood that action a, when executed at state , leads to state . \\n3. The perceptual distribution, v(o [ ) := Pt(or = o[ct - ), describes the likeli- \\nhood of observing o when the world is in state \\nA history is a sequence of states and observations. For simplicity, we assume that actions \\nand observations are alternated. We use dt to denote the history leading up to time t: \\ndt :- {ot, at-l,Ot-l, at-2,...,ao, oo} (1) \\nThe fundamental problem in POMDPs is to devise a policy for action selection that maxi- \\nmizes reward. A policy, denoted \\ncr  d > a (2) \\nis a mapping from histories to actions. Assuming that actions are chosen by a policy or, \\neach policy induces an expected cumulative (and possibly discounted by a discount factor \\n? _< 1) reward, defined as \\n: s (3) \\nHere E[ ] denotes the mathematical expectation. The POMDP problem is, thus, to find a \\npolicy or* that maximizes J\", i.e., \\nor* -- argmax J\" (4) \\no' \\n2.2 Belief States \\nTo avoid the difficulty of learning a function with unbounded input (the history can be \\narbitrarily long), it is common practice to map histories into belief states, and learn a \\nmapping from belief states to actions instead [10]. \\nFormally, a belief state (denoted 0) is a probability distribution over states conditioned on \\npast actions and observations: \\nOt- Pr(xt I dt) - Pr(xt I ot,at_l,...,oo) (5) \\nBelief are computed incrementally, using knowledge of the POMDP's defining distributions \\n', p, and ,. Initially \\n00 : r \\nFor t > O, we obtain \\nOt+l = Pr(xt+l l ot+l,at,...,oo) \\n= o Pr(ot+l \\n= o Pr(ot+l \\n= o Pr(Ot+l \\n(6) \\n(7) \\n(8) \\no0) -r(xt+l I at,..., o0) \\nzt+) f Pr(zt+l l at,...,oo, zt) Pr(zt l at,...,oo) dzt (9) \\nct+) j Pr(oet+l l at,ct) Ot dzt (10) \\n1066 $. Thrun \\n0.2 \\nlllllllllNIllllllllllllllllllllll IIIIlll II  II I I \\n2 4 6 8 10 12 \\n2 4 $ 8 10 12 \\nFigure 1: Sampling: (a) Likelihood-weighted sampling and (b) importance sampling. At the bottom \\nof each graph, samples are shown that approximate the function f shown at the top. The height of \\nthe samples illustrates their importance factors. \\nHere a denotes a constant normalizer. The derivations of (8) and (10) follow directly from \\nthe fact that the environment is a stationary Markov chain, for which future states and \\nobservations are conditionally independent from past ones given knowledge of the state. \\nEquation (9) is obtained using the theorem of total probability. \\nArmed with the notion of belief states, the policy is now a mapping from belief states \\n(instead of histories) to actions: \\no':0 >a (11) \\nThe legitimacy of conditioning a on 0, instead of d, follows directly from the fact that the \\nenvironment is Markov, which implies that 0 is all one needs to know about the past to \\nmake optimal decisions. \\n2.3 Sample Representations \\nThus far, we intentionally left open how belief states 0 are represented. In prior work, state \\nspaces have been discrete. In discrete worlds, beliefs can be represented by a collection \\nof probabilities (one for each state), hence, beliefs can be represented exactly. Here were \\nare interested in real-valued state spaces. In general, probability distributions over real- \\nvalued spaces possess infinitely many dimensions, hence cannot be represented on a digital \\ncomputer. \\nThe key idea is to represent belief states by sets of (weighted) samples drawn from the \\nbelief distribution. Figure 1 illustrates two popular schemes for sample-based approxima- \\ntion: likelihood-weighted sampling, in which samples (shown at the bottom of Figure la) \\nare drawn directly from the target distribution (labeled f in Figure la), and importance \\nsampling, where samples are drawn from some other distribution, such as the curve labeled \\n# in Figure lb. In the latter case, samples z are annotated by a numerical importance factor \\nf(z) (12) \\np(x) = g(x) \\nto account for the difference in the sampling distribution, g, and the target distribution f \\n(the height of the bars in Figure 1 b illustrates the importance factors). Importance sampling \\nrequires that f > 0 - g > 0, which will be the case throughout this paper. Obviously, both \\nsampling methods generate approximations only. Under mild assumptions, they converge \\n with N denoting the sample set size [16]. \\nto the target distribution at a rate of , \\nIn the context of POMDPs, the use of sample-based representations gives rise to the \\nfollowing algorithm for approximate belief propagation (c.f., Equation (10)): \\nAlgorithm particle_filter(Or, at, Or+  ): \\nOt+ = 0 \\ndo N times: \\ndraw random state zt from Ot \\nMonte Carlo POMDPs 1067 \\nsample xt+, according to(xt+, [ at, xt) \\nset importance factor p( z t + l ) -- v ( ot + , l z t + , ) \\nadd (zt+,,p(zt+,)) toot+, \\nnormalize all p(zt+,)  Or+, so that -p(zt+,) = 1 \\nreturn Ot + , \\nThis algorithm converges to (10) for arbitrary models /, ,, and r and arbitrary belief \\ndistributions 0, defined over discrete, continuous, or mixed continuous-discrete state and \\naction spaces. It has, with minor modifications, been proposed under names like particle \\nfilters [13], condensation algorithm [5], survival of the fittest [8], and, in the context of \\nrobotics, Monte Carlo localization [4]. \\n2.4 Projection \\nIn conventional planning, the result of applying an action at at a state zt is a distribution \\nPr(zt+,, Rt+, I at, zt) over states zt+, and rewards Rt+, at the next time step. This \\noperation is called projection. In POMDPs, the state zt is unknown. Instead, one has to \\ncompute the result of applying action at to a belief state Or. The result is a distribution \\nPt(Or+,, Rt+, I at, Or) over belief states Or+, and rewards Rt+,. Since belief states them- \\nselves are distributions, the result of a projection in POMDPs is, technically, a distribution \\nover distributions. \\nThe projection algorithm is derived as follows. Using total probability, we obtain: \\nPr(0t+,,/t+, l at,Or) - Pr(Ot+,,Rt+, l at,dr) (13) \\n,Pr(Ot+,,tt+, I ot+,,at,dt) Pt(or+, l at,dr) dot+, (14) \\n(,) (**) \\nThe term (,) has already been derived in the previous section (c.f., Equation (10)), under \\nthe observation that the reward/t+, is trivially computed from the observation \\nThe second term, (**), is obtained by integrating out the unknown variables, zt+, and zt, \\nand by once again exploiting the Markov property: \\nPr(ot+t l at,dt) -' / Pr(ot+, I Xt+l) Pr(xt+, [ at,dt) dzt+, (15) \\n= / Pt(or+, I zt+,) / Pr(zt+, I zt,at) Pr(zt Idt)dzt dzt+(i16) \\nThis leads to the following approximate algorithm for projecting belief state. In the spirit \\nof this paper, our approach uses Monte Carlo integration instead of exact integration. It \\nrepresents distributions (and distributions over distributions) by samples drawn from such \\ndistributions. \\nAlgorithm partide_projecfion(0t, at): \\nOt = 0 \\ndo N times: \\ndraw random state zt from Ot \\nsample a next state zt+ 1 according to p(zt+, I at, \\nsample an observation or+, according to '(Ot+l I \\ncompute Ot + , =particle_filter(Or, at, Ot + l) \\nadd {Ot+,,(ot+,)> toot \\nreturn Ot \\nThe result of this algorithm, Or, is a sample set of belief states Or+, and rewards \\ndrawn from the desired distribution Pv(Ot+l, Rt+l [ Or, at). As N - c, Ot converges \\nwith probability 1 to the true posterior [16]. \\n1068 S. Thrun \\n2.5 Learning Value Functions \\nFollowing the rich literature on reinforcement learning [7, 15], our approach solves the \\nPOMDP problem by value iteration in belief space. More specifically, our approach \\nrecursively learns a value function Q over belief states and action, by backing up values \\nfrom subsequent belief states: \\nQ(Ot,at) < E [/l:(ot_l_l) q- 7maaxQ(0t+l, 5)] (18) \\nLeaving open (for a moment) how Q is represented, it is easy to be seen how the algorithm \\nparticle_projection can be applied to compute a Monte Carlo approximation of the right \\nhand-side expression: Given a belief state Ot and an action at, particle_projection computes \\na sample of ](ot+l ) and Or+l, from which the expected value on the right hand side of (18) \\ncan be approximated. \\nIt has been shown [2] that if both sides of (18) are equal, the greedy policy \\no'Q(0) = argmaxQ(0, o,) (19) \\na \\nis optimal, i.e., or* -- crq?. Furthermore, it has been shown (for the discrete case!) that \\nrepetitive application of (18) leads to an optimal value function and, thus, to the optimal \\npolicy [17, 3]. \\nOur approach essentially performs model-based reinforcement learning in belief space \\nusing approximate sample-based representations. This makes it possible to apply a rich \\nbag of tricks found in the literature on MDPs. In our experiments below, we use on- \\nline reinforcement learning with counter-based exploration and experience replay [9] to \\ndetermine the order in which belief states are updated. \\n2.6 Nearest Neighbor \\nWe now return to the issue how to represent Q. Since we are operating in real-valued \\nspaces, some sort of function approximation method is called for. However, recall that \\nQ accepts a probability distribution (a sample set) as an input. This makes most existing \\nfunction approximators (e.g., neural networks) inapplicable. \\nIn our current implementation, nearest neighbor [11] is applied to represent Q. More \\nspecifically, our algorithm maintains a set of sample sets 0 (belief states) annotated by an \\naction a and a Q-value Q(O, a). When a new belief state 0 r is encountered, its Q-value is \\nobtained by finding the k nearest neighbors in the database, and linearly averaging their \\nQ-values. If there aren't sufficiently many neighbors (within a pre-specified maximum \\ndistance), 0' is added to the database; hence, the database grows over time. \\nOur approach uses KL divergence (relative entropy) as a distance function . Technically, \\nthe KL-divergence between two continuous distributions is well-defined. When applied \\nto sample sets, however, it cannot be computed. Hence, when evaluating the distance be- \\ntween two different sample sets, our approach maps them into continuous-valued densities \\nusing Gaussian kernels, and uses Monte Carlo sampling to approximate the KL divergence \\nbetween them. This algorithm is fairly generic an extension of nearest neighbors to func- \\ntion approximation in density space, where densities are represented by samples. Space \\nlimitations preclude us from providing further detail (see [11, 12]). \\n3 Experimental Results \\nPreliminary results have been obtained in a world shown in two domains, one synthetic and \\none using a simulator of a RWI B21 robot. \\nIn the synthetic environment (Figure 2a), the agents starts at the lower left corner. Its \\nobjective is to reach \"heaven\" which is either at the upper left corner or the lower right \\nStdctly speaking, KL divergence is not a distance metric, but this is ignored here. \\nMonte Carlo POMDPs 1069 \\n(a) (b![ \\n:I \\nFigure 2: (a) The environment, schematically. (b) Average performance (reward) as a function of \\ntraining episodes. The black graph corresponds to the smaller environment (25 steps min), the grey \\ngraph to the larger environment (50 steps min). (c) Same results, plotted as a function of number of \\nbackups (in thousands). \\ncomer. The opposite location is \"hell.\" The agent does not know the location of heaven, \\nbut it can ask a \"priest\" who is located in the upper right comer. Thus, an optimal solution \\nrequires the agent to go first to the priest, and then head to heaven. The state space contains \\na real-valued (coordinates of the agent) and discrete (location of heaven) component. Both \\nare unobservable: In addition to not knowing the location of heaven, the agent also cannot \\nsense its (real-valued) coordinates. 5% random motion noise is injected at each move. \\nWhen an agent hits a boundary, it is penalized, but it is also told which boundary it hit \\n(which makes it possible to infer its coordinates along one axis). However, notice that the \\ninitial coordinates of the agent are known. \\nThe optimal solution takes approximately 25 steps; thus, a successful POMDP planner must \\nbe capable of looking 25 steps ahead. We will use the term \"successful policy\" to refer \\nto a policy that always leads to heaven, even if the path is suboptimal. For a policy to be \\nsuccessful, the agent must have learned to first move to the priest (information gathering), \\nand then proceed to the right target location. \\nFigures 2b&c show performance results, averaged over 13 experiments. The solid (black) \\ncurve in both diagrams plots the average cumulative reward J as a function of the number \\nof training episodes (Figure 2b), and as a function of the number of backups (Figure 2c). \\nA successful policy was consistently found after 17 episodes (or 6,150 backups), in all \\n13 experiments. In our current implementation, 6,150 backups require approximately 29 \\nminutes on a Pentium PC. In some experiments, a successful policy was identified in 6 \\nepisodes (less than 1,500 backups or 7 minutes). After a successful policy is found, further \\nlearning gradually optimizes the path. To investigate scaling, we doubled the size of the \\nenvironment (quadrupling the size of the state space), making the optimal solution 50 steps \\nlong. The results are depicted by the gray curves in Figures 2b&c. Here a successful \\npolicy is consistently found after 33 episodes (10,250 backups, 58 minutes). In some runs, \\na successful policy is identified after only 14 episodes. \\nWe also applied MC-POMDPs to a robotic locate-and-retrieve task. Here a robot (Figure 3a) \\nis to find and grasp an object somewhere in its vicinity (at floor or table height). The robot's \\ntask is to grasp the object using its gripper. It is rewarded for successfully grasping the \\nobject, and penalized for unsuccessful grasps or for moving too far away from the object. \\nThe state space is continuous in a: and y coordinates, and discrete in the object's height. \\nThe robot uses a mono-camera system for object detection; hence, viewing the object from \\na single location is insufficient for its 3D localization. Moreover, initially the object might \\nnot be in sight of the robot's camera, so that the robot must look around first. In our \\nsimulation, we assume 30% general detection error (false-positive and false-negative), with \\nadditional Gaussian noise if the object is detected correctly. The robot's actions include \\ntums (by a variable angle), translations (by a variable distance), and grasps (at one of two \\nlegal heights). Robot control is erroneous with a variance of 20% (in :c-y-space) and 5% (in \\nrotational space). Typical belief states range from uniformly distributed sample sets (initial \\nbelief) to samples narrowly focused on a specific a:-t-z location. \\n1070 S. Thrun \\n(a) \\nFigure 3: Find and fetch task: \\n(c) \\n% success \\niteration \\n(a) The mobile robot with gripper and camera, holding the target \\nobject (experiments are carded out in simulation!), (b) three successful runs (trajectory projected into \\n2D), and (c) success rate as a function of number of planning steps. \\nFigure 3c shows the rate of successful grasps as a function of iterations (actions). While \\ninitially, the robot fails to grasp the object, after approximately 4,000 iterations its perfor- \\nmance surpasses 80%. Here the planning time is in the order of 2 hours. However, the robot \\nfails to reach 100%. This is in part because certain initial configurations make it impossible \\nto succeed (e.g., when the object is too close to the maximum allowed distance), in part \\nbecause the robot occasionally misses the object by a few centimeters. Figure 3b depicts \\nthree successful example trajectories. In all three, the robot initially searches the object, \\nthen moves towards it and grasps it successfully. \\n4 Discussion \\nWe have presented a Monte Carlo approach for learning how to act in partially observable \\nMarkov decision processes (POMDPs). Our approach represents all belief distributions \\nusing samples drawn from these distributions. Reinforcement learning in belief space is \\napplied to learn optimal policies, using a sample-based version of nearest neighbor for \\ngeneralization. Backups are performed using Monte Carlo sampling. Initial experimental \\nresults demonstrate that our approach is applicable to real-valued domains, and that it yields \\ngood performance results in environments that are--by POMDP standards--relatively large. \\nReferences \\n[1] AAAI Fall symposium on POMDPs. 1998. See http://www.cs.duke.edu/mlittman/talks/ \\npomdp-symposium. html \\n[2] R.E. Bellman. Dynamic Programming. Princeton University Press, 1957. \\n[3] P. Dayan and T. J. Sejnowski. TD(,X) converges with probability 1. 1993. \\n[4] D. Fox, W. Burgard, E Dellaert, and S. Thrun. Monte carlo localization: Efficient position estimation for mobile robots. \\nAAAI-99. \\n[5] M. Isard and A. Bake. Cndensatin: cnditina density prpagatinfr visua tracking. nternatinalJurnal f Crnputer \\nVision, 1998. \\n[6] L.P. Kaelbling, M.L. Littman, and A.R. Cassandra. Planning and acting in partially observable stochastic domains. Submitted \\nfor publication, 1997. \\n[7] L.P. Kaelbling, M.L. Littman, and A.W. Moore. Reinforcement learning: A survey. JAIR, 4, 1996. \\n[8] K. Kanazawa, D. Koller, and S.J. Russell. Stochastic simulation algorithms for dynamic probabilistic networks. UAI-95. \\n[9] L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8, \\n1992. \\n[10] M.L. Littman, A.R. Cassandra, and L.P. Kaelbling. Learning policies for partially observable environments: Scaling up. \\nICML-95. \\n[11] A.W. Moore, C.G. Atkeson, and S.A. Schaal. Locally weighted learning for control. AIReview, 11, 1997. \\n[12] D. Ormoneit and S. Sen. Kernel-based reinforcementlearning. TR 1999-8, Statistics, Stanford University, 1999. \\n[ 13] M. Pitt and N. Shephard. Filtering via simulation: auxiliary particle filter. Journal of the American Statistical Association, \\n1999. \\n[14] E. Sondik. The Optimal Control of Partially Observable Markov Processes. PhD thesis, Stanford, 1971. \\n[15] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998. \\n[16] M.A. Tanner. Tools for Statistical Inference. Springer Verlag, 1993. \\n[17] C.J.C.H. Watkins. Learning from Delayed Rewards. PhD thesis, King's College, Cambridge, 1989. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n","1739  1739      ...  Gaussian Fields for Approximate Inference \\nin Layered Sigmoid Belief Networks \\nDavid Barber* \\nStichting Neurale Netwerken \\nMedical Physics and Biophysics \\nNijmegen University, The Netherlands \\nbarberdaston. ac. uk \\nPeter Sollich \\nDepartment of Mathematics \\nKing's College, University of London \\nLondon WC2R 2LS, U.K. \\npeter. sollichkcl. ac. uk \\nAbstract \\nLayered Sigmoid Belief Networks are directed graphical models \\nin which the local conditional probabilities are parameterised by \\nweighted sums of parental states. Learning and inference in such \\nnetworks are generally intractable, and approximations need to be \\nconsidered. Progress in learning these networks has been made by \\nusing variational procedures. We demonstrate, however, that vari- \\national procedures can be inappropriate for the equally important \\nissue of inference - that is, calculating marginMs of the network. \\nWe introduce an alternative procedure, based on assuming that the \\nweighted input to a node is approximately Gaussian distributed. \\nOur approach goes beyond previous Gaussian field assumptions in \\nthat we take into account correlations between parents of nodes. \\nThis procedure is specialized for calculating marginals and is sig- \\nnificantly faster and simpler than the variational procedure. \\nI Introduction \\nLayered Sigmoid Belief Networks [1] are directed graphical models [2] in which \\nthe local conditional probabilities are parameterised by weighted sums of parental \\nstates, see fig(l). This is a graphical representation of a distribution over a set of \\nbinary variables si 6 {0, 1}. Typically, one supposes that the states of the nodes \\nat the bottom of the network are generated by states in previous layers. Whilst, in \\nprinciple, there is no restriction on the number of nodes in any layer, typically, one \\nconsiders structures similar to the \"fan out\" in fig(l) in which higher level layers \\nprovide an \"explanation\" for patterns generated in lower layers. Such graphical \\nmodels are attractive since they correspond to layers of information processors, of \\npotentially increasing complexity. Unfortunately, learning and inference in such net- \\nworks is generally intractable, and approximations need to be considered. Progress \\nin learning has been made by using variational procedures [3, 4, 5]. However, an- \\nother crucial aspect remains inference [2]. That is, given some evidence (or none), \\ncalculate the marginal of a variable, conditional on this evidence. This assumes \\nthat we have found a suitable network from some learning procedure, and now wish \\n*Present Address: NCRG, Aston University, Birmingham B4 7ET, U.K. \\n394 D. Barber and P. Sollich \\nto query this network. Whilst the variational procedure is attractive for learning, \\nsince it generally provides a bound on the likelihood of the visible units, we demon- \\nstrate that it may not always be equally appropriate for the inference problem. \\nA directed graphical model defines a distribution over \\na set of variables s = (sx ...sn) that factorises into \\nthe local conditional distributions, \\np(sx...s,) = l'Ip(silri) (1) \\ni:1 \\nwhere ri denotes the parent nodes of node i. In a \\nlayered network, these are the nodes in the proceed- \\ning layer that feed into node i. In a sigmoid belief \\nnetwork the local probabilities are defined as \\np(si--ll7ri)--r (Ewijsj-Oi)-o'(hi) \\nJ \\nFigure 1: A Layered Sig- \\nmoid Belief Network \\n(2) \\nwhere the \"field\" at node/is defined as hi = Y'.j w,jsj +Oi and (r(h) = 1/(1 +e-n). \\nwij is the strength of the connection between node i and its parent node j; if j is \\nnot a parent of i we set wlj = O. Oi is a bias term that gives a parent-independent \\nbias to the state of node i. \\nWe are interested in inference - in particular, calculating marginals of the network \\nfor cases with and without evidential nodes. In section (2) we describe how to \\napproximate the quantities p(si -- 1) and discuss in section (2.1) why our method \\ncan improve on the standard variational mean field theory. Conditional marginals, \\nsuch as p(si = 11s j = 1, sk = 0) are considered in section (3). \\n2 Gaussian Field Distributions \\nUnder the 0/1 coding for the variables si, the mean of a variable, mi is given by the \\nprobability that it is in state 1. Using the fact from (2) that the local conditional \\ndistribution of node i is dependent on its parents only through its field hi, we have \\nrni =p(si: 1)= f p, = l[hi)p(hi)dhi _= (o'(hi))p(h,) (3) \\nwhere we use the notation ((.))p to denote an average with respect to the distri- \\nbution p. If there are many parents of node i, a reasonable assumption is that the \\ndistribution of the field hi will be Gaussian, p(hi) -, N (lui, efT). Under this Gaus- \\nsian Field (GF) assumption, we need to work out the mean and variance, which are \\ngiven by \\n= = + o, = + O, (4) \\nJ J \\n(5) \\nj,k \\nwhere Rjk = (AsjAs). We use the notation A (.) -- (.) - ((.)). \\nThe diagonal terms of the node covariance matrix are Rii - mi (1 - mi). In contrast \\nto previous studies, we include off diagonal terms in the calculation of R [4]. From \\nGaussian Fields for Approximate Inference 395 \\n(5) we only need to find correlations between parents i and j of a node. These are \\neasy to calculate in the layered networks that we are considering, because neither i \\nnor j is a descendant of the other: \\nRid --p(si -- 1, sj -- 1) - mimj \\n= fp(, = = 11hj)p(hi,hj)dh-mimj \\n= {(r (hi) (r (hj))p(n,,ni) - mimj \\nAssuming that the joint distribution p(hi, hj) is Gaussian, \\nand covariance, given by \\nl \\n(6) \\n(7) \\n(8) \\nwe again need its mean \\nE,j = {AhiAhj) = Z w,kwj, {AskAs,) = Z wiwjtR, (10) \\nkl kl \\nUnder this scheme, we have a closed set of equations, (4,5,8,10) for the means \\nmi and covariance matrix tij which can be solved by forward propagation of the \\nequations. That is, we start from nodes without parents, and then consider the \\nnext layer of nodes, repeating the procedure until a full sweep through the network \\nhas been completed. The one and two dimensional field averages, equations (3) \\nand (8), are computed using Gaussian Quadrature. This results in an extremely \\nfast procedure for approximating the marginals mi, requiring only a single sweep \\nthrough the network. \\nOur approach is related to that of [6] by the common motivating assumption that \\neach node has a large number of parents. This is used in [6] to obtain actual \\nbounds on quantities of interest such as joint marginals. Our approach does not \\ngive bounds. Its advantage, however, is that it allows fluctuations in the fields hi, \\nwhich are effectively excluded in [6] by the assumed scaling of the weights wij with \\nthe number of parents per node. \\n2.1 Relation to Variational Mean Field Theory \\nIn the variational approach, one fits a tractable approximating distribution Q to \\nthe SBN. Taking Q factorised, Q(s) - 1-Ii rn'(1 - rni) x-' we have the bound \\nlnp ( sx . . . sn ) _> E {-mi lnmi - (1 - mi ) In (1 - mi ) } \\ni \\n+Z{Zmiwijmj+Oimi_(ln(l+eh'))Q} (11) \\ni j \\nThe final term in (11) causes some difficulty even in the case in which Q is a fac- \\ntorised model. Formally, this is because this term does not have the same graphical \\nstructure as the tractable model Q. One way around around this difficulty is to em- \\nploy a further bound, with associated variational parameters [7]. Another approach \\nis to make the Gaussian assumption for the field hi as in section (2). Because Q is \\nfactorised, corresponding to a diagonal correlation matrix R, this gives [4] \\n(ln (1 + e n') )Q 0 (ln (1 + e'))N(,,a) (12) \\n396 D. Barber and P. Sollich \\nwhere/ui - -.j wijmj q- Oi and eri 2 -- -]j wi2jmj(1 - mj). Note that this is a one \\ndimensional integral of a smooth function. In contrast to [4] we therefore evaluate \\nthis quantity using Gaussian Quadrature. This has the advantage that no extra \\nvariational parameters need to be introduced. Technically, the assumption of a \\nGaussian field distribution means that (11) is no longer a bound. Nevertheless, in \\npractice it is found that this has little effect on the quality of the resulting solution. \\nIn our implementation of the variational approach, we find the optimal parameters \\nmi by maximising the above equation for each component mi separately, cycling \\nthrough the nodes until the parameters mi do not change by more than 10 -. \\nThis is repeated 5 times, and the solution with the highest bound score is chosen. \\nNote that these equations cannot be solved by forward propagation alone since the \\nfinal term contains contributions from all the nodes in the network. This is in \\ncontrast to the GF approach of section (2). Finding appropriate parameters mi by \\nthe variational approach is therefore rather slower than using the GF method. \\nIn arriving at the above equations, we have made two assumptions. The first is \\nthat the intractable distribution is well approximated by a factorised model. The \\nsecond is that the field distribution is Gaussian. The first step is necessary in \\norder to obtain a bound on the likelihood of the model (although this is slightly \\ncompromised by the Gaussian fielc[ assumption). In the GF approach we dispense \\nwith this assumption of an effectively factorised network (partially because if we \\nare only interested in inference, a bound on the model likelihood is less relevant). \\nThe GF method may therefore prove useful for a broader class of networks than the \\nvariational approach. \\n2.2 Results for unconditional marginals \\nWe compared three procedures for estimating the conditional values p(si = 1) for \\nall the nodes in the network, namely the variational theory, as described in section \\n(2.1), the diagonal Gaussian field theory, and the non-diagonal Gaussian field theory \\nwhich includes correlation effects between parents. Results for small weight values \\nwij are shown in fig(2). In this case, all three methods perform reasonably well, \\nalthough there is a significant improvement in using the GF methods over the \\nvariational procedure; parental correlations are not important (compare figs(2b) \\nand (2c)). In fig(3) the weights and biases are chosen such that the exact mean \\nvariables mi are roughly 0.5 with non-trivial correlation effects between parents. \\nNote that the variational mean field theory now provides a poor solution, whereas \\nthe GF methods are relatively accurate. The effect of using the non-diagonal R \\nterms is beneficial, although not dramatically so. \\n3 Calculating Conditional Marginals \\nWe consider now how to calculate conditional marginals, given some evidential \\nnodes. (In contrast to [6], any set of nodes in the network, not just output nodes, \\ncan be considered evidential.) We write the evidence in the following manner \\nThe quantities that we are interested in are conditional marginals which, from Bayes \\nrule are related to the joint distribution by \\np(s = lIE)= p(s = 1, E) \\np(si = O,E) +p(s = 1, E) (13) \\nThat is, provided that we have a procedure for estimating joint marginals, we can \\nobtain conditional marginals too. Without loss of generality, we therefore consider \\nGaussian Fields for Approximate Inference 397 \\n2O \\n15 \\n10 \\nErro using factodeed model fit \\n0 002 004 006 \\nError using Gaussian Field. Dagonal covariance \\n0.002 0 004 0 006 0.008 0.01 \\nError using Gaussian Field, Non Diagonal covariance \\n(a) Mean error = 0.0377 \\n(b) Mean error = 0.0018 \\n(c) Mean error = 0.0017 \\nFigure 2: Error in approximating p(si = 1) for the network in fig(l), averaged over \\nall the nodes in the network. In each of 100 trials, weights were drawn from a \\nzero mean, unit variance Gaussian; biases were set to 0. Note the different scale \\nin (b) and (c). In (a) we use the variational procedure with a factorised Q, as \\nin section (2.1). In (b) we use the Gaussian field equations, assuming a diagonal \\ncovariance matrix R. This procedure was repeated in (c) including correlations \\nbetween parents. \\nE + = E U {si = 1}, which then contains n + 1 \"evidential\" variables. That is, the \\ndesired marginal variable is absorbed into the evidence set. For convenience, we \\nthen split the nodes into two sets, those containing the evidential or \"clamped\" \\nnodes, C, and the remaining \"free\" nodes F. The joint evidence is then given by \\np(E +) = Ep(Ec,,...Ec+,,sf,,...sf.) (14) \\n 7r* 71'* * \\n= \\n, (15) \\nv]ues  specified [ +.  he sgmo[d belief ewo \\n=  (2S - 1) ws + Ok ' si = otherwise \\nis therefore determined by the distribution of the field h; =  wks +0. \\nExamining (15), we see that the product over the \"free\" nodes defines a SBN in \\nwhich the local probability distributions are given by those of the original network, \\nbut with any evidential parental nodes clamped to their evidence values. Therefore, \\nConsistent with our previous assumptions, we assume that the distribution of the \\n( ') \\nfields h* - h...hc+ is jointly Gaussian. We can then find the mean and \\ncovariance matrix for the distribution of h* by repeating the calculation of section \\n(2) in which evidential nodes have been clamped to their evidence values. Once this \\nGaussian has been determined, it can be used in (17) to determine p(E +). Gaussian \\naverages of products of sigmoids are calculated by drawing 1000 samples from the \\nGaussian over which we wish to integrate x. Note that if there are evidential nodes \\nIn one and two dimensions (n = 0, 1), or n = 1, we use Gaussian Quadrature. \\n398 D. Barber and P Sollich \\nError using factorised model fit Error using Gaussian Field, Diagonal covariance Error using Gaussian Field, Non Diagonal covariance \\n30 \\n20 10 1011_ \\no o., o o o. o o. o o o. o. o. o. o o o, o' o. o. o. . \\n(a) Mean error = 0.4188 \\n(b) Mean error = 0.0253 \\n(c) Mean error = 0.0198 \\nFigure 3: All weights are set to uniformly from 0 to 50. Biases are set to -0.5 of \\nthe summed parental weights plus a uniform random number from -2.5 to 2.5. The \\nroot node is set to be 1 with probability 0.5. This has the effect of making all the \\nnodes in the exact network roughly 0.5 in mean, with non-negligible correlations \\nbetween parental nodes. 160 simulations were made. \\nin different layers, we require the correlations between their fields h to evaluate (17). \\nSuch 'inter-layer' correlations were not required in section (2), and to be able to use \\nthe same calculational scheme we simply neglect them. (We leave a study of the \\neffects of this assumption for future work.) The average in (17) then factors into \\ngroups, where each group contains evidential terms in a particular layer. \\nThe conditional marginal for node i is obtained from repeating the above procedure \\nin which the desired marginal node is clamped to its opposite value, and then using \\nthese results in (13). The above procedure is repeated for each conditional marginal \\nthat we are interested in. Although this may seem computationally expensive, the \\nmarginal for each node is computed quickly, since the equations are solved by one \\nforward propagation sweep only. \\n6O \\n50 \\n40 \\n30 \\n2O \\n10 \\n0 \\n0 \\nError using factorised model fit \\nError using Gaussian Field, Diagonal covariance \\n0.1 0.2 03 0.4 0.5 0.6 0 0.1 0.2 0.3 0.4 0.5 06 \\nError using Gaussian Field, Non Diagonal covariance \\n) 01 02 0.3 04 0.5 06 \\n(a) Mean error - 0.1534 \\n(b) Mean error = 0.0931 \\n(c) Mean error = 0.0865 \\nFigure 4: Estimating the conditional marginal of the top node being in state 1, \\ngiven that the four bottom nodes are in state 1. Weights were drawn from a zero \\nmean Gaussian with variance 5, with biases set to -0.5 the summed parental weights \\nplus a uniform random number from -2.5 to 2.5. Results of 160 simulations. \\n3.1 Results for conditional marginals \\nWe used the same structure as in the previous experiments, as shown in fig(l). We \\nare interested here in calculating the probability that the top node is in state 1, \\nGaussian Fields for Approximate Inference 399 \\ngiven that the four bottom nodes are in state 1. Weights were chosen from a zero \\nmean Gaussian with variance 5. Biases were set to negative half Of the summed \\nparent weights, plus a uniform random value from -2.5 to 2.5. Correlation effects \\nin these networks are not as.strong as in the experiments in section (2.2), although \\nthe improvement of the GF theory over the variational theory seen in fig(4) remains \\nclear. The improvement from the off diagonal terms in R is minimal. \\n4 Conclusion \\nDespite their appropriateness for learning, variational methods may not be equally \\nsuited to inference, making more tailored methods attractive. We have considered \\nan approximation procedure that is based on assuming that the distribution of the \\nweighted input to a node is approximately Gaussian. Correlation effects between \\nparents of a node were taken into account to improve the Gaussian theory, although \\nin our examples this gave only relatively modest improvements. \\nThe variational mean field theory performs poorly in networks with strong cor- \\nrelation effects between nodes. On the other hand, one may conjecture that the \\nGaussian Field approach will not generally perform catastrophically worse than the \\nfactorised variational mean field theory. One advantage of the variational theory \\nis the presence of an objective function against which competing solutions can be \\ncompared. However, finding an optimum solution for the mean parameters mi from \\nthis function is numerically complex. Since the Gaussian Field theory is extremely \\nfast to solve, an interesting compromise might be to prime the variational solution \\nwith the results from the Gaussian Field theory. \\nAcknowledgments \\nDB would like to thank Bert Kappen and Wim Wiegerinck for stimulating and \\nhelpful discussions. PS thanks the Royal Society for financial support. \\nReferences\\n[1] R. Neal. Connectionist learning of Belief Networks. Artificial Intelligence, 56:71-113, \\n1992. \\n[2] E. Castillo, J. M. Gutierrez, and A. S. Hadi. Expert Systems and Probabilistic Network \\nModels. Springer, 1997. \\n[3] M. I. Jordan, Z. Gharamani, T. S. Jaakola, and L. K. Saul. An Introduction to Vari- \\national Methods for Graphical Models. In M. I. Jordan, editor, Learning in Graphical \\nModels, pages 105-161. Kluwer, 1998. \\n[4] L. Saul and M. I. Jordan. A mean field learning algorithm for unsupervised neural \\nnetworks. In M. I. Jordan, editor, Learning in Graphical Models, 1998. \\n[5] D. Barber and W Wiegerinck. Tractable variational structures for approximating \\ngraphical models. In M.S. Kearns, S.A. Solla, and D.A. Cohn, editors, Advances in \\nNeural Information Processing Systems NIPS 11. MIT Press, 1999. \\n[6] M. Kearns and L. Saul. Inference in Multilayer Networks via Large Deviation Bounds. \\nIn Advances in Neural Information Processing Systems NIPS 11, 1999. \\n[7] L. K. Saul, T. Jaakkola, and M. I. Jordan. Mean Field Theory for Sigmoid Belief \\nNetworks. Journal of Artificial Intelligence Research, 4:61-76, 1996. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n","\n","[1740 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"vmp0qhA0dBeO","colab_type":"code","outputId":"18da8156-2e1a-471c-dbb7-e073f58be9d8","executionInfo":{"status":"ok","timestamp":1590837059455,"user_tz":-180,"elapsed":513,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["corpus_topic_df.describe()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Document</th>\n","      <th>Dominant Topic</th>\n","      <th>Contribution %</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>1740.000000</td>\n","      <td>1740.000000</td>\n","      <td>1740.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>869.500000</td>\n","      <td>10.149425</td>\n","      <td>36.746851</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>502.439051</td>\n","      <td>5.485364</td>\n","      <td>12.144926</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>10.510000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>434.750000</td>\n","      <td>6.000000</td>\n","      <td>27.600000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>869.500000</td>\n","      <td>10.000000</td>\n","      <td>34.640000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1304.250000</td>\n","      <td>15.000000</td>\n","      <td>43.760000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1739.000000</td>\n","      <td>20.000000</td>\n","      <td>81.230000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          Document  Dominant Topic  Contribution %\n","count  1740.000000  1740.000000     1740.000000   \n","mean   869.500000   10.149425       36.746851     \n","std    502.439051   5.485364        12.144926     \n","min    0.000000     1.000000        10.510000     \n","25%    434.750000   6.000000        27.600000     \n","50%    869.500000   10.000000       34.640000     \n","75%    1304.250000  15.000000       43.760000     \n","max    1739.000000  20.000000       81.230000     "]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"id":"ujn6CX8Sdbhs","colab_type":"code","outputId":"646812b9-ffae-4f67-bd56-c9800c22a87d","executionInfo":{"status":"ok","timestamp":1590837060467,"user_tz":-180,"elapsed":609,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":370}},"source":["corpus_topic_df['Dominant Topic'].value_counts()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10    145\n","9     135\n","12    108\n","5     108\n","6     108\n","1     108\n","19    106\n","8     97 \n","7     95 \n","13    90 \n","14    84 \n","16    79 \n","3     75 \n","18    71 \n","15    67 \n","4     62 \n","17    60 \n","2     53 \n","20    53 \n","11    36 \n","Name: Dominant Topic, dtype: int64"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"id":"Lo7_PpNoDTt_","colab_type":"code","colab":{}},"source":["trial_df = corpus_topic_df.copy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4UF7drvQDYlV","colab_type":"code","colab":{}},"source":["trial_df.drop(['Paper', 'Topic Desc'], axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EBNKBjGTDYqK","colab_type":"code","outputId":"0f3cecd2-bf57-4b89-c39d-e0a6f9d1afb1","executionInfo":{"status":"ok","timestamp":1590837063233,"user_tz":-180,"elapsed":434,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":402}},"source":["trial_df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Document</th>\n","      <th>Dominant Topic</th>\n","      <th>Contribution %</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>38.04</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>13</td>\n","      <td>38.55</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>13</td>\n","      <td>13.46</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>37.82</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>14</td>\n","      <td>21.81</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1735</th>\n","      <td>1735</td>\n","      <td>5</td>\n","      <td>35.56</td>\n","    </tr>\n","    <tr>\n","      <th>1736</th>\n","      <td>1736</td>\n","      <td>5</td>\n","      <td>33.06</td>\n","    </tr>\n","    <tr>\n","      <th>1737</th>\n","      <td>1737</td>\n","      <td>5</td>\n","      <td>48.96</td>\n","    </tr>\n","    <tr>\n","      <th>1738</th>\n","      <td>1738</td>\n","      <td>5</td>\n","      <td>48.52</td>\n","    </tr>\n","    <tr>\n","      <th>1739</th>\n","      <td>1739</td>\n","      <td>10</td>\n","      <td>42.87</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1740 rows × 3 columns</p>\n","</div>"],"text/plain":["      Document  Dominant Topic  Contribution %\n","0     0         1               38.04         \n","1     1         13              38.55         \n","2     2         13              13.46         \n","3     3         1               37.82         \n","4     4         14              21.81         \n","...  ..         ..                ...         \n","1735  1735      5               35.56         \n","1736  1736      5               33.06         \n","1737  1737      5               48.96         \n","1738  1738      5               48.52         \n","1739  1739      10              42.87         \n","\n","[1740 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"markdown","metadata":{"id":"AxJYTe-nbhhV","colab_type":"text"},"source":["### Dominant Topics Distribution Across Corpus\n","\n","The first thing we can do is look at the overall distribution of each topic across the corpus of research papers."]},{"cell_type":"code","metadata":{"id":"SXUGdAj-EvNE","colab_type":"code","outputId":"d1e1eb50-87dc-4497-e978-3c438ec7c6c3","executionInfo":{"status":"ok","timestamp":1590837063906,"user_tz":-180,"elapsed":522,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["corpus_topic_df.columns"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Document', 'Dominant Topic', 'Contribution %', 'Topic Desc', 'Paper'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"code","metadata":{"id":"B3sx8vISbgoR","colab_type":"code","colab":{}},"source":["pd.set_option('display.max_colwidth', 200)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aGZkZ1_QgfyS","colab_type":"code","outputId":"a9fc67c3-d486-4b89-95d2-e89e703f45ad","executionInfo":{"status":"ok","timestamp":1590837064680,"user_tz":-180,"elapsed":413,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":891}},"source":["topic_stats_df = corpus_topic_df.groupby('Dominant Topic').agg(\n","                                  Doc_Count = ('Dominant Topic', np.size),\n","                                  Total_Docs_Perc = ('Dominant Topic', np.size)).reset_index()\n","\n","# topic_stats_df = topic_stats_df['Dominant Topic'].reset_index()\n","\n","topic_stats_df['Total_Docs_Perc'] = topic_stats_df['Total_Docs_Perc'].apply(lambda row: round((row*100) / len(papers), 2))\n","topic_stats_df['Topic Desc'] = [topics_df.iloc[t]['Terms per Topic'] for t in range(len(topic_stats_df))]\n","\n","topic_stats_df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dominant Topic</th>\n","      <th>Doc_Count</th>\n","      <th>Total_Docs_Perc</th>\n","      <th>Topic Desc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>108</td>\n","      <td>6.21</td>\n","      <td>distribution, probability, prior, variable, gaussian, mixture, estimate, density, bayesian, approximation, likelihood, sample, log, expert, em, estimation, posterior, step, component, probabilistic</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>53</td>\n","      <td>3.05</td>\n","      <td>training, prediction, kernel, test, training_set, regression, estimate, selection, machine, experiment, sample, test_set, cross_validation, measure, ensemble, regularization, variance, margin, ris...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>75</td>\n","      <td>4.31</td>\n","      <td>circuit, chip, current, analog, voltage, neuron, implementation, processor, bit, design, device, computation, array, parallel, neural, digital, synapse, operation, hardware, transistor</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>62</td>\n","      <td>3.56</td>\n","      <td>image, feature, object, pixel, face, view, recognition, representation, scale, contour, surface, edge, shape, visual, part, scene, vision, digit, local, texture</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>108</td>\n","      <td>6.21</td>\n","      <td>cell, response, activity, stimulus, neuron, pattern, cortical, layer, receptive_field, cortex, connection, orientation, unit, visual, spatial, contrast, simulation, mechanism, population, synaptic</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>108</td>\n","      <td>6.21</td>\n","      <td>motion, direction, position, visual, target, control, field, velocity, movement, motor, trajectory, hand, location, arm, moving, human, response, spatial, feedback, sensory</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>95</td>\n","      <td>5.46</td>\n","      <td>equation, solution, convergence, gradient, vector, constraint, energy, iteration, rate, optimization, update, minimum, optimal, constant, gradient_descent, eq, step, matrix, condition, derivative</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>97</td>\n","      <td>5.57</td>\n","      <td>bound, theorem, class, threshold, approximation, proof, size, probability, loss, complexity, polynomial, theory, assume, linear, hypothesis, defined, definition, define, xi, constant</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>135</td>\n","      <td>7.76</td>\n","      <td>vector, linear, matrix, component, nonlinear, signal, source, filter, coefficient, operator, basis, transformation, pca, ica, projection, gaussian, representation, principal_component, rule, indep...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>145</td>\n","      <td>8.33</td>\n","      <td>map, region, subject, location, effect, change, study, condition, light, experiment, et_al, brain, pair, normal, correlation, trial, eeg, site, left, theory</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11</td>\n","      <td>36</td>\n","      <td>2.07</td>\n","      <td>state, dynamic, neuron, memory, pattern, recurrent, attractor, module, capacity, connection, phase, hopfield, fixed_point, delay, behavior, neural, oscillator, stable, oscillation, sequence</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>108</td>\n","      <td>6.21</td>\n","      <td>unit, training, layer, hidden_unit, net, architecture, pattern, trained, task, activation, back_propagation, hidden_layer, training_set, hidden, connection, learn, backpropagation, generalization,...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","      <td>90</td>\n","      <td>5.17</td>\n","      <td>rule, representation, structure, sequence, symbol, connectionist, language, level, unit, string, activation, context, pattern, role, similarity, represented, task, learned, note, part</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14</td>\n","      <td>84</td>\n","      <td>4.83</td>\n","      <td>node, class, classification, classifier, pattern, tree, vector, code, probability, feature, bit, sample, label, binary, decision, stage, labeled, decision_tree, technique, error_rate</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>67</td>\n","      <td>3.85</td>\n","      <td>neuron, signal, frequency, spike, channel, response, firing, temporal, stimulus, threshold, current, rate, neural, synaptic, auditory, event, amplitude, sound, phase, firing_rate</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>79</td>\n","      <td>4.54</td>\n","      <td>search, rate, experiment, task, strategy, table, application, user, instance, test, run, random, call, technique, average, block, feature, high, good, program</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17</td>\n","      <td>60</td>\n","      <td>3.45</td>\n","      <td>word, recognition, speech, training, character, sequence, hmm, context, speaker, feature, frame, mlp, letter, trained, state, experiment, speech_recognition, phoneme, window, segmentation</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","      <td>71</td>\n","      <td>4.08</td>\n","      <td>noise, average, distribution, curve, equation, correlation, theory, rate, limit, stochastic, optimal, random, solution, size, teacher, effect, simulation, student, eq, obtained</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19</td>\n","      <td>106</td>\n","      <td>6.09</td>\n","      <td>state, control, action, step, policy, controller, reinforcement_learning, environment, task, optimal, goal, robot, reward, td, agent, trial, transition, current, reinforcement, rl</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20</td>\n","      <td>53</td>\n","      <td>3.05</td>\n","      <td>local, distance, cluster, structure, graph, clustering, variable, dimensional, mapping, global, dimension, center, vector, cost, partition, neighborhood, prototype, constraint, interpolation, mani...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Dominant Topic  ...                                                                                                                                                                                               Topic Desc\n","0                1  ...    distribution, probability, prior, variable, gaussian, mixture, estimate, density, bayesian, approximation, likelihood, sample, log, expert, em, estimation, posterior, step, component, probabilistic\n","1                2  ...  training, prediction, kernel, test, training_set, regression, estimate, selection, machine, experiment, sample, test_set, cross_validation, measure, ensemble, regularization, variance, margin, ris...\n","2                3  ...                 circuit, chip, current, analog, voltage, neuron, implementation, processor, bit, design, device, computation, array, parallel, neural, digital, synapse, operation, hardware, transistor\n","3                4  ...                                         image, feature, object, pixel, face, view, recognition, representation, scale, contour, surface, edge, shape, visual, part, scene, vision, digit, local, texture\n","4                5  ...     cell, response, activity, stimulus, neuron, pattern, cortical, layer, receptive_field, cortex, connection, orientation, unit, visual, spatial, contrast, simulation, mechanism, population, synaptic\n","5                6  ...                             motion, direction, position, visual, target, control, field, velocity, movement, motor, trajectory, hand, location, arm, moving, human, response, spatial, feedback, sensory\n","6                7  ...      equation, solution, convergence, gradient, vector, constraint, energy, iteration, rate, optimization, update, minimum, optimal, constant, gradient_descent, eq, step, matrix, condition, derivative\n","7                8  ...                   bound, theorem, class, threshold, approximation, proof, size, probability, loss, complexity, polynomial, theory, assume, linear, hypothesis, defined, definition, define, xi, constant\n","8                9  ...  vector, linear, matrix, component, nonlinear, signal, source, filter, coefficient, operator, basis, transformation, pca, ica, projection, gaussian, representation, principal_component, rule, indep...\n","9               10  ...                                             map, region, subject, location, effect, change, study, condition, light, experiment, et_al, brain, pair, normal, correlation, trial, eeg, site, left, theory\n","10              11  ...            state, dynamic, neuron, memory, pattern, recurrent, attractor, module, capacity, connection, phase, hopfield, fixed_point, delay, behavior, neural, oscillator, stable, oscillation, sequence\n","11              12  ...  unit, training, layer, hidden_unit, net, architecture, pattern, trained, task, activation, back_propagation, hidden_layer, training_set, hidden, connection, learn, backpropagation, generalization,...\n","12              13  ...                  rule, representation, structure, sequence, symbol, connectionist, language, level, unit, string, activation, context, pattern, role, similarity, represented, task, learned, note, part\n","13              14  ...                   node, class, classification, classifier, pattern, tree, vector, code, probability, feature, bit, sample, label, binary, decision, stage, labeled, decision_tree, technique, error_rate\n","14              15  ...                       neuron, signal, frequency, spike, channel, response, firing, temporal, stimulus, threshold, current, rate, neural, synaptic, auditory, event, amplitude, sound, phase, firing_rate\n","15              16  ...                                           search, rate, experiment, task, strategy, table, application, user, instance, test, run, random, call, technique, average, block, feature, high, good, program\n","16              17  ...              word, recognition, speech, training, character, sequence, hmm, context, speaker, feature, frame, mlp, letter, trained, state, experiment, speech_recognition, phoneme, window, segmentation\n","17              18  ...                         noise, average, distribution, curve, equation, correlation, theory, rate, limit, stochastic, optimal, random, solution, size, teacher, effect, simulation, student, eq, obtained\n","18              19  ...                      state, control, action, step, policy, controller, reinforcement_learning, environment, task, optimal, goal, robot, reward, td, agent, trial, transition, current, reinforcement, rl\n","19              20  ...  local, distance, cluster, structure, graph, clustering, variable, dimensional, mapping, global, dimension, center, vector, cost, partition, neighborhood, prototype, constraint, interpolation, mani...\n","\n","[20 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"markdown","metadata":{"id":"jwfGVWfNOH4P","colab_type":"text"},"source":["### Dominant Topics in Specific Research Papers\n","\n","Another interesting perspective is to select specific papers, view the most dominant topic in each of those papers, and see if that makes sense."]},{"cell_type":"code","metadata":{"id":"f-0eIpiqbg1_","colab_type":"code","outputId":"08ff49e3-c671-490d-e09a-16c517dfc9c2","executionInfo":{"status":"ok","timestamp":1590837065720,"user_tz":-180,"elapsed":620,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":696}},"source":["pd.set_option('display.max_colwidth', 200)\n","(corpus_topic_df[corpus_topic_df['Document'].isin([681, 9, 392, 1622, 17, 906, 996, 503, 13, 733])])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Document</th>\n","      <th>Dominant Topic</th>\n","      <th>Contribution %</th>\n","      <th>Topic Desc</th>\n","      <th>Paper</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>4</td>\n","      <td>33.20</td>\n","      <td>image, feature, object, pixel, face, view, recognition, representation, scale, contour, surface, edge, shape, visual, part, scene, vision, digit, local, texture</td>\n","      <td>103 \\nNEURAL NETWORKS FOR TEMPLATE MATCHING: \\nAPPLICATION TO REAL-TIME CLASSIFICATION \\nOF THE ACTION POTENTIALS OF REAL NEURONS \\nYiu-fai Wong, Jashojiban Banik]. and James M. Bower$ \\n]'Divisi...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>16</td>\n","      <td>30.80</td>\n","      <td>search, rate, experiment, task, strategy, table, application, user, instance, test, run, random, call, technique, average, block, feature, high, good, program</td>\n","      <td>144 \\nSPEECH RECOGNITION EXPERIMENTS \\nWITH PERCEPTRONS \\nD. J. Burr \\nBell Communications Research \\nMorristown, NJ 07960 \\nABSTRACT \\nArtificial neural networks (ANNs) are capable of accurate re...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>14</td>\n","      <td>22.73</td>\n","      <td>node, class, classification, classifier, pattern, tree, vector, code, probability, feature, bit, sample, label, binary, decision, stage, labeled, decision_tree, technique, error_rate</td>\n","      <td>219 \\nNetwork Generality, Training Required, \\nand Precision Required \\nJohn S. Denker and Ben S. Wittner \\nAT&amp;T Bell Laboratories \\nHolmdel, New Jersey 07733 \\nKeep your hand on your wa]let. \\n--...</td>\n","    </tr>\n","    <tr>\n","      <th>392</th>\n","      <td>392</td>\n","      <td>7</td>\n","      <td>49.77</td>\n","      <td>equation, solution, convergence, gradient, vector, constraint, energy, iteration, rate, optimization, update, minimum, optimal, constant, gradient_descent, eq, step, matrix, condition, derivative</td>\n","      <td>Using Genetic Algorithms to Improve \\nPattern Classification Performance \\nEric I. Chang and Richard P. Lippmann \\nLincoln Laboratory, MIT \\nLexington, MA 02173-9108 \\nAbstract \\nGenetic algorithm...</td>\n","    </tr>\n","    <tr>\n","      <th>503</th>\n","      <td>503</td>\n","      <td>4</td>\n","      <td>36.34</td>\n","      <td>image, feature, object, pixel, face, view, recognition, representation, scale, contour, surface, edge, shape, visual, part, scene, vision, digit, local, texture</td>\n","      <td>Neural Network Analysis of Event Related \\nPotentials and Electroencephalogram Predicts \\nVigilance \\nRita Venturini \\nWilliam W. Lytton \\nTerrence J. Sejnowski \\nComputational Neurobiology Labora...</td>\n","    </tr>\n","    <tr>\n","      <th>681</th>\n","      <td>681</td>\n","      <td>9</td>\n","      <td>29.31</td>\n","      <td>vector, linear, matrix, component, nonlinear, signal, source, filter, coefficient, operator, basis, transformation, pca, ica, projection, gaussian, representation, principal_component, rule, indep...</td>\n","      <td>Mapping Between Neural and Physical \\nActivities of the Lobster Gastric Mill \\nKenji Doya \\nMary E.T. Boyle \\nAllen I. Selverston \\nDepartment of Biology \\nUniversity of California, San Diego \\nLa...</td>\n","    </tr>\n","    <tr>\n","      <th>733</th>\n","      <td>733</td>\n","      <td>17</td>\n","      <td>33.15</td>\n","      <td>word, recognition, speech, training, character, sequence, hmm, context, speaker, feature, frame, mlp, letter, trained, state, experiment, speech_recognition, phoneme, window, segmentation</td>\n","      <td>Optimal Brain Surgeon: \\nExtensions and performance comparisons \\nBabak Hassibi* \\nDavid G. Stork \\nGregory Wolff \\nTakahiro Watanabe \\nRicoh California Research Center \\n2882 Sand Hill Road Suite...</td>\n","    </tr>\n","    <tr>\n","      <th>906</th>\n","      <td>906</td>\n","      <td>3</td>\n","      <td>31.34</td>\n","      <td>circuit, chip, current, analog, voltage, neuron, implementation, processor, bit, design, device, computation, array, parallel, neural, digital, synapse, operation, hardware, transistor</td>\n","      <td>Extracting Rules from Artificial Neural Networks \\nwith Distributed Representations \\nSebastian Thrun \\nUniversity of Bonn \\nDepartment of Computer Science III \\nR6merstr. 164, D-53117 Bonn, Germa...</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>996</td>\n","      <td>2</td>\n","      <td>39.18</td>\n","      <td>training, prediction, kernel, test, training_set, regression, estimate, selection, machine, experiment, sample, test_set, cross_validation, measure, ensemble, regularization, variance, margin, ris...</td>\n","      <td>A Dynamical Model of Context Dependencies for the \\nVestibulo-Ocular Reflex \\nO!ivier J.M.D. Coenen* \\nTerrence J. Sejnowskit \\nComputational Neurobiology Laboratory \\nHoward Hughes Medical Instit...</td>\n","    </tr>\n","    <tr>\n","      <th>1622</th>\n","      <td>1622</td>\n","      <td>7</td>\n","      <td>36.64</td>\n","      <td>equation, solution, convergence, gradient, vector, constraint, energy, iteration, rate, optimization, update, minimum, optimal, constant, gradient_descent, eq, step, matrix, condition, derivative</td>\n","      <td>Model Selection for Support Vector Machines \\nOlivier Chapelle*,t, Vladimir Vapnik* \\n* AT&amp;T Research Labs, Red Bank, NJ \\nt LIP6, Paris, France \\n{ chapelle, vlad} @research. att. com \\nAbstract ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Document  ...                                                                                                                                                                                                    Paper\n","9            9  ...  103 \\nNEURAL NETWORKS FOR TEMPLATE MATCHING: \\nAPPLICATION TO REAL-TIME CLASSIFICATION \\nOF THE ACTION POTENTIALS OF REAL NEURONS \\nYiu-fai Wong, Jashojiban Banik]. and James M. Bower$ \\n]'Divisi...\n","13          13  ...  144 \\nSPEECH RECOGNITION EXPERIMENTS \\nWITH PERCEPTRONS \\nD. J. Burr \\nBell Communications Research \\nMorristown, NJ 07960 \\nABSTRACT \\nArtificial neural networks (ANNs) are capable of accurate re...\n","17          17  ...  219 \\nNetwork Generality, Training Required, \\nand Precision Required \\nJohn S. Denker and Ben S. Wittner \\nAT&T Bell Laboratories \\nHolmdel, New Jersey 07733 \\nKeep your hand on your wa]let. \\n--...\n","392        392  ...  Using Genetic Algorithms to Improve \\nPattern Classification Performance \\nEric I. Chang and Richard P. Lippmann \\nLincoln Laboratory, MIT \\nLexington, MA 02173-9108 \\nAbstract \\nGenetic algorithm...\n","503        503  ...  Neural Network Analysis of Event Related \\nPotentials and Electroencephalogram Predicts \\nVigilance \\nRita Venturini \\nWilliam W. Lytton \\nTerrence J. Sejnowski \\nComputational Neurobiology Labora...\n","681        681  ...  Mapping Between Neural and Physical \\nActivities of the Lobster Gastric Mill \\nKenji Doya \\nMary E.T. Boyle \\nAllen I. Selverston \\nDepartment of Biology \\nUniversity of California, San Diego \\nLa...\n","733        733  ...  Optimal Brain Surgeon: \\nExtensions and performance comparisons \\nBabak Hassibi* \\nDavid G. Stork \\nGregory Wolff \\nTakahiro Watanabe \\nRicoh California Research Center \\n2882 Sand Hill Road Suite...\n","906        906  ...  Extracting Rules from Artificial Neural Networks \\nwith Distributed Representations \\nSebastian Thrun \\nUniversity of Bonn \\nDepartment of Computer Science III \\nR6merstr. 164, D-53117 Bonn, Germa...\n","996        996  ...  A Dynamical Model of Context Dependencies for the \\nVestibulo-Ocular Reflex \\nO!ivier J.M.D. Coenen* \\nTerrence J. Sejnowskit \\nComputational Neurobiology Laboratory \\nHoward Hughes Medical Instit...\n","1622      1622  ...  Model Selection for Support Vector Machines \\nOlivier Chapelle*,t, Vladimir Vapnik* \\n* AT&T Research Labs, Red Bank, NJ \\nt LIP6, Paris, France \\n{ chapelle, vlad} @research. att. com \\nAbstract ...\n","\n","[10 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"markdown","metadata":{"id":"BlTE0SJeHZTG","colab_type":"text"},"source":["### Relevant Research Papers per Topic Based on Dominance\n","\n"]},{"cell_type":"code","metadata":{"id":"QBADJI-ybg5X","colab_type":"code","outputId":"5e7e64cb-94eb-4871-ebb1-83fd9cf35e68","executionInfo":{"status":"ok","timestamp":1590837067179,"user_tz":-180,"elapsed":594,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["corpus_topic_df.groupby('Dominant Topic').apply(lambda topic_set: (topic_set.sort_values(by=['Contribution %'], ascending=False).iloc[0])).reset_index(drop=True)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Document</th>\n","      <th>Dominant Topic</th>\n","      <th>Contribution %</th>\n","      <th>Topic Desc</th>\n","      <th>Paper</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>534</td>\n","      <td>1</td>\n","      <td>81.23</td>\n","      <td>distribution, probability, prior, variable, gaussian, mixture, estimate, density, bayesian, approximation, likelihood, sample, log, expert, em, estimation, posterior, step, component, probabilistic</td>\n","      <td>Polynomial Uniform Convergence of \\nRelative Frequencies to Probabilities \\nAlberto Bertoni, Paola Campadelll;' Anna Morpurgo, Sandra Panlzza \\nDipartimento di Scienze dell'Informazione \\nUniversi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>499</td>\n","      <td>2</td>\n","      <td>67.00</td>\n","      <td>training, prediction, kernel, test, training_set, regression, estimate, selection, machine, experiment, sample, test_set, cross_validation, measure, ensemble, regularization, variance, margin, ris...</td>\n","      <td>A Computational Mechanism To Account For \\nAveraged Modified Hand Trajectories \\nEalan A. Henis*and Tamar Flash \\nDepartment of Applied Mathematics and Computer Science \\nThe Weizmann Institute of...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>674</td>\n","      <td>3</td>\n","      <td>64.13</td>\n","      <td>circuit, chip, current, analog, voltage, neuron, implementation, processor, bit, design, device, computation, array, parallel, neural, digital, synapse, operation, hardware, transistor</td>\n","      <td>Analogy--Watershed or Waterloo? \\nStructural alignment and the development of \\nconnectionist models of analogy \\nDedre Gentner Arthur B. Markman \\nDepartment of Psychology Department of Psycholog...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1108</td>\n","      <td>4</td>\n","      <td>59.18</td>\n","      <td>image, feature, object, pixel, face, view, recognition, representation, scale, contour, surface, edge, shape, visual, part, scene, vision, digit, local, texture</td>\n","      <td>Using Feedforward Neural Networks to \\nMonitor Alertness from Changes in EEG \\nCorrelation and Coherence \\nScott Makeig \\nNaval Health Research Center, P.O. Box 85122 \\nSan Diego, CA 92186-5122 \\n...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1578</td>\n","      <td>5</td>\n","      <td>76.66</td>\n","      <td>cell, response, activity, stimulus, neuron, pattern, cortical, layer, receptive_field, cortex, connection, orientation, unit, visual, spatial, contrast, simulation, mechanism, population, synaptic</td>\n","      <td>The effect of eligibility traces on finding optimal memoryless \\npolicies in partially observable Markov decision processes \\nJohn Loch \\nDepartment of Computer Science \\nUniversity of Colorado \\n...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>765</td>\n","      <td>6</td>\n","      <td>69.49</td>\n","      <td>motion, direction, position, visual, target, control, field, velocity, movement, motor, trajectory, hand, location, arm, moving, human, response, spatial, feedback, sensory</td>\n","      <td>Development of Orientation and Ocular \\nDominance Columns in Infant Macaques \\nKlaus Obermayer \\nHoward Hughes Medical Institute \\nSMk-Institute \\nLa Jolla, CA 92037 \\nLynne Kiorpes \\nCenter for N...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1188</td>\n","      <td>7</td>\n","      <td>60.56</td>\n","      <td>equation, solution, convergence, gradient, vector, constraint, energy, iteration, rate, optimization, update, minimum, optimal, constant, gradient_descent, eq, step, matrix, condition, derivative</td>\n","      <td>Improving the Accuracy and Speed of \\nSupport Vector Machines \\nChris J.C. Burges \\nBell Laboratories \\nLucent Technologies, Room 3G429 \\n101 Crawford's Corner Road \\nHolmdel, NJ 07733-3030 \\nburg...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1194</td>\n","      <td>8</td>\n","      <td>63.18</td>\n","      <td>bound, theorem, class, threshold, approximation, proof, size, probability, loss, complexity, polynomial, theory, assume, linear, hypothesis, defined, definition, define, xi, constant</td>\n","      <td>Minimizing Statistical Bias with Queries \\nDavid A. Cohn \\nAdaptive Systems Group \\nHarlequin, Inc. \\nOne Cambridge Center \\nCambridge, MA 02142 \\ncohnharlequin. corn \\nAbstract \\nI describe a qu...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>284</td>\n","      <td>9</td>\n","      <td>75.39</td>\n","      <td>vector, linear, matrix, component, nonlinear, signal, source, filter, coefficient, operator, basis, transformation, pca, ica, projection, gaussian, representation, principal_component, rule, indep...</td>\n","      <td>A Systematic Study of the Input/Output Properties 149 \\nA Systematic Study of the Input/Output Properties \\nof a 2 Compartment Model Neuron \\nWith Active Membranes \\nPaul Rhodes \\nUniversity of Ca...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1667</td>\n","      <td>10</td>\n","      <td>61.37</td>\n","      <td>map, region, subject, location, effect, change, study, condition, light, experiment, et_al, brain, pair, normal, correlation, trial, eeg, site, left, theory</td>\n","      <td>The Infinite Gaussian Mixture Model \\nCarl Edward Rasmussen \\nDepartment of Mathematical Modelling \\nTechnical University of Denmark \\nBuilding 321, DK-2800 Kongens Lyngby, Denmark \\ncarl@imm.dtu....</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>970</td>\n","      <td>11</td>\n","      <td>61.86</td>\n","      <td>state, dynamic, neuron, memory, pattern, recurrent, attractor, module, capacity, connection, phase, hopfield, fixed_point, delay, behavior, neural, oscillator, stable, oscillation, sequence</td>\n","      <td>An Integrated Architecture of Adaptive Neural Network \\nControl for Dynamic Systems \\nLiu Ke '2 Robert L. Tokaf Brian D.McVey z \\nCenter for Nonlinear Studies, 2Applied Theoretical Physics Divis...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>174</td>\n","      <td>12</td>\n","      <td>56.83</td>\n","      <td>unit, training, layer, hidden_unit, net, architecture, pattern, trained, task, activation, back_propagation, hidden_layer, training_set, hidden, connection, learn, backpropagation, generalization,...</td>\n","      <td>297 \\nA NETWORK FOR IMAGE SEGMENTATION \\nUSING COLOR \\nAnya Hurlbert and Tomaso Poggio \\nCenter for Biological Information Processing at Whitaker College \\nDepartment of Brain and Cognitive Scienc...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>49</td>\n","      <td>13</td>\n","      <td>56.19</td>\n","      <td>rule, representation, structure, sequence, symbol, connectionist, language, level, unit, string, activation, context, pattern, role, similarity, represented, task, learned, note, part</td>\n","      <td>612 \\nConstrained Differential Optimization \\nJohn C. Platt \\nAlan H. Ban' \\nCalifornia Institute of Technology, Pasadena, CA 91125 \\nAbstract \\nMany optimization models of neural networks need co...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>241</td>\n","      <td>14</td>\n","      <td>65.96</td>\n","      <td>node, class, classification, classifier, pattern, tree, vector, code, probability, feature, bit, sample, label, binary, decision, stage, labeled, decision_tree, technique, error_rate</td>\n","      <td>524 Fahlman and Lebiere \\nThe Cascade-Correlation Learning Architecture \\nScott E. Fahlman and Christian Lebiere \\nSchool of Computer Science \\nCarnegie-Mellon University \\nPittsburgh, PA 15213 \\n...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>226</td>\n","      <td>15</td>\n","      <td>55.84</td>\n","      <td>neuron, signal, frequency, spike, channel, response, firing, temporal, stimulus, threshold, current, rate, neural, synaptic, auditory, event, amplitude, sound, phase, firing_rate</td>\n","      <td>380 Giles, Sun, Chen, Lee and Chen \\nHIGHER ORDER RECURRENT NETWORKS \\n&amp; GRAMMATICAL INFERENCE \\nC. L. Giles*, G. Z. Sun, H. H. Chen, Y. C. Lee, D. Chen \\nDepartment of Physics and Astronomy \\nand...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>658</td>\n","      <td>16</td>\n","      <td>71.47</td>\n","      <td>search, rate, experiment, task, strategy, table, application, user, instance, test, run, random, call, technique, average, block, feature, high, good, program</td>\n","      <td>Connected Letter Recognition with a \\nMulti-State Time Delay Neural Network \\nHermann Hild and Alex Waibel \\nSchool of Computer Science \\nCarnegie Mellon University \\nPittsburgh, PA 15213-3891, US...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1465</td>\n","      <td>17</td>\n","      <td>72.86</td>\n","      <td>word, recognition, speech, training, character, sequence, hmm, context, speaker, feature, frame, mlp, letter, trained, state, experiment, speech_recognition, phoneme, window, segmentation</td>\n","      <td>Dynamics of Supervised Learning with \\nRestricted Training Sets \\nA.C.C. Coolen \\nDept of Mathematics \\nKing's College London \\nStrand, London WC2R 2LS, UK \\ntcoolen @mth.kcl.ac.uk \\nD. Saad \\nNeu...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1198</td>\n","      <td>18</td>\n","      <td>50.44</td>\n","      <td>noise, average, distribution, curve, equation, correlation, theory, rate, limit, stochastic, optimal, random, solution, size, teacher, effect, simulation, student, eq, obtained</td>\n","      <td>Limitations of self-organizing maps for \\nvector quantization and multidimensional \\nscaling \\nArthur Flexer \\nThe Austrian Research Institute for Artificial Intelligence \\nSchottengasse 3, A-1010...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>44</td>\n","      <td>19</td>\n","      <td>70.92</td>\n","      <td>state, control, action, step, policy, controller, reinforcement_learning, environment, task, optimal, goal, robot, reward, td, agent, trial, transition, current, reinforcement, rl</td>\n","      <td>564 \\nPROGRAMMABLE SYNAPTIC CHIP FOR \\nELECTRONIC NEURAL NETWORKS \\nA. Moopenn, H. Langenbacher, A.P. Thakoor, and S.K. Khanna \\nJet Propulsion Laboratory \\nCalifornia Institute of Technology \\nPa...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>163</td>\n","      <td>20</td>\n","      <td>52.99</td>\n","      <td>local, distance, cluster, structure, graph, clustering, variable, dimensional, mapping, global, dimension, center, vector, cost, partition, neighborhood, prototype, constraint, interpolation, mani...</td>\n","      <td>Mapping Classifier Systems \\nInto Neural Networks \\nLawrence Davis \\nBBN Laboratories \\nBBN Systems and Technologies Corporation \\nl0 Moulton Street \\nCambridge, MA 02238 \\nJanuary 16, 1989 \\nAbst...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Document  ...                                                                                                                                                                                                    Paper\n","0        534  ...  Polynomial Uniform Convergence of \\nRelative Frequencies to Probabilities \\nAlberto Bertoni, Paola Campadelll;' Anna Morpurgo, Sandra Panlzza \\nDipartimento di Scienze dell'Informazione \\nUniversi...\n","1        499  ...  A Computational Mechanism To Account For \\nAveraged Modified Hand Trajectories \\nEalan A. Henis*and Tamar Flash \\nDepartment of Applied Mathematics and Computer Science \\nThe Weizmann Institute of...\n","2        674  ...  Analogy--Watershed or Waterloo? \\nStructural alignment and the development of \\nconnectionist models of analogy \\nDedre Gentner Arthur B. Markman \\nDepartment of Psychology Department of Psycholog...\n","3       1108  ...  Using Feedforward Neural Networks to \\nMonitor Alertness from Changes in EEG \\nCorrelation and Coherence \\nScott Makeig \\nNaval Health Research Center, P.O. Box 85122 \\nSan Diego, CA 92186-5122 \\n...\n","4       1578  ...  The effect of eligibility traces on finding optimal memoryless \\npolicies in partially observable Markov decision processes \\nJohn Loch \\nDepartment of Computer Science \\nUniversity of Colorado \\n...\n","5        765  ...  Development of Orientation and Ocular \\nDominance Columns in Infant Macaques \\nKlaus Obermayer \\nHoward Hughes Medical Institute \\nSMk-Institute \\nLa Jolla, CA 92037 \\nLynne Kiorpes \\nCenter for N...\n","6       1188  ...  Improving the Accuracy and Speed of \\nSupport Vector Machines \\nChris J.C. Burges \\nBell Laboratories \\nLucent Technologies, Room 3G429 \\n101 Crawford's Corner Road \\nHolmdel, NJ 07733-3030 \\nburg...\n","7       1194  ...  Minimizing Statistical Bias with Queries \\nDavid A. Cohn \\nAdaptive Systems Group \\nHarlequin, Inc. \\nOne Cambridge Center \\nCambridge, MA 02142 \\ncohnharlequin. corn \\nAbstract \\nI describe a qu...\n","8        284  ...  A Systematic Study of the Input/Output Properties 149 \\nA Systematic Study of the Input/Output Properties \\nof a 2 Compartment Model Neuron \\nWith Active Membranes \\nPaul Rhodes \\nUniversity of Ca...\n","9       1667  ...  The Infinite Gaussian Mixture Model \\nCarl Edward Rasmussen \\nDepartment of Mathematical Modelling \\nTechnical University of Denmark \\nBuilding 321, DK-2800 Kongens Lyngby, Denmark \\ncarl@imm.dtu....\n","10       970  ...  An Integrated Architecture of Adaptive Neural Network \\nControl for Dynamic Systems \\nLiu Ke '2 Robert L. Tokaf Brian D.McVey z \\nCenter for Nonlinear Studies, 2Applied Theoretical Physics Divis...\n","11       174  ...  297 \\nA NETWORK FOR IMAGE SEGMENTATION \\nUSING COLOR \\nAnya Hurlbert and Tomaso Poggio \\nCenter for Biological Information Processing at Whitaker College \\nDepartment of Brain and Cognitive Scienc...\n","12        49  ...  612 \\nConstrained Differential Optimization \\nJohn C. Platt \\nAlan H. Ban' \\nCalifornia Institute of Technology, Pasadena, CA 91125 \\nAbstract \\nMany optimization models of neural networks need co...\n","13       241  ...  524 Fahlman and Lebiere \\nThe Cascade-Correlation Learning Architecture \\nScott E. Fahlman and Christian Lebiere \\nSchool of Computer Science \\nCarnegie-Mellon University \\nPittsburgh, PA 15213 \\n...\n","14       226  ...  380 Giles, Sun, Chen, Lee and Chen \\nHIGHER ORDER RECURRENT NETWORKS \\n& GRAMMATICAL INFERENCE \\nC. L. Giles*, G. Z. Sun, H. H. Chen, Y. C. Lee, D. Chen \\nDepartment of Physics and Astronomy \\nand...\n","15       658  ...  Connected Letter Recognition with a \\nMulti-State Time Delay Neural Network \\nHermann Hild and Alex Waibel \\nSchool of Computer Science \\nCarnegie Mellon University \\nPittsburgh, PA 15213-3891, US...\n","16      1465  ...  Dynamics of Supervised Learning with \\nRestricted Training Sets \\nA.C.C. Coolen \\nDept of Mathematics \\nKing's College London \\nStrand, London WC2R 2LS, UK \\ntcoolen @mth.kcl.ac.uk \\nD. Saad \\nNeu...\n","17      1198  ...  Limitations of self-organizing maps for \\nvector quantization and multidimensional \\nscaling \\nArthur Flexer \\nThe Austrian Research Institute for Artificial Intelligence \\nSchottengasse 3, A-1010...\n","18        44  ...  564 \\nPROGRAMMABLE SYNAPTIC CHIP FOR \\nELECTRONIC NEURAL NETWORKS \\nA. Moopenn, H. Langenbacher, A.P. Thakoor, and S.K. Khanna \\nJet Propulsion Laboratory \\nCalifornia Institute of Technology \\nPa...\n","19       163  ...  Mapping Classifier Systems \\nInto Neural Networks \\nLawrence Davis \\nBBN Laboratories \\nBBN Systems and Technologies Corporation \\nl0 Moulton Street \\nCambridge, MA 02238 \\nJanuary 16, 1989 \\nAbst...\n","\n","[20 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"-alz1u6tSj1j","colab_type":"code","outputId":"4fc60e1c-3507-4555-8230-4992b439eba7","executionInfo":{"status":"ok","timestamp":1590837273637,"user_tz":-180,"elapsed":7892,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":605}},"source":["!pip install pyLDAvis"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting pyLDAvis\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n","\u001b[K     |████████████████████████████████| 1.6MB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.34.2)\n","Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.4)\n","Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n","Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.0.3)\n","Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.15.1)\n","Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n","Collecting funcy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n","\u001b[K     |████████████████████████████████| 552kB 17.3MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (46.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.12.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.3.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.8.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n","Building wheels for collected packages: pyLDAvis, funcy\n","  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=656574cb7cc65ba7dcf2fbe627f09e150d8f54b9e3494b3115645514b39d0619\n","  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n","  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=b49589f49de3605e4d8e066c877fd55ab962d399cc3c4e62586dad22d53925ba\n","  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n","Successfully built pyLDAvis funcy\n","Installing collected packages: funcy, pyLDAvis\n","Successfully installed funcy-1.14 pyLDAvis-2.1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CDb7PX9RR3Fd","colab_type":"code","colab":{}},"source":["import pyLDAvis.gensim as gensimvis\n","import pyLDAvis"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FBUT0iKKR3D4","colab_type":"code","outputId":"f16bc6c7-3356-4a0d-a1c9-d232dd2eb553","executionInfo":{"status":"error","timestamp":1590837386752,"user_tz":-180,"elapsed":683,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":320}},"source":["vis_data = gensimvis.prepare(opt_lda_model, bow_corpus, dictionary)\n","pyLDAvis.display(vis_data)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-86-a645596b7961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvis_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensimvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_lda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36m_extract_data\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[1;32m     46\u001b[0m           \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m           \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m       \u001b[0mdoc_topic_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'LdaMallet' object has no attribute 'inference'"]}]},{"cell_type":"code","metadata":{"id":"UlXp8ABDSixm","colab_type":"code","colab":{}},"source":["import gensim    \n","model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(opt_lda_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GdDJSSLwR3An","colab_type":"code","outputId":"26928266-3af2-4890-bcc6-7db18eaf3459","executionInfo":{"status":"ok","timestamp":1590837696957,"user_tz":-180,"elapsed":14020,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":861}},"source":["vis_data = gensimvis.prepare(model, bow_corpus, dictionary)\n","pyLDAvis.display(vis_data)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n","\n","\n","<div id=\"ldavis_el1211396432301332489065514564\"></div>\n","<script type=\"text/javascript\">\n","\n","var ldavis_el1211396432301332489065514564_data = {\"mdsDat\": {\"x\": [0.00023360362002030203, -0.0001264211716132407, -0.00048380550911517257, -0.00039334487591724434, -0.00045549425573743554, 6.841156347533084e-05, 0.0003853428411629807, 0.0004186624054402841, -0.00032692531493839236, -6.731700007063017e-05, 0.0004620427853027825, 0.0002471004282294583, 0.0002803159921455327, 2.349491840887833e-05, 0.0005957378559049027, 0.0005965543146766088, -0.000126698167746091, -0.00013563608265920897, -0.00046886708407814937, -0.0007267572628914942], \"y\": [0.0003628805003685012, 0.00015403874564159874, 0.00042611009593445, 0.0007901331982327056, -0.00046445237910639213, -0.0004333751183574462, 0.0005225032321916844, -0.00022413608292225397, -8.75325430048001e-05, 0.00034759856344242386, -0.0005602321900573325, -0.00019322800885363296, 0.00017407714034616757, -0.00021402178036841098, 0.00011074719182923432, -0.0002761666238449129, 0.00014644659805678452, 0.0002777274944248233, -0.0001920257332557358, -0.0006670923006974557], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [7.0659171630492565, 6.919488711942133, 6.865420789298475, 6.826867941988321, 6.777571158327304, 6.292930991447833, 5.885516322951093, 5.3602236085926185, 4.929795823671713, 4.90797393120636, 4.784893550733181, 4.7562019232760155, 4.312154247780768, 4.122357173823992, 4.016337921307049, 3.4161346842743163, 3.399034401266463, 3.331629343730489, 3.161332271503819, 2.8682180398288035]}, \"tinfo\": {\"Term\": [\"ance\", \"play_important\", \"paper_focus\", \"fundamentally\", \"modifies\", \"ic\", \"sequentially\", \"though\", \"ini\", \"unique\", \"p2\", \"averaged\", \"denoted\", \"gen_eral\", \"intensity\", \"solid\", \"corrupted\", \"september\", \"classifi_cation\", \"uniform_convergence\", \"equiv\", \"vapnik_estimation\", \"letting\", \"fi_fi\", \"joint_conf\", \"michael_mozer\", \"absence\", \"undergoes\", \"fulfilled\", \"distribu_tions\", \"asymptotically\", \"pyramid\", \"style\", \"world_scientific\", \"observed\", \"wright\", \"1o\", \"ed_advance\", \"partial_derivative\", \"hit\", \"par\", \"ben\", \"tolerant\", \"mason\", \"taylor_series\", \"keep\", \"eliminated\", \"1992a\", \"neu\", \"approximates\", \"phase_transition\", \"en\", \"boltzmann_machine\", \"much_simpler\", \"mdl\", \"particu\", \"etc\", \"optimum\", \"alternate\", \"essential\", \"ansatz\", \"aid\", \"robust\", \"exception\", \"acoustic_speech\", \"vari_ables\", \"dynamical_system\", \"devel\", \"capture\", \"proceed\", \"forward\", \"systematic\", \"seek\", \"conductance\", \"recovered\", \"l_\", \"charging\", \"reach\", \"compu_tational\", \"stereo\", \"lr\", \"abstract_describe\", \"sutton_barto\", \"dept_electrical\", \"allowing\", \"picked\", \"present\", \"purely\", \"mutation\", \"large_enough\", \"differentiate\", \"smc\", \"anns\", \"represents\", \"vi\", \"microelectronics\", \"extension\", \"bishop\", \"radius\", \"thomas\", \"concatenation\", \"elicited\", \"central\", \"major\", \"seemingly\", \"fire\", \"parametric\", \"tri\", \"andrew_barto\", \"generalize_well\", \"sequential\", \"paper_describe\", \"receiver\", \"millisecond\", \"fraction\", \"bringing\", \"ni\", \"numerous\", \"possibility\", \"unequal\", \"embodies\", \"topology\", \"susceptible\", \"amherst\", \"adapts\", \"ference\", \"far_away\", \"committee\", \"terminated\", \"demanding\", \"encountered\", \"guyon\", \"convolutional\", \"a_\", \"providing\", \"lateral_geniculate\", \"node\", \"computation\", \"improvement\", \"removal\", \"conveyed\", \"travel\", \"remained\", \"asynchronously\", \"differs\", \"fig_4b\", \"colorado_edu\", \"ending\", \"iw\", \"muscle\", \"nr\", \"favorable\", \"transforms\", \"kluwer_academic\", \"note\", \"sound\", \"jackel\", \"stationary\", \"conflicting\", \"article\", \"rela\", \"penalize\", \"transcription\", \"handwritten_digit\", \"upper_lower\", \"cij\", \"significantly\", \"sur\", \"ry\", \"gap\", \"roughly\", \"named\", \"induced\", \"coincide\", \"port\", \"circuitry\", \"termination\", \"itive\", \"attempt\", \"richer\", \"cost_function\", \"independently\", \"unifying\", \"edition\", \"distant\", \"ao\", \"previous\", \"object\", \"ontario\", \"oja\", \"stanford\", \"certainty\", \"np\", \"hamilton\", \"poor\", \"ously\", \"firing\", \"designing\", \"institute_technology\", \"produce\", \"route\", \"indirectly\", \"com_abstract\", \"maximally\", \"gate_voltage\", \"bartlett\", \"postsynaptic_potential\", \"ica\", \"unobserved\", \"nation\", \"wx\", \"interpolating\", \"replacing\", \"oped\", \"strongest\", \"indicative\", \"sequential_decision\", \"appropriate\", \"neu_ron\", \"capability\", \"tonic\", \"tolerance\", \"additionally\", \"sc\", \"initial\", \"digitized\", \"wagner\", \"experi_ments\", \"i0\", \"oh\", \"co\", \"le\", \"bold\", \"notation\", \"site\", \"caltech_edu\", \"viewing\", \"addi_tional\", \"stabilization\", \"rodney\", \"competition\", \"filling\", \"ft\", \"copy\", \"scott\", \"ghahramani\", \"academy_science\", \"neuromorphic\", \"relatively_simple\", \"triangular\", \"automated\", \"saturating\", \"neigh\", \"interested_reader\", \"loading\", \"black_box\", \"temporary\", \"ary\", \"changing\", \"researcher\", \"checking\", \"computer_society\", \"whilst\", \"mirror\", \"hierarchical\", \"conceptually\", \"kung\", \"chapter_page\", \"reinforcement\", \"concatenation\", \"artificially\", \"hall\", \"convolutional\", \"arithmetic\", \"isolation\", \"tuple\", \"consis\", \"located\", \"obtaining\", \"sys\", \"transition\", \"lw\", \"propagated\", \"begun\", \"us\", \"jensen_inequality\", \"row\", \"hop_field\", \"hard\", \"stretch\", \"sual\", \"vation\", \"trial\", \"computational\", \"ku\", \"uniform_distribution\", \"lq\", \"special_case\", \"code\", \"inhibitory_connection\", \"ized\", \"refractory_period\", \"preset\", \"x1\", \"na\", \"scan\", \"z0\", \"additive_gaussian\", \"infomax\", \"bernhard\", \"archi_tecture\", \"slightly_different\", \"mation\", \"default\", \"date\", \"history\", \"wang\", \"changed\", \"stored_memory\", \"cat\", \"oj\", \"deterministic_annealing\", \"constrained\", \"return\", \"nist\", \"jerusalem_israel\", \"occurring\", \"oriented\", \"recalled\", \"conditional_independence\", \"variate\", \"spring\", \"sollich\", \"successfully_applied\", \"demonstrate\", \"sition\", \"erlbaum\", \"incorrect\", \"steady\", \"pen\", \"max_planck\", \"concise\", \"nelson\", \"receives\", \"efficacy\", \"categorization\", \"periphery\", \"encoded\", \"make_sense\", \"extraction\", \"conf\", \"rich\", \"tune\", \"vlsi_implementation\", \"partitioned\", \"classi_fication\", \"compu_tation\", \"schwartz\", \"treating\", \"gaus_sian\", \"sm\", \"hubbard\", \"formally\", \"stimulated\", \"specified\", \"mining\", \"absorbing\", \"volume_page\", \"irrelevant\", \"logarithmically\", \"paired\", \"ib\", \"university_maryland\", \"sept\", \"nettalk\", \"chitecture\", \"pu\", \"rewritten\", \"key\", \"platt\", \"sens\", \"horizontal_vertical\", \"chemistry\", \"physiology\", \"digit_recognition\", \"park\", \"partitioning\", \"indication\", \"analysed\", \"penalty\", \"vg\", \"elec\", \"mations\", \"plus\", \"successfully\", \"generafive\", \"potentiation\", \"conjugate_gradient\", \"valuable\", \"depends_upon\", \"quantization\", \"page_springer\", \"class\", \"weak\", \"concluding_remark\", \"berger\", \"berkeley_ca\", \"6b\", \"handwritten_zip\", \"syst\", \"rose\", \"mech\", \"int_conf\", \"conclusion_future\", \"ill\", \"hy\", \"ratio\", \"gradient_ascent\", \"attractor\", \"fully\", \"general_ization\", \"applying\", \"kuhn\", \"writing\", \"involving\", \"short_term\", \"gene\", \"compactly\", \"exp_exp\", \"conference_acoustic\", \"ex\", \"analyzer\", \"additive\", \"tolerant\", \"cse\", \"contribute\", \"5i\", \"germany_abstract\", \"psy\", \"imagine\", \"investigated\", \"policy\", \"ave\", \"california_institute\", \"logarithmic\", \"compete\", \"massachusetts_amherst\", \"linguistic\", \"simulates\", \"connect\", \"sine_wave\", \"sequence\", \"generalizing\", \"affine\", \"poisson\", \"richard\", \"unsupervised_learning\", \"ed_advance\", \"generalised\", \"projected_onto\", \"biological_cybernetics\", \"identically\", \"constituent\", \"characterization\", \"discriminated\", \"ensemble\", \"potentiation\", \"discrimination\", \"rem\", \"touretzky_hinton\", \"prog\", \"john_denker\", \"randomized\", \"log_log\", \"steady_state\", \"visited\", \"cut\", \"ai_ai\", \"dark\", \"bu\", \"wiley\", \"placing\", \"arm\", \"silverman\", \"utility\", \"dataset\", \"relax\", \"developed\", \"mcgraw_hill\", \"ha_proven\", \"voiced\", \"special_purpose\", \"ex_ample\", \"responding\", \"main_reason\", \"illustrative\", \"look_like\", \"cp\", \"rumelhart_mcclelland\", \"royal_society\", \"head\", \"corpus\", \"overfit\", \"rectangular\", \"datasets\", \"logarithm\", \"converter\", \"processing_exploration\", \"interpretable\", \"ter\", \"purpose\", \"posteriori_probability\", \"action\", \"tran\", \"computer\", \"quiescent\", \"two_dimensional\", \"well_suited\", \"english_text\", \"time_varying\", \"aggregate\", \"initial_condition\", \"experiment_conducted\", \"deep\", \"girosi\", \"conditional_density\", \"turned\", \"structuring\", \"contributing\", \"monotonic\", \"cluster\", \"standing\", \"decomposed\", \"sv\", \"conver_gence\", \"ff\", \"copied\", \"sending\", \"emulate\", \"comparable\", \"iter\", \"englewood_cliff\", \"bright\", \"american\", \"dif_ferent\", \"mated\", \"amir\", \"pa_rameters\", \"table_summarizes\", \"expectation\", \"fragment\", \"salient\", \"pca\", \"stage\", \"floating_point\", \"derivation\", \"motor_control\", \"opt_soc\", \"uncertainty\", \"house\", \"geometry\", \"ra\", \"ingredient\", \"artificial_intelligence\", \"concrete\", \"school_computer\", \"guaranteed_converge\", \"pij\", \"repre_sentations\", \"metropolis\", \"comp_physiol\", \"vapnik_chervonenkis\", \"empirically\", \"training_set\", \"wiring\", \"tar\", \"spoken\", \"harmonic\", \"schapire\", \"berkeley_edu\", \"gl\", \"crl\", \"regression\", \"satisfies\", \"smaller\", \"graphic\", \"nervous_system\", \"concatenation\", \"gray_level\", \"convergence\", \"sensorimotor\", \"s0\", \"xp\", \"untrained\", \"component\", \"updated\", \"ck\", \"feature\", \"n2\", \"decomposed\", \"berlin\", \"actuator\", \"international_journal\", \"forced\", \"taken_account\", \"exploration_microstructure\", \"microstructure_cognition\", \"national_laboratory\", \"branch\", \"study\", \"classification\", \"iris\", \"sub\", \"daily\", \"performs\", \"log2\", \"mind\", \"conveniently\", \"stop\", \"transcription\", \"demonstration\", \"much_smaller\", \"vn\", \"doubled\", \"bayesian_restoration\", \"az\", \"lim\", \"sparsely\", \"forcing\", \"linsker\", \"mn\", \"bernhard\", \"suffers\", \"listed_table\", \"ini\", \"ensure\", \"semantics\", \"correct\", \"pointing\", \"solid\", \"ance\", \"xo\", \"cardoso\", \"closed\", \"belmont\", \"better_understanding\", \"pro_gramming\", \"obviously\", \"rest\", \"substantially\", \"much\", \"technical_report\", \"sanger\", \"fulfilled\", \"proposal\", \"equiv\", \"cam_bridge\", \"sorted\", \"prototype\", \"iii\", \"zipset\", \"cubic\", \"thirty\", \"corrupted\", \"unseen\", \"macroscopic\", \"p2\", \"solves\", \"infant\", \"fitted\", \"san_francisco\", \"reliable\", \"ad_hoc\", \"attributed\", \"averaged\", \"meeting\", \"recon\", \"annealed\", \"vowel\", \"exceeding\", \"oscillatory\", \"called\", \"zip_code\", \"subthreshold\", \"ez\", \"el\", \"shared\", \"secondly\", \"sume\", \"specification\", \"maintaining\", \"tit\", \"geman\", \"worst\", \"ant\", \"infor_mation\", \"closer\", \"posterior_distribution\", \"predefined\", \"illustrated_fig\", \"back_propagating\", \"threshold\", \"unchanged\", \"eval\", \"ko\", \"local_maximum\", \"qi\", \"vapnik_estimation\", \"charles\", \"gold\", \"ontario\", \"quiescent\", \"envelope\", \"ranking\", \"lu\", \"kr\", \"conveys\", \"hornik\", \"decision_boundary\", \"finer\", \"geiger\", \"pittsburgh_pa\", \"visible\", \"rejection\", \"fac\", \"nasa\", \"cc\", \"descriptive\", \"take_place\", \"non_zero\", \"modifies\", \"tony\", \"replacement\", \"con_tinuous\", \"recon\", \"hubel_wiesel\", \"approximation\", \"sr\", \"eye_movement\", \"committee\", \"maximized\", \"bell_lab\", \"quarter\", \"implication\", \"likelihood\", \"pre_post\", \"lateral_geniculate\", \"expands\", \"condi\", \"kn\", \"acknowledgment_author\", \"usefulness\", \"grossberg\", \"haussler\", \"selec\", \"undergoes\", \"ia\", \"score\", \"gaze\", \"text\", \"classifi_cation\", \"bayesian_inference\", \"proof_theorem\", \"expert\", \"jp\", \"dim\", \"dynamic_programming\", \"benchmark\", \"necessity\", \"thought\", \"pv\", \"suited\", \"expected_reward\", \"equipped\", \"existing\", \"degree_freedom\", \"marginal\", \"integrator\", \"kinetic\", \"cre\", \"survey\", \"fi_fi\", \"speaker_dependent\", \"expressing\", \"5c\", \"modification\", \"partially\", \"stack\", \"concatenation\", \"sensorimotor\", \"gas\", \"processing\", \"intensity\", \"gen_eral\", \"cor_responding\", \"denoted\", \"z_\", \"allow\", \"rij\", \"se_quence\", \"neuron\", \"mini\", \"duce\", \"leaky\", \"fulfill\", \"__\", \"iti\", \"multi\", \"convex\", \"sion\", \"randomized\", \"fundamentally\", \"construction\", \"operate\", \"absence\", \"determines\", \"harvard\", \"centered_around\", \"com\", \"2c\", \"emphasis\", \"wta\", \"er\", \"unclear\", \"lateral_geniculate\", \"saturated\", \"introduced\", \"self\", \"unique_solution\", \"rev\"], \"Freq\": [237.0, 236.0, 238.0, 246.0, 235.0, 240.0, 237.0, 234.0, 238.0, 230.0, 245.0, 239.0, 233.0, 252.0, 239.0, 245.0, 251.0, 242.0, 243.0, 239.0, 243.0, 253.0, 243.0, 235.0, 231.0, 241.0, 240.0, 237.0, 237.0, 235.0, 23.83685921328431, 22.925519862121064, 24.021164129706435, 23.093567539817187, 23.063366459544987, 23.057824573151287, 22.35624709985971, 22.681691050832637, 22.323715511544894, 22.309999429816507, 22.614256691286172, 21.793978011924633, 22.128183246714254, 22.26487780036087, 21.93941626679101, 21.994697578839336, 21.558688218366672, 21.789623738598642, 21.509829051444505, 21.226133182392143, 22.30974798810726, 22.48411993420493, 21.091449576043352, 21.621783131799877, 21.005700161917776, 21.3497960608078, 22.904783469168443, 22.18138941983011, 21.43588646055836, 22.504163432980356, 22.576179236186817, 21.817082903109274, 22.006905114118318, 21.97564649348145, 22.053119639170895, 21.645634462933955, 21.704060644454074, 24.214671122662935, 22.925532268524826, 23.10867118280757, 22.57716725223657, 22.940521608747385, 22.35558197704479, 21.227322340322917, 21.305525165982136, 22.443056692854665, 21.74533960755025, 22.701484417387547, 22.122564669421312, 21.663481573226154, 20.646846168045915, 21.43769365799849, 22.593491777369408, 21.61129353959077, 21.04495641643364, 21.64399449353301, 21.459226903745545, 21.633565722395556, 20.879047678125108, 21.337665395954193, 20.793075504462017, 20.91984080875902, 21.00437265110323, 20.679567192888243, 20.520989000847866, 20.463486125835992, 21.270560259293116, 20.887677510000643, 21.47079327511937, 21.75351076911185, 21.882969342112478, 21.746244255581132, 21.48216961826311, 21.482147866169793, 21.436110808527246, 21.349429728427463, 21.29797929162278, 21.248918244253197, 22.21962217393546, 20.801246514229693, 21.634673782276394, 20.83392344958687, 21.17891585666656, 22.359482926598183, 21.74195753608544, 21.886117883523188, 20.880199024026755, 22.207502805730783, 21.406664034541556, 21.339902423139137, 21.855971678955243, 21.427370321449985, 20.561959572007996, 21.02477848765027, 21.688475540791085, 21.591868129875813, 21.316432377869535, 20.677027350488633, 20.51199006502505, 20.53480845703168, 20.4459992377264, 21.081099534073942, 21.39006003921869, 20.608272237052468, 21.197837428320096, 21.5138498685779, 21.686801213467113, 20.850913491369653, 21.290436607407656, 21.849424756277635, 21.458883865744408, 21.31715108541632, 21.476048843705794, 21.38577027524307, 21.394543123329886, 21.32068710153897, 21.159100418064007, 21.098801170015978, 21.205888654830215, 21.13895498568318, 21.125741901602517, 24.614647283392443, 23.3046949882398, 23.278171820490847, 23.164907967165668, 23.143737154359126, 21.862395564978716, 22.9042531733807, 22.29578804467999, 21.479723030915572, 21.54141139767189, 20.929471998766772, 21.33312076999897, 20.89926522674084, 21.781592724095812, 20.824245703971936, 21.369440262593045, 21.49258172755448, 21.55534107774079, 20.756550749152705, 21.275477522466485, 21.235159136923023, 20.69502446370586, 21.31367676792253, 21.861968641490723, 21.120807776511654, 21.33565128315431, 21.001425839743245, 20.831303010347966, 21.699454618126804, 20.730641114784092, 21.45991258592931, 21.836130288751953, 21.780225316401175, 21.525350309483265, 21.41939313089599, 21.094096939537128, 21.073265540201824, 21.19750250401475, 21.11490485837131, 23.081136362290003, 23.373267745841158, 21.601568108275515, 22.321715633080256, 22.25464421780959, 22.832461723798463, 22.00769952474208, 22.009369957949854, 21.762629610789727, 20.637612506785143, 21.068514738337615, 21.435967519377172, 20.5085694053602, 20.57710512773694, 20.49649032685755, 21.733907704881137, 21.29051132414517, 21.266447921251302, 21.864858300447473, 21.863769081715212, 21.50427245969632, 20.102616231018906, 21.38313679238086, 20.45555389015274, 21.545675246398016, 21.04084066024442, 20.803809150089055, 20.59593844093387, 20.904434010615766, 20.697009383541243, 21.04613504908751, 21.264339293951437, 21.51999748878505, 21.18406021970548, 21.183649616707648, 21.09515851544214, 20.966928414666892, 21.054771057264784, 21.247867512266485, 21.020050294952153, 21.047758154272817, 21.247024252301973, 20.787872748384043, 19.999054121269616, 20.426502509699727, 20.58413833362617, 20.478056493733657, 20.132303550072898, 19.266809806923472, 19.97204930376149, 19.900791326883265, 19.563206004915173, 19.32079864304152, 19.36195500146606, 19.308305077348944, 18.952245431117387, 19.20035820265245, 19.442134909721094, 19.64065442588221, 19.944312938168476, 19.326121135341985, 19.03472586105758, 19.30884982318497, 20.605630168853324, 19.28335338630909, 18.767111883552808, 19.880376300168116, 19.44589442299047, 18.638144655661144, 18.882108364485852, 19.78095046686515, 20.101268438855705, 19.64063808446177, 19.376995916806013, 19.557583468976134, 19.651114503520606, 19.763812095399594, 19.718171716199233, 19.41135289997472, 19.65773658558793, 19.513709117440516, 19.39258477654046, 21.240591036642563, 19.142581475718192, 18.371161135339477, 18.52142486252161, 18.493821804432535, 18.743110688808237, 18.363501856480077, 17.938233752221016, 18.913958720934765, 18.2218915362018, 18.11842685464697, 17.979540308548792, 18.92486220492648, 17.790553426028865, 18.595158147986858, 17.854098634886075, 17.47970469898187, 17.858970079132412, 19.16427021739103, 18.22084698410392, 17.6487482889867, 18.364224585850526, 17.86715139598292, 18.611590170257067, 17.536503057515304, 18.024160581230316, 18.602876908105454, 17.842086710695295, 18.470887187191412, 17.941703955553436, 18.305611193873244, 18.569248271382353, 18.389779203553413, 18.504335885981924, 18.355826428956647, 18.215650287747174, 18.48904203224304, 18.327356941917657, 18.40756814530197, 18.250302678563564, 18.215577083849112, 18.235238943756986, 18.253300998002747, 17.709463881047864, 17.75156303925832, 17.057332312665984, 17.453341062143487, 17.066209592476984, 16.452187327293732, 16.93102795156288, 17.235335177608004, 16.938320593645145, 16.425256823997525, 16.62248743124254, 17.128879650905915, 16.166965733288688, 16.40537901592646, 16.629909224352954, 16.574172379097266, 16.47558664277032, 16.028853160652723, 16.382266936169554, 16.265641287048027, 16.26932414555398, 16.09127157744606, 15.889359317019279, 16.21241834700159, 16.256420153284697, 16.540672041541264, 16.016972367975203, 16.450512755408, 16.700430784150896, 16.21001740402278, 16.88877722865422, 16.74301725195419, 16.969261424590023, 16.57254396288983, 16.59893470328591, 16.45223960910024, 16.513583011101293, 16.476113012236883, 16.494484969788907, 16.56551442734793, 16.45315660734347, 16.462116325849784, 17.100070988775645, 16.35512541855545, 16.13896742168149, 16.445473938004906, 15.732014405433695, 15.504675658187859, 15.953816023792617, 17.102580393947154, 16.342528372894453, 16.177754237437533, 15.70843531405934, 15.52433676937924, 14.649416026395143, 15.711790043001898, 15.495495441185684, 15.48407012468596, 15.239845094072635, 15.018269257993195, 15.253644192461984, 14.94087942483438, 15.239206867245677, 15.01178136469546, 14.918106555695234, 14.729269263800159, 15.44628357547179, 15.258880461907678, 15.34003672112897, 14.413078332479712, 15.23798528290422, 14.672337736794304, 15.814464360917713, 15.420226920476228, 15.408764878558284, 15.51317480777402, 15.515487507075715, 15.368426212800824, 15.357867677760272, 15.368392327877274, 15.293193295792308, 15.282145362106547, 15.278856500974559, 17.489092671021062, 16.333937975728233, 16.907802075111658, 15.886129322060766, 15.683338834998079, 15.873236129695199, 16.02697347486893, 16.11332846521162, 15.500484078442284, 15.292402895403297, 15.23716137968539, 15.436154190958685, 15.532619557150863, 15.495691996728981, 15.611880407298699, 15.644854532537156, 15.179411010998525, 15.287785485866166, 14.875175813740276, 15.448131536424896, 14.953242640745131, 15.653341624122126, 14.994205804401084, 15.957729326168797, 14.833520423949196, 14.915596466227202, 15.443194542638766, 15.39155947700133, 15.352911682812923, 14.875737545714959, 15.321508546832357, 15.031703084116527, 15.289169551327136, 15.17828388630598, 15.087082093759834, 15.037964363824775, 15.065602867294707, 16.097477164182163, 15.56641777072467, 15.322014129106028, 15.248022976627952, 15.103029192665733, 15.710194185927623, 15.344241476125442, 14.708994754484474, 14.91201829788938, 14.896870818798517, 14.37973361859757, 15.412009297595683, 15.109599956867484, 15.544196196556943, 14.989027693325621, 15.36487012542798, 14.637360543759932, 14.723262188728214, 14.914356401441166, 14.779469694057978, 15.221651440728936, 14.677801954142108, 15.11041921381619, 14.876562835931754, 15.021380463285226, 14.504540831854253, 14.39610082322229, 14.381641935287663, 14.91517801588346, 14.884168055630086, 15.215704130484252, 14.94142366145382, 14.67864075537446, 14.865985217683043, 14.748443924132498, 14.799486748962048, 14.758181695669002, 14.782419791023951, 14.751587055739057, 15.845672004645815, 15.677094451212527, 15.266969887188269, 15.132584889151557, 15.14363425280936, 15.527890579943945, 14.85297287489124, 14.694906485089417, 15.406133913517325, 14.757548619847416, 15.122197570000642, 15.136545278321208, 15.20052189300652, 14.838768760495817, 14.622919068695579, 14.272665503852975, 14.691904850917274, 14.344570451849354, 14.452788450702624, 14.007355380402386, 14.074992156403992, 14.549335198085396, 14.610844129319835, 14.305540209675698, 14.397481396295616, 14.93816061836057, 14.205177836130128, 15.384834551868556, 13.955254980814859, 14.406900586034938, 14.301596509126384, 14.878103453828745, 14.583898975597602, 14.62238124872697, 14.760807515388889, 14.9418115535904, 14.39846607639294, 14.514651792345195, 14.475847801570522, 14.471532136464477, 14.418846327042175, 14.422566371641164, 14.233523585458974, 14.162527484250296, 14.167229431385671, 14.012427964582225, 14.084609798184951, 14.970589365870943, 14.278841722469657, 13.763673723791733, 14.171124815205602, 13.785740665207161, 13.851025474313632, 13.450818439907104, 13.47674793472788, 13.469694445800235, 14.028565370679438, 12.881480688522029, 13.65391557970237, 13.637419148659973, 13.526109044847079, 13.067446910898417, 13.435807134600266, 13.166713328209932, 13.377874136208298, 13.354257290787407, 13.748964767385717, 12.872203315485095, 13.589104139388693, 13.295625981058434, 12.524318902531872, 13.187378237859823, 13.655908863833893, 13.51022698083198, 13.779418627859283, 13.465832942301695, 13.400931780453002, 13.400351591305679, 13.3683139902805, 14.18226791539631, 13.5926224006748, 13.887415742119899, 13.553124180080896, 12.946199807858298, 12.80426018040159, 13.484818780681803, 13.049292821369168, 13.42557183382, 13.130679878254945, 12.974631376305686, 12.756353336980474, 12.893416065681446, 13.090662503576652, 13.261059828214304, 12.671443542679965, 12.466129380729898, 12.69293421752513, 12.832851756383365, 12.65318187625676, 12.635967033576039, 12.438150903592438, 12.263123774013192, 12.321793092637925, 12.647076103268109, 12.363892137134675, 12.690242672244423, 12.683481929985268, 13.09814462595298, 12.572631410544595, 13.106810328414667, 12.823654071884182, 12.85403548513742, 13.147190200834014, 13.19389411744935, 12.887821166603482, 12.749317051318968, 12.663012911768737, 12.665057184108141, 12.664079689460781, 12.828255109853949, 12.955938710369104, 13.11680818094409, 12.613887409001547, 12.914564128129369, 12.651389687622572, 12.820501373583044, 12.32931762750164, 12.061259683742728, 12.713408028696948, 12.955857364293896, 12.766114410536314, 12.067548877535515, 12.253773811404768, 12.336750992271218, 12.76212125386983, 12.163023178939241, 12.856291514686879, 12.439070643583051, 12.301805658517482, 12.564677166267709, 12.159461889648492, 12.40801690326611, 12.255983625428854, 12.547785799822694, 12.230878678638218, 12.19024537767375, 11.99116496181943, 11.72066296235316, 12.41950455366366, 12.186340308636954, 12.406973369251311, 12.46252018677939, 12.490681097283723, 12.639547944492765, 12.434780220097478, 12.295309580527679, 12.270531074036628, 12.381176444792967, 11.763333085233667, 11.570233351791753, 11.292333803375243, 11.372402485105672, 11.194968737630491, 10.876462099498392, 10.810607033547436, 11.030305859743331, 10.641697999708875, 10.656670019308894, 11.011839674747451, 10.728921962796301, 10.627620651572977, 10.772203527782766, 10.50918202157019, 10.611936188713107, 10.769076374998347, 10.19705648640382, 10.782610660678628, 10.661844245133718, 10.441075991059279, 10.413342067406676, 10.517000182395131, 10.630281662636099, 10.432055734755421, 10.526045989865006, 10.45063732597539, 10.222442067065709, 10.469448415286083, 10.754958908045905, 10.62926081532189, 10.842094463530733, 10.660546696330172, 10.587777021107309, 10.55490332449366, 10.55177957504744, 10.513751334707848, 11.942869451860568, 11.59485233090824, 10.892439390899915, 11.098970040875482, 11.464772761136835, 10.721410183479735, 10.838550749625105, 10.503594194025087, 10.273821390871744, 10.19280017308076, 10.793742860740252, 10.255568526728197, 10.201609621412011, 10.366944030038944, 10.32759973023409, 10.491756263709792, 10.636953706104823, 10.641661803773296, 10.394386463764286, 10.978826383761803, 10.231286788048688, 10.23031659840095, 10.368262629256797, 10.623119329008539, 10.492512574539214, 10.579559184279141, 10.126462525339154, 10.600937671563885, 10.146683518387997, 10.070444022719048, 10.532756033706928, 10.413628102219917, 10.418976009942279, 10.454053758708174, 10.440189600012165, 10.615382595247423, 10.456796161877717, 10.50274244205757, 10.631714763042554, 10.524802286856918, 10.552626614644522, 10.602501543765216, 10.58376649491381, 10.504890190281, 10.50144080143221, 11.167107754091823, 11.141605681804615, 10.71561532495252, 10.642948637265222, 10.40454812867729, 10.462695432671659, 10.5182982200392, 10.359993136213294, 10.456135539722753, 10.638788996428167, 10.633083809486248, 10.249999703058402, 10.127953492044917, 10.303205698053485, 10.500827002334667, 10.060131576645693, 10.10210614684679, 10.495257946314723, 10.246975567205057, 10.0615320588031, 9.977259008549515, 10.639759841178133, 10.687573143298664, 10.42224557248327, 10.116502545187087, 9.998953414463186, 10.4988433870747, 10.425190828054527, 9.99983776053648, 9.921452155183543, 10.312528002452462, 10.5763034571472, 10.43946462655212, 10.324004276932914, 10.291834685049432, 10.24615874000069, 10.317864528769348, 10.278053071327108, 10.256538292408829, 10.19807241392793, 10.2827825537323, 10.201258435091983, 9.996313421315103, 9.995723623219881, 9.766111802016749, 9.802135356138677, 10.119659993532073, 10.097214232601841, 10.108953640607636, 9.97095174436072, 10.257052898005691, 9.934713770452866, 10.01630348174424, 10.087950579356704, 9.793758343888284, 10.112331777951297, 9.748809938239066, 9.821814277877545, 9.789147754529981, 9.813466328443132, 9.43831062340139, 9.389076692239316, 9.706430061872956, 9.800916397803183, 9.788199619606507, 9.531289337872051, 9.712108668995326, 9.47689855454729, 9.290789853792623, 9.590550400015347, 9.976885860710624, 9.860533325728753, 9.87597020067495, 9.75294280495886, 10.122338484807715, 9.774226582376523, 9.898264701739512, 9.979239594302122, 9.6691217012799, 9.912974564336327, 9.344072828874102, 8.96780781815864, 9.083259362774527, 9.311330835897332, 9.493022462050593, 8.939386902024982, 8.898810829199903, 8.997552280591496, 9.219793416793351, 8.755872630256587, 8.725906085938986, 9.063463387299697, 9.139516141738138, 8.954221397560328, 9.15246522905377, 9.14930952604634, 9.254196944473076, 9.110125969593179, 8.86579699786202, 8.943400616300558, 8.835024267005068, 8.67971552349088, 8.72324459944023, 8.881635088803025, 8.85449886556666, 8.756152169982718, 8.717876804695784, 8.855149776759466, 8.979738354399025, 8.920275347131263, 8.983831550494177, 8.96190418817707, 8.925310153850532, 8.906900202952233, 8.862282391631933, 8.862258295232031], \"Total\": [237.0, 236.0, 238.0, 246.0, 235.0, 240.0, 237.0, 234.0, 238.0, 230.0, 245.0, 239.0, 233.0, 252.0, 239.0, 245.0, 251.0, 242.0, 243.0, 239.0, 243.0, 253.0, 243.0, 235.0, 231.0, 241.0, 240.0, 237.0, 237.0, 235.0, 238.9409617177473, 238.03994877246134, 250.76402909184506, 243.42720158646995, 243.19674166164373, 246.08215565532188, 241.7363446144676, 246.0582036193468, 242.21257252321115, 242.6527552546984, 246.9191682503643, 238.97422473610905, 243.84146215962912, 246.72340399659774, 243.23595524424155, 243.87550983909688, 239.4517136949385, 242.37764540047166, 240.2991379164153, 237.1454052989234, 249.53444937681925, 251.89128544991937, 236.41850665079016, 242.39753972395283, 235.97227067915222, 239.89583353098826, 257.79876052101446, 249.90256466713913, 241.73430239552314, 253.8249322552919, 257.4933859866814, 247.61413355966886, 250.82814005639946, 252.65255800178926, 260.82714905300304, 248.12598921530727, 255.39804326199356, 247.38318489582463, 244.9200685892891, 247.86082756293175, 243.47184105647062, 247.794690327744, 243.44402828614062, 231.91457813127823, 233.3157298507142, 248.2404497724532, 240.99837483846167, 251.9940611843724, 247.44939378778201, 242.41847771057166, 231.1063340625992, 240.82234633288928, 256.21114022990724, 246.10156420974596, 239.95374510836152, 246.78461018997476, 244.8811838978585, 247.62404244078678, 239.60581191058986, 244.94517588253447, 238.84487476545488, 240.80230877562747, 241.85940190433817, 239.34991941426983, 237.5850336454914, 237.3408719760093, 246.91248975663643, 242.6295382903831, 250.58734718096292, 254.7696375982582, 257.4723341807731, 255.69497142513148, 252.58416963190655, 253.09780006119507, 253.38812022862868, 252.10973328731717, 252.0152098626576, 251.6230509612551, 245.71875415214097, 233.14319270422484, 242.5528003379169, 234.13556982974774, 238.36426906097185, 252.0322665623294, 245.34970221561952, 247.1589146993043, 235.92485477044485, 251.10750912286957, 242.24995911116525, 242.36692579131397, 248.38336149437643, 243.70367259929048, 234.29932951392522, 239.90096654180834, 247.6782962483219, 246.59306981520706, 243.98026168626723, 236.7116060216895, 235.63250673439845, 235.92529709373318, 235.211038446413, 242.95888858092462, 246.56211766976745, 238.7784467210772, 246.27106929833374, 250.00075985987738, 252.28583523693212, 242.57159324724904, 247.91093138628247, 256.5918644134643, 251.0837586636312, 249.3818665588248, 252.39752130342623, 251.02970656815666, 252.08978398114303, 251.18082674791293, 247.5003375994762, 246.07160865043397, 251.78676570082968, 249.15215697498945, 249.56615576736405, 250.1290147153856, 243.9408375539477, 247.85238876676638, 247.4751792384271, 249.44793058150552, 237.96189586821595, 249.60732294387006, 245.52934862110345, 237.31268826011154, 239.49381687359602, 233.40097346307192, 240.01950007792826, 235.15117909693774, 245.29062566562578, 234.98914673009057, 241.53037185919496, 243.09085544633925, 244.57165694471652, 236.48990626322637, 242.44092241372394, 242.0538858616708, 236.54065828410836, 243.6798691758306, 250.06978114487885, 241.96269305583706, 245.27929917432115, 242.297024757473, 240.74825154631512, 250.88940084508292, 239.8540632415918, 248.46201241373893, 253.27005151341905, 253.9679528938583, 251.44093835279952, 251.77592629964388, 245.02786018840607, 245.9164696957999, 257.66472365324836, 253.98206694411903, 238.95350402242832, 248.47142573914078, 231.53143099716291, 242.20308890419807, 243.23194643345948, 249.64560783655503, 242.69490348342725, 243.34902310701696, 241.80998493542955, 229.46169394757018, 235.66696141350562, 239.93660119941052, 231.65488418777105, 232.88256143872877, 232.81212165983717, 248.13550886229774, 243.89042769385577, 244.43088854165816, 251.34021672112047, 252.01785904569914, 248.0439053759929, 232.1335373530381, 247.80952459439183, 237.22171183265448, 250.1708459758543, 244.47518466316538, 241.8263668074702, 240.1402684897742, 244.1817888076182, 242.0186236506236, 246.10878237351127, 248.82486533235166, 253.49106566125184, 248.43054415391543, 248.67765409781455, 247.819056272772, 245.76280322642876, 248.57486425933948, 259.2879460565689, 251.82931020606995, 254.99594622592633, 249.55893325446252, 244.46808640148055, 239.2513430210013, 247.54096764352894, 249.8853563220239, 248.7215998607924, 244.85315233556088, 234.81926918066247, 243.4502091353211, 243.37952825749522, 239.27943261927703, 236.7102926010985, 237.35998502136377, 237.00838444791708, 232.64579134829282, 236.50578796907948, 239.63716639787643, 243.1107415329406, 247.4725775536865, 239.89435051389873, 236.3249136585226, 240.43750096337195, 256.6471364864755, 240.54772104532566, 234.2021598643177, 248.30732211684293, 243.17497918383555, 233.50402395207635, 236.8278200532579, 248.14251177626065, 252.72084918211792, 246.7205270233367, 244.1807927951767, 247.77295936352854, 249.6691600384814, 256.22646887544033, 256.45655141903103, 245.6741074659551, 257.4723341807731, 253.12572888429403, 249.54210007601193, 246.56211766976745, 243.07130130019584, 237.00564311059935, 241.0685934295344, 241.24810294509228, 244.66555943501837, 240.22828367719282, 235.75148801063818, 248.72093242269986, 239.8008567376361, 238.61142263046906, 237.94319759113546, 250.90005009510173, 236.10677587678845, 247.42346804632777, 238.13972700364934, 233.49179907122118, 238.91276850337843, 256.46449909602603, 245.12927952856165, 237.48461559076694, 247.30647729324718, 241.26229919790927, 251.64466389915833, 237.176453552216, 244.35711854598262, 252.35949531609526, 242.23937508944746, 250.79687128607782, 243.7035470041735, 248.74502997309045, 253.05788244403982, 250.8988509082665, 253.58743964444056, 251.62749281874423, 248.77289066755247, 256.14742768860117, 252.47571333279717, 255.26633568964974, 250.46455470646916, 250.52129669180164, 256.2663668055176, 241.58892533692264, 239.86974125433642, 247.27628978618864, 241.7296452184753, 247.92348007791057, 244.13474751989602, 235.58796570127694, 242.5682134457198, 247.5635322936095, 243.3024543502157, 236.80616860340217, 239.92235346266767, 247.7729818037711, 234.04995493941647, 237.77687001328624, 241.09833705214447, 240.89759595539567, 239.8760172908872, 233.88671962201164, 239.45013474102822, 238.22185499998832, 238.54765282131598, 235.9861858442649, 233.21170970055945, 238.81861872406972, 239.86527307180842, 244.18394912433652, 236.45335879661215, 242.92301852342788, 246.8086744015897, 239.61313359198863, 251.010276129274, 248.734389146665, 253.68340919275687, 246.1743540991425, 246.8557610560239, 244.99536479141, 246.71125800438892, 245.89425470752263, 249.13366681843613, 255.1378098572842, 249.21115779694733, 254.61203528864178, 241.9841329012564, 239.81849906381413, 239.70093407722004, 245.65832658443767, 236.60924791841296, 233.23094199944853, 240.46615260838433, 258.58426350689075, 249.2589928667618, 248.09645230778568, 241.34926713466433, 238.624871776948, 225.73467371728404, 244.08263057276406, 243.67264739575472, 244.93306415771076, 241.734183430134, 238.96327472085738, 242.7289783185514, 238.77641697626407, 243.81532744476655, 240.38233333514748, 238.96671404220402, 236.14349954766402, 247.86002766646496, 245.20091026396577, 246.56987662679438, 231.80358121592903, 245.09825759402707, 236.06150794401955, 254.71990342807428, 248.82456347580032, 248.8810017871825, 251.0142874441184, 251.7537941936977, 248.56888423946734, 248.80901883845632, 249.8481402573037, 247.3059076671896, 247.21887585334431, 250.86481948037937, 243.06000213303156, 239.91724468806726, 254.2743633648131, 239.88514911525095, 238.0214466601478, 244.1257335988007, 247.32914203870803, 249.43583482986992, 240.69482103265693, 237.50660222943065, 236.87675966088275, 240.55882829415904, 242.9247562554456, 242.6132064912678, 245.4128036012476, 245.97118435066182, 239.00209248553008, 240.83916235566755, 234.72747286416939, 244.03623763295215, 236.50981758171815, 248.23037001260184, 238.40844216203473, 253.88376972983178, 236.51865273171248, 238.05935241848465, 246.55446659133327, 245.77727473548904, 245.7864725983894, 238.15754756015767, 245.4981682354973, 241.1299654285943, 248.57536728255445, 248.0808940350672, 244.99267878026888, 243.84146215962912, 248.79731337505143, 244.36374784479833, 241.13864859907048, 240.38335880633883, 239.6555689122233, 238.1580959280357, 249.4967106635206, 243.8919519555609, 233.96158081760848, 237.50757949523586, 237.2910145245132, 229.0934592828476, 246.14314706138452, 241.90936031865488, 248.88386344120303, 240.0601116193178, 246.75029212673672, 235.10330874930648, 236.6139914682547, 240.66803172229285, 239.71291292151994, 246.96538914902894, 238.52055752407605, 246.0582036193468, 242.37324854959485, 244.82665934352627, 237.11745305918717, 235.4543550074102, 235.25221135178919, 244.268919456009, 243.8751091996434, 250.3382444315112, 247.21887585334431, 241.93597017411037, 247.31090693568177, 244.34343741095182, 247.34465469038528, 245.8794767263117, 250.52795633513475, 255.33432397700352, 240.938454155156, 242.90356617183969, 240.9854881241904, 239.94235373277962, 240.6123468904952, 247.61667218642583, 237.53013171602456, 236.25027132026932, 249.85237996922913, 239.8126424041192, 247.14731008145418, 247.5544086103747, 248.7955078114431, 245.26847669608142, 242.048170486459, 236.3674292893723, 243.4047481796516, 237.91119563857956, 240.54509176666414, 233.23026783116623, 234.36329963594332, 242.2727329008135, 243.41347870097752, 238.3934536210889, 240.20275441629292, 249.5680380776397, 237.45704044702433, 257.7153683460897, 234.54717616401234, 242.31937278735012, 240.7401281908391, 251.15496834548028, 246.40884160709967, 247.30947255687033, 252.62649864602392, 258.97842818610275, 243.3870062514912, 250.24664043760038, 248.26270149094807, 249.80201679447305, 247.10015427074873, 256.3354702668288, 236.9953644877743, 240.92879580995202, 242.39224101692866, 239.98674903193, 241.8660738748018, 257.30383212758034, 246.3803736890528, 239.00921964357676, 246.08818796562625, 240.8499869697779, 242.10663733041991, 235.7242198312025, 238.30704942409463, 239.04993053793177, 249.36555338621451, 230.29201817667675, 245.0291719831225, 244.93843082858115, 242.96797820629172, 235.24588004260758, 242.03409327372057, 237.4038104395748, 241.58353458646928, 241.80095674021268, 249.1162783366902, 233.48709205928176, 246.5677144682, 241.8407330768714, 228.0862922699476, 240.67287903805652, 249.96112688531454, 247.4084620389939, 253.54911971491032, 246.85561671373324, 248.05904311811253, 248.68100029444008, 257.53092315917036, 232.9117394706103, 240.25735005662762, 249.5517426372234, 244.26140672179594, 234.65245800702795, 233.4224364611893, 246.07464324267002, 239.945094960699, 247.11881791838476, 241.72513481529754, 240.43024703202627, 238.55771179750613, 241.12869436895733, 244.95384421730668, 248.20745171581393, 240.04442056749957, 236.29940072044388, 240.84287169048096, 243.6898709516094, 240.41608622365865, 240.10973757225509, 236.3624633929886, 233.22114675055437, 234.40277944708376, 240.7763304646101, 235.56178820378568, 242.36787301581984, 242.32166234412855, 250.54753599688422, 240.85293482877887, 253.5962109375445, 246.88235533342473, 247.67571815699654, 255.2458446082965, 257.4723341807731, 252.02572484180484, 248.34794625782317, 245.0644794527644, 247.80686747908217, 251.29612432669646, 239.0533464383497, 244.5130887992162, 248.76490273624523, 239.36231628640795, 245.54622187974945, 240.96470718455797, 245.0291719831225, 236.66829262245463, 231.62727358034982, 244.33716460317459, 249.49867119605, 247.0993727719665, 233.5960703265811, 237.92487116213783, 239.5484729051627, 247.9195734404493, 236.71534799627526, 250.28773144180724, 242.18988976054212, 240.25846776558112, 245.49580914971807, 237.75910623792487, 242.62506876720707, 240.46164826545748, 246.20830295163105, 240.04368909490216, 240.01950007792826, 236.26966036681227, 231.04606137320965, 245.1112470185787, 240.54255010354674, 245.1709026637121, 247.01101688069102, 248.38845921361778, 258.1698781357614, 249.50919940152198, 244.49272579985632, 243.09304466160845, 252.47571333279717, 243.89284364440397, 242.93542377320335, 238.67477830780646, 240.61244437576127, 241.32048137208545, 241.52940095022757, 240.2934371035911, 245.48718115110404, 237.55842834704654, 238.32703604236943, 246.868096145461, 240.54987757294703, 238.73696449250826, 241.9977511557279, 236.91038508183362, 239.39669194480715, 243.39974161461333, 230.61294373429723, 243.86324709927268, 241.93170587564236, 237.40184843029533, 237.585604741145, 240.65974768667775, 243.484750234138, 239.81038325945937, 242.35319123730872, 240.63857581843592, 235.484759933948, 241.64869679671764, 248.29692013142548, 245.63118583335867, 251.0063576727359, 246.77450249791895, 245.5985447065768, 245.64942081904206, 246.00922452711274, 243.84390556619, 243.98088222407634, 241.30296569628334, 231.9122285242789, 236.6161895636847, 254.31125446871937, 239.3807044085707, 242.70560889680937, 235.27388370994728, 231.29395679074844, 230.46056850504158, 244.4085641890552, 233.96255573377408, 232.73420396356082, 236.98979467169573, 236.26270282249624, 240.1755665328667, 243.53754623011744, 243.8874441747012, 238.3288514748948, 251.76274691833964, 234.76673186579686, 234.76640662267678, 238.05104825912463, 243.92810277131665, 241.0064414709448, 243.29914378685257, 233.18983780245443, 244.49781643124925, 234.07341880337032, 232.66443883104205, 243.97481982932624, 241.08549892319195, 241.34693446575037, 242.7153342665332, 242.28014242559144, 250.0348769295655, 243.75852468978837, 246.1421876671841, 253.0917197307296, 250.19380215266693, 253.11378547605972, 257.66472365324836, 256.3354702668288, 249.5103516278594, 250.95923423490365, 243.1600237001395, 246.80582400115756, 238.97931581460026, 240.63555131355525, 235.42389102223876, 237.5120313165713, 239.05839727924484, 235.74521144251423, 238.02862696167566, 243.40415827234517, 243.91088787951008, 235.5164193192358, 234.8671589909769, 238.93398216200598, 244.44458730668384, 234.9710336231187, 235.99400296634613, 245.1860581422146, 240.09965307670993, 236.3318454953938, 235.27388370994728, 251.0594693400794, 252.41965477274786, 246.4220501212015, 239.3784398935115, 236.7116060216895, 248.63652939758222, 247.10662295571908, 237.12766149020342, 235.82859499908614, 245.59746304263817, 252.31664568549633, 250.00075985987738, 248.59674371374535, 249.03749634844533, 246.77038020838137, 256.43869071854584, 253.3390678487231, 252.6933634066156, 227.86948848909734, 238.3839068828981, 237.88736839426312, 235.15380202597504, 237.35478585835338, 233.02108441575177, 236.11401063350743, 243.98123071478733, 243.56482230557876, 243.88239535313593, 240.77291182342447, 248.02334060438287, 240.42780048469268, 242.89115114317556, 244.7256456032015, 237.82966406493475, 245.62251263525866, 237.2979207447005, 239.30901472751432, 238.85512199187687, 239.5299706327072, 232.28484005892815, 231.60368094540416, 239.73798238862972, 242.11744416300917, 242.31729432483934, 236.0615692966634, 240.72385039415641, 235.1066397470605, 230.55359611669084, 237.99997260768987, 248.16424502661175, 245.91646784027225, 246.63106614986947, 243.74435230987928, 257.4723341807731, 245.0644794527644, 255.98625950648037, 242.93924709140663, 239.07149559066823, 252.93102825926064, 239.88839496626295, 233.05978123791607, 236.85115623087495, 243.72483617576094, 249.054528363617, 234.59706276326713, 234.02422028058575, 238.45506093036323, 244.73324203425489, 232.92790312845665, 233.6142434983585, 242.66450164086157, 244.843224259675, 240.48964993583465, 246.7424706541914, 247.31830682008172, 250.52795633513475, 246.90992995949782, 240.57264380785526, 242.68777952679173, 240.59141750132568, 236.37098734009302, 237.9049234747191, 242.39400519512688, 241.7995871547185, 239.31023886568622, 238.56815268120792, 242.53300348298194, 247.29726518053326, 246.91623708149842, 250.00075985987738, 252.0932811072348, 249.67873004334973, 248.71867002471132, 244.7554267999557, 244.80747434177312], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.623, -8.662, -8.6153, -8.6547, -8.656, -8.6563, -8.6872, -8.6727, -8.6886, -8.6892, -8.6757, -8.7126, -8.6974, -8.6913, -8.706, -8.7035, -8.7235, -8.7128, -8.7258, -8.739, -8.6892, -8.6815, -8.7454, -8.7206, -8.7495, -8.7332, -8.6629, -8.695, -8.7292, -8.6806, -8.6774, -8.7116, -8.7029, -8.7043, -8.7008, -8.7195, -8.7168, -8.5864, -8.6411, -8.6331, -8.6564, -8.6404, -8.6662, -8.718, -8.7144, -8.6623, -8.6939, -8.6509, -8.6767, -8.6977, -8.7458, -8.7082, -8.6557, -8.7001, -8.7267, -8.6986, -8.7072, -8.6991, -8.7346, -8.7129, -8.7387, -8.7326, -8.7286, -8.7442, -8.7519, -8.7547, -8.716, -8.7342, -8.7066, -8.6935, -8.6876, -8.6939, -8.7061, -8.7061, -8.7082, -8.7123, -8.7147, -8.717, -8.6645, -8.7305, -8.6912, -8.7289, -8.7125, -8.6582, -8.6862, -8.6796, -8.7267, -8.665, -8.7018, -8.7049, -8.681, -8.7008, -8.742, -8.7198, -8.6887, -8.6932, -8.706, -8.7365, -8.7445, -8.7434, -8.7477, -8.7171, -8.7026, -8.7398, -8.7116, -8.6968, -8.6888, -8.7281, -8.7072, -8.6813, -8.6993, -8.706, -8.6985, -8.7028, -8.7023, -8.7058, -8.7134, -8.7163, -8.7112, -8.7144, -8.715, -8.5565, -8.6112, -8.6123, -8.6172, -8.6181, -8.6751, -8.6285, -8.6555, -8.6927, -8.6899, -8.7187, -8.6996, -8.7201, -8.6788, -8.7237, -8.6979, -8.6921, -8.6892, -8.727, -8.7023, -8.7042, -8.73, -8.7005, -8.6751, -8.7096, -8.6995, -8.7153, -8.7234, -8.6826, -8.7282, -8.6937, -8.6763, -8.6788, -8.6906, -8.6956, -8.7109, -8.7118, -8.706, -8.7099, -8.6136, -8.601, -8.6798, -8.647, -8.6501, -8.6244, -8.6612, -8.6611, -8.6724, -8.7255, -8.7048, -8.6875, -8.7318, -8.7284, -8.7323, -8.6737, -8.6943, -8.6955, -8.6677, -8.6678, -8.6843, -8.7518, -8.69, -8.7343, -8.6824, -8.7061, -8.7175, -8.7275, -8.7126, -8.7226, -8.7059, -8.6956, -8.6836, -8.6994, -8.6994, -8.7036, -8.7097, -8.7055, -8.6963, -8.7071, -8.7058, -8.6222, -8.644, -8.6827, -8.6616, -8.6539, -8.6591, -8.6761, -8.72, -8.6841, -8.6876, -8.7048, -8.7172, -8.7151, -8.7179, -8.7365, -8.7235, -8.711, -8.7008, -8.6855, -8.717, -8.7321, -8.7178, -8.6528, -8.7192, -8.7463, -8.6887, -8.7108, -8.7532, -8.7402, -8.6937, -8.6776, -8.7008, -8.7143, -8.705, -8.7003, -8.6946, -8.6969, -8.7126, -8.6999, -8.7073, -8.7135, -8.5556, -8.6596, -8.7007, -8.6925, -8.694, -8.6807, -8.7011, -8.7245, -8.6716, -8.7089, -8.7145, -8.7222, -8.671, -8.7328, -8.6886, -8.7292, -8.7504, -8.729, -8.6584, -8.7089, -8.7408, -8.7011, -8.7285, -8.6877, -8.7472, -8.7198, -8.6882, -8.7299, -8.6953, -8.7243, -8.7043, -8.69, -8.6997, -8.6935, -8.7015, -8.7092, -8.6943, -8.7031, -8.6987, -8.7073, -8.7092, -8.7081, -8.6136, -8.6439, -8.6415, -8.6814, -8.6585, -8.6809, -8.7175, -8.6888, -8.671, -8.6884, -8.7192, -8.7072, -8.6772, -8.735, -8.7204, -8.7068, -8.7101, -8.7161, -8.7436, -8.7218, -8.7289, -8.7287, -8.7397, -8.7523, -8.7322, -8.7295, -8.7122, -8.7443, -8.7176, -8.7026, -8.7324, -8.6913, -8.7, -8.6866, -8.7102, -8.7086, -8.7175, -8.7138, -8.7161, -8.715, -8.7107, -8.7175, -8.7169, -8.5952, -8.6397, -8.653, -8.6342, -8.6786, -8.6931, -8.6646, -8.5951, -8.6405, -8.6506, -8.6801, -8.6919, -8.7499, -8.6799, -8.6937, -8.6945, -8.7104, -8.725, -8.7095, -8.7302, -8.7104, -8.7254, -8.7317, -8.7444, -8.6969, -8.7091, -8.7038, -8.7661, -8.7105, -8.7483, -8.6734, -8.6986, -8.6993, -8.6926, -8.6924, -8.702, -8.7027, -8.702, -8.7069, -8.7076, -8.7078, -8.5683, -8.6366, -8.6021, -8.6644, -8.6772, -8.6652, -8.6556, -8.6502, -8.689, -8.7025, -8.7061, -8.6931, -8.6869, -8.6893, -8.6818, -8.6797, -8.7099, -8.7028, -8.7301, -8.6924, -8.7249, -8.6792, -8.7222, -8.6599, -8.733, -8.7274, -8.6927, -8.696, -8.6985, -8.7301, -8.7006, -8.7197, -8.7027, -8.71, -8.716, -8.7193, -8.7174, -8.6258, -8.6593, -8.6752, -8.68, -8.6896, -8.6501, -8.6737, -8.716, -8.7023, -8.7033, -8.7386, -8.6693, -8.6891, -8.6608, -8.6971, -8.6724, -8.7209, -8.715, -8.7021, -8.7112, -8.6817, -8.7181, -8.6891, -8.7047, -8.695, -8.73, -8.7375, -8.7385, -8.7021, -8.7041, -8.6821, -8.7003, -8.7181, -8.7054, -8.7133, -8.7099, -8.7126, -8.711, -8.7131, -8.6355, -8.6462, -8.6727, -8.6816, -8.6809, -8.6558, -8.7002, -8.7109, -8.6637, -8.7067, -8.6823, -8.6813, -8.6771, -8.7012, -8.7158, -8.7401, -8.7111, -8.7351, -8.7275, -8.7588, -8.754, -8.7209, -8.7167, -8.7378, -8.7314, -8.6945, -8.7448, -8.665, -8.7626, -8.7307, -8.7381, -8.6985, -8.7185, -8.7159, -8.7065, -8.6943, -8.7313, -8.7233, -8.7259, -8.7262, -8.7299, -8.7296, -8.6448, -8.6498, -8.6495, -8.6605, -8.6553, -8.5943, -8.6416, -8.6784, -8.6492, -8.6768, -8.6721, -8.7014, -8.6995, -8.7, -8.6593, -8.7446, -8.6864, -8.6876, -8.6958, -8.7303, -8.7025, -8.7227, -8.7068, -8.7086, -8.6795, -8.7453, -8.6912, -8.713, -8.7727, -8.7212, -8.6862, -8.697, -8.6772, -8.7003, -8.7051, -8.7051, -8.7075, -8.6034, -8.6459, -8.6244, -8.6488, -8.6946, -8.7056, -8.6538, -8.6867, -8.6582, -8.6805, -8.6924, -8.7094, -8.6987, -8.6835, -8.6706, -8.7161, -8.7324, -8.7144, -8.7034, -8.7175, -8.7189, -8.7346, -8.7488, -8.744, -8.718, -8.7406, -8.7146, -8.7151, -8.6829, -8.7239, -8.6823, -8.7041, -8.7017, -8.6792, -8.6757, -8.6991, -8.7099, -8.7167, -8.7166, -8.7166, -8.6777, -8.6678, -8.6555, -8.6946, -8.671, -8.6916, -8.6783, -8.7174, -8.7394, -8.6867, -8.6678, -8.6826, -8.7388, -8.7235, -8.7168, -8.6829, -8.731, -8.6755, -8.7085, -8.7196, -8.6985, -8.7312, -8.711, -8.7233, -8.6998, -8.7254, -8.7287, -8.7452, -8.768, -8.7101, -8.729, -8.7111, -8.7066, -8.7044, -8.6925, -8.7089, -8.7201, -8.7222, -8.7132, -8.6025, -8.6191, -8.6434, -8.6363, -8.652, -8.6809, -8.687, -8.6668, -8.7027, -8.7013, -8.6685, -8.6945, -8.704, -8.6905, -8.7152, -8.7055, -8.6908, -8.7454, -8.6896, -8.7008, -8.7217, -8.7244, -8.7145, -8.7038, -8.7226, -8.7136, -8.7208, -8.7429, -8.719, -8.6921, -8.7039, -8.6841, -8.7009, -8.7078, -8.7109, -8.7112, -8.7148, -8.5823, -8.6119, -8.6744, -8.6556, -8.6232, -8.6902, -8.6794, -8.7108, -8.7329, -8.7408, -8.6835, -8.7347, -8.7399, -8.7239, -8.7277, -8.7119, -8.6981, -8.6977, -8.7212, -8.6665, -8.737, -8.7371, -8.7237, -8.6994, -8.7118, -8.7035, -8.7473, -8.7015, -8.7453, -8.7529, -8.708, -8.7194, -8.7188, -8.7155, -8.7168, -8.7002, -8.7152, -8.7108, -8.6986, -8.7087, -8.7061, -8.7014, -8.7032, -8.7106, -8.711, -8.6295, -8.6318, -8.6707, -8.6775, -8.7002, -8.6946, -8.6893, -8.7045, -8.6953, -8.6779, -8.6785, -8.7152, -8.7271, -8.71, -8.691, -8.7339, -8.7297, -8.6915, -8.7155, -8.7337, -8.7421, -8.6778, -8.6734, -8.6985, -8.7283, -8.74, -8.6912, -8.6982, -8.7399, -8.7477, -8.7091, -8.6838, -8.6969, -8.708, -8.7111, -8.7155, -8.7086, -8.7124, -8.7145, -8.6678, -8.6595, -8.6675, -8.6878, -8.6878, -8.7111, -8.7074, -8.6755, -8.6777, -8.6766, -8.6903, -8.662, -8.6939, -8.6858, -8.6786, -8.7082, -8.6762, -8.7128, -8.7054, -8.7087, -8.7062, -8.7452, -8.7504, -8.7172, -8.7075, -8.7088, -8.7354, -8.7166, -8.7411, -8.761, -8.7292, -8.6897, -8.7014, -8.6999, -8.7124, -8.6752, -8.7102, -8.6976, -8.5922, -8.6237, -8.5988, -8.6579, -8.699, -8.6862, -8.6614, -8.6421, -8.7022, -8.7068, -8.6957, -8.6713, -8.7229, -8.7264, -8.6884, -8.6801, -8.7005, -8.6786, -8.679, -8.6676, -8.6833, -8.7105, -8.7018, -8.7139, -8.7317, -8.7267, -8.7087, -8.7117, -8.7229, -8.7273, -8.7117, -8.6977, -8.7043, -8.6972, -8.6997, -8.7038, -8.7058, -8.7109, -8.7109], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.3449, 0.3097, 0.3043, 0.2946, 0.2943, 0.2822, 0.2691, 0.2659, 0.2657, 0.2633, 0.2594, 0.2552, 0.2502, 0.2446, 0.2441, 0.244, 0.2423, 0.2408, 0.2365, 0.2364, 0.2353, 0.2337, 0.2332, 0.233, 0.231, 0.2307, 0.2291, 0.2281, 0.2271, 0.2269, 0.2158, 0.2207, 0.2165, 0.2078, 0.1795, 0.2108, 0.1846, 0.3468, 0.3021, 0.2982, 0.2928, 0.2911, 0.283, 0.2797, 0.2774, 0.2674, 0.2654, 0.2639, 0.2562, 0.2558, 0.2555, 0.2519, 0.2425, 0.2383, 0.237, 0.237, 0.2362, 0.2332, 0.2306, 0.2303, 0.2296, 0.2276, 0.2272, 0.222, 0.2218, 0.22, 0.2191, 0.2185, 0.2137, 0.2102, 0.2056, 0.2063, 0.2063, 0.2043, 0.201, 0.202, 0.2, 0.1992, 0.2755, 0.262, 0.2618, 0.2594, 0.2579, 0.2564, 0.2552, 0.2545, 0.254, 0.2532, 0.2524, 0.2488, 0.2482, 0.2474, 0.2455, 0.2441, 0.2433, 0.2433, 0.2411, 0.2409, 0.2374, 0.2373, 0.236, 0.2342, 0.234, 0.2288, 0.2261, 0.2259, 0.2248, 0.2248, 0.2239, 0.2154, 0.219, 0.2192, 0.2146, 0.2158, 0.212, 0.2122, 0.2193, 0.2223, 0.2044, 0.2117, 0.2094, 0.3657, 0.336, 0.319, 0.3156, 0.3068, 0.297, 0.2957, 0.2853, 0.282, 0.2758, 0.2727, 0.2638, 0.2638, 0.2629, 0.2609, 0.2593, 0.2586, 0.2554, 0.2513, 0.2511, 0.2508, 0.2481, 0.2478, 0.2473, 0.2458, 0.2423, 0.2387, 0.237, 0.2366, 0.2359, 0.2352, 0.2334, 0.2281, 0.2263, 0.2201, 0.2319, 0.2273, 0.1865, 0.197, 0.3543, 0.3278, 0.3196, 0.3073, 0.3001, 0.2997, 0.2911, 0.2885, 0.2836, 0.2829, 0.2769, 0.2762, 0.2671, 0.2652, 0.2616, 0.2564, 0.2531, 0.2497, 0.2496, 0.2469, 0.2462, 0.2451, 0.2415, 0.2408, 0.2396, 0.2389, 0.2385, 0.2354, 0.2336, 0.2325, 0.2325, 0.2318, 0.2252, 0.2296, 0.2286, 0.2279, 0.2301, 0.2229, 0.1899, 0.2083, 0.1971, 0.3023, 0.301, 0.2839, 0.271, 0.2693, 0.2688, 0.2674, 0.2653, 0.2652, 0.2619, 0.2618, 0.2601, 0.2595, 0.2582, 0.2581, 0.2547, 0.2541, 0.2498, 0.2474, 0.247, 0.2468, 0.2438, 0.2436, 0.2421, 0.2417, 0.2408, 0.2396, 0.2378, 0.2366, 0.2365, 0.2342, 0.2351, 0.2319, 0.2266, 0.2237, 0.2035, 0.2003, 0.2276, 0.1933, 0.203, 0.211, 0.381, 0.2912, 0.2754, 0.2665, 0.2643, 0.2636, 0.2615, 0.2568, 0.2562, 0.2555, 0.2548, 0.2499, 0.2481, 0.2471, 0.2445, 0.2421, 0.2406, 0.2391, 0.2387, 0.2335, 0.2332, 0.2325, 0.2298, 0.2284, 0.2282, 0.2258, 0.2251, 0.2243, 0.2242, 0.2239, 0.2235, 0.2206, 0.2194, 0.215, 0.2147, 0.2184, 0.2041, 0.2098, 0.2031, 0.2135, 0.2114, 0.1898, 0.3433, 0.3202, 0.2921, 0.2749, 0.2726, 0.2655, 0.2645, 0.264, 0.2615, 0.2614, 0.2577, 0.2566, 0.2544, 0.2536, 0.2524, 0.2522, 0.2496, 0.2479, 0.2457, 0.244, 0.242, 0.2409, 0.2407, 0.2399, 0.2362, 0.2346, 0.2341, 0.2341, 0.2338, 0.233, 0.2328, 0.2273, 0.2278, 0.2215, 0.2279, 0.2267, 0.2254, 0.2221, 0.2232, 0.2112, 0.1917, 0.2084, 0.1875, 0.3601, 0.3245, 0.3117, 0.306, 0.2992, 0.299, 0.297, 0.2939, 0.2852, 0.2797, 0.2778, 0.2774, 0.2749, 0.2668, 0.2546, 0.2487, 0.2459, 0.2428, 0.2427, 0.2384, 0.2373, 0.2365, 0.2361, 0.2353, 0.2344, 0.233, 0.2327, 0.2321, 0.232, 0.2317, 0.2306, 0.2288, 0.2278, 0.2261, 0.2233, 0.2265, 0.2248, 0.2213, 0.2267, 0.2263, 0.2114, 0.3826, 0.3273, 0.3037, 0.2996, 0.2945, 0.2813, 0.2779, 0.2748, 0.2717, 0.2715, 0.2705, 0.2681, 0.2645, 0.2634, 0.2594, 0.2592, 0.2578, 0.2572, 0.2556, 0.2545, 0.2532, 0.2506, 0.248, 0.2474, 0.2452, 0.2442, 0.2439, 0.2437, 0.2412, 0.2411, 0.2403, 0.2391, 0.2257, 0.2204, 0.2269, 0.2284, 0.2101, 0.3197, 0.2995, 0.2868, 0.285, 0.2817, 0.2746, 0.2737, 0.273, 0.2717, 0.2716, 0.2714, 0.2689, 0.2665, 0.2664, 0.2661, 0.2634, 0.2633, 0.2627, 0.2586, 0.2535, 0.2532, 0.2516, 0.2495, 0.249, 0.2486, 0.2456, 0.2451, 0.245, 0.2438, 0.2433, 0.2392, 0.2336, 0.2374, 0.2281, 0.2323, 0.2235, 0.2267, 0.2096, 0.1885, 0.3241, 0.3053, 0.2867, 0.2822, 0.2801, 0.2765, 0.2736, 0.2683, 0.2596, 0.2576, 0.2519, 0.2512, 0.2504, 0.2406, 0.2392, 0.2387, 0.2383, 0.2372, 0.2337, 0.2333, 0.2332, 0.2332, 0.2327, 0.2324, 0.2313, 0.2299, 0.2293, 0.2272, 0.2239, 0.2232, 0.2224, 0.2195, 0.2186, 0.2176, 0.2058, 0.1931, 0.2182, 0.1984, 0.2037, 0.1972, 0.2045, 0.168, 0.3313, 0.3098, 0.3041, 0.3031, 0.3004, 0.2996, 0.2956, 0.2893, 0.2892, 0.2832, 0.2827, 0.2801, 0.2711, 0.2675, 0.2659, 0.2602, 0.2564, 0.2555, 0.2554, 0.2532, 0.2526, 0.2517, 0.2501, 0.2475, 0.2468, 0.2457, 0.2454, 0.2429, 0.2417, 0.2396, 0.2366, 0.2361, 0.2314, 0.2351, 0.2254, 0.2228, 0.1855, 0.3901, 0.3166, 0.3001, 0.2971, 0.2914, 0.2857, 0.2847, 0.2771, 0.276, 0.2759, 0.2693, 0.2602, 0.2601, 0.2596, 0.2593, 0.2473, 0.2467, 0.2456, 0.2449, 0.2443, 0.2442, 0.2441, 0.2434, 0.2431, 0.2423, 0.2416, 0.2391, 0.2388, 0.2376, 0.2361, 0.2261, 0.2311, 0.2303, 0.2227, 0.2176, 0.2155, 0.2194, 0.2259, 0.2149, 0.2009, 0.2898, 0.2771, 0.2722, 0.2716, 0.2697, 0.2679, 0.2645, 0.2601, 0.2597, 0.2589, 0.2569, 0.2518, 0.2517, 0.2487, 0.2486, 0.2482, 0.2463, 0.246, 0.2459, 0.2428, 0.2424, 0.2416, 0.2416, 0.2383, 0.2382, 0.2379, 0.2347, 0.234, 0.2335, 0.2324, 0.2322, 0.2311, 0.2281, 0.2248, 0.198, 0.2158, 0.2248, 0.2286, 0.1997, 0.3449, 0.3323, 0.3257, 0.3247, 0.306, 0.2763, 0.2753, 0.2741, 0.271, 0.2692, 0.2668, 0.2667, 0.2648, 0.2647, 0.2612, 0.2605, 0.2586, 0.258, 0.258, 0.2547, 0.2527, 0.2492, 0.2463, 0.2453, 0.2417, 0.2401, 0.24, 0.2396, 0.2376, 0.2374, 0.2364, 0.2346, 0.2347, 0.2327, 0.2293, 0.2276, 0.2328, 0.3647, 0.3462, 0.3234, 0.3221, 0.2824, 0.2759, 0.2729, 0.2726, 0.2676, 0.2633, 0.2618, 0.2543, 0.2543, 0.2523, 0.2516, 0.2509, 0.2507, 0.2497, 0.2493, 0.2492, 0.2485, 0.2484, 0.2479, 0.2478, 0.2475, 0.2463, 0.245, 0.2434, 0.2432, 0.2417, 0.2391, 0.2396, 0.2391, 0.2368, 0.2372, 0.2224, 0.2328, 0.2274, 0.2118, 0.2132, 0.2042, 0.1911, 0.1945, 0.214, 0.2079, 0.321, 0.3038, 0.297, 0.2833, 0.2826, 0.2793, 0.2781, 0.2769, 0.2765, 0.2715, 0.2689, 0.2672, 0.258, 0.258, 0.2542, 0.2508, 0.2506, 0.2506, 0.2476, 0.2452, 0.2413, 0.2406, 0.2397, 0.2386, 0.2378, 0.2373, 0.237, 0.2361, 0.2357, 0.2333, 0.2314, 0.2296, 0.2258, 0.2203, 0.2155, 0.2202, 0.1887, 0.197, 0.1974, 0.3476, 0.3108, 0.3049, 0.2962, 0.2868, 0.282, 0.2725, 0.2716, 0.2711, 0.2709, 0.27, 0.2686, 0.2678, 0.2658, 0.2654, 0.2644, 0.2641, 0.262, 0.261, 0.2596, 0.2593, 0.251, 0.2487, 0.2474, 0.2472, 0.2451, 0.2447, 0.2439, 0.243, 0.2427, 0.2427, 0.2404, 0.2377, 0.2364, 0.2356, 0.218, 0.2324, 0.2014, 0.3592, 0.3437, 0.3122, 0.306, 0.2938, 0.2905, 0.2867, 0.2844, 0.2841, 0.282, 0.2743, 0.2727, 0.2705, 0.2641, 0.2641, 0.2635, 0.2609, 0.2572, 0.2545, 0.253, 0.2518, 0.2507, 0.2506, 0.2471, 0.2471, 0.2456, 0.2449, 0.2443, 0.2435, 0.2422, 0.2413, 0.2359, 0.2308, 0.2254, 0.2147, 0.2202, 0.222, 0.233, 0.2328]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \"Freq\": [0.09076744665808685, 0.07839006756834772, 0.06601268847860861, 0.07013848150852166, 0.07013848150852166, 0.06601268847860861, 0.04950951635895646, 0.04950951635895646, 0.037132137269217345, 0.0536353093888695, 0.041257930299130385, 0.04950951635895646, 0.04950951635895646, 0.037132137269217345, 0.037132137269217345, 0.02888055120939127, 0.037132137269217345, 0.037132137269217345, 0.02475475817947823, 0.02888055120939127, 0.09100824303059031, 0.07446128975230117, 0.07446128975230117, 0.057914336474012014, 0.07032455143272888, 0.0661878131131566, 0.04964085983486744, 0.05377759815443973, 0.04136738319572287, 0.045504121515295155, 0.045504121515295155, 0.045504121515295155, 0.045504121515295155, 0.045504121515295155, 0.03723064487615058, 0.0330939065565783, 0.028957168237006007, 0.03723064487615058, 0.028957168237006007, 0.028957168237006007, 0.06268014302730611, 0.07939484783458774, 0.07521617163276732, 0.06268014302730611, 0.06268014302730611, 0.06268014302730611, 0.05432279062366529, 0.058501466825485696, 0.05014411442184488, 0.045965438220024475, 0.03760808581638366, 0.04178676201820407, 0.04178676201820407, 0.03760808581638366, 0.04178676201820407, 0.033429409614563255, 0.033429409614563255, 0.033429409614563255, 0.03760808581638366, 0.03760808581638366, 0.06044384032192665, 0.05641425096713154, 0.07656219774110709, 0.06850301903151687, 0.06850301903151687, 0.06447342967672176, 0.05641425096713154, 0.04835507225754132, 0.05238466161233643, 0.04835507225754132, 0.04835507225754132, 0.05238466161233643, 0.0402958935479511, 0.04835507225754132, 0.0402958935479511, 0.03626630419315599, 0.03626630419315599, 0.03626630419315599, 0.0402958935479511, 0.02820712548356577, 0.062204877099314645, 0.062204877099314645, 0.07464585251917757, 0.06635186890593561, 0.0704988607125566, 0.06635186890593561, 0.05391089348607269, 0.05391089348607269, 0.049763901679451716, 0.05391089348607269, 0.06635186890593561, 0.049763901679451716, 0.03732292625958879, 0.04146991806620976, 0.04146991806620976, 0.033175934452967806, 0.033175934452967806, 0.03732292625958879, 0.033175934452967806, 0.029028942646346832, 0.06736654833933436, 0.06315613906812596, 0.07578736688175117, 0.06315613906812596, 0.06736654833933436, 0.05894572979691757, 0.054735320525709175, 0.05052491125450077, 0.04631450198329238, 0.06315613906812596, 0.05052491125450077, 0.05894572979691757, 0.04631450198329238, 0.04631450198329238, 0.03368327416966718, 0.037893683440875583, 0.03368327416966718, 0.037893683440875583, 0.029472864898458784, 0.025262455627250387, 0.07005557007740525, 0.07417648596431145, 0.07005557007740525, 0.07417648596431145, 0.07005557007740525, 0.06593465419049906, 0.06181373830359287, 0.057692822416686676, 0.053571906529780486, 0.045330074755968106, 0.045330074755968106, 0.045330074755968106, 0.041209158869061915, 0.03296732709524953, 0.037088242982155725, 0.037088242982155725, 0.03296732709524953, 0.03296732709524953, 0.03296732709524953, 0.037088242982155725, 0.058631757565429404, 0.06700772293191933, 0.0879476363481441, 0.06700772293191933, 0.07119570561516428, 0.07119570561516428, 0.05444377488218445, 0.05025579219893949, 0.05025579219893949, 0.041879826832449576, 0.04606780951569454, 0.05025579219893949, 0.037691844149204616, 0.041879826832449576, 0.041879826832449576, 0.037691844149204616, 0.037691844149204616, 0.03350386146595966, 0.029315878782714702, 0.025127896099469745, 0.0706592121055454, 0.06650278786404273, 0.0706592121055454, 0.06650278786404273, 0.0706592121055454, 0.04987709089803205, 0.05403351513953472, 0.05818993938103739, 0.04572066665652938, 0.04572066665652938, 0.04987709089803205, 0.04987709089803205, 0.04572066665652938, 0.04572066665652938, 0.04156424241502671, 0.03740781817352404, 0.03740781817352404, 0.03325139393202137, 0.03740781817352404, 0.03740781817352404, 0.06449104713577516, 0.076583118473733, 0.06046035668978921, 0.06852173758176111, 0.06449104713577516, 0.07255242802774706, 0.05239897579781732, 0.05239897579781732, 0.06449104713577516, 0.04836828535183137, 0.04433759490584542, 0.04433759490584542, 0.04433759490584542, 0.040306904459859474, 0.03224552356788758, 0.03627621401387353, 0.03627621401387353, 0.03224552356788758, 0.03224552356788758, 0.028214833121901633, 0.07059145573019565, 0.08720121001965346, 0.07474389430256011, 0.062286578585466754, 0.06643901715783121, 0.062286578585466754, 0.06643901715783121, 0.053981701440737856, 0.04567682429600895, 0.037371947151280054, 0.04567682429600895, 0.04982926286837341, 0.037371947151280054, 0.037371947151280054, 0.037371947151280054, 0.033219508578915605, 0.033219508578915605, 0.029067070006551152, 0.033219508578915605, 0.029067070006551152, 0.08033630027897685, 0.07187984761803191, 0.07187984761803191, 0.059195168626614524, 0.06342339495708699, 0.08033630027897685, 0.05073871596566959, 0.05073871596566959, 0.05073871596566959, 0.05496694229614206, 0.046510489635197126, 0.05073871596566959, 0.04228226330472466, 0.04228226330472466, 0.03805403697425219, 0.029597584313307262, 0.03382581064377973, 0.029597584313307262, 0.03805403697425219, 0.029597584313307262, 0.08189091880463756, 0.08189091880463756, 0.07019221611826076, 0.07799135124251196, 0.06629264855613516, 0.05459394586975837, 0.05459394586975837, 0.046794810745507176, 0.04289524318338158, 0.04289524318338158, 0.04289524318338158, 0.05459394586975837, 0.03509610805913038, 0.03899567562125598, 0.04289524318338158, 0.03509610805913038, 0.031196540497004783, 0.03899567562125598, 0.027296972934879184, 0.031196540497004783, 0.08434704776660097, 0.07667913433327361, 0.06517726418328257, 0.06517726418328257, 0.06134330746661889, 0.06517726418328257, 0.06134330746661889, 0.05367539403329153, 0.04984143731662785, 0.04984143731662785, 0.04984143731662785, 0.038339567166636807, 0.038339567166636807, 0.038339567166636807, 0.03450561044997313, 0.038339567166636807, 0.03450561044997313, 0.030671653733309445, 0.030671653733309445, 0.026837697016645765, 0.06444786068914737, 0.0765318345683625, 0.06847585198221907, 0.056391878103003945, 0.07250384327529079, 0.06444786068914737, 0.056391878103003945, 0.056391878103003945, 0.044307904223788816, 0.04833589551686052, 0.04833589551686052, 0.056391878103003945, 0.04833589551686052, 0.0402799129307171, 0.036251921637645394, 0.036251921637645394, 0.03222393034457369, 0.03222393034457369, 0.036251921637645394, 0.028195939051501973, 0.06475921323140994, 0.06907649411350393, 0.07339377499559793, 0.07339377499559793, 0.06044193234931594, 0.056124651467221946, 0.06044193234931594, 0.056124651467221946, 0.05180737058512795, 0.04749008970303396, 0.04749008970303396, 0.04749008970303396, 0.03885552793884596, 0.03885552793884596, 0.05180737058512795, 0.034538247056751965, 0.03885552793884596, 0.034538247056751965, 0.02158640441046998, 0.03022096617465797, 0.06762005604732146, 0.07184630955027906, 0.06762005604732146, 0.07184630955027906, 0.06339380254436387, 0.05916754904140628, 0.05494129553844869, 0.05494129553844869, 0.046488788532533507, 0.05494129553844869, 0.046488788532533507, 0.042262535029575915, 0.042262535029575915, 0.042262535029575915, 0.038036281526618324, 0.038036281526618324, 0.046488788532533507, 0.03381002802366073, 0.03381002802366073, 0.02958377452070314, 0.06863742305040658, 0.07267491852395991, 0.08882490041817322, 0.06459992757685325, 0.06863742305040658, 0.05652493662974659, 0.05248744115619327, 0.04844994568263994, 0.04844994568263994, 0.04441245020908661, 0.04441245020908661, 0.05248744115619327, 0.04441245020908661, 0.036337459261979954, 0.036337459261979954, 0.040374954735533285, 0.036337459261979954, 0.028262468314873296, 0.036337459261979954, 0.028262468314873296, 0.061261208430117065, 0.061261208430117065, 0.06942936955413267, 0.061261208430117065, 0.06942936955413267, 0.08168161124015609, 0.06942936955413267, 0.053093047306101454, 0.04492488618208585, 0.05717712786810926, 0.04900896674409365, 0.04900896674409365, 0.03675672505807024, 0.04492488618208585, 0.040840805620078045, 0.032672644496062435, 0.032672644496062435, 0.032672644496062435, 0.02858856393405463, 0.02858856393405463, 0.07889824419581964, 0.07495333198602866, 0.05917368314686473, 0.06311859535665572, 0.08678806861540161, 0.05917368314686473, 0.05917368314686473, 0.047338946517491785, 0.055228770937073755, 0.055228770937073755, 0.047338946517491785, 0.047338946517491785, 0.03944912209790982, 0.03944912209790982, 0.04339403430770081, 0.035504209888118844, 0.03155929767832786, 0.03155929767832786, 0.027614385468536878, 0.03155929767832786, 0.06530807401942904, 0.06122631939321472, 0.06938982864564334, 0.08571684715050061, 0.053062810140786086, 0.06530807401942904, 0.06122631939321472, 0.04489930088835746, 0.04898105551457178, 0.06122631939321472, 0.04489930088835746, 0.0571445647670004, 0.040817546262143144, 0.03673579163592883, 0.040817546262143144, 0.03265403700971452, 0.03673579163592883, 0.03265403700971452, 0.03265403700971452, 0.0285722823835002, 0.06431568953138705, 0.06431568953138705, 0.08039461191423382, 0.06431568953138705, 0.06833542012709874, 0.06029595893567536, 0.07235515072281043, 0.05225649774425198, 0.04823676714854029, 0.0442170365528286, 0.04823676714854029, 0.05627622833996367, 0.04019730595711691, 0.036177575361405215, 0.04019730595711691, 0.03215784476569353, 0.028138114169981836, 0.036177575361405215, 0.03215784476569353, 0.028138114169981836, 0.07894692063599093, 0.07063671846378135, 0.058171415205466995, 0.06232651629157178, 0.07063671846378135, 0.06232651629157178, 0.049861213033257426, 0.05401631411936221, 0.049861213033257426, 0.049861213033257426, 0.06232651629157178, 0.04570611194715264, 0.037395909774943066, 0.03324080868883828, 0.037395909774943066, 0.037395909774943066, 0.03324080868883828, 0.037395909774943066, 0.037395909774943066, 0.029085707602733497, 0.05788327306808181, 0.06615231207780778, 0.06615231207780778, 0.06615231207780778, 0.07028683158267077, 0.06615231207780778, 0.05788327306808181, 0.049614234058355834, 0.045479714553492846, 0.045479714553492846, 0.0620177925729448, 0.045479714553492846, 0.05788327306808181, 0.041345195048629865, 0.045479714553492846, 0.03307615603890389, 0.03307615603890389, 0.028941636534040905, 0.028941636534040905, 0.03307615603890389, 0.0708503510761283, 0.0708503510761283, 0.06668268336576781, 0.0708503510761283, 0.0708503510761283, 0.06251501565540732, 0.05417968023468635, 0.04584434481396537, 0.05417968023468635, 0.041676677103604884, 0.04584434481396537, 0.06251501565540732, 0.03750900939324439, 0.03750900939324439, 0.041676677103604884, 0.03750900939324439, 0.033341341682883906, 0.029173673972523417, 0.033341341682883906, 0.029173673972523417, 0.08884791705437342, 0.06865520863292492, 0.060578125264345514, 0.07673229200150432, 0.07673229200150432, 0.060578125264345514, 0.06461666694863522, 0.04846250021147641, 0.04442395852718671, 0.05250104189576611, 0.04442395852718671, 0.04442395852718671, 0.04846250021147641, 0.04038541684289701, 0.03634687515860731, 0.03230833347431761, 0.03230833347431761, 0.03634687515860731, 0.03230833347431761, 0.024231250105738206, 0.08616274126800912, 0.0656478028708641, 0.07385377822972211, 0.061544815191435086, 0.0697507905502931, 0.0697507905502931, 0.053338839832577076, 0.057441827512006084, 0.045132864473719066, 0.04923585215314807, 0.04923585215314807, 0.04923585215314807, 0.03282390143543205, 0.04102987679429006, 0.03282390143543205, 0.036926889114861056, 0.03282390143543205, 0.03282390143543205, 0.028720913756003042, 0.036926889114861056, 0.07084698758222303, 0.08751686701333435, 0.0666795177244452, 0.07084698758222303, 0.0666795177244452, 0.054177108151111736, 0.054177108151111736, 0.05834457800888956, 0.054177108151111736, 0.05000963829333391, 0.04584216843555608, 0.05000963829333391, 0.04584216843555608, 0.0333397588622226, 0.03750722872000043, 0.0333397588622226, 0.0333397588622226, 0.0333397588622226, 0.0333397588622226, 0.025004819146666954, 0.08687223861858057, 0.06618837228082329, 0.057914825745720384, 0.07032514554837475, 0.07032514554837475, 0.057914825745720384, 0.06618837228082329, 0.05377805247816893, 0.05377805247816893, 0.04964127921061747, 0.04136773267551456, 0.04964127921061747, 0.0372309594079631, 0.04136773267551456, 0.045504505943066015, 0.033094186140411645, 0.028957412872860192, 0.028957412872860192, 0.028957412872860192, 0.028957412872860192, 0.057889338251178576, 0.06202429098340562, 0.07442914918008674, 0.0702941964478597, 0.0702941964478597, 0.06202429098340562, 0.053754385518951536, 0.049619432786724496, 0.053754385518951536, 0.053754385518951536, 0.049619432786724496, 0.045484480054497456, 0.053754385518951536, 0.041349527322270416, 0.03721457459004337, 0.028944669125589288, 0.03721457459004337, 0.03721457459004337, 0.03307962185781633, 0.028944669125589288, 0.07503096073130276, 0.06669418731671357, 0.08753612085318656, 0.06669418731671357, 0.06669418731671357, 0.05835741390212437, 0.045852253780240576, 0.05418902719482977, 0.045852253780240576, 0.045852253780240576, 0.05002064048753518, 0.05002064048753518, 0.05002064048753518, 0.03751548036565138, 0.04168386707294598, 0.03334709365835679, 0.03751548036565138, 0.03334709365835679, 0.03334709365835679, 0.02501032024376759, 0.0600093309984244, 0.0760118192646709, 0.0600093309984244, 0.0760118192646709, 0.07201119719810928, 0.05600870893186277, 0.06801057513154765, 0.052008086865301145, 0.04400684273217789, 0.04400684273217789, 0.04800746479873952, 0.04800746479873952, 0.05600870893186277, 0.040006220665616266, 0.03600559859905464, 0.03600559859905464, 0.03200497653249301, 0.03200497653249301, 0.028004354465931386, 0.02400373239936976, 0.06832122907211832, 0.06430233324434666, 0.07234012489988999, 0.07234012489988999, 0.06028343741657499, 0.06028343741657499, 0.05224564576103166, 0.05224564576103166, 0.06028343741657499, 0.05224564576103166, 0.05626454158880333, 0.048226749933259996, 0.04018895827771666, 0.036170062449944994, 0.036170062449944994, 0.036170062449944994, 0.036170062449944994, 0.03215116662217333, 0.03215116662217333, 0.028132270794401663, 0.06449509166045789, 0.08061886457557237, 0.07255697811801513, 0.06449509166045789, 0.06852603488923652, 0.06852603488923652, 0.06046414843167927, 0.04837131874534342, 0.04837131874534342, 0.06046414843167927, 0.0443403755165648, 0.0443403755165648, 0.0443403755165648, 0.0443403755165648, 0.04030943228778618, 0.032247545830228944, 0.03627848905900757, 0.032247545830228944, 0.028216602601450327, 0.02418565937267171, 0.07156134226972105, 0.058932870104476155, 0.058932870104476155, 0.06735185154797275, 0.06314236082622446, 0.05472337938272786, 0.05472337938272786, 0.05472337938272786, 0.046304397939231266, 0.05472337938272786, 0.04209490721748297, 0.05051388866097956, 0.04209490721748297, 0.04209490721748297, 0.046304397939231266, 0.046304397939231266, 0.037885416495734674, 0.037885416495734674, 0.03367592577398638, 0.03367592577398638, 0.07732417521633625, 0.08546356208121375, 0.0895332555136525, 0.0569757080541425, 0.061045401486581254, 0.06918478835145875, 0.05290601462170375, 0.048836321189265, 0.04476662775682625, 0.0406969343243875, 0.04476662775682625, 0.048836321189265, 0.048836321189265, 0.0406969343243875, 0.03662724089194875, 0.03255754745951, 0.02848785402707125, 0.02848785402707125, 0.02848785402707125, 0.02848785402707125, 0.07349954246915277, 0.0605290349745964, 0.06917603997096732, 0.0605290349745964, 0.07782304496733822, 0.06485253747278186, 0.05620553247641094, 0.05188202997822548, 0.05188202997822548, 0.04323502498185457, 0.05620553247641094, 0.047558527480040025, 0.04323502498185457, 0.03891152248366911, 0.04323502498185457, 0.03458801998548366, 0.04323502498185457, 0.0302645174872982, 0.02594101498911274, 0.03458801998548366, 0.07442340408631076, 0.08682730476736257, 0.06615413696560957, 0.062019503405258976, 0.06615413696560957, 0.07442340408631076, 0.062019503405258976, 0.04961560272420718, 0.04961560272420718, 0.03721170204315538, 0.045480969163856584, 0.05375023628455778, 0.041346335603505986, 0.03721170204315538, 0.041346335603505986, 0.03307706848280478, 0.02894243492245419, 0.02894243492245419, 0.03307706848280478, 0.02894243492245419, 0.08932268264626282, 0.0660211132602812, 0.05825392346495401, 0.0699047081579448, 0.0699047081579448, 0.0699047081579448, 0.05437032856729041, 0.05825392346495401, 0.04660313877196321, 0.05048673366962681, 0.05048673366962681, 0.04660313877196321, 0.03883594897663601, 0.0349523540789724, 0.03883594897663601, 0.0349523540789724, 0.0349523540789724, 0.027185164283645206, 0.031068759181308806, 0.023301569385981605, 0.0698728311797645, 0.06165249809979221, 0.07809316425973681, 0.057542331559806065, 0.0698728311797645, 0.0698728311797645, 0.0698728311797645, 0.05343216501981991, 0.04932199847983377, 0.04110166539986147, 0.045211831939847624, 0.04932199847983377, 0.04110166539986147, 0.04110166539986147, 0.045211831939847624, 0.04110166539986147, 0.045211831939847624, 0.03288133231988918, 0.024660999239916884, 0.024660999239916884, 0.05560499848320805, 0.07149214090698178, 0.07149214090698178, 0.08340749772481207, 0.06752035530103835, 0.06354856969509491, 0.05163321287726462, 0.06354856969509491, 0.05163321287726462, 0.04766142727132119, 0.04766142727132119, 0.043689641665377754, 0.03971785605943432, 0.03971785605943432, 0.03971785605943432, 0.03574607045349089, 0.031774284847547456, 0.031774284847547456, 0.031774284847547456, 0.031774284847547456, 0.07483739516007142, 0.0630209643453233, 0.07877620543165413, 0.0630209643453233, 0.07089858488848871, 0.059082154073740595, 0.059082154073740595, 0.047265723258992476, 0.047265723258992476, 0.0630209643453233, 0.047265723258992476, 0.039388102715827066, 0.039388102715827066, 0.04332691298740977, 0.039388102715827066, 0.039388102715827066, 0.03151048217266165, 0.03151048217266165, 0.027571671901078943, 0.027571671901078943, 0.07079195882852903, 0.06662772595626261, 0.06662772595626261, 0.06662772595626261, 0.08744889031759467, 0.062463493083996194, 0.05829926021172978, 0.05413502733946337, 0.045806561594930545, 0.045806561594930545, 0.049970794467196954, 0.04164232872266413, 0.045806561594930545, 0.037478095850397714, 0.045806561594930545, 0.02914963010586489, 0.033313862978131305, 0.033313862978131305, 0.02914963010586489, 0.02914963010586489, 0.08855326534169766, 0.0632523323869269, 0.07168597670518383, 0.0632523323869269, 0.0632523323869269, 0.054818688068669984, 0.06746915454605536, 0.046385043750413066, 0.059035510227798446, 0.046385043750413066, 0.05060186590954152, 0.046385043750413066, 0.03795139943215614, 0.0421682215912846, 0.0421682215912846, 0.03795139943215614, 0.03373457727302768, 0.029517755113899223, 0.03373457727302768, 0.029517755113899223, 0.07130981942038273, 0.07130981942038273, 0.07527147605484844, 0.07130981942038273, 0.06734816278591703, 0.07527147605484844, 0.047539879613588486, 0.047539879613588486, 0.047539879613588486, 0.051501536248054196, 0.03961656634465707, 0.047539879613588486, 0.03961656634465707, 0.04357822297912278, 0.03565490971019136, 0.03565490971019136, 0.03169325307572566, 0.04357822297912278, 0.03169325307572566, 0.023769939806794243, 0.06659710907069404, 0.06267963206653555, 0.07443206307901097, 0.0705145860748525, 0.07443206307901097, 0.05484467805821861, 0.0705145860748525, 0.04700972404990167, 0.050927201054060144, 0.04700972404990167, 0.04700972404990167, 0.04700972404990167, 0.043092247045743194, 0.039174770041584726, 0.03525729303742625, 0.039174770041584726, 0.039174770041584726, 0.03133981603326778, 0.03525729303742625, 0.027422339029109306, 0.08228038395737954, 0.06582430716590362, 0.06582430716590362, 0.0699383263637726, 0.04936823037442772, 0.06582430716590362, 0.07816636475951055, 0.04936823037442772, 0.04936823037442772, 0.04936823037442772, 0.037026172780820786, 0.04936823037442772, 0.04114019197868977, 0.04114019197868977, 0.04114019197868977, 0.03291215358295181, 0.04114019197868977, 0.03291215358295181, 0.03291215358295181, 0.028798134385082837, 0.0600354497397517, 0.07204253968770204, 0.06403781305573515, 0.06403781305573515, 0.0680401763717186, 0.06403781305573515, 0.06403781305573515, 0.05603308642376825, 0.04802835979180136, 0.04402599647581791, 0.04402599647581791, 0.0600354497397517, 0.03602126984385102, 0.04402599647581791, 0.04402599647581791, 0.03602126984385102, 0.04002363315983447, 0.028016543211884126, 0.028016543211884126, 0.032018906527867576, 0.06320774548539493, 0.06742159518442126, 0.06742159518442126, 0.0884908436795529, 0.0589938957863686, 0.05478004608734227, 0.05478004608734227, 0.05478004608734227, 0.05056619638831594, 0.05478004608734227, 0.05056619638831594, 0.046352346689289614, 0.046352346689289614, 0.03371079759221063, 0.03792464729123696, 0.03792464729123696, 0.03792464729123696, 0.03371079759221063, 0.0294969478931843, 0.02528309819415797, 0.06654736746940469, 0.06654736746940469, 0.062388157002566905, 0.07486578840308028, 0.05822894653572911, 0.06654736746940469, 0.05822894653572911, 0.054069736068891316, 0.054069736068891316, 0.04575131513521573, 0.04991052560205352, 0.04575131513521573, 0.041592104668377934, 0.054069736068891316, 0.04575131513521573, 0.03743289420154014, 0.03743289420154014, 0.029114473267864555, 0.033273683734702346, 0.02495526280102676, 0.06320969452818342, 0.07506151225221781, 0.07111090634420635, 0.05530848271216049, 0.07111090634420635, 0.07901211816022927, 0.04740727089613756, 0.05530848271216049, 0.05135787680414903, 0.0434566649881261, 0.05925908862017196, 0.05135787680414903, 0.0434566649881261, 0.03950605908011463, 0.03950605908011463, 0.03950605908011463, 0.03555545317210317, 0.03160484726409171, 0.027654241356080245, 0.02370363544806878, 0.07402077327368599, 0.057571712546200214, 0.0657962429099431, 0.07813303845555743, 0.07402077327368599, 0.07813303845555743, 0.04523491700058588, 0.057571712546200214, 0.04523491700058588, 0.04523491700058588, 0.04934718218245732, 0.04934718218245732, 0.04112265181871444, 0.03289812145497155, 0.04112265181871444, 0.037010386636842994, 0.03289812145497155, 0.037010386636842994, 0.028785856273100107, 0.03289812145497155, 0.1004432217375536, 0.062777013585971, 0.07114728206410047, 0.0502216108687768, 0.06696214782503573, 0.0502216108687768, 0.062777013585971, 0.054406745107841534, 0.04603647662971207, 0.05859187934690627, 0.0502216108687768, 0.04603647662971207, 0.0502216108687768, 0.041851342390647334, 0.041851342390647334, 0.029295939673453134, 0.029295939673453134, 0.029295939673453134, 0.0251108054343884, 0.03348107391251787, 0.0677210686830977, 0.07170466095857404, 0.08365543778500305, 0.07967184550952672, 0.07568825323405037, 0.0557702918566687, 0.051786699581192364, 0.047803107305716026, 0.047803107305716026, 0.047803107305716026, 0.051786699581192364, 0.047803107305716026, 0.04381951503023969, 0.04381951503023969, 0.03983592275476336, 0.031868738203810684, 0.02788514592833435, 0.02788514592833435, 0.02788514592833435, 0.02788514592833435, 0.07061318157373841, 0.07061318157373841, 0.07061318157373841, 0.08722804782638276, 0.07061318157373841, 0.06230574844741625, 0.05815203188425517, 0.049844598757933, 0.049844598757933, 0.045690882194771916, 0.03738344906844975, 0.045690882194771916, 0.041537165631610834, 0.041537165631610834, 0.03738344906844975, 0.03322973250528867, 0.03322973250528867, 0.03738344906844975, 0.029076015942127584, 0.03322973250528867, 0.07187862294184136, 0.07187862294184136, 0.05496600577905515, 0.0761067772325379, 0.0591941600697517, 0.07187862294184136, 0.0591941600697517, 0.050737851488358604, 0.050737851488358604, 0.06342231436044825, 0.050737851488358604, 0.050737851488358604, 0.0422815429069655, 0.0422815429069655, 0.03805338861626895, 0.033825234325572405, 0.025368925744179302, 0.02959708003487585, 0.02959708003487585, 0.025368925744179302, 0.06684721852170732, 0.07077940784651363, 0.08257597582093257, 0.06684721852170732, 0.062915029196901, 0.05505065054728838, 0.062915029196901, 0.051118461222482064, 0.03932189324806313, 0.051118461222482064, 0.047186271897675755, 0.04325408257286944, 0.047186271897675755, 0.03932189324806313, 0.04325408257286944, 0.02752532527364419, 0.04325408257286944, 0.03538970392325681, 0.02752532527364419, 0.0314575145984505, 0.0625275250036826, 0.08337003333824346, 0.0625275250036826, 0.06669602667059477, 0.07920153167133129, 0.07920153167133129, 0.05835902333677043, 0.05002202000294608, 0.05002202000294608, 0.05002202000294608, 0.045853518336033905, 0.045853518336033905, 0.03751651500220956, 0.03751651500220956, 0.03751651500220956, 0.03334801333529738, 0.029179511668385213, 0.02501101000147304, 0.03334801333529738, 0.029179511668385213, 0.07266150254495349, 0.07266150254495349, 0.059838884448785226, 0.07693570857700957, 0.059838884448785226, 0.0683872965128974, 0.05556467841672914, 0.047016266352616964, 0.051290472384673054, 0.042742060320560875, 0.06411309048084131, 0.047016266352616964, 0.038467854288504785, 0.038467854288504785, 0.038467854288504785, 0.0341936482564487, 0.0341936482564487, 0.0341936482564487, 0.029919442224392613, 0.0341936482564487, 0.058484245982103264, 0.06266169212368207, 0.06266169212368207, 0.0835489228315761, 0.06683913826526088, 0.06266169212368207, 0.045951907557366854, 0.05012935369894565, 0.05012935369894565, 0.058484245982103264, 0.05012935369894565, 0.04177446141578805, 0.04177446141578805, 0.04177446141578805, 0.045951907557366854, 0.03759701527420924, 0.045951907557366854, 0.03341956913263044, 0.03341956913263044, 0.029242122991051632, 0.0809680485209298, 0.05262923153860437, 0.06477443881674384, 0.07287124366883682, 0.05667763396465086, 0.06072603639069735, 0.05667763396465086, 0.05667763396465086, 0.05667763396465086, 0.05262923153860437, 0.04858082911255788, 0.05262923153860437, 0.04453242668651139, 0.03238721940837192, 0.04858082911255788, 0.03238721940837192, 0.03238721940837192, 0.03643562183441841, 0.02833881698232543, 0.02833881698232543, 0.07051440288167674, 0.0663664968298134, 0.0663664968298134, 0.07466230893354008, 0.06221859077795006, 0.06221859077795006, 0.06221859077795006, 0.04977487262236005, 0.05392277867422338, 0.04562696657049671, 0.04562696657049671, 0.04977487262236005, 0.04147906051863338, 0.04147906051863338, 0.04147906051863338, 0.0331832484149067, 0.04147906051863338, 0.0331832484149067, 0.0331832484149067, 0.024887436311180025, 0.07254100826814376, 0.06851095225324688, 0.07657106428304064, 0.06045084022345314, 0.08866123232773127, 0.06045084022345314, 0.05642078420855626, 0.05642078420855626, 0.04433061616386563, 0.048360672178762505, 0.04433061616386563, 0.04433061616386563, 0.04433061616386563, 0.03627050413407188, 0.03627050413407188, 0.03627050413407188, 0.03224044811917501, 0.03224044811917501, 0.02821039210427813, 0.03224044811917501, 0.0656909312623407, 0.0656909312623407, 0.06979661446623699, 0.07800798087402958, 0.07390229767013329, 0.06979661446623699, 0.05747956485454811, 0.053373881650651815, 0.045162515242859226, 0.04926819844675552, 0.04105683203896293, 0.053373881650651815, 0.04105683203896293, 0.036951148835066644, 0.036951148835066644, 0.036951148835066644, 0.03284546563117035, 0.03284546563117035, 0.04105683203896293, 0.028739782427274055, 0.07749696148103387, 0.06933938658829346, 0.06526059914192325, 0.05710302424918285, 0.06526059914192325, 0.061181811695553055, 0.061181811695553055, 0.053024236802812645, 0.04486666191007224, 0.053024236802812645, 0.04894544935644244, 0.053024236802812645, 0.04078787446370204, 0.04486666191007224, 0.04894544935644244, 0.03263029957096163, 0.03670908701733183, 0.028551512124591425, 0.028551512124591425, 0.02447272467822122, 0.07564830674810848, 0.05883757191519548, 0.07985099045633672, 0.07564830674810848, 0.05883757191519548, 0.06304025562342373, 0.07564830674810848, 0.04622952079051073, 0.05043220449873898, 0.04622952079051073, 0.05043220449873898, 0.042026837082282484, 0.05043220449873898, 0.042026837082282484, 0.042026837082282484, 0.033621469665825986, 0.033621469665825986, 0.033621469665825986, 0.02941878595759774, 0.021013418541141242, 0.064749377449374, 0.06879621353995988, 0.064749377449374, 0.07688988572113162, 0.064749377449374, 0.06070254135878812, 0.06070254135878812, 0.0485620330870305, 0.0485620330870305, 0.056655705268202246, 0.04451519699644462, 0.04046836090585875, 0.0485620330870305, 0.04046836090585875, 0.04046836090585875, 0.032374688724687, 0.032374688724687, 0.04046836090585875, 0.036421524815272874, 0.028327852634101123, 0.06701936599559173, 0.07958549711976519, 0.07120807637031622, 0.07120807637031622, 0.05864194524614277, 0.06701936599559173, 0.05445323487141829, 0.05445323487141829, 0.0502645244966938, 0.04188710374724484, 0.046075814121969316, 0.046075814121969316, 0.046075814121969316, 0.04188710374724484, 0.033509682997795866, 0.046075814121969316, 0.033509682997795866, 0.033509682997795866, 0.029320972623071383, 0.033509682997795866, 0.0920601375495363, 0.06276827560195657, 0.07113737901555078, 0.07113737901555078, 0.07532193072234789, 0.058583723895159465, 0.05439917218836236, 0.06276827560195657, 0.04603006877476815, 0.050214620481565254, 0.04603006877476815, 0.04603006877476815, 0.04603006877476815, 0.03766096536117394, 0.033476413654376834, 0.033476413654376834, 0.03766096536117394, 0.029291861947579732, 0.029291861947579732, 0.025107310240782627, 0.06129312668898224, 0.06946554358084654, 0.06129312668898224, 0.0653793351349144, 0.0653793351349144, 0.07355175202677869, 0.05720691824305009, 0.044948292905253644, 0.044948292905253644, 0.04903450135118579, 0.044948292905253644, 0.05720691824305009, 0.04903450135118579, 0.04086208445932149, 0.04086208445932149, 0.0326896675674572, 0.036775876013389344, 0.0326896675674572, 0.04086208445932149, 0.024517250675592895, 0.0681538000006896, 0.0681538000006896, 0.0721628470595537, 0.0761718941184178, 0.0721628470595537, 0.0641447529418255, 0.06013570588296142, 0.044099517647505034, 0.04810856470636913, 0.0641447529418255, 0.04810856470636913, 0.03608142352977685, 0.040090470588640945, 0.03608142352977685, 0.03608142352977685, 0.03207237647091275, 0.03608142352977685, 0.040090470588640945, 0.02806332941204866, 0.03207237647091275, 0.06647421798007509, 0.07893813385133917, 0.06647421798007509, 0.0623195793563204, 0.07062885660382978, 0.0623195793563204, 0.049855663485056316, 0.04570102486130163, 0.04570102486130163, 0.06647421798007509, 0.04570102486130163, 0.049855663485056316, 0.03739174761379224, 0.04154638623754693, 0.03739174761379224, 0.033237108990037546, 0.033237108990037546, 0.03739174761379224, 0.029082470366282854, 0.03739174761379224, 0.06601535014071624, 0.07014130952451102, 0.06188939075692148, 0.07426726890830578, 0.06601535014071624, 0.06601535014071624, 0.057763431373126714, 0.049511512605537183, 0.049511512605537183, 0.04538555322174242, 0.057763431373126714, 0.049511512605537183, 0.03713363445415289, 0.05363747198933195, 0.04125959383794765, 0.03713363445415289, 0.028881715686563357, 0.03300767507035812, 0.028881715686563357, 0.024755756302768592, 0.05915452317194647, 0.07605581550678832, 0.06337984625565693, 0.08028113859049878, 0.07183049242307786, 0.06337984625565693, 0.054929200088236006, 0.04647855392081508, 0.05070387700452555, 0.04647855392081508, 0.04647855392081508, 0.04225323083710462, 0.03802790775339416, 0.04225323083710462, 0.05070387700452555, 0.033802584669683694, 0.033802584669683694, 0.033802584669683694, 0.029577261585973236, 0.029577261585973236, 0.07129398611213572, 0.06733320910590596, 0.06337243209967619, 0.07525476311836547, 0.07129398611213572, 0.05941165509344643, 0.07129398611213572, 0.05149010108098691, 0.04752932407475714, 0.043568547068527384, 0.043568547068527384, 0.04752932407475714, 0.031686216049838095, 0.03960777006229762, 0.04752932407475714, 0.031686216049838095, 0.03960777006229762, 0.031686216049838095, 0.031686216049838095, 0.027725439043608333, 0.07438085649158295, 0.07024858668649502, 0.07438085649158295, 0.061984047076319125, 0.07024858668649502, 0.06611631688140707, 0.061984047076319125, 0.04545496785596736, 0.04545496785596736, 0.04545496785596736, 0.04545496785596736, 0.04545496785596736, 0.04132269805087942, 0.04132269805087942, 0.04132269805087942, 0.04545496785596736, 0.02892588863561559, 0.02892588863561559, 0.03305815844070353, 0.02892588863561559, 0.07169442730036667, 0.06747710804740392, 0.06325978879444118, 0.08012906580629216, 0.05904246954147843, 0.05904246954147843, 0.05904246954147843, 0.05482515028851569, 0.04217319252962745, 0.046390511782590196, 0.06325978879444118, 0.046390511782590196, 0.04217319252962745, 0.046390511782590196, 0.03373855402370196, 0.03373855402370196, 0.03373855402370196, 0.03373855402370196, 0.037955873276664706, 0.03373855402370196, 0.05770113605559668, 0.08655170408339502, 0.07418717492862431, 0.06182264577385359, 0.06182264577385359, 0.05770113605559668, 0.05770113605559668, 0.04945811661908287, 0.04945811661908287, 0.04945811661908287, 0.041215097182569056, 0.04945811661908287, 0.05357962633733978, 0.041215097182569056, 0.04945811661908287, 0.03297207774605525, 0.03297207774605525, 0.037093587464312155, 0.02885056802779834, 0.03297207774605525, 0.06404723171080094, 0.06831704715818766, 0.06404723171080094, 0.06831704715818766, 0.0725868626055744, 0.08112649350034785, 0.05977741626341421, 0.042698154473867295, 0.05123778536864075, 0.04696796992125402, 0.042698154473867295, 0.04696796992125402, 0.042698154473867295, 0.03842833902648057, 0.042698154473867295, 0.03415852357909383, 0.042698154473867295, 0.029888708131707103, 0.029888708131707103, 0.025618892684320375, 0.06544821549314137, 0.0695387289614627, 0.07362924242978403, 0.07771975589810537, 0.0695387289614627, 0.08590078283474803, 0.053176675088177354, 0.049086161619856024, 0.04090513468321335, 0.049086161619856024, 0.04090513468321335, 0.049086161619856024, 0.04090513468321335, 0.03681462121489201, 0.04090513468321335, 0.03272410774657068, 0.028633594278249345, 0.03681462121489201, 0.028633594278249345, 0.03272410774657068, 0.08882553357389551, 0.07190638432172493, 0.0676765970086823, 0.0676765970086823, 0.059217022382597004, 0.054987235069554365, 0.054987235069554365, 0.059217022382597004, 0.05075744775651172, 0.054987235069554365, 0.05075744775651172, 0.04229787313042643, 0.05075744775651172, 0.03806808581738379, 0.03383829850434115, 0.03383829850434115, 0.029608511191298502, 0.029608511191298502, 0.02537872387825586, 0.02537872387825586, 0.06857062459444505, 0.06857062459444505, 0.06857062459444505, 0.06050349228921622, 0.06857062459444505, 0.05646992613660181, 0.06453705844183064, 0.05243635998398739, 0.04840279383137298, 0.05243635998398739, 0.04840279383137298, 0.04840279383137298, 0.04033566152614415, 0.04436922767875856, 0.05243635998398739, 0.03226852922091532, 0.03226852922091532, 0.028234963068300905, 0.028234963068300905, 0.02420139691568649, 0.07705793940208033, 0.0730022583809182, 0.06489089633859396, 0.06894657735975608, 0.07705793940208033, 0.06489089633859396, 0.060835215317431836, 0.04866817225394547, 0.04866817225394547, 0.05272385327510759, 0.04866817225394547, 0.044612491232783345, 0.05677953429626972, 0.0365011291904591, 0.040556810211621226, 0.03244544816929698, 0.02838976714813486, 0.0365011291904591, 0.024334086126972736, 0.024334086126972736, 0.06068969844057266, 0.0728276381286872, 0.0890115577128399, 0.06878165823264902, 0.06878165823264902, 0.05664371854453449, 0.06473567833661084, 0.05259773864849631, 0.04855175875245813, 0.04045979896038178, 0.04855175875245813, 0.04450577885641995, 0.04855175875245813, 0.04045979896038178, 0.0364138190643436, 0.03236783916830542, 0.0364138190643436, 0.03236783916830542, 0.03236783916830542, 0.028321859272267245, 0.0726930050430859, 0.06461600448274302, 0.06865450476291446, 0.06057750420257158, 0.05653900392240014, 0.06057750420257158, 0.04846200336205726, 0.05653900392240014, 0.05653900392240014, 0.04442350308188583, 0.05653900392240014, 0.06461600448274302, 0.040385002801714386, 0.03634650252154295, 0.04442350308188583, 0.03230800224137151, 0.040385002801714386, 0.03634650252154295, 0.03230800224137151, 0.02826950196120007, 0.071576663094834, 0.06315587920132411, 0.06315587920132411, 0.06315587920132411, 0.07578705504158893, 0.06315587920132411, 0.058945487254569176, 0.05052470336105929, 0.04631431141430435, 0.05052470336105929, 0.06315587920132411, 0.04631431141430435, 0.04631431141430435, 0.037893527520794466, 0.03368313557403953, 0.03368313557403953, 0.037893527520794466, 0.029472743627284588, 0.03368313557403953, 0.03368313557403953, 0.06874795250338502, 0.06874795250338502, 0.06445120547192346, 0.07304469953484657, 0.06015445844046189, 0.055857711409000324, 0.06015445844046189, 0.055857711409000324, 0.0472642173460772, 0.051560964377538765, 0.0472642173460772, 0.055857711409000324, 0.04296747031461563, 0.0472642173460772, 0.0472642173460772, 0.03437397625169251, 0.04296747031461563, 0.030077229220230945, 0.030077229220230945, 0.030077229220230945, 0.06402936224634553, 0.07603486766753532, 0.07203303252713873, 0.06803119738674214, 0.06803119738674214, 0.08403853794832852, 0.05202385682515575, 0.05202385682515575, 0.04802202168475915, 0.04402018654436256, 0.04001835140396596, 0.04001835140396596, 0.04402018654436256, 0.04402018654436256, 0.04001835140396596, 0.03201468112317277, 0.04001835140396596, 0.03201468112317277, 0.03201468112317277, 0.028012845982776174, 0.07088934085730181, 0.0625494184035016, 0.0625494184035016, 0.06671937963040171, 0.0625494184035016, 0.07088934085730181, 0.05420949594970138, 0.05003953472280128, 0.05420949594970138, 0.05003953472280128, 0.04586957349590117, 0.05003953472280128, 0.04586957349590117, 0.04586957349590117, 0.03752965104210096, 0.04169961226900107, 0.033359689815200856, 0.029189728588300747, 0.033359689815200856, 0.029189728588300747, 0.06197870136495733, 0.06197870136495733, 0.07850635506227929, 0.057846787940626844, 0.08677018191094027, 0.04958296109196587, 0.07024252821361832, 0.057846787940626844, 0.04545104766763538, 0.053714874516296356, 0.04958296109196587, 0.04545104766763538, 0.04131913424330489, 0.0371872208189744, 0.04131913424330489, 0.033055307394643914, 0.033055307394643914, 0.033055307394643914, 0.024791480545982934, 0.033055307394643914, 0.057161506121725186, 0.09390818862854852, 0.07757632973662704, 0.06532743556768593, 0.06532743556768593, 0.0694104002906663, 0.06124447084470555, 0.053078541398744815, 0.053078541398744815, 0.04491261195278407, 0.04899557667576444, 0.04491261195278407, 0.03674668250682333, 0.0408296472298037, 0.03674668250682333, 0.032663717783842965, 0.032663717783842965, 0.032663717783842965, 0.02449778833788222, 0.028580753060862593, 0.0688626852373146, 0.07291343142774487, 0.08101492380860542, 0.0688626852373146, 0.05671044666602379, 0.05671044666602379, 0.06076119285645406, 0.05265970047559352, 0.05671044666602379, 0.04455820809473298, 0.036456715713872434, 0.04455820809473298, 0.04860895428516325, 0.04050746190430271, 0.036456715713872434, 0.04455820809473298, 0.032405969523442164, 0.036456715713872434, 0.028355223333011895, 0.032405969523442164, 0.07782587359241672, 0.06553757776203513, 0.06553757776203513, 0.06144147915190794, 0.06144147915190794, 0.06144147915190794, 0.05734538054178075, 0.06963367637216233, 0.05734538054178075, 0.04505708471139915, 0.04096098610127196, 0.05324928193165355, 0.04915318332152635, 0.04096098610127196, 0.03686488749114476, 0.032768788881017565, 0.032768788881017565, 0.028672690270890373, 0.028672690270890373, 0.032768788881017565, 0.0717102115402229, 0.0677263108990994, 0.0717102115402229, 0.0717102115402229, 0.0677263108990994, 0.05975850961685242, 0.05975850961685242, 0.0677263108990994, 0.04382290705235844, 0.04780680769348193, 0.04382290705235844, 0.04382290705235844, 0.04780680769348193, 0.04780680769348193, 0.039839006411234945, 0.02788730448786446, 0.03187120512898795, 0.03187120512898795, 0.023903403846740966, 0.03187120512898795, 0.0638658894008092, 0.07663906728097104, 0.0638658894008092, 0.05960816344075525, 0.07663906728097104, 0.05960816344075525, 0.0638658894008092, 0.05109271152064736, 0.046834985560593415, 0.03831953364048552, 0.05109271152064736, 0.05109271152064736, 0.05109271152064736, 0.03831953364048552, 0.042577259600539465, 0.03406180768043157, 0.03831953364048552, 0.042577259600539465, 0.029804081720377625, 0.02554635576032368, 0.06188271854299787, 0.06600823311253107, 0.05775720397346468, 0.08251029139066382, 0.06600823311253107, 0.06188271854299787, 0.05363168940393149, 0.06600823311253107, 0.049506174834398294, 0.049506174834398294, 0.049506174834398294, 0.045380660264865105, 0.05363168940393149, 0.04125514569533191, 0.04125514569533191, 0.02887860198673234, 0.033004116556265534, 0.02887860198673234, 0.02887860198673234, 0.03712963112579872, 0.0791815260202023, 0.08314060232121241, 0.06730429711717195, 0.0514679919131315, 0.07126337341818208, 0.0514679919131315, 0.05542706821414161, 0.05542706821414161, 0.04750891561212138, 0.04750891561212138, 0.0514679919131315, 0.04354983931111127, 0.04354983931111127, 0.03959076301010115, 0.03563168670909104, 0.03167261040808092, 0.03563168670909104, 0.03563168670909104, 0.03563168670909104, 0.03167261040808092, 0.064393722346157, 0.06036911469952219, 0.06841832999279182, 0.064393722346157, 0.0925659758726007, 0.06036911469952219, 0.05634450705288738, 0.04427068411298294, 0.04427068411298294, 0.05634450705288738, 0.048295291759617755, 0.048295291759617755, 0.040246076466348125, 0.04427068411298294, 0.03622146881971331, 0.0321968611730785, 0.03622146881971331, 0.0321968611730785, 0.03622146881971331, 0.0321968611730785, 0.07446335340347518, 0.07860020637033491, 0.07446335340347518, 0.06205279450289598, 0.05791594153603625, 0.05377908856917652, 0.06618964746975571, 0.07032650043661545, 0.05377908856917652, 0.04964223560231679, 0.041368529668597324, 0.04964223560231679, 0.03723167670173759, 0.03723167670173759, 0.041368529668597324, 0.033094823734877855, 0.033094823734877855, 0.033094823734877855, 0.024821117801158393, 0.028957970768018124, 0.05567355876774132, 0.05567355876774132, 0.05995614021141373, 0.06852130309875855, 0.07708646598610337, 0.08136904742977578, 0.051390977324068915, 0.05995614021141373, 0.0471083958803965, 0.0471083958803965, 0.051390977324068915, 0.051390977324068915, 0.038543232993051686, 0.04282581443672409, 0.0471083958803965, 0.034260651549379274, 0.034260651549379274, 0.034260651549379274, 0.034260651549379274, 0.029978070105706866, 0.06238873568044352, 0.07018732764049897, 0.07018732764049897, 0.07018732764049897, 0.07408662362052668, 0.0779859196005544, 0.0584894397004158, 0.0584894397004158, 0.046791551760332645, 0.05459014372038808, 0.046791551760332645, 0.046791551760332645, 0.035093663820249484, 0.04289225578030492, 0.0389929598002772, 0.03119436784022176, 0.03119436784022176, 0.02729507186019404, 0.02729507186019404, 0.03119436784022176, 0.06550157930706972, 0.061407730600377866, 0.07368927672045344, 0.06550157930706972, 0.057313881893686004, 0.07368927672045344, 0.06550157930706972, 0.061407730600377866, 0.049126184480302294, 0.05322003318699415, 0.061407730600377866, 0.03684463836022672, 0.04093848706691858, 0.03684463836022672, 0.03684463836022672, 0.03275078965353486, 0.03275078965353486, 0.03275078965353486, 0.028656940946843002, 0.03275078965353486, 0.06639048919198981, 0.091286922638986, 0.053942272468491724, 0.07053989476648918, 0.06639048919198981, 0.06224108361749045, 0.06639048919198981, 0.053942272468491724, 0.045643461319493, 0.045643461319493, 0.049792866893992364, 0.049792866893992364, 0.03734465017049427, 0.04149405574499364, 0.04149405574499364, 0.029045839021495545, 0.033195244595994905, 0.03734465017049427, 0.033195244595994905, 0.029045839021495545, 0.07594112978228894, 0.07594112978228894, 0.06794732664731115, 0.07194422821480004, 0.06395042507982227, 0.07194422821480004, 0.05995352351233337, 0.051959720377355585, 0.04796281880986669, 0.04796281880986669, 0.04796281880986669, 0.043965917242377806, 0.043965917242377806, 0.03996901567488891, 0.043965917242377806, 0.02797831097242224, 0.043965917242377806, 0.03197521253991113, 0.02797831097242224, 0.02797831097242224, 0.07253896106375284, 0.06850901878243323, 0.07253896106375284, 0.06850901878243323, 0.07656890334507244, 0.08059884562639204, 0.05238924965715482, 0.056419191938474424, 0.056419191938474424, 0.04432936509451562, 0.04029942281319602, 0.04432936509451562, 0.03626948053187642, 0.03626948053187642, 0.04029942281319602, 0.032239538250556816, 0.028209595969237212, 0.032239538250556816, 0.028209595969237212, 0.032239538250556816, 0.06933090085880601, 0.05709603600136966, 0.06933090085880601, 0.06933090085880601, 0.06525261257299389, 0.05709603600136966, 0.061174324287181774, 0.061174324287181774, 0.061174324287181774, 0.05301774771555754, 0.05301774771555754, 0.04893945942974542, 0.040782882858121185, 0.03670459457230907, 0.040782882858121185, 0.032626306286496945, 0.032626306286496945, 0.032626306286496945, 0.032626306286496945, 0.02446972971487271, 0.06591713981097883, 0.070036961049165, 0.070036961049165, 0.07415678228735118, 0.07415678228735118, 0.05767749733460647, 0.05767749733460647, 0.053557676096420294, 0.061797318572792644, 0.04531803362004794, 0.04531803362004794, 0.04943785485823411, 0.04119821238186176, 0.03707839114367559, 0.03707839114367559, 0.03295856990548941, 0.028838748667303234, 0.04119821238186176, 0.03295856990548941, 0.028838748667303234, 0.07659928235185631, 0.059577219606999354, 0.08085479803807055, 0.08936582941049903, 0.06808825097942783, 0.059577219606999354, 0.055321703920785115, 0.055321703920785115, 0.051066188234570875, 0.046810672548356635, 0.038299641175928156, 0.042555156862142396, 0.042555156862142396, 0.03404412548971392, 0.038299641175928156, 0.029788609803499677, 0.03404412548971392, 0.03404412548971392, 0.03404412548971392, 0.025533094117285438, 0.061993027976996816, 0.0661258965087966, 0.057860159445197026, 0.08679023916779555, 0.07439163357239617, 0.061993027976996816, 0.057860159445197026, 0.045461553849797665, 0.041328685317997875, 0.049594422381597454, 0.049594422381597454, 0.049594422381597454, 0.037195816786198085, 0.05372729091339724, 0.037195816786198085, 0.0330629482543983, 0.0330629482543983, 0.037195816786198085, 0.037195816786198085, 0.024797211190798727, 0.06266650587576958, 0.06684427293415422, 0.06266650587576958, 0.06684427293415422, 0.06266650587576958, 0.06684427293415422, 0.05848873881738494, 0.04595543764223103, 0.0543109717590003, 0.05848873881738494, 0.0543109717590003, 0.05013320470061566, 0.041777670583846384, 0.041777670583846384, 0.0543109717590003, 0.03342213646707711, 0.02924436940869247, 0.041777670583846384, 0.02924436940869247, 0.02924436940869247, 0.0630195312669338, 0.058818229182471544, 0.06722083335139606, 0.06722083335139606, 0.07562343752032057, 0.058818229182471544, 0.05461692709800929, 0.05041562501354704, 0.04621432292908478, 0.06722083335139606, 0.05041562501354704, 0.04621432292908478, 0.05041562501354704, 0.04201302084462253, 0.04201302084462253, 0.029409114591235772, 0.03781171876016028, 0.029409114591235772, 0.03361041667569803, 0.029409114591235772, 0.05617725997407765, 0.07222790568095698, 0.07624056710767682, 0.07624056710767682, 0.06420258282751733, 0.06821524425423715, 0.060189921400797486, 0.06420258282751733, 0.040126614267198324, 0.04815193712063799, 0.052164598547357825, 0.04413927569391816, 0.040126614267198324, 0.03611395284047849, 0.040126614267198324, 0.040126614267198324, 0.028088629987038826, 0.03210129141375866, 0.028088629987038826, 0.028088629987038826, 0.057381463151835316, 0.057381463151835316, 0.06967749097008574, 0.07787484284891935, 0.06557881503066894, 0.06557881503066894, 0.06148013909125213, 0.057381463151835316, 0.04508543533358489, 0.04508543533358489, 0.04508543533358489, 0.053282787212418505, 0.04508543533358489, 0.03688808345475127, 0.040986759394168085, 0.040986759394168085, 0.03278940751533447, 0.03278940751533447, 0.040986759394168085, 0.03278940751533447, 0.07990803178720755, 0.05993102384040566, 0.06392642542976604, 0.05993102384040566, 0.0719172286084868, 0.06392642542976604, 0.06392642542976604, 0.055935622251045286, 0.047944819072324535, 0.03995401589360378, 0.047944819072324535, 0.04394941748296415, 0.047944819072324535, 0.04394941748296415, 0.05194022066168491, 0.03196321271488302, 0.0359586143042434, 0.027967811125522643, 0.0359586143042434, 0.027967811125522643, 0.07067141405983351, 0.07067141405983351, 0.07067141405983351, 0.06235713005279428, 0.0665142720563139, 0.06235713005279428, 0.05819998804927466, 0.05819998804927466, 0.04572856203871581, 0.049885704042235426, 0.04572856203871581, 0.041571420035196184, 0.04572856203871581, 0.041571420035196184, 0.037414278031676566, 0.04572856203871581, 0.037414278031676566, 0.03325713602815695, 0.03325713602815695, 0.024942852021117713, 0.07362028938635307, 0.0818003215403923, 0.07362028938635307, 0.06544025723231384, 0.06953027330933345, 0.06135024115529422, 0.05726022507827461, 0.05317020900125499, 0.04499017684721576, 0.04090016077019615, 0.03681014469317653, 0.04499017684721576, 0.04499017684721576, 0.04090016077019615, 0.04499017684721576, 0.03272012861615692, 0.04499017684721576, 0.03272012861615692, 0.03272012861615692, 0.02454009646211769, 0.06817300853767318, 0.06416283156486888, 0.07218318551047749, 0.06416283156486888, 0.06817300853767318, 0.05614247761926027, 0.06015265459206457, 0.06015265459206457, 0.04812212367365166, 0.04812212367365166, 0.04010176972804305, 0.04812212367365166, 0.05614247761926027, 0.04010176972804305, 0.04411194670084735, 0.03208141578243444, 0.036091592755238745, 0.036091592755238745, 0.03208141578243444, 0.028071238809630136, 0.07058935746394966, 0.058824464553291385, 0.0627460955235108, 0.06666772649373023, 0.08235425037460795, 0.06666772649373023, 0.058824464553291385, 0.0627460955235108, 0.04705957164263311, 0.04705957164263311, 0.050981202612852536, 0.04705957164263311, 0.04313794067241369, 0.04313794067241369, 0.04313794067241369, 0.027451416791535982, 0.027451416791535982, 0.03529467873197483, 0.03529467873197483, 0.027451416791535982, 0.0673642177747522, 0.07925202091147318, 0.07132681882032586, 0.0673642177747522, 0.05547641463803123, 0.06340161672917854, 0.07528941986589952, 0.04358861150131025, 0.04755121254688391, 0.04755121254688391, 0.04358861150131025, 0.04755121254688391, 0.04358861150131025, 0.04755121254688391, 0.03566340941016293, 0.03170080836458927, 0.03566340941016293, 0.03170080836458927, 0.03170080836458927, 0.027738207319015613, 0.07797115151227485, 0.0656599170629683, 0.06155617224653277, 0.08617864114514588, 0.0697636618794038, 0.04924493779722622, 0.06155617224653277, 0.05745242743009726, 0.053348682613661734, 0.0451411929807907, 0.04924493779722622, 0.04103744816435518, 0.04103744816435518, 0.03282995853148415, 0.0451411929807907, 0.03282995853148415, 0.03282995853148415, 0.036933703347919664, 0.02872621371504863, 0.02462246889861311, 0.07676757205376923, 0.05656557940804049, 0.08484836911206073, 0.06868677499547773, 0.06464637646633198, 0.06060597793718624, 0.05656557940804049, 0.06060597793718624, 0.04444438382060324, 0.04444438382060324, 0.04848478234974899, 0.04848478234974899, 0.040403985291457487, 0.03636358676231174, 0.04444438382060324, 0.028282789704020244, 0.03232318823316599, 0.03232318823316599, 0.03232318823316599, 0.028282789704020244, 0.07444181444562382, 0.07030615808753361, 0.07030615808753361, 0.07030615808753361, 0.05789918901326297, 0.06203484537135319, 0.05789918901326297, 0.04962787629708255, 0.045492219938992336, 0.05789918901326297, 0.045492219938992336, 0.045492219938992336, 0.041356563580902124, 0.041356563580902124, 0.03722090722281191, 0.0330852508647217, 0.03722090722281191, 0.0330852508647217, 0.0330852508647217, 0.03722090722281191, 0.07770179361040302, 0.06906826098702491, 0.06043472836364679, 0.06906826098702491, 0.09065209254547019, 0.05611796205195774, 0.06043472836364679, 0.047484429428579625, 0.05180119574026868, 0.04316766311689057, 0.05180119574026868, 0.04316766311689057, 0.03885089680520151, 0.034534130493512456, 0.04316766311689057, 0.030217364181823397, 0.034534130493512456, 0.034534130493512456, 0.02590059787013434, 0.034534130493512456, 0.07181734890701691, 0.0549191491641894, 0.08871554864984442, 0.07181734890701691, 0.06336824903560316, 0.050694599228482524, 0.0549191491641894, 0.050694599228482524, 0.050694599228482524, 0.050694599228482524, 0.0549191491641894, 0.050694599228482524, 0.038020949421361895, 0.038020949421361895, 0.04224549935706877, 0.038020949421361895, 0.038020949421361895, 0.04224549935706877, 0.025347299614241262, 0.02957184954994814, 0.06228127033494783, 0.06643335502394436, 0.06643335502394436, 0.07058543971294089, 0.0747375244019374, 0.07058543971294089, 0.05397710095695479, 0.04567293157896175, 0.049825016267958266, 0.049825016267958266, 0.04152084688996523, 0.049825016267958266, 0.04567293157896175, 0.05397710095695479, 0.0373687622009687, 0.03321667751197218, 0.0373687622009687, 0.029064592822975657, 0.03321667751197218, 0.029064592822975657, 0.07138131952633525, 0.07138131952633525, 0.06298351722911934, 0.0671824183777273, 0.06298351722911934, 0.06298351722911934, 0.06298351722911934, 0.05458571493190343, 0.046187912634687515, 0.06298351722911934, 0.046187912634687515, 0.046187912634687515, 0.03779011033747161, 0.041989011486079565, 0.03779011033747161, 0.041989011486079565, 0.03359120918886365, 0.029392308040255694, 0.03359120918886365, 0.029392308040255694, 0.07444139279952862, 0.07030575986622148, 0.07030575986622148, 0.08271265866614291, 0.06617012693291432, 0.07030575986622148, 0.05376322813299289, 0.062034493999607185, 0.04135632933307146, 0.04962759519968575, 0.03722069639976431, 0.045491962266378604, 0.05376322813299289, 0.03722069639976431, 0.03308506346645716, 0.03308506346645716, 0.02894943053315002, 0.03308506346645716, 0.02894943053315002, 0.02894943053315002, 0.065475461617088, 0.065475461617088, 0.061110430842615464, 0.07420552316603307, 0.06984049239156054, 0.061110430842615464, 0.05674540006814293, 0.052380369293670404, 0.043650307744725336, 0.043650307744725336, 0.061110430842615464, 0.043650307744725336, 0.043650307744725336, 0.0392852769702528, 0.043650307744725336, 0.03492024619578027, 0.03492024619578027, 0.03492024619578027, 0.030555215421307732, 0.030555215421307732, 0.07395856228694808, 0.06984975327100652, 0.06984975327100652, 0.06984975327100652, 0.0616321352391234, 0.0821761803188312, 0.05752332622318184, 0.053414517207240275, 0.04930570819129872, 0.04930570819129872, 0.04519689917535716, 0.0410880901594156, 0.0410880901594156, 0.0410880901594156, 0.03697928114347404, 0.032870472127532475, 0.0410880901594156, 0.03697928114347404, 0.02876166311159092, 0.02465285409564936, 0.0654361698123184, 0.0654361698123184, 0.0613464091990485, 0.0613464091990485, 0.0736156910388582, 0.0654361698123184, 0.0572566485857786, 0.0572566485857786, 0.0531668879725087, 0.049077127359238804, 0.049077127359238804, 0.0449873667459689, 0.0368078455194291, 0.040897606132699, 0.0531668879725087, 0.0327180849061592, 0.040897606132699, 0.0327180849061592, 0.0327180849061592, 0.0327180849061592, 0.06676824990117963, 0.06676824990117963, 0.07462333812484782, 0.06676824990117963, 0.07069579401301372, 0.06676824990117963, 0.05498561756567734, 0.06284070578934553, 0.04713052934200915, 0.04320298523017505, 0.04713052934200915, 0.04320298523017505, 0.03534789700650686, 0.04713052934200915, 0.039275441118340956, 0.03534789700650686, 0.031420352894672765, 0.031420352894672765, 0.031420352894672765, 0.02749280878283867, 0.08082460697863918, 0.0889070676765031, 0.0687009159318433, 0.07678337662970722, 0.06465968558291134, 0.05657722488504743, 0.06465968558291134, 0.04849476418718351, 0.04445353383825155, 0.04445353383825155, 0.04445353383825155, 0.04445353383825155, 0.03637107314038763, 0.04445353383825155, 0.03637107314038763, 0.028288612442523713, 0.03232984279145567, 0.03232984279145567, 0.03232984279145567, 0.028288612442523713, 0.08244988513397092, 0.06183741385047819, 0.08657237939066947, 0.06595990810717674, 0.06595990810717674, 0.0535924253370811, 0.06595990810717674, 0.0535924253370811, 0.04534743682368401, 0.04534743682368401, 0.049469931080382556, 0.04122494256698546, 0.03710244831028692, 0.04534743682368401, 0.04534743682368401, 0.03297995405358837, 0.03297995405358837, 0.03297995405358837, 0.028857459796889823, 0.028857459796889823, 0.0687406176581538, 0.06469705191355653, 0.06469705191355653, 0.0687406176581538, 0.0687406176581538, 0.06469705191355653, 0.07278418340275108, 0.05256635467976467, 0.05256635467976467, 0.04447922319057011, 0.04852278893516739, 0.04447922319057011, 0.04043565744597283, 0.04447922319057011, 0.04447922319057011, 0.032348525956778264, 0.032348525956778264, 0.03639209170137554, 0.028304960212180978, 0.028304960212180978, 0.06879801451428082, 0.05665718842352538, 0.06879801451428082, 0.06879801451428082, 0.06879801451428082, 0.064751072484029, 0.064751072484029, 0.06070413045377719, 0.04856330436302175, 0.04856330436302175, 0.052610246393273566, 0.05665718842352538, 0.04451636233276994, 0.036422478272266315, 0.036422478272266315, 0.02832859421176269, 0.0323755362420145, 0.036422478272266315, 0.0323755362420145, 0.024281652181510877, 0.06331096168670254, 0.06726789679212146, 0.07913870210837819, 0.059354026581283637, 0.06331096168670254, 0.07913870210837819, 0.059354026581283637, 0.059354026581283637, 0.04748322126502691, 0.043526286159608, 0.043526286159608, 0.04748322126502691, 0.043526286159608, 0.03561241594877018, 0.043526286159608, 0.03165548084335127, 0.03165548084335127, 0.03165548084335127, 0.03561241594877018, 0.027698545737932363, 0.06347007517568069, 0.05923873683063532, 0.07616409021081684, 0.06347007517568069, 0.06770141352072608, 0.05923873683063532, 0.05077606014054456, 0.05500739848558994, 0.05077606014054456, 0.05500739848558994, 0.0423133834504538, 0.05500739848558994, 0.0423133834504538, 0.03808204510540842, 0.0423133834504538, 0.03385070676036304, 0.03808204510540842, 0.0423133834504538, 0.03385070676036304, 0.02961936841531766, 0.05825868650209384, 0.08544607353640431, 0.05437477406862092, 0.06991042380251261, 0.06602651136903968, 0.07767824866945845, 0.05437477406862092, 0.05437477406862092, 0.050490861635147995, 0.05825868650209384, 0.042723036768202154, 0.042723036768202154, 0.038839124334729226, 0.050490861635147995, 0.038839124334729226, 0.02718738703431046, 0.03107129946778338, 0.03107129946778338, 0.038839124334729226, 0.03107129946778338, 0.056074206353088164, 0.07209540816825621, 0.06809010771446421, 0.07209540816825621, 0.06809010771446421, 0.08010600907584024, 0.05206890589929615, 0.04806360544550414, 0.04806360544550414, 0.04806360544550414, 0.05206890589929615, 0.04806360544550414, 0.03604770408412811, 0.04005300453792012, 0.03604770408412811, 0.03204240363033609, 0.04005300453792012, 0.03604770408412811, 0.03204240363033609, 0.028037103176544082, 0.0634374579254863, 0.0676666217871854, 0.07612494951058356, 0.0634374579254863, 0.0676666217871854, 0.050749966340389044, 0.05920829406378721, 0.0676666217871854, 0.05497913020208813, 0.046520802478689956, 0.046520802478689956, 0.046520802478689956, 0.046520802478689956, 0.046520802478689956, 0.04229163861699087, 0.0338333108935927, 0.0338333108935927, 0.0338333108935927, 0.0338333108935927, 0.025374983170194522, 0.07277751360647557, 0.07277751360647557, 0.07277751360647557, 0.06873431840611581, 0.05256153760467679, 0.0606479280053963, 0.0606479280053963, 0.04447514720395729, 0.04851834240431704, 0.06469112320575605, 0.04447514720395729, 0.04851834240431704, 0.04851834240431704, 0.03638875680323778, 0.03234556160287803, 0.03234556160287803, 0.03638875680323778, 0.04043195200359753, 0.028302366402518275, 0.03234556160287803, 0.06504826995177637, 0.06504826995177637, 0.06911378682376239, 0.06504826995177637, 0.05691723620780432, 0.06098275307979034, 0.06098275307979034, 0.05691723620780432, 0.04878620246383227, 0.06504826995177637, 0.04878620246383227, 0.04878620246383227, 0.04472068559184625, 0.04472068559184625, 0.040655168719860225, 0.032524134975888186, 0.03658965184787421, 0.02845861810390216, 0.02845861810390216, 0.032524134975888186, 0.06706972446810358, 0.06287786668884711, 0.07126158224736005, 0.07545344002661654, 0.06287786668884711, 0.05868600890959064, 0.05868600890959064, 0.054494151130334165, 0.050302293351077686, 0.046110435571821214, 0.050302293351077686, 0.046110435571821214, 0.03772672001330827, 0.054494151130334165, 0.03772672001330827, 0.03353486223405179, 0.03353486223405179, 0.03353486223405179, 0.02934300445479532, 0.03353486223405179, 0.07227827240446946, 0.06826281282644338, 0.07227827240446946, 0.06826281282644338, 0.0642473532484173, 0.0642473532484173, 0.06826281282644338, 0.05621643409236513, 0.04417005535828689, 0.04417005535828689, 0.04417005535828689, 0.04818551493631297, 0.04015459578026081, 0.04417005535828689, 0.03613913620223473, 0.03212367662420865, 0.04015459578026081, 0.04015459578026081, 0.028108217046182566, 0.028108217046182566, 0.07058335445180314, 0.07888727850495644, 0.07473531647837979, 0.06643139242522648, 0.07058335445180314, 0.05812746837207317, 0.05812746837207317, 0.04567158229234321, 0.04982354431891986, 0.041519620265766555, 0.037367658239189895, 0.04982354431891986, 0.05812746837207317, 0.04567158229234321, 0.041519620265766555, 0.037367658239189895, 0.03321569621261324, 0.029063734186036586, 0.029063734186036586, 0.02491177215945993, 0.07465833327629705, 0.06636296291226404, 0.07051064809428055, 0.05806759254823104, 0.07880601845831356, 0.05806759254823104, 0.053919907366214534, 0.07051064809428055, 0.04977222218419804, 0.04562453700218153, 0.04977222218419804, 0.04147685182016503, 0.04147685182016503, 0.037329166638148524, 0.037329166638148524, 0.03318148145613202, 0.03318148145613202, 0.02903379627411552, 0.02903379627411552, 0.03318148145613202, 0.06899091953996525, 0.09055058189620438, 0.06467898706871741, 0.06467898706871741, 0.05174318965497393, 0.05605512212622176, 0.06036705459746959, 0.05605512212622176, 0.04743125718372611, 0.05174318965497393, 0.05174318965497393, 0.05174318965497393, 0.04311932471247828, 0.04743125718372611, 0.04311932471247828, 0.030183527298734796, 0.034495459769982625, 0.034495459769982625, 0.025871594827486966, 0.025871594827486966, 0.07347077776481718, 0.057143938261524474, 0.061225648137347655, 0.06530735801317084, 0.06530735801317084, 0.06530735801317084, 0.061225648137347655, 0.06530735801317084, 0.04898051850987812, 0.0530622283857013, 0.0530622283857013, 0.04898051850987812, 0.044898808634054946, 0.040817098758231765, 0.040817098758231765, 0.03265367900658542, 0.03265367900658542, 0.03265367900658542, 0.03265367900658542, 0.028571969130762237, 0.0746485405412225, 0.0746485405412225, 0.08294282282358055, 0.06635425825886444, 0.07050139940004346, 0.04976569369414833, 0.062207117117685416, 0.0456185525529693, 0.041471411411790275, 0.062207117117685416, 0.041471411411790275, 0.04976569369414833, 0.041471411411790275, 0.041471411411790275, 0.041471411411790275, 0.03317712912943222, 0.03732427027061125, 0.03317712912943222, 0.024882846847074164, 0.029029987988253195, 0.06516532581484145, 0.06109249295141386, 0.07738382440512423, 0.089602322995407, 0.06923815867826905, 0.06109249295141386, 0.05294682722455868, 0.05294682722455868, 0.048873994361131094, 0.048873994361131094, 0.048873994361131094, 0.03665549577084832, 0.04072832863427591, 0.04072832863427591, 0.03665549577084832, 0.04072832863427591, 0.03258266290742073, 0.028509830043993135, 0.03258266290742073, 0.024436997180565547, 0.0797242117943295, 0.07175179061489655, 0.05979315884574712, 0.0637793694354636, 0.0637793694354636, 0.0637793694354636, 0.05979315884574712, 0.05580694825603064, 0.05979315884574712, 0.047834527076597695, 0.04384831648688122, 0.047834527076597695, 0.03986210589716475, 0.03986210589716475, 0.03986210589716475, 0.03587589530744827, 0.0318896847177318, 0.03587589530744827, 0.0318896847177318, 0.0318896847177318, 0.07081559649925614, 0.06664997317577048, 0.05831872652879917, 0.07081559649925614, 0.06248434985228483, 0.07081559649925614, 0.05415310320531352, 0.06248434985228483, 0.04582185655834221, 0.04165623323485655, 0.06248434985228483, 0.04165623323485655, 0.04582185655834221, 0.04582185655834221, 0.0374906099113709, 0.0374906099113709, 0.0374906099113709, 0.03332498658788524, 0.029159363264399586, 0.029159363264399586, 0.06632176504053827, 0.06632176504053827, 0.06632176504053827, 0.08290220630067284, 0.06632176504053827, 0.05803154441047099, 0.07461198567060556, 0.049741323780403704, 0.05388643409543735, 0.045596213465370065, 0.049741323780403704, 0.049741323780403704, 0.03730599283530278, 0.033160882520269136, 0.04145110315033642, 0.033160882520269136, 0.033160882520269136, 0.033160882520269136, 0.029015772205235494, 0.029015772205235494, 0.06376135600939982, 0.06376135600939982, 0.06801211307669314, 0.06801211307669314, 0.06376135600939982, 0.05100908480751986, 0.06376135600939982, 0.05100908480751986, 0.05100908480751986, 0.05525984187481318, 0.0595105989421065, 0.05100908480751986, 0.04250757067293321, 0.03825681360563989, 0.04250757067293321, 0.02975529947105325, 0.04250757067293321, 0.02975529947105325, 0.03400605653834657, 0.02975529947105325, 0.07270860870837818, 0.07270860870837818, 0.06059050725698182, 0.06462987440744727, 0.06866924155791272, 0.06462987440744727, 0.05251177295605091, 0.06866924155791272, 0.048472405805585456, 0.04443303865512, 0.04443303865512, 0.05251177295605091, 0.04443303865512, 0.040393671504654546, 0.03635430435418909, 0.03635430435418909, 0.040393671504654546, 0.03635430435418909, 0.024236202902792728, 0.020196835752327273, 0.0748214747740668, 0.049880983182711204, 0.07066472617550754, 0.0748214747740668, 0.07066472617550754, 0.04572423458415193, 0.062351228978389, 0.05403773178127047, 0.041567485985592664, 0.049880983182711204, 0.049880983182711204, 0.05819448037982974, 0.04572423458415193, 0.0374107373870334, 0.041567485985592664, 0.03325398878847413, 0.03325398878847413, 0.0374107373870334, 0.02909724018991487, 0.0374107373870334, 0.07775294071879839, 0.0695684206431354, 0.07366068068096689, 0.06138390056747241, 0.07366068068096689, 0.05729164052964091, 0.06138390056747241, 0.05729164052964091, 0.045014860416146434, 0.04910712045397792, 0.0654761606053039, 0.04910712045397792, 0.04092260037831494, 0.03683034034048344, 0.045014860416146434, 0.028645820264820456, 0.03273808030265195, 0.028645820264820456, 0.028645820264820456, 0.02455356022698896, 0.07133653847455665, 0.07553280544364822, 0.058747737567281945, 0.07553280544364822, 0.050355203629098816, 0.054551470598190384, 0.050355203629098816, 0.054551470598190384, 0.04615893666000725, 0.050355203629098816, 0.050355203629098816, 0.050355203629098816, 0.054551470598190384, 0.04196266969091568, 0.04615893666000725, 0.03776640272182411, 0.033570135752732544, 0.033570135752732544, 0.03776640272182411, 0.02098133484545784, 0.07310882608023254, 0.06498562318242893, 0.06498562318242893, 0.06092402173352712, 0.06498562318242893, 0.06904722463133074, 0.06498562318242893, 0.05686242028462531, 0.048739217386821694, 0.048739217386821694, 0.052800818835723504, 0.048739217386821694, 0.04467761593791989, 0.03655441304011627, 0.052800818835723504, 0.03655441304011627, 0.03249281159121446, 0.03249281159121446, 0.028431210142312657, 0.028431210142312657, 0.0699680679137321, 0.06585229921292433, 0.06585229921292433, 0.0699680679137321, 0.07408383661453988, 0.05762076181130879, 0.05762076181130879, 0.04938922440969325, 0.05762076181130879, 0.041157687008077704, 0.04938922440969325, 0.04938922440969325, 0.05762076181130879, 0.03704191830726994, 0.041157687008077704, 0.028810380905654395, 0.041157687008077704, 0.028810380905654395, 0.028810380905654395, 0.028810380905654395, 0.06442573913371344, 0.07247895652542762, 0.08053217391714179, 0.07247895652542762, 0.06442573913371344, 0.06039913043785635, 0.06039913043785635, 0.05234591304614217, 0.04831930435028508, 0.04429269565442799, 0.04429269565442799, 0.04831930435028508, 0.03623947826271381, 0.05234591304614217, 0.040266086958570896, 0.03221286956685672, 0.03221286956685672, 0.03623947826271381, 0.03221286956685672, 0.028186260870999628, 0.06899103087829372, 0.073049326812311, 0.07710762274632828, 0.06493273494427644, 0.06899103087829372, 0.073049326812311, 0.05275784714222461, 0.04464125527419006, 0.04464125527419006, 0.040582959340172774, 0.04464125527419006, 0.060874439010259164, 0.0365246634061555, 0.04464125527419006, 0.0365246634061555, 0.0365246634061555, 0.028408071538120944, 0.0365246634061555, 0.028408071538120944, 0.03246636747213822, 0.08105617142834692, 0.05673931999984285, 0.07295055428551224, 0.0607921285712602, 0.06484493714267754, 0.06484493714267754, 0.0526865114284255, 0.05673931999984285, 0.04863370285700816, 0.04863370285700816, 0.0526865114284255, 0.04458089428559081, 0.0526865114284255, 0.04458089428559081, 0.04052808571417346, 0.03242246857133877, 0.03242246857133877, 0.03647527714275612, 0.028369659999921425, 0.03647527714275612, 0.07168922472645481, 0.06770649001942954, 0.0836374288475306, 0.07567195943348007, 0.059741020605379, 0.059741020605379, 0.06770649001942954, 0.055758285898353736, 0.04381008177727794, 0.04381008177727794, 0.047792816484303204, 0.04381008177727794, 0.03982734707025267, 0.03982734707025267, 0.03186187765620214, 0.027879142949176868, 0.03982734707025267, 0.03982734707025267, 0.03186187765620214, 0.027879142949176868, 0.07532032610706932, 0.06695140098406162, 0.07113586354556548, 0.08368925123007702, 0.06276693842255777, 0.05439801329955007, 0.05858247586105392, 0.05439801329955007, 0.04602908817654236, 0.05021355073804622, 0.04602908817654236, 0.05021355073804622, 0.04184462561503851, 0.03766016305353466, 0.03347570049203081, 0.03347570049203081, 0.03766016305353466, 0.04602908817654236, 0.02929123793052696, 0.02929123793052696, 0.0689481424018629, 0.05678082315447532, 0.08517123473171298, 0.060836596236937844, 0.0689481424018629, 0.05678082315447532, 0.08517123473171298, 0.05678082315447532, 0.04055773082462523, 0.04055773082462523, 0.04461350390708775, 0.04461350390708775, 0.04461350390708775, 0.04055773082462523, 0.03650195774216271, 0.032446184659700186, 0.032446184659700186, 0.04055773082462523, 0.02839041157723766, 0.024334638494775136, 0.08676463598973527, 0.0785013373240462, 0.06197473999266805, 0.06610638932551259, 0.06610638932551259, 0.07023803865835712, 0.05371144132697898, 0.057843090659823515, 0.0454481426612899, 0.04131649332844537, 0.04957979199413444, 0.033053194662756295, 0.05371144132697898, 0.0454481426612899, 0.03718484399560083, 0.03718484399560083, 0.033053194662756295, 0.028921545329911758, 0.028921545329911758, 0.02478989599706722, 0.06740816064072429, 0.06319515060067903, 0.07162117068076956, 0.06740816064072429, 0.07162117068076956, 0.08004719076086009, 0.04634311044049795, 0.06319515060067903, 0.05476913052058849, 0.04213010040045268, 0.04634311044049795, 0.04634311044049795, 0.04213010040045268, 0.04634311044049795, 0.04213010040045268, 0.029491070280316875, 0.033704080320362144, 0.033704080320362144, 0.029491070280316875, 0.02527806024027161, 0.06669768248793437, 0.06669768248793437, 0.06669768248793437, 0.07086628764343027, 0.058360472176942575, 0.06669768248793437, 0.054191867021446676, 0.058360472176942575, 0.054191867021446676, 0.050023261865950776, 0.04585465671045488, 0.04585465671045488, 0.04585465671045488, 0.04168605155495898, 0.04168605155495898, 0.033348841243967184, 0.033348841243967184, 0.029180236088471288, 0.033348841243967184, 0.037517446399463084, 0.07760499549697678, 0.07372474572212794, 0.07372474572212794, 0.062083996397581415, 0.05432349684788374, 0.062083996397581415, 0.062083996397581415, 0.05432349684788374, 0.0504432470730349, 0.04268274752333722, 0.04268274752333722, 0.058203746622732574, 0.04268274752333722, 0.03492224797363955, 0.03880249774848839, 0.03492224797363955, 0.031041998198790707, 0.031041998198790707, 0.03492224797363955, 0.02716174842394187, 0.0703848058791949, 0.07866537127674725, 0.0703848058791949, 0.06210424048164256, 0.06624452318041874, 0.0703848058791949, 0.057963957782866386, 0.04968339238531405, 0.04140282698776171, 0.04554310968653788, 0.04140282698776171, 0.04968339238531405, 0.04968339238531405, 0.03312226159020937, 0.03726254428898554, 0.04554310968653788, 0.03726254428898554, 0.03312226159020937, 0.028981978891433193, 0.024841696192657026, 0.0717113310072749, 0.06374340533979991, 0.06374340533979991, 0.06374340533979991, 0.05577547967232492, 0.05577547967232492, 0.06374340533979991, 0.051791516838587424, 0.05975944250606242, 0.04382359117111244, 0.04382359117111244, 0.051791516838587424, 0.047807554004849935, 0.04382359117111244, 0.03585566550363745, 0.04382359117111244, 0.03187170266989996, 0.03187170266989996, 0.03585566550363745, 0.03187170266989996, 0.07504563298504387, 0.06670722932003899, 0.06253802748753656, 0.08755323848255119, 0.06670722932003899, 0.07087643115254143, 0.05836882565503412, 0.050030421990029245, 0.050030421990029245, 0.04586122015752681, 0.054199623822531684, 0.04586122015752681, 0.03752281649252193, 0.04586122015752681, 0.03752281649252193, 0.033353614660019494, 0.033353614660019494, 0.033353614660019494, 0.02918441282751706, 0.025015210995014622, 0.07550542905683075, 0.06711593693940511, 0.07131068299811794, 0.07131068299811794, 0.06711593693940511, 0.05872644482197947, 0.054531698763266655, 0.050336952704553836, 0.0419474605871282, 0.04614220664584102, 0.050336952704553836, 0.05872644482197947, 0.04614220664584102, 0.0419474605871282, 0.0419474605871282, 0.033557968469702555, 0.033557968469702555, 0.037752714528415374, 0.029363222410989737, 0.029363222410989737, 0.07201511050973211, 0.07201511050973211, 0.08048747645205354, 0.059306561596249974, 0.059306561596249974, 0.059306561596249974, 0.06354274456741069, 0.05083419565392855, 0.05083419565392855, 0.046598012682767836, 0.042361829711607124, 0.05083419565392855, 0.03812564674044641, 0.03812564674044641, 0.03812564674044641, 0.03812564674044641, 0.0338894637692857, 0.0338894637692857, 0.042361829711607124, 0.0338894637692857, 0.07184265424276153, 0.07583391281180384, 0.07583391281180384, 0.059868878535634606, 0.06785139567371921, 0.06386013710467692, 0.059868878535634606, 0.05188636139754999, 0.055877619966592296, 0.039912585690423066, 0.039912585690423066, 0.04789510282850768, 0.03592132712138076, 0.05188636139754999, 0.03592132712138076, 0.03193006855233846, 0.03592132712138076, 0.03193006855233846, 0.03592132712138076, 0.03193006855233846, 0.07636738412588204, 0.07636738412588204, 0.06029004009938055, 0.06430937610600593, 0.06832871211263129, 0.07234804811925666, 0.05627070409275518, 0.05225136808612981, 0.04823203207950444, 0.06029004009938055, 0.04823203207950444, 0.05225136808612981, 0.03617402405962833, 0.0401933600662537, 0.032154688053002964, 0.032154688053002964, 0.032154688053002964, 0.032154688053002964, 0.03617402405962833, 0.02813535204637759, 0.06443897891093887, 0.06443897891093887, 0.06846641509287255, 0.06443897891093887, 0.06846641509287255, 0.07652128745673992, 0.056384106547071516, 0.04832923418320415, 0.05235667036513783, 0.04832923418320415, 0.04832923418320415, 0.056384106547071516, 0.040274361819336794, 0.040274361819336794, 0.040274361819336794, 0.04430179800127047, 0.03221948945546944, 0.036246925637403116, 0.03221948945546944, 0.028192053273535758, 0.06639403942761274, 0.06639403942761274, 0.07054366689183854, 0.06639403942761274, 0.07054366689183854, 0.062244411963386946, 0.05809478449916115, 0.04979552957070956, 0.04564590210648376, 0.053945157034935354, 0.04979552957070956, 0.062244411963386946, 0.04564590210648376, 0.04564590210648376, 0.03734664717803217, 0.03319701971380637, 0.02489776478535478, 0.03319701971380637, 0.03319701971380637, 0.029047392249580577, 0.07332100723977134, 0.057027450075377704, 0.07739439653086974, 0.06924761794867293, 0.06924761794867293, 0.057027450075377704, 0.06517422865757452, 0.048880671493180886, 0.040733892910984074, 0.040733892910984074, 0.03258711432878726, 0.0529540607842793, 0.048880671493180886, 0.040733892910984074, 0.0529540607842793, 0.03666050361988567, 0.03666050361988567, 0.03258711432878726, 0.03258711432878726, 0.03258711432878726, 0.07065306589498024, 0.0623409404955708, 0.0623409404955708, 0.07065306589498024, 0.0623409404955708, 0.06649700319527552, 0.05818487779586608, 0.04987275239645664, 0.05402881509616136, 0.04987275239645664, 0.05818487779586608, 0.0623409404955708, 0.0415606269970472, 0.03324850159763776, 0.03324850159763776, 0.03324850159763776, 0.03324850159763776, 0.03324850159763776, 0.03324850159763776, 0.03324850159763776, 0.06867177238098093, 0.0646322563585703, 0.0646322563585703, 0.06867177238098093, 0.06867177238098093, 0.06867177238098093, 0.0646322563585703, 0.04847419226892772, 0.04847419226892772, 0.04443467624651708, 0.05251370829133836, 0.060592740336159655, 0.03231612817928515, 0.040395160224106434, 0.03635564420169579, 0.03635564420169579, 0.028276612156874506, 0.028276612156874506, 0.03231612817928515, 0.03231612817928515, 0.062307850846159, 0.06646170756923626, 0.062307850846159, 0.07061556429231353, 0.06646170756923626, 0.062307850846159, 0.054000137400004464, 0.05815399412308173, 0.0498462806769272, 0.054000137400004464, 0.04153856723077266, 0.05815399412308173, 0.0498462806769272, 0.04153856723077266, 0.04569242395384993, 0.03323085378461813, 0.0373847105076954, 0.03323085378461813, 0.03323085378461813, 0.029076997061540866, 0.07036746397332415, 0.057949676213325765, 0.07036746397332415, 0.06208893879999189, 0.07036746397332415, 0.04967115103999351, 0.04967115103999351, 0.07450672655999027, 0.053810413626659635, 0.04967115103999351, 0.053810413626659635, 0.053810413626659635, 0.04967115103999351, 0.037253363279995136, 0.037253363279995136, 0.037253363279995136, 0.037253363279995136, 0.028974838106662883, 0.033114100693329006, 0.024835575519996756, 0.06796251616828726, 0.06796251616828726, 0.0637148589077693, 0.06796251616828726, 0.06796251616828726, 0.0552195443867334, 0.059467201647251354, 0.0637148589077693, 0.04672422986569749, 0.05097188712621545, 0.042476572605179536, 0.04672422986569749, 0.042476572605179536, 0.042476572605179536, 0.038228915344661586, 0.03398125808414363, 0.03398125808414363, 0.042476572605179536, 0.029733600823625677, 0.029733600823625677, 0.06529834741922923, 0.07346064084663287, 0.06937949413293105, 0.07346064084663287, 0.06121720070552739, 0.053054907278123745, 0.053054907278123745, 0.04489261385072009, 0.04897376056442192, 0.053054907278123745, 0.04897376056442192, 0.04897376056442192, 0.05713605399182557, 0.036730320423316434, 0.053054907278123745, 0.04081146713701826, 0.028568026995912786, 0.028568026995912786, 0.032649173709614614, 0.028568026995912786, 0.0669430243061755, 0.06275908528703954, 0.05857514626790357, 0.07531090234444746, 0.0669430243061755, 0.0669430243061755, 0.05020726822963163, 0.05020726822963163, 0.05857514626790357, 0.0543912072487676, 0.05020726822963163, 0.046023329210495664, 0.05857514626790357, 0.041839390191359696, 0.03765545117222373, 0.029287573133951784, 0.03347151215308775, 0.03765545117222373, 0.029287573133951784, 0.025103634114815816, 0.06633722642543032, 0.062435036635699125, 0.07023941621516151, 0.06633722642543032, 0.06633722642543032, 0.062435036635699125, 0.07023941621516151, 0.05463065705623674, 0.04292408768704315, 0.05463065705623674, 0.05072846726650554, 0.05072846726650554, 0.04682627747677434, 0.04682627747677434, 0.039021897897311954, 0.031217518317849562, 0.035119708107580756, 0.031217518317849562, 0.02731532852811837, 0.02731532852811837, 0.06476581001981546, 0.06476581001981546, 0.07340125135579086, 0.08203669269176625, 0.060448089351827765, 0.060448089351827765, 0.060448089351827765, 0.05181264801585237, 0.05181264801585237, 0.04317720667987698, 0.05181264801585237, 0.047494927347864674, 0.04317720667987698, 0.04317720667987698, 0.03885948601188927, 0.025906324007926185, 0.030224044675913882, 0.03885948601188927, 0.03885948601188927, 0.030224044675913882, 0.06781807715025658, 0.06781807715025658, 0.08901122625971175, 0.07205670697214761, 0.059340817506474505, 0.06357944732836554, 0.05510218768458347, 0.059340817506474505, 0.04662492804080139, 0.04662492804080139, 0.04238629821891036, 0.05510218768458347, 0.04238629821891036, 0.04238629821891036, 0.04238629821891036, 0.03390903857512829, 0.029670408753237253, 0.03390903857512829, 0.029670408753237253, 0.025431778931346214, 0.07975758563378213, 0.06716428263897443, 0.06296651497403853, 0.07136205030391034, 0.06716428263897443, 0.054570979644166726, 0.04617544431429492, 0.06716428263897443, 0.05037321197923082, 0.04617544431429492, 0.05037321197923082, 0.054570979644166726, 0.03777990898442312, 0.04617544431429492, 0.03358214131948722, 0.03358214131948722, 0.03777990898442312, 0.03358214131948722, 0.029384373654551314, 0.029384373654551314, 0.07618413626216215, 0.07618413626216215, 0.06348678021846846, 0.06348678021846846, 0.06348678021846846, 0.0592543282039039, 0.05078942417477477, 0.05078942417477477, 0.05078942417477477, 0.05078942417477477, 0.05078942417477477, 0.038092068131081076, 0.05078942417477477, 0.033859616116516514, 0.05078942417477477, 0.033859616116516514, 0.038092068131081076, 0.038092068131081076, 0.02962716410195195, 0.025394712087387385, 0.06436116914006473, 0.05577967992138943, 0.07294265835874002, 0.07294265835874002, 0.05577967992138943, 0.06436116914006473, 0.05148893531205178, 0.05577967992138943, 0.05577967992138943, 0.04719819070271413, 0.05577967992138943, 0.04719819070271413, 0.03861670148403883, 0.04290744609337648, 0.03861670148403883, 0.034325956874701184, 0.03861670148403883, 0.03861670148403883, 0.034325956874701184, 0.03861670148403883, 0.06668966218248584, 0.06252155829608047, 0.06252155829608047, 0.058353454409675105, 0.07502586995529657, 0.058353454409675105, 0.06252155829608047, 0.050017246636864375, 0.050017246636864375, 0.06668966218248584, 0.050017246636864375, 0.04584914275045901, 0.041681038864053645, 0.041681038864053645, 0.04584914275045901, 0.03334483109124292, 0.037512934977648286, 0.029176727204837553, 0.029176727204837553, 0.025008623318432188, 0.06907717167336386, 0.08939398687141205, 0.06907717167336386, 0.06501380863375422, 0.06501380863375422, 0.0528237195149253, 0.0528237195149253, 0.048760356475315667, 0.048760356475315667, 0.0528237195149253, 0.048760356475315667, 0.044696993435706024, 0.044696993435706024, 0.036570267356486746, 0.036570267356486746, 0.036570267356486746, 0.028443541277267472, 0.036570267356486746, 0.03250690431687711, 0.028443541277267472, 0.07212933001300188, 0.056100590010112576, 0.0761365150137242, 0.0761365150137242, 0.07212933001300188, 0.06411496001155723, 0.04808622000866792, 0.06411496001155723, 0.04407903500794559, 0.04007185000722327, 0.04808622000866792, 0.04808622000866792, 0.04407903500794559, 0.056100590010112576, 0.04007185000722327, 0.03205748000577861, 0.028050295005056288, 0.03205748000577861, 0.03205748000577861, 0.028050295005056288, 0.058593590887827285, 0.06696410387180261, 0.07533461685577794, 0.06696410387180261, 0.06277884737981494, 0.07114936036379027, 0.058593590887827285, 0.058593590887827285, 0.04603782141186429, 0.050223077903851956, 0.04603782141186429, 0.050223077903851956, 0.04185256491987663, 0.04185256491987663, 0.04185256491987663, 0.03766730842788897, 0.03348205193590131, 0.04185256491987663, 0.029296795443913642, 0.029296795443913642, 0.05753053709134167, 0.07396783340315358, 0.06574918524724763, 0.0698585093252006, 0.09040512971496548, 0.05753053709134167, 0.061639861169294645, 0.053421213013388695, 0.053421213013388695, 0.04931188893543571, 0.03698391670157679, 0.04520256485748274, 0.03698391670157679, 0.03698391670157679, 0.03698391670157679, 0.04109324077952976, 0.03698391670157679, 0.028765268545670835, 0.03287459262362381, 0.028765268545670835, 0.06769020250771739, 0.06769020250771739, 0.07192084016444973, 0.06345956485098506, 0.07192084016444973, 0.06769020250771739, 0.05499828953752038, 0.046537014224055706, 0.046537014224055706, 0.050767651880788044, 0.050767651880788044, 0.046537014224055706, 0.046537014224055706, 0.04230637656732337, 0.04230637656732337, 0.03384510125385869, 0.02961446359712636, 0.02961446359712636, 0.02961446359712636, 0.03807573891059103, 0.07420593054756497, 0.07008337885047804, 0.06596082715339109, 0.0577157237592172, 0.0577157237592172, 0.07008337885047804, 0.0577157237592172, 0.07008337885047804, 0.049470620365043315, 0.0577157237592172, 0.049470620365043315, 0.04534806866795637, 0.037102965273782484, 0.037102965273782484, 0.04122551697086943, 0.037102965273782484, 0.032980413576695546, 0.0288578618796086, 0.0288578618796086, 0.0288578618796086, 0.06467699090679001, 0.097015486360185, 0.06871930283846438, 0.06063467897511563, 0.06063467897511563, 0.06467699090679001, 0.056592367043441255, 0.05255005511176688, 0.056592367043441255, 0.044465431248418126, 0.05255005511176688, 0.044465431248418126, 0.0485077431800925, 0.04042311931674376, 0.04042311931674376, 0.032338495453395004, 0.032338495453395004, 0.032338495453395004, 0.028296183521720628, 0.02425387159004625, 0.057080307215132936, 0.07746613122053755, 0.06523463681729479, 0.07338896641945662, 0.08154329602161847, 0.05300314241405201, 0.061157472016213854, 0.05300314241405201, 0.03261731840864739, 0.048925977612971086, 0.040771648010809236, 0.061157472016213854, 0.05300314241405201, 0.04484881281189016, 0.03261731840864739, 0.03669448320972831, 0.03669448320972831, 0.03261731840864739, 0.028540153607566468, 0.024462988806485543, 0.0613802778793499, 0.052611666753728484, 0.07014888900497131, 0.052611666753728484, 0.07453319456778203, 0.0657645834421606, 0.0657645834421606, 0.04822736119091778, 0.052611666753728484, 0.04384305562810707, 0.04822736119091778, 0.05699597231653919, 0.05699597231653919, 0.04822736119091778, 0.035074444502485654, 0.035074444502485654, 0.04384305562810707, 0.035074444502485654, 0.026305833376864242, 0.026305833376864242, 0.0628022686889554, 0.08792317616453756, 0.07117590451414946, 0.06698908660155242, 0.0628022686889554, 0.05861545077635837, 0.0628022686889554, 0.054428632863761345, 0.05024181495116432, 0.05024181495116432, 0.04605499703856729, 0.04605499703856729, 0.04186817912597027, 0.03349454330077621, 0.03768136121337324, 0.029307725388179186, 0.03768136121337324, 0.03768136121337324, 0.03349454330077621, 0.03349454330077621, 0.07140313151819927, 0.07536997215809924, 0.08330365343789915, 0.06346945023839935, 0.06743629087829932, 0.051568928318699477, 0.059502609598499394, 0.04363524703889955, 0.04760208767879952, 0.059502609598499394, 0.051568928318699477, 0.039668406398999594, 0.039668406398999594, 0.04363524703889955, 0.04363524703889955, 0.031734725119199676, 0.031734725119199676, 0.031734725119199676, 0.031734725119199676, 0.027767884479299718, 0.07333795237686258, 0.05608196358230668, 0.06470995797958463, 0.07333795237686258, 0.06470995797958463, 0.06470995797958463, 0.060395960780945654, 0.0517679663836677, 0.060395960780945654, 0.04745396918502873, 0.043139971986389754, 0.038825974787750774, 0.038825974787750774, 0.038825974787750774, 0.038825974787750774, 0.038825974787750774, 0.0345119775891118, 0.0345119775891118, 0.030197980390472827, 0.0345119775891118, 0.07263363952200502, 0.06859843732633808, 0.07263363952200502, 0.056492830739337245, 0.08473924610900586, 0.056492830739337245, 0.06859843732633808, 0.06052803293500419, 0.0443872241523364, 0.0443872241523364, 0.0443872241523364, 0.04842242634800335, 0.0443872241523364, 0.04035202195666946, 0.04035202195666946, 0.03631681976100251, 0.028246415369668623, 0.03228161756533557, 0.028246415369668623, 0.028246415369668623, 0.06238879185252541, 0.07902580301319885, 0.07070729743286214, 0.06654804464269377, 0.0748665502230305, 0.06238879185252541, 0.04991103348202033, 0.05407028627218869, 0.04991103348202033, 0.04991103348202033, 0.04575178069185197, 0.04991103348202033, 0.04159252790168361, 0.03327402232134689, 0.04159252790168361, 0.03327402232134689, 0.03327402232134689, 0.03327402232134689, 0.04159252790168361, 0.029114769531178527, 0.06560735145341104, 0.06150689198757285, 0.06150689198757285, 0.05330597305589647, 0.06560735145341104, 0.06970781091924923, 0.05740643252173466, 0.05740643252173466, 0.04920551359005828, 0.04920551359005828, 0.06150689198757285, 0.05740643252173466, 0.03690413519254371, 0.0410045946583819, 0.0410045946583819, 0.03690413519254371, 0.0410045946583819, 0.03280367572670552, 0.03280367572670552, 0.03280367572670552, 0.07439985045235818, 0.06199987537696515, 0.06613320040209617, 0.07026652542722717, 0.06613320040209617, 0.06199987537696515, 0.06613320040209617, 0.05786655035183414, 0.0413332502513101, 0.05373322532670313, 0.06199987537696515, 0.04546657527644111, 0.04546657527644111, 0.0413332502513101, 0.0413332502513101, 0.03306660020104808, 0.02893327517591707, 0.03306660020104808, 0.02893327517591707, 0.02893327517591707, 0.07954154216501444, 0.07954154216501444, 0.06363323373201156, 0.08749569638151589, 0.05170200240725939, 0.06363323373201156, 0.047724925299008666, 0.05170200240725939, 0.047724925299008666, 0.05170200240725939, 0.04374784819075794, 0.04374784819075794, 0.05170200240725939, 0.03977077108250722, 0.04374784819075794, 0.03181661686600578, 0.035793693974256496, 0.03181661686600578, 0.027839539757755053, 0.027839539757755053, 0.06804089341009306, 0.06804089341009306, 0.05953578173383142, 0.05953578173383142, 0.05953578173383142, 0.06804089341009306, 0.06378833757196224, 0.05528322589570061, 0.042525558381308164, 0.05528322589570061, 0.051030670057569796, 0.046778114219438977, 0.046778114219438977, 0.046778114219438977, 0.038273002543177344, 0.02976789086691571, 0.03402044670504653, 0.038273002543177344, 0.038273002543177344, 0.02976789086691571, 0.06651629823127947, 0.058201760952369536, 0.0748308355101894, 0.07898810414964437, 0.06651629823127947, 0.0623590295918245, 0.05404449231291457, 0.058201760952369536, 0.04157268639454967, 0.045729955034004635, 0.04157268639454967, 0.05404449231291457, 0.04157268639454967, 0.045729955034004635, 0.0498872236734596, 0.033258149115639735, 0.033258149115639735, 0.0374154177550947, 0.029100880476184768, 0.029100880476184768, 0.0776355506185817, 0.061291224172564496, 0.061291224172564496, 0.061291224172564496, 0.0776355506185817, 0.05720514256106019, 0.06946338739557309, 0.053119060949555895, 0.0490329793380516, 0.044946897726547294, 0.0490329793380516, 0.044946897726547294, 0.040860816115043, 0.03677473450353869, 0.03677473450353869, 0.03677473450353869, 0.03677473450353869, 0.032688652892034396, 0.03677473450353869, 0.03677473450353869, 0.06175605792719078, 0.06175605792719078, 0.06175605792719078, 0.07410726951262893, 0.06587312845567016, 0.06587312845567016, 0.06175605792719078, 0.04117070528479385, 0.04940484634175262, 0.05352191687023201, 0.05352191687023201, 0.04940484634175262, 0.045287775813273236, 0.037053634756314464, 0.045287775813273236, 0.03293656422783508, 0.037053634756314464, 0.037053634756314464, 0.04117070528479385, 0.02470242317087631, 0.08614004915234164, 0.07047822203373406, 0.05873185169477838, 0.07439367881338596, 0.06264730847443027, 0.06264730847443027, 0.05481639491512649, 0.050900938135474603, 0.050900938135474603, 0.04698548135582271, 0.04698548135582271, 0.03915456779651892, 0.04698548135582271, 0.04698548135582271, 0.04307002457617082, 0.03523911101686703, 0.03523911101686703, 0.031323654237215136, 0.03523911101686703, 0.027408197457563246, 0.09347381904641192, 0.0650252654235909, 0.0650252654235909, 0.06908934451256532, 0.0650252654235909, 0.04876894906769317, 0.06096118633461646, 0.0650252654235909, 0.04876894906769317, 0.04876894906769317, 0.06096118633461646, 0.04876894906769317, 0.03657671180076988, 0.03251263271179545, 0.03657671180076988, 0.03251263271179545, 0.03657671180076988, 0.03251263271179545, 0.028448553622821015, 0.03251263271179545, 0.0708750840210253, 0.0708750840210253, 0.07481258868886002, 0.08662510269236424, 0.0708750840210253, 0.0708750840210253, 0.06693757935319056, 0.055125065349686336, 0.03543754201051265, 0.051187560681851596, 0.04725005601401686, 0.03937504667834738, 0.03937504667834738, 0.03150003734267791, 0.03937504667834738, 0.03937504667834738, 0.03150003734267791, 0.03150003734267791, 0.02362502800700843, 0.027562532674843168, 0.06677430306155369, 0.0709476970029008, 0.0709476970029008, 0.06260090912020659, 0.05842751517885948, 0.0709476970029008, 0.05842751517885948, 0.06677430306155369, 0.04173393941347106, 0.05008072729616527, 0.04590733335481816, 0.05008072729616527, 0.03756054547212395, 0.04173393941347106, 0.04173393941347106, 0.03756054547212395, 0.03756054547212395, 0.03338715153077684, 0.03338715153077684, 0.025040363648082636, 0.06980443164988114, 0.06569828861165283, 0.06569828861165283, 0.06980443164988114, 0.06980443164988114, 0.05337985949696793, 0.049273716458739625, 0.049273716458739625, 0.05337985949696793, 0.05337985949696793, 0.04516757342051132, 0.04516757342051132, 0.04516757342051132, 0.049273716458739625, 0.03695528734405472, 0.03284914430582642, 0.04516757342051132, 0.03695528734405472, 0.028743001267598114, 0.028743001267598114, 0.07547056067556832, 0.06355415635837332, 0.06752629113077165, 0.06752629113077165, 0.05958202158597498, 0.06752629113077165, 0.06355415635837332, 0.047665617268779985, 0.06355415635837332, 0.047665617268779985, 0.047665617268779985, 0.047665617268779985, 0.04369348249638166, 0.035749212951584994, 0.035749212951584994, 0.03177707817918666, 0.035749212951584994, 0.03177707817918666, 0.03177707817918666, 0.03177707817918666, 0.07039637854305818, 0.08604001821929333, 0.06648546862399939, 0.07039637854305818, 0.07821819838117576, 0.05866364878588182, 0.054752738866823025, 0.054752738866823025, 0.04693091902870545, 0.04693091902870545, 0.04693091902870545, 0.05084182894776424, 0.03519818927152909, 0.03910909919058788, 0.03519818927152909, 0.03519818927152909, 0.031287279352470304, 0.031287279352470304, 0.03519818927152909, 0.023465459514352725, 0.0918765610841608, 0.07099552447412426, 0.06264310983010964, 0.07517173179613157, 0.06681931715211695, 0.05846690250810233, 0.05846690250810233, 0.05846690250810233, 0.05011448786408771, 0.041762073220073095, 0.05429069518609502, 0.0459382805420804, 0.041762073220073095, 0.041762073220073095, 0.037585865898065784, 0.033409658576058474, 0.033409658576058474, 0.029233451254051166, 0.025057243932043855, 0.029233451254051166, 0.07649465683082872, 0.06844258769074148, 0.08857276054095957, 0.06844258769074148, 0.0724686222607851, 0.056364483980610634, 0.06039051855065425, 0.0483124148405234, 0.0483124148405234, 0.044286380270479785, 0.04026034570043616, 0.05233844941056701, 0.03623431113039255, 0.04026034570043616, 0.04026034570043616, 0.04026034570043616, 0.032208276560348935, 0.028182241990305317, 0.032208276560348935, 0.0241562074202617, 0.07125846349959641, 0.06287511485258508, 0.06706678917609074, 0.06706678917609074, 0.06706678917609074, 0.0586834405290794, 0.05449176620557373, 0.0586834405290794, 0.04610841755856239, 0.05449176620557373, 0.04610841755856239, 0.041916743235056717, 0.041916743235056717, 0.04610841755856239, 0.041916743235056717, 0.037725068911551043, 0.03353339458804537, 0.03353339458804537, 0.0293417202645397, 0.037725068911551043, 0.06239183174309529, 0.07487019809171434, 0.07487019809171434, 0.06239183174309529, 0.07071074264217467, 0.06655128719263498, 0.049913465394476234, 0.049913465394476234, 0.04159455449539686, 0.054072920844015916, 0.045754009944936544, 0.045754009944936544, 0.049913465394476234, 0.054072920844015916, 0.03743509904585717, 0.03327564359631749, 0.03743509904585717, 0.03327564359631749, 0.03327564359631749, 0.02079727724769843, 0.08278709902243549, 0.06209032426682661, 0.06622967921794838, 0.07450838912019193, 0.06622967921794838, 0.06622967921794838, 0.04967225941346129, 0.053811614364583066, 0.04553290446233951, 0.04967225941346129, 0.04553290446233951, 0.04553290446233951, 0.053811614364583066, 0.04139354951121774, 0.04139354951121774, 0.03725419456009597, 0.03311483960897419, 0.028975484657852418, 0.03311483960897419, 0.028975484657852418, 0.08733926606751946, 0.07145939950978865, 0.07939933278865405, 0.06748943287035594, 0.06748943287035594, 0.05160956631262514, 0.059549499591490536, 0.05160956631262514, 0.04763959967319243, 0.05160956631262514, 0.039699666394327024, 0.039699666394327024, 0.04366963303375973, 0.04366963303375973, 0.039699666394327024, 0.027789766476028918, 0.035729699754894324, 0.03175973311546162, 0.027789766476028918, 0.03175973311546162, 0.05912881748054132, 0.07489650214201901, 0.07095458097664958, 0.05912881748054132, 0.06701265981128017, 0.07095458097664958, 0.0551868963151719, 0.06701265981128017, 0.04730305398443306, 0.03941921165369422, 0.04336113281906364, 0.05124497514980248, 0.04730305398443306, 0.03547729048832479, 0.04336113281906364, 0.03547729048832479, 0.03941921165369422, 0.03547729048832479, 0.02759344815758595, 0.02759344815758595, 0.06802401836955115, 0.05952101607335726, 0.08503002296193894, 0.06802401836955115, 0.0722755195176481, 0.05952101607335726, 0.05952101607335726, 0.05952101607335726, 0.04676651262906641, 0.05101801377716336, 0.04251501148096947, 0.03401200918477557, 0.04676651262906641, 0.03826351033287252, 0.03826351033287252, 0.03401200918477557, 0.03401200918477557, 0.03401200918477557, 0.02976050803667863, 0.02550900688858168, 0.07314943848548801, 0.07314943848548801, 0.08534101156640268, 0.06095786540457335, 0.05689400771093512, 0.05689400771093512, 0.06095786540457335, 0.04876629232365868, 0.04063857693638223, 0.05689400771093512, 0.0528301500172969, 0.0528301500172969, 0.04470243463002045, 0.036574719242744004, 0.036574719242744004, 0.032510861549105784, 0.032510861549105784, 0.036574719242744004, 0.032510861549105784, 0.02844700385546756, 0.06852627209018322, 0.06852627209018322, 0.06424338008454678, 0.06852627209018322, 0.07280916409581967, 0.06424338008454678, 0.05567759607327387, 0.04711181206200096, 0.04711181206200096, 0.04711181206200096, 0.04711181206200096, 0.04711181206200096, 0.05567759607327387, 0.04711181206200096, 0.04711181206200096, 0.03426313604509161, 0.03426313604509161, 0.029980244039455158, 0.029980244039455158, 0.029980244039455158, 0.0701342581292143, 0.06600871353337816, 0.06600871353337816, 0.0701342581292143, 0.07425980272505044, 0.05775762434170589, 0.05775762434170589, 0.04950653515003362, 0.04950653515003362, 0.04538099055419748, 0.053632079745869754, 0.04538099055419748, 0.05775762434170589, 0.04950653515003362, 0.04125544595836135, 0.028878812170852947, 0.028878812170852947, 0.03300435676668908, 0.028878812170852947, 0.028878812170852947, 0.07190271722515226, 0.07190271722515226, 0.06790812182375491, 0.06790812182375491, 0.06391352642235758, 0.05991893102096022, 0.05592433561956287, 0.04793514481676818, 0.05192974021816553, 0.03994595401397348, 0.05991893102096022, 0.04793514481676818, 0.03994595401397348, 0.03994595401397348, 0.04394054941537083, 0.03595135861257613, 0.03195676321117879, 0.03595135861257613, 0.03994595401397348, 0.027962167809781436, 0.07896515930127018, 0.07480909828541386, 0.07480909828541386, 0.07896515930127018, 0.058184854221988554, 0.054028793206132235, 0.058184854221988554, 0.04987273219027591, 0.04156061015856326, 0.04987273219027591, 0.04571667117441958, 0.04571667117441958, 0.04571667117441958, 0.033248488126850606, 0.04156061015856326, 0.04571667117441958, 0.033248488126850606, 0.033248488126850606, 0.029092427110994277, 0.029092427110994277, 0.06813344572314668, 0.0561098964778855, 0.07614914521998746, 0.07614914521998746, 0.07214129547156707, 0.06011774622630589, 0.0561098964778855, 0.052102046729465105, 0.0561098964778855, 0.04809419698104471, 0.040078497484203925, 0.04809419698104471, 0.04408634723262432, 0.04408634723262432, 0.04408634723262432, 0.02805494823894275, 0.04408634723262432, 0.03206279798736314, 0.03206279798736314, 0.024047098490522355, 0.07097232949636864, 0.07514717240791974, 0.06262264367326645, 0.05844780076171535, 0.07932201531947083, 0.06262264367326645, 0.054272957850164255, 0.054272957850164255, 0.04592327202706206, 0.041748429115510965, 0.05009811493861316, 0.041748429115510965, 0.04592327202706206, 0.04592327202706206, 0.04592327202706206, 0.03339874329240877, 0.03757358620395987, 0.03339874329240877, 0.041748429115510965, 0.029223900380857676, 0.07803363447496962, 0.06571253429471126, 0.05749846750787235, 0.06160550090129181, 0.06571253429471126, 0.06160550090129181, 0.05749846750787235, 0.0533914341144529, 0.04928440072103345, 0.045177367327613994, 0.045177367327613994, 0.045177367327613994, 0.045177367327613994, 0.04928440072103345, 0.04107033393419454, 0.045177367327613994, 0.03285626714735563, 0.03285626714735563, 0.03696330054077509, 0.03285626714735563, 0.08491804381528226, 0.06469946195450077, 0.07278689469881337, 0.07278689469881337, 0.06874317832665706, 0.060655745582344475, 0.060655745582344475, 0.04448088009371928, 0.04448088009371928, 0.04852459646587558, 0.04852459646587558, 0.04852459646587558, 0.04852459646587558, 0.036393447349406687, 0.04043716372156298, 0.032349730977250385, 0.028306014605094087, 0.032349730977250385, 0.028306014605094087, 0.036393447349406687, 0.06780057884641998, 0.07627565120222247, 0.07627565120222247, 0.050850434134814985, 0.07203811502432123, 0.06356304266851873, 0.050850434134814985, 0.06780057884641998, 0.046612897956913736, 0.04237536177901249, 0.050850434134814985, 0.046612897956913736, 0.046612897956913736, 0.04237536177901249, 0.04237536177901249, 0.03390028942320999, 0.03390028942320999, 0.03390028942320999, 0.03390028942320999, 0.025425217067407493, 0.0906136359247289, 0.07485474272042823, 0.05515612621505238, 0.07485474272042823, 0.07091501941935306, 0.06303557281720272, 0.05909584951612755, 0.05515612621505238, 0.05515612621505238, 0.03545750970967653, 0.043336956311826866, 0.04727667961290204, 0.043336956311826866, 0.043336956311826866, 0.03545750970967653, 0.03545750970967653, 0.03151778640860136, 0.03151778640860136, 0.03151778640860136, 0.02757806310752619, 0.08921687580466531, 0.07370089740385395, 0.06206391360324543, 0.07370089740385395, 0.06206391360324543, 0.06594290820344827, 0.05430592440283975, 0.05430592440283975, 0.05042692980263691, 0.046547935202434075, 0.04266894060223123, 0.046547935202434075, 0.03878994600202839, 0.03878994600202839, 0.03878994600202839, 0.03878994600202839, 0.031031956801622716, 0.03878994600202839, 0.027152962201419877, 0.031031956801622716, 0.07016670796790953, 0.07429416137778655, 0.06191180114815546, 0.06603925455803249, 0.05778434773827843, 0.06603925455803249, 0.04952944091852437, 0.04952944091852437, 0.04540198750864734, 0.04952944091852437, 0.0536568943284014, 0.0536568943284014, 0.041274534098770306, 0.037147080688893275, 0.037147080688893275, 0.03301962727901624, 0.041274534098770306, 0.03301962727901624, 0.037147080688893275, 0.03301962727901624, 0.06838972093592918, 0.06034387141405516, 0.06838972093592918, 0.06838972093592918, 0.07241264569686619, 0.06838972093592918, 0.06034387141405516, 0.04827509713124413, 0.04827509713124413, 0.06034387141405516, 0.05229802189218114, 0.040229247609370106, 0.040229247609370106, 0.04425217237030712, 0.036206322848433094, 0.03218339808749609, 0.036206322848433094, 0.036206322848433094, 0.028160473326559073, 0.036206322848433094, 0.07067282011511797, 0.07067282011511797, 0.062358370689809976, 0.06651559540246398, 0.062358370689809976, 0.05404392126450198, 0.062358370689809976, 0.05404392126450198, 0.05820114597715598, 0.04988669655184798, 0.04988669655184798, 0.05820114597715598, 0.03741502241388599, 0.04572947183919398, 0.04572947183919398, 0.02910057298857799, 0.03325779770123199, 0.03325779770123199, 0.03325779770123199, 0.02494334827592399, 0.0736471737793796, 0.07773868343378956, 0.0736471737793796, 0.0531896255073297, 0.06137264481614966, 0.06546415447055964, 0.05728113516173968, 0.06137264481614966, 0.0531896255073297, 0.0531896255073297, 0.04091509654409977, 0.04091509654409977, 0.04500660619850975, 0.03273207723527982, 0.04091509654409977, 0.03273207723527982, 0.04500660619850975, 0.03273207723527982, 0.024549057926459863, 0.03273207723527982, 0.08707610235176878, 0.059370069785296894, 0.07124408374235627, 0.06728607909000314, 0.06332807443765003, 0.07124408374235627, 0.06728607909000314, 0.05541206513294377, 0.051454060480590647, 0.047496055828237516, 0.047496055828237516, 0.04353805117588439, 0.047496055828237516, 0.03562204187117814, 0.03958004652353126, 0.03166403721882501, 0.03166403721882501, 0.03166403721882501, 0.027706032566471885, 0.023748027914118758, 0.0645758887932362, 0.0688809480461186, 0.0645758887932362, 0.0645758887932362, 0.055965770287471364, 0.0645758887932362, 0.055965770287471364, 0.051660711034588955, 0.051660711034588955, 0.06027082954035378, 0.04735565178170654, 0.04735565178170654, 0.04305059252882413, 0.04305059252882413, 0.03874553327594171, 0.0344404740230593, 0.03013541477017689, 0.0344404740230593, 0.03874553327594171, 0.03013541477017689, 0.06110025222514514, 0.06924695252183116, 0.06110025222514514, 0.06924695252183116, 0.05702690207680213, 0.07332030267017417, 0.052953551928459124, 0.05702690207680213, 0.04888020178011611, 0.06110025222514514, 0.052953551928459124, 0.04888020178011611, 0.0448068516317731, 0.0448068516317731, 0.036660151335087085, 0.036660151335087085, 0.03258680118674408, 0.03258680118674408, 0.028513451038401066, 0.028513451038401066, 0.08447415555925833, 0.06436126137848254, 0.0683838402146377, 0.07240641905079286, 0.060338682542327375, 0.0683838402146377, 0.060338682542327375, 0.0482709460338619, 0.03620320952539643, 0.0482709460338619, 0.044248367197706744, 0.044248367197706744, 0.03620320952539643, 0.0482709460338619, 0.040225788361551586, 0.03620320952539643, 0.03620320952539643, 0.040225788361551586, 0.03218063068924127, 0.03218063068924127, 0.0729171174617175, 0.06481521552152666, 0.06886616649162208, 0.06886616649162208, 0.06481521552152666, 0.05266236261124042, 0.060764264551431246, 0.056713313581335835, 0.04456046067104958, 0.056713313581335835, 0.04456046067104958, 0.04456046067104958, 0.05266236261124042, 0.04456046067104958, 0.03645855873085875, 0.03240760776076333, 0.03240760776076333, 0.03645855873085875, 0.03240760776076333, 0.028356656790667917, 0.07117285096602678, 0.06698621267390756, 0.07117285096602678, 0.05861293608966912, 0.06279957438178835, 0.05861293608966912, 0.0544262977975499, 0.050239659505430675, 0.0544262977975499, 0.04605302121331145, 0.04605302121331145, 0.050239659505430675, 0.04605302121331145, 0.04186638292119223, 0.037679744629073, 0.03349310633695378, 0.02930646804483456, 0.037679744629073, 0.04186638292119223, 0.02930646804483456, 0.07241279223317006, 0.07241279223317006, 0.0643669264294845, 0.06838985933132728, 0.0844815909386984, 0.05632106062579894, 0.05632106062579894, 0.05632106062579894, 0.05229812772395616, 0.05632106062579894, 0.04425226192027059, 0.04425226192027059, 0.03620639611658503, 0.04022932901842781, 0.03620639611658503, 0.03218346321474225, 0.03620639611658503, 0.03218346321474225, 0.03620639611658503, 0.02816053031289947, 0.0771165321146041, 0.07305776726646705, 0.06088147272205587, 0.06494023757019293, 0.07305776726646705, 0.06494023757019293, 0.06088147272205587, 0.0487051781776447, 0.0487051781776447, 0.036528883633233526, 0.0487051781776447, 0.0487051781776447, 0.05682270787391881, 0.036528883633233526, 0.04464641332950764, 0.028411353936959406, 0.028411353936959406, 0.032470118785096466, 0.032470118785096466, 0.028411353936959406, 0.07475924041322661, 0.06229936701102217, 0.05399278474288588, 0.07475924041322661, 0.07060594927915846, 0.06229936701102217, 0.05814607587695403, 0.05814607587695403, 0.04568620247474959, 0.04153291134068145, 0.04568620247474959, 0.05399278474288588, 0.04568620247474959, 0.037379620206613305, 0.033226329072545156, 0.037379620206613305, 0.033226329072545156, 0.04153291134068145, 0.04153291134068145, 0.02491974680440887, 0.0770560907759918, 0.06421340897999317, 0.05993251504799363, 0.06849430291199272, 0.07277519684399227, 0.06421340897999317, 0.05565162111599409, 0.04708983325199499, 0.05137072718399454, 0.05137072718399454, 0.05137072718399454, 0.05565162111599409, 0.03424715145599636, 0.03424715145599636, 0.05137072718399454, 0.03424715145599636, 0.029966257523996814, 0.03424715145599636, 0.02568536359199727, 0.029966257523996814, 0.0630252173378416, 0.07983194196126601, 0.0630252173378416, 0.0714285796495538, 0.0630252173378416, 0.058823536181985485, 0.0672268984936977, 0.058823536181985485, 0.050420173870273276, 0.04621849271441717, 0.04201681155856106, 0.050420173870273276, 0.03361344924684885, 0.04621849271441717, 0.03361344924684885, 0.03361344924684885, 0.03361344924684885, 0.03361344924684885, 0.04201681155856106, 0.03361344924684885, 0.07290032196322382, 0.08505037562376112, 0.06885030407637806, 0.06480028618953229, 0.05670025041584075, 0.06480028618953229, 0.06480028618953229, 0.052650232528994985, 0.04860021464214921, 0.052650232528994985, 0.04860021464214921, 0.04860021464214921, 0.04050017886845768, 0.03645016098161191, 0.03645016098161191, 0.03645016098161191, 0.028350125207920376, 0.03645016098161191, 0.032400143094766146, 0.028350125207920376, 0.0729170748253872, 0.07696802342679761, 0.060764229021156003, 0.0648151776225664, 0.06886612622397681, 0.056713280419745604, 0.056713280419745604, 0.06886612622397681, 0.0445604346155144, 0.0445604346155144, 0.048611383216924806, 0.048611383216924806, 0.048611383216924806, 0.0364585374126936, 0.0445604346155144, 0.0364585374126936, 0.0364585374126936, 0.028356640209872802, 0.0324075888112832, 0.024305691608462403, 0.07519474188238245, 0.058484799241853014, 0.06266228490198537, 0.0710172562222501, 0.06266228490198537, 0.058484799241853014, 0.058484799241853014, 0.05430731358172066, 0.05430731358172066, 0.0501298279215883, 0.04595234226145594, 0.04595234226145594, 0.04177485660132358, 0.04595234226145594, 0.04595234226145594, 0.03759737094119123, 0.029242399620926507, 0.04177485660132358, 0.03341988528105887, 0.02506491396079415, 0.0666179338347079, 0.0749451755640464, 0.0666179338347079, 0.07078155469937715, 0.07078155469937715, 0.05829069210536942, 0.054127071240700174, 0.054127071240700174, 0.04579982951136168, 0.04579982951136168, 0.04579982951136168, 0.04996345037603093, 0.041636208646692444, 0.041636208646692444, 0.041636208646692444, 0.03330896691735395, 0.041636208646692444, 0.0374725877820232, 0.02914534605268471, 0.024981725188015464, 0.06969758565430607, 0.07379744363397114, 0.06149786969497595, 0.06559772767464102, 0.07379744363397114, 0.06149786969497595, 0.057398011715310886, 0.057398011715310886, 0.03689872181698557, 0.04919829575598076, 0.04919829575598076, 0.04919829575598076, 0.04919829575598076, 0.0450984377763157, 0.03689872181698557, 0.03689872181698557, 0.03279886383732051, 0.0450984377763157, 0.02459914787799038, 0.02459914787799038, 0.06967776771163645, 0.06148038327497333, 0.08607253658496267, 0.06557907549330488, 0.06557907549330488, 0.06148038327497333, 0.06967776771163645, 0.04918430661997866, 0.05328299883831022, 0.04098692218331555, 0.04508561440164711, 0.04918430661997866, 0.04918430661997866, 0.04098692218331555, 0.036888229964984, 0.036888229964984, 0.03278953774665244, 0.03278953774665244, 0.028690845528320887, 0.02459215330998933, 0.07596079975615598, 0.06796492609761325, 0.05996905243907052, 0.09994842073178418, 0.05996905243907052, 0.06396698926834188, 0.05996905243907052, 0.06396698926834188, 0.051973178780527776, 0.04797524195125641, 0.051973178780527776, 0.04397730512198504, 0.03598143146344231, 0.03598143146344231, 0.03598143146344231, 0.03198349463417094, 0.03598143146344231, 0.03198349463417094, 0.027985557804899575, 0.027985557804899575, 0.06516084783351148, 0.07737850680229488, 0.06516084783351148, 0.07330595381270041, 0.07330595381270041, 0.06108829484391701, 0.052943188864728076, 0.052943188864728076, 0.052943188864728076, 0.052943188864728076, 0.04072552989594467, 0.04479808288553914, 0.04072552989594467, 0.03665297690635021, 0.052943188864728076, 0.03258042391675574, 0.02850787092716127, 0.03665297690635021, 0.02850787092716127, 0.03258042391675574, 0.06082896008083586, 0.06893948809161397, 0.08921580811855925, 0.07299475209700303, 0.06082896008083586, 0.06082896008083586, 0.06082896008083586, 0.04866316806466869, 0.0567736960754468, 0.0567736960754468, 0.05271843207005775, 0.03649737604850151, 0.04055264005389057, 0.04055264005389057, 0.04055264005389057, 0.03244211204311246, 0.03649737604850151, 0.0283868480377234, 0.0283868480377234, 0.0283868480377234, 0.07226481499663659, 0.06376307205585581, 0.0680139435262462, 0.0680139435262462, 0.0680139435262462, 0.05951220058546543, 0.046759586174294265, 0.05101045764468465, 0.05101045764468465, 0.042508714703903876, 0.046759586174294265, 0.05101045764468465, 0.05526132911507504, 0.046759586174294265, 0.042508714703903876, 0.029756100292732714, 0.029756100292732714, 0.0340069717631231, 0.038257843233513486, 0.029756100292732714, 0.05529405725838305, 0.0680542243180099, 0.07656100235776114, 0.07230761333788553, 0.06380083529813428, 0.05529405725838305, 0.06380083529813428, 0.05529405725838305, 0.04253389019875619, 0.04678727921863181, 0.05104066823850743, 0.04253389019875619, 0.04253389019875619, 0.04678727921863181, 0.04253389019875619, 0.04253389019875619, 0.03402711215900495, 0.03402711215900495, 0.03828050117888057, 0.03402711215900495, 0.0716615206385356, 0.0716615206385356, 0.08360510741162488, 0.0636991294564761, 0.07962391182059513, 0.0636991294564761, 0.047774347092357075, 0.05175554268338683, 0.05175554268338683, 0.047774347092357075, 0.047774347092357075, 0.043793151501327315, 0.043793151501327315, 0.0358307603192678, 0.043793151501327315, 0.0358307603192678, 0.03184956472823805, 0.023887173546178538, 0.03184956472823805, 0.027868369137208294, 0.06686742702812225, 0.0626882128388646, 0.0626882128388646, 0.06686742702812225, 0.0626882128388646, 0.0835842837851528, 0.05432978446034932, 0.05015057027109168, 0.05015057027109168, 0.05432978446034932, 0.045971356081834044, 0.045971356081834044, 0.045971356081834044, 0.033433713514061124, 0.045971356081834044, 0.03761292770331876, 0.033433713514061124, 0.033433713514061124, 0.03761292770331876, 0.029254499324803483, 0.0757856345222716, 0.06736500846424143, 0.0757856345222716, 0.06315469543522634, 0.0757856345222716, 0.05894438240621125, 0.05473406937719616, 0.05473406937719616, 0.04210313029015089, 0.04631344331916598, 0.04210313029015089, 0.05473406937719616, 0.04631344331916598, 0.0378928172611358, 0.0378928172611358, 0.033682504232120714, 0.029472191203105624, 0.04210313029015089, 0.033682504232120714, 0.025261878174090535, 0.07139748142720963, 0.0832970616650779, 0.07139748142720963, 0.051564847697429174, 0.07139748142720963, 0.06743095468125354, 0.05553137444338527, 0.04759832095147309, 0.05553137444338527, 0.05553137444338527, 0.0396652674595609, 0.051564847697429174, 0.0396652674595609, 0.0396652674595609, 0.035698740713604814, 0.03173221396764872, 0.03173221396764872, 0.035698740713604814, 0.035698740713604814, 0.03173221396764872, 0.08240799338155747, 0.07416719404340172, 0.06592639470524597, 0.0618059950361681, 0.09064879271971321, 0.057685595367090226, 0.045324396359856606, 0.057685595367090226, 0.04944479602893448, 0.04944479602893448, 0.05356519569801235, 0.04120399669077873, 0.03708359702170086, 0.04120399669077873, 0.03708359702170086, 0.028842797683545113, 0.028842797683545113, 0.032963197352622986, 0.032963197352622986, 0.02472239801446724, 0.06967759049410642, 0.06148022690656449, 0.07787495408164835, 0.057381545112793524, 0.07377627228787739, 0.06967759049410642, 0.05328286331902256, 0.05328286331902256, 0.04918418152525159, 0.04098681793770966, 0.04098681793770966, 0.04508549973148063, 0.04918418152525159, 0.036888136143938696, 0.04098681793770966, 0.028690772556396762, 0.04918418152525159, 0.03278945435016773, 0.036888136143938696, 0.028690772556396762, 0.07908186782015944, 0.07075746068119529, 0.07075746068119529, 0.07075746068119529, 0.07075746068119529, 0.06243305354223113, 0.058270849972749054, 0.0499464428337849, 0.0499464428337849, 0.033297628555856607, 0.045784239264302834, 0.045784239264302834, 0.045784239264302834, 0.058270849972749054, 0.033297628555856607, 0.033297628555856607, 0.03745983212533868, 0.029135424986374527, 0.029135424986374527, 0.02497322141689245, 0.07214467281012506, 0.07615271018846534, 0.0641285980534445, 0.0641285980534445, 0.07214467281012506, 0.06012056067510421, 0.06012056067510421, 0.052104485918423656, 0.048096448540083374, 0.04008037378340281, 0.048096448540083374, 0.04408841116174309, 0.04008037378340281, 0.04008037378340281, 0.052104485918423656, 0.03206429902672225, 0.03206429902672225, 0.04008037378340281, 0.028056261648381965, 0.028056261648381965, 0.07214162861800358, 0.06412589210489207, 0.07614949687455934, 0.08015736513111509, 0.07614949687455934, 0.06011802384833632, 0.056110155591780564, 0.056110155591780564, 0.04809441907866906, 0.04809441907866906, 0.04007868256555754, 0.056110155591780564, 0.03607081430900179, 0.04007868256555754, 0.04809441907866906, 0.028055077795890282, 0.032062946052446036, 0.032062946052446036, 0.028055077795890282, 0.028055077795890282, 0.06860153229599283, 0.06431393652749329, 0.06431393652749329, 0.06860153229599283, 0.07288912806449238, 0.051451149221994626, 0.06431393652749329, 0.06431393652749329, 0.06860153229599283, 0.051451149221994626, 0.04287595768499552, 0.04287595768499552, 0.04287595768499552, 0.03858836191649597, 0.04287595768499552, 0.03430076614799642, 0.030013170379496863, 0.03430076614799642, 0.03430076614799642, 0.02143797884249776, 0.0657160184544256, 0.0944667765282368, 0.0657160184544256, 0.0657160184544256, 0.0575015161476224, 0.0575015161476224, 0.0657160184544256, 0.061608767301024, 0.0451797626874176, 0.0533942649942208, 0.0451797626874176, 0.049287013840819204, 0.041072511534016, 0.041072511534016, 0.041072511534016, 0.0369652603806144, 0.0287507580738112, 0.0369652603806144, 0.0287507580738112, 0.024643506920409602, 0.07744048526825731, 0.0733646702541385, 0.08966793031061372, 0.0652130402259009, 0.0692888552400197, 0.06113722521178208, 0.052985595183544475, 0.04890978016942567, 0.04890978016942567, 0.052985595183544475, 0.040758150141188054, 0.04890978016942567, 0.03668233512706925, 0.040758150141188054, 0.040758150141188054, 0.03260652011295045, 0.03260652011295045, 0.03260652011295045, 0.03260652011295045, 0.02853070509883164, 0.06450077287600295, 0.06450077287600295, 0.06853207118075313, 0.08062596609500369, 0.0765946677902535, 0.056438176266502584, 0.056438176266502584, 0.06046947457125277, 0.04434428135225203, 0.056438176266502584, 0.04434428135225203, 0.04434428135225203, 0.052406877961752404, 0.040312983047501846, 0.040312983047501846, 0.032250386438001474, 0.028219088133251292, 0.03628168474275166, 0.032250386438001474, 0.02418778982850111, 0.07604232077197165, 0.06336860064330971, 0.05069488051464777, 0.071817747395751, 0.07604232077197165, 0.0802668941481923, 0.05914402726708906, 0.05069488051464777, 0.04647030713842712, 0.05069488051464777, 0.04647030713842712, 0.04647030713842712, 0.04647030713842712, 0.038021160385985825, 0.03379658700976518, 0.03379658700976518, 0.03379658700976518, 0.03379658700976518, 0.03379658700976518, 0.02957201363354453, 0.06848897464641288, 0.0813306573926153, 0.06420841373101208, 0.07276953556181369, 0.06420841373101208, 0.06420841373101208, 0.05992785281561127, 0.038525048238607244, 0.051366730984809664, 0.04280560915400805, 0.04708617006940886, 0.038525048238607244, 0.038525048238607244, 0.04708617006940886, 0.038525048238607244, 0.038525048238607244, 0.03424448732320644, 0.038525048238607244, 0.025683365492404832, 0.038525048238607244, 0.05471711981104158, 0.06313513824350951, 0.05892612902727555, 0.06734414745974349, 0.05892612902727555, 0.07155315667597745, 0.06734414745974349, 0.050508110594807615, 0.05892612902727555, 0.05471711981104158, 0.050508110594807615, 0.04629910137857365, 0.03788108294610571, 0.03788108294610571, 0.04209009216233968, 0.04209009216233968, 0.03367207372987174, 0.03367207372987174, 0.03367207372987174, 0.029463064513637775, 0.06848477081646764, 0.06848477081646764, 0.060427738955706745, 0.06848477081646764, 0.0725132867468481, 0.060427738955706745, 0.056399223025326295, 0.056399223025326295, 0.056399223025326295, 0.0644562548860872, 0.040285159303804496, 0.048342191164565396, 0.040285159303804496, 0.048342191164565396, 0.040285159303804496, 0.03625664337342405, 0.03625664337342405, 0.028199611512663147, 0.028199611512663147, 0.024171095582282698, 0.06885101787031658, 0.05670083824614307, 0.06075089812086757, 0.06885101787031658, 0.07290107774504109, 0.06480095799559207, 0.06075089812086757, 0.04455065862196955, 0.052650778371418565, 0.04050059874724505, 0.05670083824614307, 0.04860071849669406, 0.04455065862196955, 0.04455065862196955, 0.04050059874724505, 0.03240047899779604, 0.036450538872520546, 0.04050059874724505, 0.03240047899779604, 0.036450538872520546, 0.07611318505900629, 0.06765616449689447, 0.06765616449689447, 0.088798715902174, 0.05074212337267086, 0.05919914393478267, 0.06765616449689447, 0.05074212337267086, 0.054970633653726765, 0.04228510281055905, 0.04228510281055905, 0.046513613091614954, 0.04228510281055905, 0.04228510281055905, 0.04228510281055905, 0.03805659252950314, 0.03805659252950314, 0.029599571967391335, 0.029599571967391335, 0.029599571967391335, 0.06640981446728644, 0.07812919349092522, 0.06250335479274018, 0.07812919349092522, 0.058596895118193916, 0.06250335479274018, 0.054690435443647656, 0.054690435443647656, 0.058596895118193916, 0.054690435443647656, 0.03906459674546261, 0.04687751609455513, 0.04687751609455513, 0.03906459674546261, 0.03906459674546261, 0.03515813707091635, 0.03125167739637009, 0.03125167739637009, 0.03906459674546261, 0.027345217721823828, 0.06872494389865864, 0.0730202528923248, 0.05583901691766015, 0.05583901691766015, 0.0859061798733233, 0.06013432591132631, 0.06013432591132631, 0.05154370792399398, 0.05154370792399398, 0.05154370792399398, 0.04724839893032782, 0.04724839893032782, 0.04724839893032782, 0.04295308993666165, 0.04295308993666165, 0.030067162955663154, 0.030067162955663154, 0.030067162955663154, 0.03865778094299548, 0.02577185396199699, 0.06674984418227413, 0.06257797892088199, 0.06257797892088199, 0.06674984418227413, 0.06674984418227413, 0.0750935747050584, 0.05006238313670559, 0.05423424839809773, 0.06674984418227413, 0.04589051787531346, 0.05006238313670559, 0.05006238313670559, 0.0375467873525292, 0.0375467873525292, 0.0375467873525292, 0.033374922091137064, 0.0375467873525292, 0.033374922091137064, 0.033374922091137064, 0.0375467873525292, 0.07295477163632508, 0.05149748586093535, 0.07724622879140303, 0.07724622879140303, 0.060080400171091244, 0.055788943016013295, 0.05149748586093535, 0.047206028705857404, 0.047206028705857404, 0.04291457155077946, 0.047206028705857404, 0.055788943016013295, 0.047206028705857404, 0.047206028705857404, 0.04291457155077946, 0.03862311439570151, 0.03433165724062357, 0.030040200085545622, 0.04291457155077946, 0.030040200085545622, 0.058563096546014895, 0.07529540984487629, 0.07111233152016094, 0.05438001822129954, 0.0669292531954456, 0.05438001822129954, 0.0669292531954456, 0.05019693989658419, 0.058563096546014895, 0.05019693989658419, 0.05019693989658419, 0.04601386157186885, 0.04601386157186885, 0.04601386157186885, 0.041830783247153495, 0.0334646265977228, 0.029281548273007448, 0.04601386157186885, 0.029281548273007448, 0.029281548273007448, 0.06559309820484296, 0.06969266684264565, 0.07379223548044833, 0.07379223548044833, 0.07379223548044833, 0.06559309820484296, 0.05329439229143491, 0.0573939609292376, 0.04919482365363222, 0.04509525501582954, 0.04509525501582954, 0.04509525501582954, 0.04509525501582954, 0.036896117740224166, 0.040995686378026856, 0.0286969804646188, 0.04509525501582954, 0.036896117740224166, 0.03279654910242148, 0.02459741182681611, 0.0711656459228464, 0.06325835193141903, 0.06721199892713271, 0.0711656459228464, 0.06325835193141903, 0.05930470493570533, 0.05930470493570533, 0.05139741094427796, 0.05139741094427796, 0.04349011695285058, 0.05535105793999164, 0.05139741094427796, 0.04744376394856427, 0.03953646995713689, 0.0355828229614232, 0.0355828229614232, 0.031629175965709515, 0.031629175965709515, 0.0355828229614232, 0.03953646995713689, 0.07323429890062205, 0.06916572673947638, 0.07730287106176773, 0.06102858241718504, 0.06916572673947638, 0.06916572673947638, 0.05696001025603937, 0.04882286593374803, 0.044754293772602365, 0.06102858241718504, 0.04068572161145669, 0.04882286593374803, 0.044754293772602365, 0.04068572161145669, 0.04068572161145669, 0.032548577289165354, 0.032548577289165354, 0.032548577289165354, 0.032548577289165354, 0.032548577289165354, 0.07278435104843266, 0.06469720093194013, 0.06065362587369388, 0.06065362587369388, 0.06065362587369388, 0.06065362587369388, 0.07278435104843266, 0.0485229006989551, 0.06065362587369388, 0.05661005081544762, 0.052566475757201366, 0.0485229006989551, 0.04447932564070885, 0.032348600465970066, 0.03639217552421633, 0.032348600465970066, 0.040435750582462585, 0.032348600465970066, 0.02830502540772381, 0.02830502540772381, 0.07130619975464593, 0.05872275273912018, 0.06291723507762877, 0.07550068209315451, 0.07550068209315451, 0.05872275273912018, 0.06291723507762877, 0.054528270400611596, 0.05872275273912018, 0.06291723507762877, 0.041944823385085844, 0.046139305723594426, 0.041944823385085844, 0.037750341046577256, 0.037750341046577256, 0.02936137636956009, 0.02936137636956009, 0.033555858708068674, 0.02936137636956009, 0.025166894031051507, 0.05776215025287865, 0.0701397538784955, 0.07426562175370112, 0.07426562175370112, 0.0701397538784955, 0.05776215025287865, 0.05363628237767303, 0.06188801812808427, 0.04125867875205618, 0.04951041450246742, 0.06188801812808427, 0.045384546627261796, 0.03713281087685056, 0.03713281087685056, 0.03713281087685056, 0.03713281087685056, 0.033006943001644945, 0.033006943001644945, 0.028881075126439323, 0.033006943001644945, 0.0814949807438907, 0.06433814269254529, 0.0900733997695634, 0.07720577123105433, 0.055759723666872575, 0.055759723666872575, 0.06004893317970893, 0.05147051415403622, 0.04289209512836352, 0.04718130464119987, 0.05147051415403622, 0.05147051415403622, 0.03860288561552717, 0.03860288561552717, 0.04289209512836352, 0.034313676102690815, 0.034313676102690815, 0.034313676102690815, 0.02573525707701811, 0.030024466589854464, 0.06762068422376721, 0.07184697698775266, 0.06339439145978176, 0.06762068422376721, 0.06762068422376721, 0.06339439145978176, 0.06762068422376721, 0.0507155131678254, 0.0507155131678254, 0.04226292763985451, 0.06339439145978176, 0.04648922040383995, 0.04226292763985451, 0.04226292763985451, 0.04226292763985451, 0.03381034211188361, 0.03381034211188361, 0.03381034211188361, 0.0253577565839127, 0.0253577565839127, 0.06668192155634882, 0.06251430145907702, 0.07084954165362062, 0.06668192155634882, 0.07084954165362062, 0.05417906126453342, 0.058346681361805215, 0.058346681361805215, 0.04584382106998981, 0.05417906126453342, 0.04584382106998981, 0.05001144116726162, 0.041676200972718014, 0.05417906126453342, 0.03750858087544621, 0.03334096077817441, 0.03750858087544621, 0.03334096077817441, 0.03334096077817441, 0.029173340680902608, 0.07072036968122984, 0.06656034793527515, 0.07072036968122984, 0.062400326189320454, 0.058240304443365754, 0.06656034793527515, 0.058240304443365754, 0.045760239205501665, 0.04992026095145636, 0.045760239205501665, 0.062400326189320454, 0.05408028269741106, 0.041600217459546965, 0.041600217459546965, 0.041600217459546965, 0.03744019571359227, 0.03328017396763758, 0.03328017396763758, 0.03328017396763758, 0.029120152221682877, 0.06017731900011323, 0.06447569892869275, 0.07307245878585178, 0.06877407885727227, 0.06017731900011323, 0.08166921864301081, 0.06447569892869275, 0.0515805591429542, 0.0515805591429542, 0.04298379928579517, 0.047282179214374685, 0.03868541935721565, 0.047282179214374685, 0.04298379928579517, 0.03868541935721565, 0.03868541935721565, 0.034387039428636135, 0.034387039428636135, 0.034387039428636135, 0.030088659500056614, 0.07314451030260034, 0.06095375858550029, 0.06908092639690033, 0.06501734249120031, 0.06908092639690033, 0.06095375858550029, 0.05282659077410025, 0.05689017467980027, 0.04876300686840023, 0.04876300686840023, 0.04876300686840023, 0.044699422962700214, 0.05689017467980027, 0.032508671245600154, 0.03657225515130017, 0.032508671245600154, 0.032508671245600154, 0.03657225515130017, 0.03657225515130017, 0.032508671245600154, 0.07015468545217295, 0.053647700639896954, 0.061901193046034954, 0.07428143165524194, 0.07015468545217295, 0.06602793924910395, 0.061901193046034954, 0.04952095443682796, 0.04952095443682796, 0.04539420823375896, 0.04539420823375896, 0.04952095443682796, 0.04539420823375896, 0.053647700639896954, 0.03714071582762097, 0.03714071582762097, 0.03714071582762097, 0.028887223421482977, 0.028887223421482977, 0.03301396962455198, 0.05926188481511508, 0.06716346945713043, 0.0711142617781381, 0.07901584642015344, 0.0711142617781381, 0.06716346945713043, 0.055311092494107414, 0.04740950785209207, 0.043458715531084394, 0.04740950785209207, 0.03950792321007672, 0.05136030017309974, 0.04740950785209207, 0.03950792321007672, 0.04740950785209207, 0.03950792321007672, 0.043458715531084394, 0.027655546247053707, 0.027655546247053707, 0.027655546247053707, 0.06556403325667205, 0.06966178533521405, 0.08605279364938208, 0.06146628117813006, 0.06556403325667205, 0.06556403325667205, 0.05736852909958805, 0.04917302494250404, 0.04917302494250404, 0.06146628117813006, 0.04507527286396204, 0.04507527286396204, 0.036879768706878036, 0.04917302494250404, 0.036879768706878036, 0.036879768706878036, 0.032782016628336026, 0.036879768706878036, 0.02458651247125202, 0.028684264549794024, 0.0686381375069802, 0.0646006000065696, 0.060563062506159, 0.0686381375069802, 0.0686381375069802, 0.0524879875053378, 0.0646006000065696, 0.060563062506159, 0.0524879875053378, 0.056525525005748395, 0.044412912504516594, 0.044412912504516594, 0.040375375004106, 0.0524879875053378, 0.036337837503695396, 0.036337837503695396, 0.0323003000032848, 0.036337837503695396, 0.036337837503695396, 0.028262762502874197, 0.06745343163151621, 0.06348558271201526, 0.06745343163151621, 0.06745343163151621, 0.06745343163151621, 0.07142128055101717, 0.05951773379251431, 0.06348558271201526, 0.0515820359535124, 0.04364633811451049, 0.04761418703401144, 0.04364633811451049, 0.039678489195009534, 0.0515820359535124, 0.039678489195009534, 0.03174279135600763, 0.03174279135600763, 0.03174279135600763, 0.03174279135600763, 0.03174279135600763, 0.06331784809976974, 0.06331784809976974, 0.06727521360600536, 0.07123257911224096, 0.06331784809976974, 0.06331784809976974, 0.05936048259353413, 0.055403117087298524, 0.047488386074827305, 0.055403117087298524, 0.047488386074827305, 0.055403117087298524, 0.03957365506235609, 0.0435310205685917, 0.0435310205685917, 0.03561628955612048, 0.03561628955612048, 0.03957365506235609, 0.027701558543649262, 0.027701558543649262, 0.061236026109037105, 0.05715362436843463, 0.06531842784963958, 0.06940082959024206, 0.06940082959024206, 0.06940082959024206, 0.06531842784963958, 0.05715362436843463, 0.04898882088722969, 0.05715362436843463, 0.05307122262783216, 0.04082401740602474, 0.04490641914662721, 0.05307122262783216, 0.036741615665422264, 0.03265921392481979, 0.028576812184217315, 0.028576812184217315, 0.03265921392481979, 0.028576812184217315, 0.07820253093424688, 0.057622917530497704, 0.08643437629574656, 0.06173884021124754, 0.07408660825349705, 0.06585476289199738, 0.057622917530497704, 0.04939107216899803, 0.057622917530497704, 0.057622917530497704, 0.0452751494882482, 0.04115922680749836, 0.04115922680749836, 0.03704330412674853, 0.03704330412674853, 0.03292738144599869, 0.028811458765248852, 0.03704330412674853, 0.03292738144599869, 0.024695536084499016, 0.0719219227924495, 0.06346052011098485, 0.0719219227924495, 0.0719219227924495, 0.06769122145171717, 0.06346052011098485, 0.046537714748055556, 0.05076841608878788, 0.054999117429520204, 0.05076841608878788, 0.042307013407323235, 0.059229818770252525, 0.042307013407323235, 0.03807631206659091, 0.042307013407323235, 0.03384561072585859, 0.03807631206659091, 0.029614909385126263, 0.029614909385126263, 0.029614909385126263, 0.060110097636554775, 0.060110097636554775, 0.07213211716386574, 0.06812477732142874, 0.0801467968487397, 0.07613945700630272, 0.05610275779411779, 0.060110097636554775, 0.05610275779411779, 0.044080738266806835, 0.04808807810924382, 0.04007339842436985, 0.044080738266806835, 0.03205871873949588, 0.044080738266806835, 0.028051378897058894, 0.03606605858193287, 0.03606605858193287, 0.028051378897058894, 0.028051378897058894, 0.07018903052357126, 0.0660602640221847, 0.07018903052357126, 0.05780273101941162, 0.09083286303050397, 0.0660602640221847, 0.05367396451802508, 0.05367396451802508, 0.041287665013865445, 0.04541643151525199, 0.04541643151525199, 0.04954519801663853, 0.041287665013865445, 0.04541643151525199, 0.041287665013865445, 0.037158898512478895, 0.037158898512478895, 0.03303013201109235, 0.03303013201109235, 0.02890136550970581, 0.06378875095419556, 0.06378875095419556, 0.0595361675572492, 0.08930425133587379, 0.05528358416030282, 0.0595361675572492, 0.06378875095419556, 0.05528358416030282, 0.05528358416030282, 0.046778417366410086, 0.046778417366410086, 0.05528358416030282, 0.046778417366410086, 0.04252583396946371, 0.04252583396946371, 0.0297680837786246, 0.03402066717557097, 0.0297680837786246, 0.03402066717557097, 0.0297680837786246, 0.0759888814156743, 0.06332406784639524, 0.0759888814156743, 0.06332406784639524, 0.059102463323302226, 0.06754567236948826, 0.06332406784639524, 0.054880858800209216, 0.04643764975402318, 0.06332406784639524, 0.042216045230930165, 0.0506592542771162, 0.03377283618474413, 0.042216045230930165, 0.042216045230930165, 0.03799444070783715, 0.0253296271385581, 0.029551231661651113, 0.03799444070783715, 0.0253296271385581, 0.0642420849882809, 0.0685248906541663, 0.0642420849882809, 0.0728076963200517, 0.0642420849882809, 0.0642420849882809, 0.0728076963200517, 0.051393667990624725, 0.04282805665885394, 0.05567647365651012, 0.051393667990624725, 0.04282805665885394, 0.03426244532708315, 0.038545250992968544, 0.04282805665885394, 0.03426244532708315, 0.03426244532708315, 0.03426244532708315, 0.03426244532708315, 0.029979639661197755, 0.06229848245903364, 0.0706049467869048, 0.07475817895084037, 0.06645171462296921, 0.058145250295098067, 0.06645171462296921, 0.06645171462296921, 0.04568555380329133, 0.04983878596722691, 0.053992018131162486, 0.04983878596722691, 0.04568555380329133, 0.033225857311484606, 0.053992018131162486, 0.04153232163935576, 0.029072625147549033, 0.033225857311484606, 0.03737908947542019, 0.033225857311484606, 0.029072625147549033, 0.0630503975324158, 0.06725375736791019, 0.07145711720340457, 0.06725375736791019, 0.058847037696921414, 0.058847037696921414, 0.05464367786142703, 0.05044031802593264, 0.05044031802593264, 0.04623695819043826, 0.05464367786142703, 0.042033598354943864, 0.05044031802593264, 0.042033598354943864, 0.04623695819043826, 0.03362687868395509, 0.03783023851944948, 0.02522015901296632, 0.03783023851944948, 0.03783023851944948, 0.06143867743254203, 0.06143867743254203, 0.06582715439200931, 0.06582715439200931, 0.07899258527041118, 0.06582715439200931, 0.06143867743254203, 0.05705020047307474, 0.04827324655414016, 0.03949629263520559, 0.052661723513607456, 0.03949629263520559, 0.03949629263520559, 0.04388476959467288, 0.04388476959467288, 0.0351078156757383, 0.0351078156757383, 0.0351078156757383, 0.04388476959467288, 0.030719338716271014, 0.07159189707745317, 0.07580318514083277, 0.07159189707745317, 0.06316932095069397, 0.06316932095069397, 0.058958032887314375, 0.054746744823934775, 0.05053545676055518, 0.04211288063379598, 0.04632416869717558, 0.05053545676055518, 0.058958032887314375, 0.04211288063379598, 0.037901592570416386, 0.04211288063379598, 0.033690304507036785, 0.037901592570416386, 0.037901592570416386, 0.029479016443657188, 0.02526772838027759, 0.07668310556893136, 0.06861119971957017, 0.06457524679488957, 0.06457524679488957, 0.06861119971957017, 0.08071905849361197, 0.05650334094552837, 0.05650334094552837, 0.04439548217148658, 0.04843143509616718, 0.04439548217148658, 0.04439548217148658, 0.04439548217148658, 0.040359529246805984, 0.040359529246805984, 0.03228762339744479, 0.036323576322125385, 0.03228762339744479, 0.028251670472764185, 0.028251670472764185, 0.06670286930036344, 0.06670286930036344, 0.058365010637818016, 0.06253393996909073, 0.07087179863163616, 0.0541960813065453, 0.050027151975272584, 0.07504072796290888, 0.04585822264399987, 0.0541960813065453, 0.058365010637818016, 0.0541960813065453, 0.04168929331272715, 0.04168929331272715, 0.04168929331272715, 0.03335143465018172, 0.029182505318909008, 0.03335143465018172, 0.029182505318909008, 0.029182505318909008, 0.09066453820772769, 0.0659378459692565, 0.0659378459692565, 0.053574499850020905, 0.06181673059617797, 0.06181673059617797, 0.05769561522309944, 0.04945338447694238, 0.041211153730785315, 0.04533226910386384, 0.053574499850020905, 0.053574499850020905, 0.04533226910386384, 0.04533226910386384, 0.03709003835770678, 0.03296892298462825, 0.03296892298462825, 0.03709003835770678, 0.03709003835770678, 0.02472669223847119, 0.06718744579628586, 0.06298823043401798, 0.06298823043401798, 0.06718744579628586, 0.07978509188308945, 0.07138666115855372, 0.07558587652082158, 0.05458979970948225, 0.05039058434721439, 0.046191368984946524, 0.041992153622678656, 0.041992153622678656, 0.046191368984946524, 0.03359372289814293, 0.041992153622678656, 0.03359372289814293, 0.03359372289814293, 0.02939450753587506, 0.03359372289814293, 0.025195292173607196, 0.06858709796835898, 0.060518027619140276, 0.06455256279374963, 0.08069070349218704, 0.06858709796835898, 0.060518027619140276, 0.05648349244453093, 0.05648349244453093, 0.060518027619140276, 0.052448957269921576, 0.04841442209531222, 0.04841442209531222, 0.04034535174609352, 0.04034535174609352, 0.03631081657148417, 0.032276281396874815, 0.032276281396874815, 0.032276281396874815, 0.028241746222265465, 0.028241746222265465, 0.06233492897503975, 0.0748019147700477, 0.06649059090670907, 0.06649059090670907, 0.06649059090670907, 0.07064625283837839, 0.05817926704337043, 0.045712281248362484, 0.0498679431800318, 0.045712281248362484, 0.0498679431800318, 0.045712281248362484, 0.04155661931669317, 0.04155661931669317, 0.045712281248362484, 0.033245295453354536, 0.033245295453354536, 0.045712281248362484, 0.029089633521685216, 0.0249339715900159, 0.07721234398484073, 0.06502092125039219, 0.06502092125039219, 0.07721234398484073, 0.0690847288285417, 0.06095711367224268, 0.0690847288285417, 0.04876569093779414, 0.04063807578149512, 0.05282949851594365, 0.04876569093779414, 0.04470188335964463, 0.04876569093779414, 0.05282949851594365, 0.04063807578149512, 0.028446653047046584, 0.028446653047046584, 0.032510460625196094, 0.02438284546889707, 0.028446653047046584, 0.06762203988542782, 0.06762203988542782, 0.06339566239258858, 0.06762203988542782, 0.06762203988542782, 0.05916928489974934, 0.050716529914070864, 0.0549429074069101, 0.06762203988542782, 0.050716529914070864, 0.0549429074069101, 0.04226377492839239, 0.03803739743555315, 0.03381101994271391, 0.04226377492839239, 0.03381101994271391, 0.03803739743555315, 0.02958464244987467, 0.03381101994271391, 0.03381101994271391, 0.07169616046474477, 0.07169616046474477, 0.06372992041310645, 0.0597468003872873, 0.0677130404389256, 0.06372992041310645, 0.05576368036146814, 0.05576368036146814, 0.05576368036146814, 0.05178056033564899, 0.04381432028401069, 0.04779744030982984, 0.04779744030982984, 0.04381432028401069, 0.039831200258191535, 0.031864960206553224, 0.031864960206553224, 0.04381432028401069, 0.031864960206553224, 0.02788184018073407, 0.07058652684937786, 0.06643437821117916, 0.07889082412577525, 0.06643437821117916, 0.07058652684937786, 0.062282229572980456, 0.05397793229658306, 0.05397793229658306, 0.049825783658384366, 0.062282229572980456, 0.04567363502018567, 0.04567363502018567, 0.037369337743788276, 0.037369337743788276, 0.04152148638198697, 0.03321718910558958, 0.03321718910558958, 0.03321718910558958, 0.03321718910558958, 0.03321718910558958, 0.06942089007127596, 0.07327760618634685, 0.07327760618634685, 0.06942089007127596, 0.08099103841648862, 0.050137309495921524, 0.0578507417260633, 0.05399402561099241, 0.04628059338085064, 0.050137309495921524, 0.050137309495921524, 0.050137309495921524, 0.04242387726577975, 0.04628059338085064, 0.03471044503563798, 0.026997012805496205, 0.03471044503563798, 0.026997012805496205, 0.03085372892056709, 0.02314029669042532, 0.07229311137449601, 0.05953550348487907, 0.05528296752167342, 0.06804057541129037, 0.07229311137449601, 0.06378803944808471, 0.06378803944808471, 0.05103043155846777, 0.05103043155846777, 0.046777895595262124, 0.05103043155846777, 0.04252535963205648, 0.04252535963205648, 0.04252535963205648, 0.04252535963205648, 0.034020287705645184, 0.029767751742439535, 0.034020287705645184, 0.04252535963205648, 0.034020287705645184, 0.061557996600406825, 0.0738695959204882, 0.05335026372035258, 0.061557996600406825, 0.06976572948046107, 0.0574541301603797, 0.0574541301603797, 0.061557996600406825, 0.061557996600406825, 0.04924639728032546, 0.04924639728032546, 0.04924639728032546, 0.04514253084029834, 0.04924639728032546, 0.041038664400271214, 0.032830931520216974, 0.032830931520216974, 0.032830931520216974, 0.02872706508018985, 0.02872706508018985, 0.07061679124152835, 0.05400107565528638, 0.06646286234496786, 0.07061679124152835, 0.062308933448407364, 0.062308933448407364, 0.062308933448407364, 0.04984714675872589, 0.04153928896560491, 0.05400107565528638, 0.05400107565528638, 0.05400107565528638, 0.04153928896560491, 0.04153928896560491, 0.04153928896560491, 0.037385360069044415, 0.03323143117248393, 0.037385360069044415, 0.03323143117248393, 0.037385360069044415, 0.06545817550089678, 0.07364044743850888, 0.06954931146970283, 0.06136703953209073, 0.08591385534492703, 0.06136703953209073, 0.057275903563284686, 0.057275903563284686, 0.04909363162567259, 0.04500249565686654, 0.04500249565686654, 0.04909363162567259, 0.040911359688060485, 0.040911359688060485, 0.040911359688060485, 0.03682022371925444, 0.03682022371925444, 0.03272908775044839, 0.028637951781642343, 0.028637951781642343, 0.0679537229179577, 0.08069504596507478, 0.07644793828270242, 0.059459507553213, 0.07220083060033007, 0.06370661523558535, 0.050965292188468285, 0.050965292188468285, 0.046718184506095925, 0.050965292188468285, 0.059459507553213, 0.03822396914135121, 0.03822396914135121, 0.03822396914135121, 0.042471076823723565, 0.03822396914135121, 0.03397686145897885, 0.0297297537766065, 0.0297297537766065, 0.0297297537766065, 0.06794494898305903, 0.06794494898305903, 0.05945183036017665, 0.06794494898305903, 0.06794494898305903, 0.05945183036017665, 0.05945183036017665, 0.05095871173729427, 0.04246559311441189, 0.05095871173729427, 0.05095871173729427, 0.04671215242585308, 0.04246559311441189, 0.04246559311441189, 0.0382190338029707, 0.04246559311441189, 0.0382190338029707, 0.0382190338029707, 0.033972474491529514, 0.029725915180088325, 0.07531314815199693, 0.06694502057955283, 0.07112908436577488, 0.07112908436577488, 0.06694502057955283, 0.05857689300710873, 0.054392829220886674, 0.04184063786222052, 0.04602470164844257, 0.06276095679333078, 0.05020876543466462, 0.04602470164844257, 0.04184063786222052, 0.033472510289776415, 0.04602470164844257, 0.03765657407599846, 0.03765657407599846, 0.033472510289776415, 0.03765657407599846, 0.02510438271733231, 0.06967932187384104, 0.06148175459456563, 0.06148175459456563, 0.06148175459456563, 0.06967932187384104, 0.06558053823420334, 0.06558053823420334, 0.0491854036756525, 0.045086620036014796, 0.0491854036756525, 0.0491854036756525, 0.045086620036014796, 0.045086620036014796, 0.040987836396377084, 0.045086620036014796, 0.03279026911710167, 0.045086620036014796, 0.02869148547746396, 0.03279026911710167, 0.03279026911710167, 0.07842401318756165, 0.0619136946217592, 0.06604127426320981, 0.0619136946217592, 0.07429643354611104, 0.057786114980308584, 0.057786114980308584, 0.049530955697407356, 0.049530955697407356, 0.049530955697407356, 0.04540337605595675, 0.0619136946217592, 0.041275796414506134, 0.03714821677305552, 0.041275796414506134, 0.028893057490154292, 0.028893057490154292, 0.03714821677305552, 0.03714821677305552, 0.028893057490154292, 0.06718226368770905, 0.06298337220722723, 0.07138115516819087, 0.07138115516819087, 0.054585589246263604, 0.050386697765781786, 0.058784480726745414, 0.054585589246263604, 0.04618780628529997, 0.054585589246263604, 0.06298337220722723, 0.04618780628529997, 0.050386697765781786, 0.04198891480481815, 0.04618780628529997, 0.033591131843854524, 0.03779002332433634, 0.029392240363372707, 0.033591131843854524, 0.029392240363372707, 0.06360551823691324, 0.08056698976675677, 0.072086254001835, 0.06360551823691324, 0.06360551823691324, 0.06360551823691324, 0.05512478247199148, 0.050884414589530594, 0.04664404670706971, 0.05512478247199148, 0.042403678824608826, 0.042403678824608826, 0.042403678824608826, 0.042403678824608826, 0.042403678824608826, 0.033922943059687065, 0.03816331094214794, 0.042403678824608826, 0.02968257517722618, 0.02968257517722618, 0.06857301493297786, 0.06857301493297786, 0.08470784197603148, 0.06453930817221447, 0.06453930817221447, 0.05647189465068766, 0.06050560141145106, 0.052438187889924257, 0.052438187889924257, 0.044370774368397446, 0.052438187889924257, 0.044370774368397446, 0.044370774368397446, 0.040337067607634045, 0.036303360846870636, 0.036303360846870636, 0.032269654086107234, 0.036303360846870636, 0.032269654086107234, 0.02823594732534383, 0.06431924031284617, 0.06431924031284617, 0.07289513902122564, 0.06003129095865642, 0.06003129095865642, 0.06431924031284617, 0.06431924031284617, 0.06860718966703591, 0.05145539225027693, 0.04716744289608719, 0.05574334160446667, 0.04716744289608719, 0.038591544187707695, 0.04287949354189744, 0.038591544187707695, 0.03001564547932821, 0.038591544187707695, 0.034303594833517956, 0.034303594833517956, 0.025727696125138463, 0.06037140186654369, 0.07647044236428867, 0.07244568223985243, 0.08451996261316116, 0.05634664174210744, 0.06037140186654369, 0.06037140186654369, 0.04024760124436246, 0.0523218816176712, 0.04829712149323495, 0.05634664174210744, 0.04829712149323495, 0.04024760124436246, 0.0442723613687987, 0.036222841119926213, 0.032198080995489965, 0.032198080995489965, 0.036222841119926213, 0.02817332087105372, 0.02817332087105372, 0.07066585593725577, 0.07459173682265888, 0.06673997505185268, 0.07066585593725577, 0.07459173682265888, 0.05888821328104648, 0.05496233239564338, 0.047110570624837186, 0.06281409416644958, 0.051036451510240284, 0.051036451510240284, 0.04318468973943408, 0.047110570624837186, 0.047110570624837186, 0.039258808854030984, 0.03140704708322479, 0.03140704708322479, 0.03140704708322479, 0.02748116619782169, 0.023555285312418593, 0.06953670992588623, 0.07362710462740894, 0.061355920522840786, 0.0654463152243635, 0.08589828873197711, 0.05317513111979535, 0.05726552582131807, 0.061355920522840786, 0.05317513111979535, 0.04908473641827263, 0.03681355231370447, 0.04499434171674991, 0.03681355231370447, 0.04090394701522719, 0.04090394701522719, 0.04090394701522719, 0.03681355231370447, 0.028632762910659033, 0.028632762910659033, 0.03272315761218175, 0.06668428209792991, 0.06668428209792991, 0.0625165144668093, 0.06668428209792991, 0.08752312025353301, 0.0625165144668093, 0.054180979204568054, 0.0625165144668093, 0.0416776763112062, 0.04584544394232681, 0.04584544394232681, 0.0416776763112062, 0.04584544394232681, 0.0416776763112062, 0.033342141048964956, 0.03750990868008558, 0.0416776763112062, 0.033342141048964956, 0.033342141048964956, 0.02917437341784434, 0.06764164823107258, 0.0591864422021885, 0.06764164823107258, 0.08877966330328275, 0.06341404521663054, 0.0718692512455146, 0.05495883918774647, 0.05495883918774647, 0.04650363315886239, 0.04650363315886239, 0.05495883918774647, 0.04650363315886239, 0.04227603014442036, 0.04227603014442036, 0.04227603014442036, 0.03804842712997832, 0.02959322110109425, 0.03382082411553629, 0.02959322110109425, 0.02959322110109425, 0.07791869948885215, 0.06151476275435696, 0.06151476275435696, 0.06971673112160455, 0.06971673112160455, 0.05331279438710937, 0.05331279438710937, 0.05331279438710937, 0.04921181020348557, 0.04511082601986177, 0.04511082601986177, 0.04100984183623797, 0.04921181020348557, 0.04511082601986177, 0.04511082601986177, 0.04511082601986177, 0.036908857652614174, 0.036908857652614174, 0.02870688928536658, 0.02870688928536658, 0.06636802935482501, 0.0624640276280706, 0.06636802935482501, 0.07417603280833383, 0.0624640276280706, 0.0624640276280706, 0.07027203108157942, 0.05075202244780736, 0.05075202244780736, 0.04684802072105295, 0.05465602417456178, 0.05075202244780736, 0.04684802072105295, 0.03904001726754413, 0.03904001726754413, 0.0312320138140353, 0.0312320138140353, 0.03904001726754413, 0.03513601554078971, 0.023424010360526475, 0.06003692155684772, 0.06003692155684772, 0.06861362463639739, 0.06432527309662256, 0.07290197617617224, 0.06432527309662256, 0.06003692155684772, 0.05146021847729804, 0.05146021847729804, 0.04717186693752321, 0.04717186693752321, 0.05574857001707288, 0.04288351539774837, 0.04288351539774837, 0.038595163857973534, 0.034306812318198696, 0.04288351539774837, 0.034306812318198696, 0.038595163857973534, 0.02573010923864902, 0.07446474283178628, 0.06619088251714336, 0.05791702220250044, 0.04964316188785752, 0.07032781267446482, 0.062053952359821904, 0.06619088251714336, 0.05791702220250044, 0.04550623173053606, 0.04964316188785752, 0.04550623173053606, 0.04550623173053606, 0.04964316188785752, 0.05378009204517898, 0.0413693015732146, 0.0413693015732146, 0.03309544125857168, 0.02895851110125022, 0.02895851110125022, 0.02482158094392876, 0.07843481264342018, 0.07017851657569175, 0.06192222050796331, 0.06605036854182753, 0.06605036854182753, 0.06605036854182753, 0.07430666460955597, 0.04540962837250643, 0.04540962837250643, 0.04128148033864221, 0.04128148033864221, 0.053665924440234866, 0.04128148033864221, 0.04128148033864221, 0.04540962837250643, 0.037153332304777985, 0.037153332304777985, 0.03302518427091376, 0.028897036237049544, 0.028897036237049544, 0.07122662947685232, 0.06703682774291983, 0.06284702600898734, 0.06703682774291983, 0.054467422541122364, 0.06284702600898734, 0.05027762080718987, 0.05027762080718987, 0.05865722427505485, 0.05027762080718987, 0.05027762080718987, 0.05027762080718987, 0.04189801733932489, 0.04189801733932489, 0.04608781907325738, 0.04608781907325738, 0.033518413871459915, 0.033518413871459915, 0.033518413871459915, 0.029328612137527424, 0.07640413075686553, 0.06836159067719547, 0.06836159067719547, 0.06434032063736045, 0.08444667083653558, 0.06031905059752542, 0.06434032063736045, 0.05629778055769039, 0.05227651051785536, 0.04021270039835028, 0.044233970438185305, 0.044233970438185305, 0.03619143035851525, 0.048255240478020336, 0.04021270039835028, 0.032170160318680224, 0.032170160318680224, 0.028148890278845196, 0.028148890278845196, 0.028148890278845196, 0.07384266236104532, 0.06995620644730609, 0.058296838706088414, 0.07384266236104532, 0.06606975053356687, 0.06606975053356687, 0.058296838706088414, 0.05052392687860995, 0.046637470964870725, 0.03886455913739227, 0.05052392687860995, 0.05052392687860995, 0.058296838706088414, 0.034978103223653044, 0.034978103223653044, 0.03886455913739227, 0.03886455913739227, 0.034978103223653044, 0.03109164730991382, 0.027205191396174593, 0.06616765641117953, 0.06203217788548081, 0.07030313493687824, 0.06616765641117953, 0.09098052756537185, 0.06616765641117953, 0.06203217788548081, 0.05376122083408336, 0.05376122083408336, 0.049625742308384645, 0.0413547852569872, 0.045490263782685926, 0.0413547852569872, 0.0413547852569872, 0.03308382820558976, 0.03721930673128848, 0.02894834967989104, 0.02894834967989104, 0.02894834967989104, 0.03308382820558976, 0.06519627242430746, 0.06519627242430746, 0.06112150539778824, 0.06519627242430746, 0.05704673837126902, 0.06927103945082667, 0.048897204318230594, 0.05704673837126902, 0.048897204318230594, 0.06519627242430746, 0.05704673837126902, 0.048897204318230594, 0.04074767026519216, 0.04074767026519216, 0.03667290323867294, 0.03667290323867294, 0.03667290323867294, 0.03259813621215373, 0.03259813621215373, 0.03259813621215373, 0.07434408562433541, 0.07434408562433541, 0.07021385864520566, 0.07434408562433541, 0.05782317770781643, 0.05782317770781643, 0.05782317770781643, 0.05782317770781643, 0.04543249677042719, 0.04543249677042719, 0.04543249677042719, 0.04130226979129745, 0.037172042812167704, 0.037172042812167704, 0.04130226979129745, 0.03304181583303796, 0.03304181583303796, 0.03304181583303796, 0.04130226979129745, 0.03304181583303796, 0.06274273711694428, 0.06274273711694428, 0.06692558625807389, 0.06274273711694428, 0.06692558625807389, 0.06692558625807389, 0.058559887975814656, 0.05019418969355542, 0.046011340552425804, 0.05019418969355542, 0.04182849141129619, 0.05019418969355542, 0.046011340552425804, 0.046011340552425804, 0.04182849141129619, 0.029279943987907328, 0.03764564227016657, 0.033462793129036945, 0.033462793129036945, 0.04182849141129619, 0.06623880645126873, 0.06234240607178233, 0.06623880645126873, 0.07013520683075512, 0.05844600569229594, 0.08182440796921431, 0.06623880645126873, 0.046756804553836746, 0.046756804553836746, 0.046756804553836746, 0.05065320493332314, 0.05065320493332314, 0.046756804553836746, 0.046756804553836746, 0.038964003794863956, 0.031171203035891167, 0.031171203035891167, 0.031171203035891167, 0.031171203035891167, 0.031171203035891167, 0.0695759895045419, 0.06139057897459579, 0.0695759895045419, 0.06139057897459579, 0.06548328423956884, 0.0695759895045419, 0.057297873709622735, 0.05320516844464968, 0.05320516844464968, 0.04911246317967663, 0.057297873709622735, 0.04501975791470358, 0.040927052649730525, 0.03683434738475747, 0.05320516844464968, 0.028648936854811367, 0.028648936854811367, 0.040927052649730525, 0.03274164211978442, 0.03274164211978442, 0.06031011356497029, 0.06461797881961102, 0.06461797881961102, 0.06461797881961102, 0.0861573050928147, 0.06892584407425176, 0.05169438305568882, 0.06461797881961102, 0.04307865254640735, 0.05169438305568882, 0.047386517801048085, 0.047386517801048085, 0.03877078729176661, 0.047386517801048085, 0.03877078729176661, 0.03446292203712588, 0.03446292203712588, 0.03446292203712588, 0.030155056782485146, 0.030155056782485146, 0.07125143283255216, 0.07125143283255216, 0.059376194027126794, 0.07125143283255216, 0.07125143283255216, 0.06333460696226857, 0.06333460696226857, 0.05541778109198501, 0.04750095522170143, 0.03958412935141786, 0.05145936815684322, 0.059376194027126794, 0.03958412935141786, 0.04354254228655965, 0.04750095522170143, 0.027708890545992505, 0.027708890545992505, 0.027708890545992505, 0.031667303481134286, 0.023750477610850716, 0.07609779173701012, 0.06408235093642958, 0.07209264480348328, 0.07609779173701012, 0.06808749786995642, 0.06808749786995642, 0.06007720400290273, 0.06007720400290273, 0.04806176320232218, 0.05206691013584903, 0.05206691013584903, 0.03604632240174164, 0.040051469335268486, 0.040051469335268486, 0.03204117546821479, 0.03204117546821479, 0.02803602853468794, 0.03604632240174164, 0.02803602853468794, 0.03604632240174164, 0.07214523971931393, 0.07615330859260915, 0.07214523971931393, 0.06813717084601871, 0.07615330859260915, 0.06813717084601871, 0.052104895352837834, 0.04008068873295218, 0.048096826479542616, 0.04008068873295218, 0.06412910197272349, 0.048096826479542616, 0.0440887576062474, 0.036072619859656964, 0.04008068873295218, 0.032064550986361746, 0.036072619859656964, 0.028056482113066526, 0.028056482113066526, 0.028056482113066526, 0.07706207988311446, 0.06895028200068136, 0.06895028200068136, 0.06895028200068136, 0.06895028200068136, 0.06083848411824826, 0.05678258517703171, 0.04867078729459861, 0.05678258517703171, 0.06083848411824826, 0.04461488835338206, 0.04461488835338206, 0.04055898941216551, 0.032447191529732404, 0.04055898941216551, 0.036503090470948955, 0.028391292588515856, 0.032447191529732404, 0.024335393647299305, 0.028391292588515856, 0.07019285576622639, 0.066063864250566, 0.07019285576622639, 0.07019285576622639, 0.07019285576622639, 0.06193487273490563, 0.04541890667226413, 0.04541890667226413, 0.0495478981879245, 0.053676889703584876, 0.0495478981879245, 0.0495478981879245, 0.041289915156603754, 0.03716092364094338, 0.0495478981879245, 0.033031932125283, 0.033031932125283, 0.03716092364094338, 0.033031932125283, 0.02890294060962263, 0.07543220396918769, 0.05447881397774666, 0.06705084797261128, 0.05447881397774666, 0.06286016997432307, 0.07124152597089949, 0.050288135979458456, 0.05866949197603487, 0.06705084797261128, 0.05447881397774666, 0.050288135979458456, 0.04609745798117025, 0.037716101984593846, 0.03352542398630564, 0.04190677998288205, 0.03352542398630564, 0.037716101984593846, 0.037716101984593846, 0.029334745988017434, 0.029334745988017434, 0.06750894109526985, 0.07172824991372422, 0.06328963227681549, 0.05907032345836112, 0.07172824991372422, 0.05907032345836112, 0.07594755873217858, 0.050631705821452384, 0.050631705821452384, 0.050631705821452384, 0.042193088184543655, 0.050631705821452384, 0.050631705821452384, 0.042193088184543655, 0.033754470547634925, 0.02953516172918056, 0.033754470547634925, 0.033754470547634925, 0.033754470547634925, 0.02953516172918056, 0.06422703528982313, 0.07225541470105101, 0.06824122499543707, 0.05619865587859523, 0.060212845584209175, 0.060212845584209175, 0.060212845584209175, 0.06422703528982313, 0.048170276467367344, 0.05218446617298129, 0.048170276467367344, 0.05218446617298129, 0.05619865587859523, 0.04014189705613945, 0.04014189705613945, 0.03211351764491156, 0.03211351764491156, 0.036127707350525506, 0.024085138233683672, 0.028099327939297616, 0.07351643099140706, 0.06943218482521778, 0.06534793865902849, 0.0857691694899749, 0.061263692492839215, 0.053095200160460654, 0.061263692492839215, 0.053095200160460654, 0.053095200160460654, 0.04492670782808209, 0.049010953994271376, 0.04084246166189281, 0.03675821549570353, 0.04084246166189281, 0.03675821549570353, 0.032673969329514246, 0.03675821549570353, 0.03675821549570353, 0.02858972316332497, 0.03675821549570353, 0.07016181901951184, 0.05365315572080317, 0.07016181901951184, 0.08667048231822051, 0.07016181901951184, 0.05778032154548034, 0.061907487370157506, 0.05778032154548034, 0.049525989896126006, 0.04127165824677167, 0.04539882407144884, 0.049525989896126006, 0.04539882407144884, 0.04127165824677167, 0.037144492422094506, 0.03301732659741734, 0.037144492422094506, 0.02889016077274017, 0.03301732659741734, 0.02889016077274017, 0.06751744855485858, 0.059574219313110514, 0.08340390703835472, 0.07546067779660666, 0.07148906317573261, 0.06354583393398455, 0.059574219313110514, 0.051630990071362445, 0.039716146208740345, 0.047659375450488414, 0.039716146208740345, 0.051630990071362445, 0.043687760829614376, 0.039716146208740345, 0.047659375450488414, 0.03574453158786631, 0.031772916966992276, 0.03574453158786631, 0.023829687725244207, 0.031772916966992276, 0.08373310198134735, 0.079745811410807, 0.06778393969918596, 0.0637966491286456, 0.06778393969918596, 0.0637966491286456, 0.0717712302697263, 0.05183477741702455, 0.047847486846484204, 0.043860196275943855, 0.0398729057054035, 0.047847486846484204, 0.043860196275943855, 0.0398729057054035, 0.0398729057054035, 0.0318983245643228, 0.0318983245643228, 0.0318983245643228, 0.02791103399378245, 0.02791103399378245, 0.0714400090736151, 0.06723765559869657, 0.06723765559869657, 0.09245177644820778, 0.0714400090736151, 0.06303530212377803, 0.05463059517394096, 0.05042824169902243, 0.042023534749185354, 0.05463059517394096, 0.05042824169902243, 0.04622588822410389, 0.04622588822410389, 0.042023534749185354, 0.033618827799348286, 0.033618827799348286, 0.033618827799348286, 0.02941647432442975, 0.033618827799348286, 0.02941647432442975, 0.06776595013245003, 0.07200132201572815, 0.0635305782491719, 0.0635305782491719, 0.0635305782491719, 0.06776595013245003, 0.07623669389900628, 0.05082446259933752, 0.055059834482615644, 0.05082446259933752, 0.04658909071605939, 0.042353718832781266, 0.03811834694950314, 0.03811834694950314, 0.042353718832781266, 0.033882975066225014, 0.042353718832781266, 0.033882975066225014, 0.029647603182946885, 0.029647603182946885, 0.08336030266188613, 0.05418419673022598, 0.0666882421295089, 0.0666882421295089, 0.06252022699641459, 0.05835221186332029, 0.05835221186332029, 0.07085625726260321, 0.05001618159713168, 0.04584816646403737, 0.05001618159713168, 0.04584816646403737, 0.04584816646403737, 0.03751213619784876, 0.03751213619784876, 0.03751213619784876, 0.03334412106475445, 0.029176105931660146, 0.03334412106475445, 0.029176105931660146, 0.08134066440308065, 0.06507253152246452, 0.06100549830231049, 0.06913956474261855, 0.06507253152246452, 0.07320659796277258, 0.052871431862002424, 0.06100549830231049, 0.04473736542169436, 0.04067033220154032, 0.06100549830231049, 0.04473736542169436, 0.03660329898138629, 0.04067033220154032, 0.04067033220154032, 0.028469232541078228, 0.03660329898138629, 0.03660329898138629, 0.028469232541078228, 0.028469232541078228, 0.07336300980580404, 0.056101125145614855, 0.06473206747570945, 0.06904753864075674, 0.06041659631066215, 0.056101125145614855, 0.056101125145614855, 0.056101125145614855, 0.04747018281552026, 0.05178565398056756, 0.04747018281552026, 0.04747018281552026, 0.05178565398056756, 0.04747018281552026, 0.03883924048542567, 0.02589282699028378, 0.03883924048542567, 0.03883924048542567, 0.03452376932037837, 0.03452376932037837, 0.06854193624912247, 0.06451005764623291, 0.07257381485201203, 0.06854193624912247, 0.06451005764623291, 0.07660569345490159, 0.0564463004404538, 0.0564463004404538, 0.04838254323467469, 0.04435066463178513, 0.05241442183756424, 0.04838254323467469, 0.032255028823116455, 0.04838254323467469, 0.036286907426006015, 0.036286907426006015, 0.032255028823116455, 0.032255028823116455, 0.040318786028895576, 0.024191271617337345, 0.09020996005099104, 0.06560724367344803, 0.06560724367344803, 0.07790860186221953, 0.061506790943857526, 0.05740633821426702, 0.05330588548467652, 0.04510498002549552, 0.04510498002549552, 0.04510498002549552, 0.04510498002549552, 0.049205432755086015, 0.04510498002549552, 0.04510498002549552, 0.03690407456631451, 0.041004527295905015, 0.032803621836724015, 0.032803621836724015, 0.03690407456631451, 0.02870316910713351, 0.06240059238915282, 0.07488071086698339, 0.066560631881763, 0.06240059238915282, 0.0707206713743732, 0.066560631881763, 0.05408051340393245, 0.04576043441871207, 0.06240059238915282, 0.05408051340393245, 0.04160039492610188, 0.049920473911322254, 0.04576043441871207, 0.04160039492610188, 0.04160039492610188, 0.037440355433491695, 0.037440355433491695, 0.0332803159408815, 0.029120276448271316, 0.0332803159408815, 0.07840959124663005, 0.06602912947084635, 0.06602912947084635, 0.06602912947084635, 0.06602912947084635, 0.05777548828699056, 0.06190230887891846, 0.045395026511206873, 0.053648667695062664, 0.049521847103134765, 0.045395026511206873, 0.045395026511206873, 0.041268205919278975, 0.041268205919278975, 0.041268205919278975, 0.02888774414349528, 0.03301456473542318, 0.037141385327351076, 0.041268205919278975, 0.03301456473542318, 0.07665853088823504, 0.060519892806501344, 0.07262387136780162, 0.09279716896996873, 0.05648523328606792, 0.060519892806501344, 0.05648523328606792, 0.0524505737656345, 0.05648523328606792, 0.0524505737656345, 0.04438125472476766, 0.04034659520433423, 0.04438125472476766, 0.03631193568390081, 0.03631193568390081, 0.04034659520433423, 0.032277276163467386, 0.032277276163467386, 0.02824261664303396, 0.02824261664303396, 0.07699465383145153, 0.06888995342814083, 0.07294230362979619, 0.06888995342814083, 0.06888995342814083, 0.06078525302483015, 0.05673290282317481, 0.04457585221820878, 0.052680552621519466, 0.04457585221820878, 0.04862820241986412, 0.052680552621519466, 0.04457585221820878, 0.04457585221820878, 0.036471151814898094, 0.032418801613242745, 0.028366451411587404, 0.040523502016553435, 0.028366451411587404, 0.028366451411587404, 0.07598939889234842, 0.06799051479841701, 0.0639910727514513, 0.07598939889234842, 0.06799051479841701, 0.06799051479841701, 0.05199274661055418, 0.05199274661055418, 0.047993304563588475, 0.047993304563588475, 0.043993862516622774, 0.043993862516622774, 0.039994420469657066, 0.03599497842269136, 0.03599497842269136, 0.03199553637572565, 0.043993862516622774, 0.03599497842269136, 0.03199553637572565, 0.03199553637572565, 0.06077652365257656, 0.07293182838309187, 0.06888006013958677, 0.06482829189608166, 0.07293182838309187, 0.06077652365257656, 0.06482829189608166, 0.05267298716556635, 0.05267298716556635, 0.05267298716556635, 0.044569450678556145, 0.05267298716556635, 0.04051768243505104, 0.036465914191545934, 0.036465914191545934, 0.03241414594804083, 0.02836237770453573, 0.044569450678556145, 0.02836237770453573, 0.02836237770453573, 0.07875246179434839, 0.0663178625636618, 0.05802812974320408, 0.07046272897389066, 0.07460759538411953, 0.06217299615343294, 0.07460759538411953, 0.04973839692274635, 0.05388326333297521, 0.04559353051251749, 0.04973839692274635, 0.04559353051251749, 0.04559353051251749, 0.03730379769205976, 0.0331589312818309, 0.02901406487160204, 0.0331589312818309, 0.0331589312818309, 0.0331589312818309, 0.02901406487160204, 0.06341994522104259, 0.06341994522104259, 0.07610393426525111, 0.06341994522104259, 0.059191948872973084, 0.054963952524903575, 0.06341994522104259, 0.0676479415691121, 0.046507959828764565, 0.06341994522104259, 0.054963952524903575, 0.04227996348069506, 0.038051967132625555, 0.04227996348069506, 0.038051967132625555, 0.03382397078455605, 0.025367978088417033, 0.03382397078455605, 0.038051967132625555, 0.029595974436486542, 0.058541961202657665, 0.07805594827021022, 0.06634755602967869, 0.062444758616168175, 0.0702503534431892, 0.07805594827021022, 0.054639163789147155, 0.058541961202657665, 0.050736366375636645, 0.046833568962126135, 0.050736366375636645, 0.042930771548615625, 0.042930771548615625, 0.042930771548615625, 0.03902797413510511, 0.0351251767215946, 0.031222379308084088, 0.031222379308084088, 0.0351251767215946, 0.027319581894573577, 0.07653869471077794, 0.08862375177037446, 0.06445363765118142, 0.0725103423575791, 0.06848199000438027, 0.052368580591584904, 0.06445363765118142, 0.060425285297982585, 0.03625517117878955, 0.04028352353198839, 0.052368580591584904, 0.04834022823838607, 0.03625517117878955, 0.03625517117878955, 0.03625517117878955, 0.03222681882559071, 0.028198466472391874, 0.03222681882559071, 0.03625517117878955, 0.03625517117878955, 0.06940328560768429, 0.08573347045655118, 0.06940328560768429, 0.06532073939546756, 0.06532073939546756, 0.06940328560768429, 0.053073100758817396, 0.053073100758817396, 0.04490800833438395, 0.053073100758817396, 0.04490800833438395, 0.04082546212216723, 0.04490800833438395, 0.04082546212216723, 0.04082546212216723, 0.03266036969773378, 0.03266036969773378, 0.03266036969773378, 0.03266036969773378, 0.02857782348551706, 0.0839997446878572, 0.06399980547646264, 0.08799973253013613, 0.059999817634183716, 0.059999817634183716, 0.06399980547646264, 0.06399980547646264, 0.05199984194962589, 0.04799985410734697, 0.03999987842278915, 0.03999987842278915, 0.04399986626506806, 0.03999987842278915, 0.03999987842278915, 0.03199990273823132, 0.03199990273823132, 0.03199990273823132, 0.03999987842278915, 0.03599989058051023, 0.03599989058051023, 0.08014139080970914, 0.07613432126922369, 0.060106043107281855, 0.06812018218825278, 0.06812018218825278, 0.0841484603501946, 0.060106043107281855, 0.0560989735667964, 0.05209190402631094, 0.048084834485825484, 0.04007069540485457, 0.04407776494534003, 0.03606362586436911, 0.04007069540485457, 0.03606362586436911, 0.03606362586436911, 0.0280494867833982, 0.0280494867833982, 0.032056556323883656, 0.024042417242912742, 0.0729839567165327, 0.06869078279203077, 0.06439760886752885, 0.060104434943026926, 0.06869078279203077, 0.060104434943026926, 0.055811261018525, 0.05151808709402308, 0.05151808709402308, 0.047224913169521154, 0.055811261018525, 0.047224913169521154, 0.047224913169521154, 0.03863856532051731, 0.04293173924501923, 0.03863856532051731, 0.03434539139601538, 0.030052217471513463, 0.030052217471513463, 0.03863856532051731, 0.07406796437262116, 0.06995307746303109, 0.06995307746303109, 0.061723303643850966, 0.0576084167342609, 0.05349352982467084, 0.0576084167342609, 0.04526375600549071, 0.04526375600549071, 0.061723303643850966, 0.041148869095900646, 0.05349352982467084, 0.04526375600549071, 0.04526375600549071, 0.03703398218631058, 0.03703398218631058, 0.03291909527672052, 0.041148869095900646, 0.03703398218631058, 0.03291909527672052, 0.07329065934559356, 0.07329065934559356, 0.06921895604861615, 0.07736236264257099, 0.07736236264257099, 0.07329065934559356, 0.052932142860706466, 0.057003846157683884, 0.052932142860706466, 0.04478873626675162, 0.040717032969774206, 0.040717032969774206, 0.04478873626675162, 0.03257362637581936, 0.03664532967279678, 0.03257362637581936, 0.03664532967279678, 0.040717032969774206, 0.024430219781864524, 0.024430219781864524, 0.0764930869177771, 0.07246713497473621, 0.0644152310886544, 0.05636332720257261, 0.06844118303169532, 0.06038927914561351, 0.06844118303169532, 0.05233737525953171, 0.06038927914561351, 0.048311423316490805, 0.04428547137344991, 0.04428547137344991, 0.04025951943040901, 0.04025951943040901, 0.048311423316490805, 0.036233567487368105, 0.0322076155443272, 0.0322076155443272, 0.0322076155443272, 0.028181663601286305, 0.06200669531861547, 0.08267559375815396, 0.06200669531861547, 0.06200669531861547, 0.06200669531861547, 0.06614047500652316, 0.053739135942800074, 0.049605356254892374, 0.045471576566984674, 0.049605356254892374, 0.06200669531861547, 0.045471576566984674, 0.04133779687907698, 0.045471576566984674, 0.03720401719116928, 0.03720401719116928, 0.03307023750326158, 0.03307023750326158, 0.028936457815353884, 0.028936457815353884, 0.06544161977685065, 0.06544161977685065, 0.06953172101290381, 0.061351518540797484, 0.08180202472106331, 0.05317131606869115, 0.049081214832637984, 0.05317131606869115, 0.057261417304744315, 0.049081214832637984, 0.04499111359658482, 0.049081214832637984, 0.03681091112447849, 0.04499111359658482, 0.049081214832637984, 0.03681091112447849, 0.028630708652372158, 0.03681091112447849, 0.028630708652372158, 0.028630708652372158, 0.0617448034832644, 0.06997744394769966, 0.07821008441213491, 0.07409376417991728, 0.07409376417991728, 0.06586112371548203, 0.05351216301882915, 0.05351216301882915, 0.045279522554393895, 0.04939584278661152, 0.04116320232217627, 0.045279522554393895, 0.03704688208995864, 0.04116320232217627, 0.04116320232217627, 0.04939584278661152, 0.032930561857741014, 0.032930561857741014, 0.028814241625523387, 0.028814241625523387, 0.07067204763414384, 0.05820050981635375, 0.07067204763414384, 0.07482922690674054, 0.062357689088950445, 0.07898640617933723, 0.05820050981635375, 0.054043330543757054, 0.04572897199856366, 0.04572897199856366, 0.04988615127116036, 0.04988615127116036, 0.04572897199856366, 0.04157179272596696, 0.03741461345337027, 0.03325743418077357, 0.03741461345337027, 0.029100254908176876, 0.03325743418077357, 0.02494307563558018, 0.061536309423800784, 0.07384357130856094, 0.06563873005205417, 0.08204841256506772, 0.06563873005205417, 0.057433888795547394, 0.061536309423800784, 0.04922904753904062, 0.04512662691078724, 0.04512662691078724, 0.057433888795547394, 0.04102420628253386, 0.04512662691078724, 0.04922904753904062, 0.04102420628253386, 0.028716944397773697, 0.04102420628253386, 0.032819365026027086, 0.028716944397773697, 0.02461452376952031, 0.06130817935568045, 0.06130817935568045, 0.0776570271838619, 0.06539539131272581, 0.06539539131272581, 0.06130817935568045, 0.0776570271838619, 0.049046543484544354, 0.049046543484544354, 0.044959331527498994, 0.049046543484544354, 0.044959331527498994, 0.044959331527498994, 0.044959331527498994, 0.04087211957045363, 0.03678490761340827, 0.03269769565636291, 0.03269769565636291, 0.03269769565636291, 0.024523271742272177, 0.07006695592659921, 0.07006695592659921, 0.07006695592659921, 0.06594537028385808, 0.05358061335563469, 0.05770219899837582, 0.04945902771289356, 0.06182378464111695, 0.05358061335563469, 0.04533744207015243, 0.04945902771289356, 0.05358061335563469, 0.0412158564274113, 0.0412158564274113, 0.04945902771289356, 0.03709427078467017, 0.03709427078467017, 0.02885109949918791, 0.03709427078467017, 0.02472951385644678, 0.07832867782320282, 0.07049581004088254, 0.07049581004088254, 0.06266294225856225, 0.07049581004088254, 0.07441224393204268, 0.05091364058508183, 0.05091364058508183, 0.03916433891160141, 0.054830074476241976, 0.05874650836740212, 0.04699720669392169, 0.04308077280276155, 0.04308077280276155, 0.03524790502044127, 0.03916433891160141, 0.03133147112928113, 0.03524790502044127, 0.027415037238120988, 0.027415037238120988, 0.07565050424909071, 0.05972408230191372, 0.05574247681511947, 0.07565050424909071, 0.07166889876229647, 0.06768729327550221, 0.05176087132832523, 0.05176087132832523, 0.04777926584153098, 0.04777926584153098, 0.04777926584153098, 0.05972408230191372, 0.05176087132832523, 0.03981605486794248, 0.04379766035473673, 0.027871238407559736, 0.031852843894353985, 0.035834449381148235, 0.031852843894353985, 0.02388963292076549, 0.06742775335198009, 0.07164198793647884, 0.06321351876748132, 0.0758562225209776, 0.06742775335198009, 0.06321351876748132, 0.05057081501398506, 0.05057081501398506, 0.04635658042948631, 0.04214234584498755, 0.06321351876748132, 0.04635658042948631, 0.029499642091491286, 0.04635658042948631, 0.04214234584498755, 0.033713876675990045, 0.033713876675990045, 0.029499642091491286, 0.0379281112604888, 0.029499642091491286, 0.05758973482417476, 0.06644969402789395, 0.07973963283347275, 0.07087967362975356, 0.06644969402789395, 0.06201971442603436, 0.05758973482417476, 0.05315975522231516, 0.06644969402789395, 0.048729775620455565, 0.048729775620455565, 0.04429979601859597, 0.048729775620455565, 0.03986981641673638, 0.03986981641673638, 0.03543983681487678, 0.03543983681487678, 0.02657987761115758, 0.03543983681487678, 0.022149898009297984, 0.07394824681057284, 0.06984001087665212, 0.06984001087665212, 0.06984001087665212, 0.06984001087665212, 0.053407067140969273, 0.0616235390088107, 0.04108235933920713, 0.04519059527312785, 0.04519059527312785, 0.053407067140969273, 0.0616235390088107, 0.04519059527312785, 0.04519059527312785, 0.03286588747136571, 0.03697412340528642, 0.028757651537444993, 0.03697412340528642, 0.03286588747136571, 0.02464941560352428, 0.08010913273824216, 0.06746032230588814, 0.06746032230588814, 0.06746032230588814, 0.06324405216177013, 0.06324405216177013, 0.07589286259412416, 0.0463789715852981, 0.0463789715852981, 0.0463789715852981, 0.0463789715852981, 0.0463789715852981, 0.03794643129706208, 0.04216270144118009, 0.04216270144118009, 0.029513891008826062, 0.03373016115294407, 0.03373016115294407, 0.03373016115294407, 0.029513891008826062, 0.06923220025491002, 0.0908672628345694, 0.056251162707114395, 0.06490518773897815, 0.06923220025491002, 0.06057817522304627, 0.056251162707114395, 0.051924150191182517, 0.051924150191182517, 0.04759713767525064, 0.04759713767525064, 0.051924150191182517, 0.043270125159318766, 0.043270125159318766, 0.043270125159318766, 0.030289087611523133, 0.03461610012745501, 0.03461610012745501, 0.030289087611523133, 0.030289087611523133, 0.06991280779345575, 0.07813784400445055, 0.06580028968795835, 0.06991280779345575, 0.06991280779345575, 0.061687771582460964, 0.05346273537146617, 0.05346273537146617, 0.04935021726596877, 0.04523769916047137, 0.037012662949476575, 0.041125181054973974, 0.041125181054973974, 0.041125181054973974, 0.041125181054973974, 0.03290014484397918, 0.037012662949476575, 0.04523769916047137, 0.03290014484397918, 0.028787626738481783, 0.06672203017817176, 0.06255190329203603, 0.06672203017817176, 0.0708921570643075, 0.06672203017817176, 0.06672203017817176, 0.07506228395044323, 0.05421164951976456, 0.045871395747493084, 0.04170126886135735, 0.05421164951976456, 0.045871395747493084, 0.037531141975221616, 0.04170126886135735, 0.04170126886135735, 0.037531141975221616, 0.037531141975221616, 0.037531141975221616, 0.029190888202950148, 0.02502076131681441, 0.06921865119482022, 0.0773620219236226, 0.061075280466017845, 0.06921865119482022, 0.06514696583041903, 0.061075280466017845, 0.05700359510161665, 0.04886022437281427, 0.061075280466017845, 0.05293190973721546, 0.03664516827961071, 0.044788539008413085, 0.0407168536440119, 0.03664516827961071, 0.0407168536440119, 0.044788539008413085, 0.03257348291520951, 0.0407168536440119, 0.028501797550808325, 0.03257348291520951, 0.06827007481484591, 0.06827007481484591, 0.05973631546299017, 0.06827007481484591, 0.06827007481484591, 0.06827007481484591, 0.05973631546299017, 0.0554694357870623, 0.04693567643520656, 0.05120255611113443, 0.0426687967592787, 0.05973631546299017, 0.03840191708335083, 0.0426687967592787, 0.03840191708335083, 0.029868157731495086, 0.034135037407422957, 0.03840191708335083, 0.034135037407422957, 0.029868157731495086, 0.05963374488455325, 0.06389329809059277, 0.07667195770871132, 0.05963374488455325, 0.06815285129663229, 0.055374191678513734, 0.06815285129663229, 0.051114638472474216, 0.055374191678513734, 0.0468550852664347, 0.0468550852664347, 0.051114638472474216, 0.03833597885435566, 0.04259553206039518, 0.04259553206039518, 0.034076425648316144, 0.04259553206039518, 0.034076425648316144, 0.029816872442276626, 0.025557319236237108, 0.06716771143759316, 0.08297187883467391, 0.06321666958832298, 0.06321666958832298, 0.06716771143759316, 0.05531458588978261, 0.05531458588978261, 0.05136354404051242, 0.05531458588978261, 0.04741250219124223, 0.05136354404051242, 0.05136354404051242, 0.039510418492701864, 0.04346146034197205, 0.039510418492701864, 0.03160833479416149, 0.03555937664343167, 0.027657292944891304, 0.039510418492701864, 0.03160833479416149, 0.06093242350484245, 0.07311890820581095, 0.06499458507183195, 0.06905674663882144, 0.06093242350484245, 0.06905674663882144, 0.05280810037086346, 0.06905674663882144, 0.04874593880387396, 0.05280810037086346, 0.044683777236884464, 0.04874593880387396, 0.040621615669894966, 0.036559454102905474, 0.040621615669894966, 0.036559454102905474, 0.040621615669894966, 0.032497292535915975, 0.032497292535915975, 0.032497292535915975, 0.07091074943828457, 0.06673952888309136, 0.06673952888309136, 0.07091074943828457, 0.07925319054867098, 0.05422586721751173, 0.05422586721751173, 0.058397087772704935, 0.05005464666231852, 0.05422586721751173, 0.05422586721751173, 0.037540984996738885, 0.0417122055519321, 0.0417122055519321, 0.037540984996738885, 0.03336976444154568, 0.037540984996738885, 0.029198543886352468, 0.0417122055519321, 0.029198543886352468, 0.08916867894828241, 0.06890307009640004, 0.06890307009640004, 0.06484994832602357, 0.08511555717790593, 0.060796826555647096, 0.05269058301489415, 0.056743704785270625, 0.044584339474141206, 0.04053121770376473, 0.04863746124451768, 0.044584339474141206, 0.04863746124451768, 0.04053121770376473, 0.03647809593338826, 0.03242497416301179, 0.03647809593338826, 0.03242497416301179, 0.028371852392635313, 0.02431873062225884, 0.06500282535190725, 0.07719085510538985, 0.06094014876741304, 0.06094014876741304, 0.06500282535190725, 0.06094014876741304, 0.056877472182918845, 0.056877472182918845, 0.04062676584494203, 0.05281479559842464, 0.06094014876741304, 0.04875211901393044, 0.04875211901393044, 0.036564089260447824, 0.04468944242943623, 0.03250141267595363, 0.03250141267595363, 0.03250141267595363, 0.03250141267595363, 0.03250141267595363, 0.06648027839260606, 0.0581702435935303, 0.06648027839260606, 0.06232526099306818, 0.06648027839260606, 0.06232526099306818, 0.06232526099306818, 0.05401522619399242, 0.04986020879445454, 0.05401522619399242, 0.04570519139491666, 0.04986020879445454, 0.05401522619399242, 0.0373951565958409, 0.0373951565958409, 0.0373951565958409, 0.0373951565958409, 0.03324013919630303, 0.0373951565958409, 0.02908512179676515, 0.0678585023488597, 0.0678585023488597, 0.0598751491313468, 0.06386682574010324, 0.07983353217512906, 0.0598751491313468, 0.07185017895761615, 0.05189179591383389, 0.04790011930507744, 0.05189179591383389, 0.04790011930507744, 0.05588347252259034, 0.043908442696320986, 0.03592508947880808, 0.03991676608756453, 0.03193341287005162, 0.03193341287005162, 0.03193341287005162, 0.03193341287005162, 0.02794173626129517, 0.06839150464071146, 0.06034544527121599, 0.06034544527121599, 0.06034544527121599, 0.07241453432545919, 0.06436847495596372, 0.06034544527121599, 0.044253326532225065, 0.06034544527121599, 0.05229938590172053, 0.05229938590172053, 0.04827635621697279, 0.044253326532225065, 0.044253326532225065, 0.036207267162729595, 0.036207267162729595, 0.03218423747798186, 0.03218423747798186, 0.03218423747798186, 0.036207267162729595, 0.061429099061552646, 0.06552437233232282, 0.069619645603093, 0.06552437233232282, 0.061429099061552646, 0.06552437233232282, 0.061429099061552646, 0.069619645603093, 0.05733382579078247, 0.04914327924924212, 0.040952732707701764, 0.045048005978471944, 0.040952732707701764, 0.040952732707701764, 0.04914327924924212, 0.03276218616616141, 0.03276218616616141, 0.03276218616616141, 0.028666912895391236, 0.028666912895391236, 0.0729981665221107, 0.060116137135855875, 0.06870415672669243, 0.060116137135855875, 0.09017420570378382, 0.05152811754501932, 0.0558221273404376, 0.0558221273404376, 0.047234107749601045, 0.03864608815876449, 0.047234107749601045, 0.04294009795418277, 0.047234107749601045, 0.04294009795418277, 0.04294009795418277, 0.03864608815876449, 0.03864608815876449, 0.030058068567927938, 0.030058068567927938, 0.030058068567927938, 0.06837289774430591, 0.06837289774430591, 0.07239483290573567, 0.06032902742144639, 0.06435096258287615, 0.06435096258287615, 0.056307092260016636, 0.052285157098586874, 0.052285157098586874, 0.056307092260016636, 0.044241286775727355, 0.044241286775727355, 0.04021935161429759, 0.04021935161429759, 0.04826322193715711, 0.028153546130008318, 0.032175481291438074, 0.04021935161429759, 0.032175481291438074, 0.028153546130008318, 0.07023395370365354, 0.06610254466226216, 0.061971135620870775, 0.061971135620870775, 0.06610254466226216, 0.06610254466226216, 0.07023395370365354, 0.04957690849669662, 0.041314090413913845, 0.04957690849669662, 0.053708317538088, 0.061971135620870775, 0.04544549945530523, 0.037182681372522466, 0.041314090413913845, 0.028919863289739695, 0.03305127233113108, 0.028919863289739695, 0.037182681372522466, 0.03305127233113108, 0.08899350732846645, 0.06780457701216491, 0.07204236307542522, 0.06780457701216491, 0.0593290048856443, 0.0593290048856443, 0.0593290048856443, 0.04661564669586338, 0.050853432759123685, 0.04237786063260307, 0.04661564669586338, 0.050853432759123685, 0.050853432759123685, 0.03814007456934276, 0.04237786063260307, 0.02966450244282215, 0.02966450244282215, 0.03814007456934276, 0.02966450244282215, 0.02966450244282215, 0.08655753041521191, 0.08243574325258278, 0.06594859460206622, 0.0741921689273245, 0.06594859460206622, 0.06182680743943708, 0.06182680743943708, 0.049461445951549665, 0.04121787162629139, 0.06182680743943708, 0.04121787162629139, 0.04121787162629139, 0.03709608446366225, 0.03709608446366225, 0.03709608446366225, 0.02885251013840397, 0.02885251013840397, 0.02885251013840397, 0.03297429730103311, 0.02885251013840397, 0.07416392262962915, 0.07416392262962915, 0.057683050934156, 0.07416392262962915, 0.06592348678189257, 0.057683050934156, 0.057683050934156, 0.057683050934156, 0.057683050934156, 0.04944261508641943, 0.04120217923868286, 0.04532239716255114, 0.037081961314814574, 0.04532239716255114, 0.037081961314814574, 0.032961743390946284, 0.04532239716255114, 0.037081961314814574, 0.028841525467078, 0.028841525467078, 0.07194262849659953, 0.07617454781992893, 0.07194262849659953, 0.07617454781992893, 0.06347878984994076, 0.05924687052661139, 0.055014951203282, 0.055014951203282, 0.05078303187995262, 0.03808727390996446, 0.04655111255662323, 0.04655111255662323, 0.04231919323329385, 0.05078303187995262, 0.04655111255662323, 0.02539151593997631, 0.029623435263305693, 0.029623435263305693, 0.03385535458663508, 0.029623435263305693, 0.07459307977805249, 0.058016839827374156, 0.0704490197903829, 0.06216089981504374, 0.058016839827374156, 0.06216089981504374, 0.058016839827374156, 0.04972871985203499, 0.05387277983970457, 0.04972871985203499, 0.04972871985203499, 0.04558465986436541, 0.04558465986436541, 0.04144059987669583, 0.04558465986436541, 0.037296539889026245, 0.04144059987669583, 0.037296539889026245, 0.03315247990135666, 0.029008419913687078, 0.05477354107519271, 0.08426698626952725, 0.07162693832909815, 0.06320023970214543, 0.08426698626952725, 0.06320023970214543, 0.05477354107519271, 0.05056019176171635, 0.042133493134763625, 0.05056019176171635, 0.046346842448239986, 0.046346842448239986, 0.042133493134763625, 0.042133493134763625, 0.03792014382128726, 0.033706794507810896, 0.029493445194334535, 0.033706794507810896, 0.033706794507810896, 0.029493445194334535, 0.06724811879417404, 0.05463909652026641, 0.05884210394490229, 0.07985714106808169, 0.05884210394490229, 0.06724811879417404, 0.05884210394490229, 0.05463909652026641, 0.050436089095630535, 0.050436089095630535, 0.050436089095630535, 0.04623308167099466, 0.04203007424635878, 0.04203007424635878, 0.050436089095630535, 0.037827066821722904, 0.037827066821722904, 0.03362405939708702, 0.03362405939708702, 0.025218044547815267, 0.07935491860941486, 0.07141942674847337, 0.08729041047035635, 0.06745168081800264, 0.07141942674847337, 0.05951618895706114, 0.05951618895706114, 0.055548443026590406, 0.043645205235178175, 0.05158069709611966, 0.03967745930470743, 0.03967745930470743, 0.03967745930470743, 0.043645205235178175, 0.043645205235178175, 0.031741967443765944, 0.031741967443765944, 0.02380647558282446, 0.027774221513295203, 0.031741967443765944, 0.06238000990262181, 0.06653867722946326, 0.058221342575780356, 0.06653867722946326, 0.07485601188314617, 0.07069734455630472, 0.04990400792209745, 0.04990400792209745, 0.0540626752489389, 0.04990400792209745, 0.04990400792209745, 0.04158667326841454, 0.04574534059525599, 0.037428005941573084, 0.04990400792209745, 0.037428005941573084, 0.037428005941573084, 0.03326933861473163, 0.03326933861473163, 0.03326933861473163, 0.06709859684912509, 0.05871127224298445, 0.06290493454605477, 0.07548592145526571, 0.06290493454605477, 0.06290493454605477, 0.06290493454605477, 0.046130285333773495, 0.05451760993991413, 0.050323947636843815, 0.046130285333773495, 0.050323947636843815, 0.037742960727632856, 0.041936623030703175, 0.037742960727632856, 0.037742960727632856, 0.037742960727632856, 0.041936623030703175, 0.029355636121492223, 0.037742960727632856, 0.08023782720927042, 0.060178370406952814, 0.06419026176741634, 0.06419026176741634, 0.07221404448834337, 0.06419026176741634, 0.05616647904648929, 0.05616647904648929, 0.06419026176741634, 0.04413080496509873, 0.04413080496509873, 0.04011891360463521, 0.04413080496509873, 0.04413080496509873, 0.04011891360463521, 0.036107022244171685, 0.036107022244171685, 0.036107022244171685, 0.03209513088370817, 0.028083239523244645, 0.06962054552038378, 0.06142989310622098, 0.06552521931330238, 0.06552521931330238, 0.06552521931330238, 0.07781119793454658, 0.06552521931330238, 0.057334566899139586, 0.04914391448497679, 0.04914391448497679, 0.04914391448497679, 0.04504858827789539, 0.04504858827789539, 0.036857935863732594, 0.036857935863732594, 0.03276260965665119, 0.036857935863732594, 0.036857935863732594, 0.028667283449569793, 0.024571957242488396, 0.06993207075778092, 0.053477465873597176, 0.06170476831568905, 0.07404572197882686, 0.07404572197882686, 0.06170476831568905, 0.06170476831568905, 0.05759111709464312, 0.05759111709464312, 0.04525016343150531, 0.04525016343150531, 0.04936381465255124, 0.041136512210459365, 0.03702286098941343, 0.04936381465255124, 0.02468190732627562, 0.032909209768367496, 0.032909209768367496, 0.032909209768367496, 0.032909209768367496, 0.06912916466839401, 0.06099632176623001, 0.073195586119476, 0.06506274321731201, 0.06099632176623001, 0.05692990031514801, 0.05692990031514801, 0.052863478864066005, 0.052863478864066005, 0.05692990031514801, 0.04473063596190201, 0.048797057412984006, 0.04066421451082001, 0.04473063596190201, 0.032531371608656004, 0.032531371608656004, 0.04066421451082001, 0.032531371608656004, 0.04066421451082001, 0.028464950157574005, 0.06356093719101444, 0.05508614556554584, 0.05508614556554584, 0.06356093719101444, 0.06779833300374873, 0.06356093719101444, 0.06356093719101444, 0.06779833300374873, 0.04237395812734295, 0.04237395812734295, 0.05508614556554584, 0.05084874975281155, 0.03813656231460866, 0.04661135394007725, 0.04237395812734295, 0.03813656231460866, 0.03813656231460866, 0.04237395812734295, 0.02966177068914007, 0.02966177068914007, 0.07529807668002568, 0.06693162371557838, 0.06693162371557838, 0.07111485019780203, 0.05856517075113109, 0.06274839723335474, 0.05019871778668379, 0.05438194426890744, 0.04601549130446014, 0.04183226482223649, 0.04601549130446014, 0.05019871778668379, 0.05438194426890744, 0.04183226482223649, 0.04601549130446014, 0.03764903834001284, 0.03346581185778919, 0.03764903834001284, 0.029282585375565544, 0.025099358893341896, 0.08187949241928016, 0.07778551779831616, 0.06140961931446012, 0.06550359393542414, 0.06140961931446012, 0.057315644693496114, 0.057315644693496114, 0.05322167007253211, 0.0491276954515681, 0.0491276954515681, 0.05322167007253211, 0.04503372083060409, 0.03684577158867607, 0.057315644693496114, 0.03684577158867607, 0.03275179696771207, 0.03275179696771207, 0.03275179696771207, 0.028657822346748057, 0.028657822346748057, 0.06971120167640342, 0.06561054275426205, 0.06971120167640342, 0.08201317844282756, 0.05330856598783791, 0.06561054275426205, 0.061509883832120665, 0.05740922490997929, 0.045107248143555156, 0.045107248143555156, 0.045107248143555156, 0.049207907065696534, 0.04100658922141378, 0.049207907065696534, 0.04100658922141378, 0.045107248143555156, 0.032805271377131025, 0.032805271377131025, 0.028704612454989644, 0.024603953532848267, 0.0907599970901274, 0.07013272502418935, 0.07425817943737696, 0.06600727061100174, 0.06600727061100174, 0.05363090737143892, 0.05775636178462653, 0.0453799985450637, 0.04950545295825131, 0.0453799985450637, 0.0453799985450637, 0.05363090737143892, 0.0453799985450637, 0.04125454413187609, 0.03300363530550087, 0.03712908971868848, 0.03300363530550087, 0.03300363530550087, 0.03300363530550087, 0.028878180892313265, 0.07357840206823447, 0.07790654336636592, 0.0649221194719716, 0.060593978173840164, 0.06925026077010304, 0.060593978173840164, 0.060593978173840164, 0.051937695577577284, 0.04760955427944584, 0.04760955427944584, 0.0432814129813144, 0.04760955427944584, 0.03895327168318296, 0.03895327168318296, 0.051937695577577284, 0.03462513038505152, 0.03462513038505152, 0.03462513038505152, 0.030296989086920082, 0.030296989086920082, 0.07068911283515025, 0.07068911283515025, 0.05821456351130021, 0.06653092972720023, 0.06237274661925022, 0.06653092972720023, 0.05821456351130021, 0.049898197295400175, 0.049898197295400175, 0.054056380403350195, 0.054056380403350195, 0.04574001418745016, 0.04158183107950015, 0.04158183107950015, 0.04158183107950015, 0.024949098647700087, 0.03326546486360012, 0.037423647971550136, 0.03326546486360012, 0.037423647971550136, 0.07224500971029879, 0.06823139805972663, 0.08428584466201525, 0.06421778640915447, 0.06823139805972663, 0.06823139805972663, 0.06020417475858232, 0.04816333980686586, 0.05217695145743801, 0.04816333980686586, 0.0441497281562937, 0.0441497281562937, 0.0441497281562937, 0.04013611650572155, 0.0441497281562937, 0.03210889320457724, 0.02809528155400508, 0.02809528155400508, 0.03210889320457724, 0.02809528155400508, 0.06677634349692019, 0.08764395083970775, 0.06260282202836268, 0.0709498649654777, 0.06260282202836268, 0.06260282202836268, 0.05008225762269014, 0.05008225762269014, 0.05425577909124765, 0.04590873615413263, 0.05425577909124765, 0.05425577909124765, 0.04590873615413263, 0.04173521468557512, 0.04173521468557512, 0.033388171748460094, 0.037561693217017605, 0.033388171748460094, 0.029214650279902582, 0.02504112881134507, 0.07054975061961867, 0.07054975061961867, 0.062249779958487066, 0.07054975061961867, 0.07884972128075028, 0.06639976528905286, 0.05809979462792126, 0.041499853305658044, 0.04979982396678965, 0.04564983863622384, 0.053949809297355454, 0.041499853305658044, 0.04564983863622384, 0.03734986797509224, 0.053949809297355454, 0.03319988264452643, 0.03734986797509224, 0.03319988264452643, 0.02904989731396063, 0.024899911983394824, 0.0717420583427907, 0.07572772825072352, 0.05978504861899225, 0.05978504861899225, 0.06377071852692506, 0.05978504861899225, 0.0717420583427907, 0.051813708803126614, 0.0478280388951938, 0.051813708803126614, 0.039856699079328164, 0.0478280388951938, 0.0478280388951938, 0.039856699079328164, 0.0478280388951938, 0.03188535926346253, 0.03587102917139535, 0.03587102917139535, 0.03188535926346253, 0.03188535926346253, 0.07436360683045037, 0.057838360868128075, 0.06196967235870865, 0.08675754130219211, 0.07436360683045037, 0.057838360868128075, 0.04957573788696692, 0.0537070493775475, 0.04957573788696692, 0.057838360868128075, 0.04131311490580577, 0.0537070493775475, 0.04131311490580577, 0.04544442639638634, 0.04131311490580577, 0.03305049192464461, 0.028919180434064037, 0.028919180434064037, 0.028919180434064037, 0.028919180434064037, 0.08491976932143558, 0.06368982699107668, 0.059443838525004906, 0.06793581545714847, 0.05519785005893313, 0.059443838525004906, 0.06368982699107668, 0.05095186159286135, 0.05095186159286135, 0.05095186159286135, 0.04245988466071779, 0.05095186159286135, 0.05095186159286135, 0.04245988466071779, 0.03821389619464601, 0.033967907728574234, 0.029721919262502453, 0.04245988466071779, 0.03821389619464601, 0.029721919262502453, 0.06745553693842522, 0.05951959141625755, 0.06348756417734139, 0.07539148246059289, 0.08729540074384441, 0.07142350969950906, 0.06348756417734139, 0.051583645894089876, 0.043647700371922206, 0.043647700371922206, 0.04761567313300604, 0.05555161865517372, 0.043647700371922206, 0.03967972761083837, 0.043647700371922206, 0.02777580932758686, 0.03571175484975453, 0.031743782088670694, 0.02777580932758686, 0.02777580932758686, 0.062617806818324, 0.06679232727287894, 0.062617806818324, 0.07514136818198881, 0.06679232727287894, 0.062617806818324, 0.06679232727287894, 0.058443286363769074, 0.041745204545549336, 0.0500942454546592, 0.04591972500010427, 0.041745204545549336, 0.041745204545549336, 0.041745204545549336, 0.0500942454546592, 0.041745204545549336, 0.03339616363643947, 0.029221643181884537, 0.03339616363643947, 0.03339616363643947, 0.06307034935685964, 0.06727503931398361, 0.07147972927110759, 0.058865659399735655, 0.07147972927110759, 0.058865659399735655, 0.050456279485487705, 0.05466096944261168, 0.04625158952836373, 0.050456279485487705, 0.058865659399735655, 0.05466096944261168, 0.04625158952836373, 0.04625158952836373, 0.03784220961411578, 0.033637519656991806, 0.033637519656991806, 0.029432829699867828, 0.042046899571239756, 0.029432829699867828, 0.0707044447388004, 0.062386274769529765, 0.06654535975416509, 0.062386274769529765, 0.06654535975416509, 0.07902261470807104, 0.05406810480025913, 0.05406810480025913, 0.05406810480025913, 0.041590849846353174, 0.037431764861717856, 0.04990901981562381, 0.04990901981562381, 0.041590849846353174, 0.037431764861717856, 0.033272679877082545, 0.033272679877082545, 0.033272679877082545, 0.037431764861717856, 0.029113594892447223, 0.06998101745702, 0.05351489570242706, 0.06586448701837176, 0.06174795657972353, 0.06998101745702, 0.06586448701837176, 0.06586448701837176, 0.06586448701837176, 0.0576314261410753, 0.05351489570242706, 0.04528183482513059, 0.04116530438648235, 0.04116530438648235, 0.03704877394783412, 0.04116530438648235, 0.03293224350918588, 0.03293224350918588, 0.03293224350918588, 0.03293224350918588, 0.024699182631889412, 0.07052024697061388, 0.07835582996734874, 0.07052024697061388, 0.07052024697061388, 0.06660245547224644, 0.07052024697061388, 0.05093128947877668, 0.06268466397387899, 0.04701349798040925, 0.04701349798040925, 0.043095706482041814, 0.043095706482041814, 0.03526012348530694, 0.05093128947877668, 0.03917791498367437, 0.031342331986939496, 0.031342331986939496, 0.03526012348530694, 0.031342331986939496, 0.027424540488572062, 0.07114063874400108, 0.07114063874400108, 0.07951012565506002, 0.054401664921883176, 0.0669558952884716, 0.06277115183294213, 0.04603217801082422, 0.0502169214663537, 0.06277115183294213, 0.0502169214663537, 0.04603217801082422, 0.0502169214663537, 0.0502169214663537, 0.0502169214663537, 0.04184743455529475, 0.029293204188706325, 0.029293204188706325, 0.0334779476442358, 0.029293204188706325, 0.029293204188706325, 0.09155255483127198, 0.07490663577104072, 0.06242219647586726, 0.05826071671080944, 0.049937757180693806, 0.054099236945751625, 0.06658367624092508, 0.054099236945751625, 0.04577627741563599, 0.05826071671080944, 0.03745331788552036, 0.05826071671080944, 0.04577627741563599, 0.03745331788552036, 0.03745331788552036, 0.03745331788552036, 0.03745331788552036, 0.03329183812046254, 0.02913035835540472, 0.024968878590346903, 0.06962026153962558, 0.0737155710419565, 0.08190619004661834, 0.05733433303263284, 0.08600149954894926, 0.05733433303263284, 0.05733433303263284, 0.04504840452564009, 0.049143714027971006, 0.04095309502330917, 0.049143714027971006, 0.049143714027971006, 0.03685778552097825, 0.04095309502330917, 0.04095309502330917, 0.03685778552097825, 0.03276247601864734, 0.02866716651631642, 0.03276247601864734, 0.024571857013985503, 0.06676760637969964, 0.0751135571771621, 0.06676760637969964, 0.06676760637969964, 0.06259463098096842, 0.07928653257589333, 0.05424868018350596, 0.06259463098096842, 0.045902729386043506, 0.045902729386043506, 0.05007570478477474, 0.045902729386043506, 0.03755677858858105, 0.04172975398731228, 0.03755677858858105, 0.03338380318984982, 0.03755677858858105, 0.029210827791118596, 0.03338380318984982, 0.02503785239238737, 0.0769151157876681, 0.06409592982305676, 0.06836899181126053, 0.06409592982305676, 0.06836899181126053, 0.05982286783485297, 0.06836899181126053, 0.042730619882037836, 0.051276743858445405, 0.05554980584664919, 0.04700368187024162, 0.042730619882037836, 0.042730619882037836, 0.03845755789383405, 0.042730619882037836, 0.03418449590563027, 0.029911433917426487, 0.03845755789383405, 0.029911433917426487, 0.03845755789383405, 0.06357956653017766, 0.07629547983621318, 0.08901139314224872, 0.06357956653017766, 0.06357956653017766, 0.05934092876149915, 0.05934092876149915, 0.05934092876149915, 0.04662501545546362, 0.042386377686785104, 0.042386377686785104, 0.05086365322414212, 0.042386377686785104, 0.042386377686785104, 0.042386377686785104, 0.033909102149428084, 0.033909102149428084, 0.033909102149428084, 0.02543182661207106, 0.029670464380749575, 0.059120081552634283, 0.06334294452067958, 0.0717886704567702, 0.07601153342481551, 0.06334294452067958, 0.059120081552634283, 0.05489721858458897, 0.0675658074887249, 0.05067435561654367, 0.04645149264849836, 0.05067435561654367, 0.05067435561654367, 0.038005766712407754, 0.04222862968045306, 0.04222862968045306, 0.03378290374436245, 0.03378290374436245, 0.03378290374436245, 0.029560040776317142, 0.029560040776317142, 0.07927516018177228, 0.06342012814541781, 0.0872026761999495, 0.05549261212724059, 0.07134764416359504, 0.06738388615450643, 0.05152885411815197, 0.05549261212724059, 0.04360133809997475, 0.047565096109063364, 0.047565096109063364, 0.047565096109063364, 0.04360133809997475, 0.04360133809997475, 0.04360133809997475, 0.027746306063620295, 0.03963758009088614, 0.027746306063620295, 0.031710064072708904, 0.027746306063620295, 0.05532596848023116, 0.0680934996679768, 0.0766051871264739, 0.06383765593872825, 0.0680934996679768, 0.05532596848023116, 0.07234934339722536, 0.05532596848023116, 0.051070124750982604, 0.04681428102173405, 0.051070124750982604, 0.04681428102173405, 0.0340467498339884, 0.03830259356323695, 0.03830259356323695, 0.03830259356323695, 0.0340467498339884, 0.042558437292485506, 0.0340467498339884, 0.02979090610473985, 0.0752346873907411, 0.06687527768065875, 0.06687527768065875, 0.06687527768065875, 0.06269557282561758, 0.08359409710082344, 0.054336163115535234, 0.05851586797057641, 0.045976753405452896, 0.04179704855041172, 0.050156458260494065, 0.050156458260494065, 0.03761734369537055, 0.03761734369537055, 0.03761734369537055, 0.029257933985288205, 0.03761734369537055, 0.03761734369537055, 0.033437638840329374, 0.025078229130247032, 0.06869375770255144, 0.060612139149310094, 0.06869375770255144, 0.09293861336227548, 0.060612139149310094, 0.06869375770255144, 0.056571329872689424, 0.056571329872689424, 0.0444489020428274, 0.0444489020428274, 0.03636728348958606, 0.0444489020428274, 0.0444489020428274, 0.04040809276620673, 0.03636728348958606, 0.03232647421296538, 0.03636728348958606, 0.03232647421296538, 0.028285664936344712, 0.03636728348958606, 0.06910508837219624, 0.0734241563954585, 0.060466952325671716, 0.06910508837219624, 0.09501949651176983, 0.060466952325671716, 0.05614788430240945, 0.04319068023262265, 0.047509748255884915, 0.047509748255884915, 0.05614788430240945, 0.047509748255884915, 0.04319068023262265, 0.03887161220936039, 0.03455254418609812, 0.04319068023262265, 0.030233476162835858, 0.030233476162835858, 0.030233476162835858, 0.025914408139573592, 0.06811821077152282, 0.07212516434631829, 0.08414602507070466, 0.07212516434631829, 0.06411125719672736, 0.0601043036219319, 0.0601043036219319, 0.05209039647234098, 0.05609735004713644, 0.04808344289754552, 0.04407648932275006, 0.04808344289754552, 0.03606258217315914, 0.0400695357479546, 0.0400695357479546, 0.03606258217315914, 0.03205562859836368, 0.03205562859836368, 0.03205562859836368, 0.02404172144877276, 0.06770008614788862, 0.07168244415658796, 0.08761187619138529, 0.0637177281391893, 0.07168244415658796, 0.0637177281391893, 0.05973537013048997, 0.047788296104391975, 0.047788296104391975, 0.051770654113091306, 0.051770654113091306, 0.047788296104391975, 0.03982358008699331, 0.03982358008699331, 0.03982358008699331, 0.03185886406959465, 0.02787650606089532, 0.03584122207829398, 0.023894148052195988, 0.02787650606089532, 0.07726200698758856, 0.06912916414678975, 0.07319558556718915, 0.08539484982838735, 0.07319558556718915, 0.056929899885591564, 0.05286347846519217, 0.044730635624393375, 0.04066421420399398, 0.05286347846519217, 0.044730635624393375, 0.044730635624393375, 0.04879705704479277, 0.036597792783594575, 0.04066421420399398, 0.03253137136319518, 0.03253137136319518, 0.036597792783594575, 0.03253137136319518, 0.024398528522396386, 0.09457363549713829, 0.06579035512844403, 0.06579035512844403, 0.06579035512844403, 0.06990225232397178, 0.06579035512844403, 0.04934276634633302, 0.04934276634633302, 0.04934276634633302, 0.04111897195527752, 0.04934276634633302, 0.04523086915080527, 0.04111897195527752, 0.04111897195527752, 0.04934276634633302, 0.032895177564222014, 0.032895177564222014, 0.032895177564222014, 0.032895177564222014, 0.032895177564222014, 0.066603314793274, 0.07909143631701288, 0.066603314793274, 0.062440607618694376, 0.066603314793274, 0.062440607618694376, 0.07492872914243325, 0.062440607618694376, 0.0499524860949555, 0.0499524860949555, 0.04162707174579625, 0.045789778920375876, 0.03746436457121662, 0.04162707174579625, 0.03746436457121662, 0.033301657396637, 0.029138950222057376, 0.029138950222057376, 0.029138950222057376, 0.029138950222057376, 0.06265750741225048, 0.07518900889470058, 0.06265750741225048, 0.05848034025143379, 0.06683467457306719, 0.07101184173388389, 0.05848034025143379, 0.05848034025143379, 0.05012600592980039, 0.05012600592980039, 0.054303173090617086, 0.054303173090617086, 0.04177167160816699, 0.03759450444735029, 0.03759450444735029, 0.04594883876898369, 0.029240170125716895, 0.03759450444735029, 0.029240170125716895, 0.025063002964900195, 0.06457524094645448, 0.06861119350560788, 0.06457524094645448, 0.06861119350560788, 0.06861119350560788, 0.06457524094645448, 0.04843143070984086, 0.06861119350560788, 0.05650333582814767, 0.04439547815068745, 0.04843143070984086, 0.052467383268994264, 0.04439547815068745, 0.04035952559153405, 0.04035952559153405, 0.036323573032380645, 0.03228762047322724, 0.028251667914073835, 0.03228762047322724, 0.028251667914073835, 0.07147698568236852, 0.07147698568236852, 0.06750604203334803, 0.06353509838432757, 0.08338981662942993, 0.06750604203334803, 0.05559321108628662, 0.06353509838432757, 0.051622267437266146, 0.0436803801392252, 0.0436803801392252, 0.0436803801392252, 0.03970943649020473, 0.03970943649020473, 0.03573849284118426, 0.031767549192163784, 0.031767549192163784, 0.02779660554314331, 0.02779660554314331, 0.023825661894122836, 0.05518108686622755, 0.0764045818147766, 0.05518108686622755, 0.06791518383535698, 0.0721598828250668, 0.06367048484564718, 0.059425785855937364, 0.06791518383535698, 0.05518108686622755, 0.05093638787651774, 0.05093638787651774, 0.042446989897098115, 0.042446989897098115, 0.0382022909073883, 0.03395759191767849, 0.0382022909073883, 0.0382022909073883, 0.029712892927968682, 0.03395759191767849, 0.02546819393825887, 0.0787457171312823, 0.06693385956158995, 0.06299657370502583, 0.08268300298784641, 0.05905928784846172, 0.05905928784846172, 0.0551220019918976, 0.04331014442220526, 0.0551220019918976, 0.05118471613533349, 0.0551220019918976, 0.0551220019918976, 0.03937285856564115, 0.04331014442220526, 0.04331014442220526, 0.031498286852512915, 0.03543557270907703, 0.031498286852512915, 0.0275610009959488, 0.0275610009959488, 0.06209619917366709, 0.07762024896708386, 0.06985822407037548, 0.08150126141543805, 0.054334174276958705, 0.07373923651872967, 0.058215186725312895, 0.05045316182860451, 0.04657214938025032, 0.05045316182860451, 0.04657214938025032, 0.04269113693189613, 0.04657214938025032, 0.03881012448354193, 0.03492911203518774, 0.031048099586833546, 0.04269113693189613, 0.03492911203518774, 0.027167087138479352, 0.027167087138479352, 0.05901652041814853, 0.06744745190645546, 0.06744745190645546, 0.063231986162302, 0.08430931488306934, 0.06744745190645546, 0.05480105467399506, 0.0505855889298416, 0.04215465744153467, 0.05901652041814853, 0.0379391916973812, 0.04215465744153467, 0.04215465744153467, 0.0379391916973812, 0.04215465744153467, 0.0379391916973812, 0.0379391916973812, 0.03372372595322773, 0.03372372595322773, 0.0252927944649208, 0.08241041241960056, 0.061807809314700424, 0.06592832993568044, 0.06592832993568044, 0.06592832993568044, 0.05768728869372039, 0.06592832993568044, 0.049446247451760336, 0.05356676807274036, 0.04120520620980028, 0.04532572683078031, 0.04532572683078031, 0.04120520620980028, 0.04532572683078031, 0.04120520620980028, 0.037084685588820256, 0.03296416496784022, 0.028843644346860195, 0.037084685588820256, 0.037084685588820256, 0.08097081173311614, 0.051139460041968086, 0.06392432505246011, 0.07670919006295213, 0.06392432505246011, 0.06392432505246011, 0.0596627033822961, 0.04687783837180408, 0.04687783837180408, 0.04687783837180408, 0.04687783837180408, 0.051139460041968086, 0.04687783837180408, 0.055401081712132094, 0.04261621670164007, 0.034092973361312055, 0.034092973361312055, 0.02983135169114805, 0.034092973361312055, 0.025569730020984043, 0.08803431060942962, 0.06402495317049428, 0.07602963188996195, 0.06402495317049428, 0.07202807231680605, 0.06402495317049428, 0.06002339359733838, 0.052020274451026596, 0.048018714877870704, 0.04401715530471481, 0.048018714877870704, 0.04401715530471481, 0.04001559573155892, 0.04001559573155892, 0.04001559573155892, 0.03201247658524714, 0.028010917012091244, 0.036014036158403026, 0.028010917012091244, 0.024009357438935352, 0.07263406653678027, 0.06836147438755791, 0.05981629008911316, 0.07263406653678027, 0.06408888223833553, 0.05981629008911316, 0.05554369793989079, 0.06836147438755791, 0.051271105790668424, 0.046998513641446055, 0.046998513641446055, 0.046998513641446055, 0.04272592149222369, 0.04272592149222369, 0.034180737193778954, 0.034180737193778954, 0.03845332934300132, 0.02990814504455658, 0.034180737193778954, 0.02990814504455658, 0.05983863510163821, 0.07266119976627497, 0.07266119976627497, 0.06838701154472938, 0.06838701154472938, 0.05556444688009263, 0.051290258658547035, 0.051290258658547035, 0.04701607043700145, 0.05556444688009263, 0.04701607043700145, 0.051290258658547035, 0.04274188221545586, 0.04274188221545586, 0.03419350577236469, 0.03419350577236469, 0.04274188221545586, 0.03846769399391028, 0.029919317550819106, 0.029919317550819106, 0.06809653150849758, 0.08011356648058539, 0.08011356648058539, 0.0640908531844683, 0.0921306014526732, 0.06809653150849758, 0.0520738182123805, 0.0520738182123805, 0.04406246156432196, 0.04406246156432196, 0.0520738182123805, 0.04406246156432196, 0.04406246156432196, 0.03605110491626343, 0.03605110491626343, 0.03204542659223415, 0.028039748268204885, 0.028039748268204885, 0.028039748268204885, 0.028039748268204885, 0.06821655353808925, 0.06395301894195868, 0.06821655353808925, 0.08100715732648099, 0.059689484345828096, 0.07248008813421983, 0.059689484345828096, 0.059689484345828096, 0.046898880557436365, 0.046898880557436365, 0.04263534596130578, 0.059689484345828096, 0.03837181136517521, 0.03837181136517521, 0.03837181136517521, 0.034108276769044626, 0.034108276769044626, 0.029844742172914048, 0.034108276769044626, 0.029844742172914048, 0.06106263124898539, 0.06106263124898539, 0.052920947082454005, 0.07327515749878247, 0.07327515749878247, 0.06920431541551678, 0.044779262915922616, 0.056991789165719696, 0.052920947082454005, 0.040708420832656925, 0.052920947082454005, 0.052920947082454005, 0.036637578749391234, 0.044779262915922616, 0.044779262915922616, 0.044779262915922616, 0.036637578749391234, 0.03256673666612554, 0.03256673666612554, 0.028495894582859848, 0.07679607982448645, 0.07679607982448645, 0.06467038301009385, 0.07679607982448645, 0.05658658513383212, 0.06871228194822472, 0.06467038301009385, 0.04850278725757039, 0.04446088831943952, 0.052544686195701254, 0.04850278725757039, 0.04850278725757039, 0.05658658513383212, 0.04446088831943952, 0.036377090443177794, 0.03233519150504693, 0.02829329256691606, 0.02829329256691606, 0.02829329256691606, 0.02829329256691606, 0.0666985849645612, 0.07086724652484629, 0.06252992340427613, 0.06252992340427613, 0.07503590808513136, 0.0666985849645612, 0.050023938723420903, 0.050023938723420903, 0.050023938723420903, 0.0666985849645612, 0.04168661560285075, 0.04585527716313583, 0.04168661560285075, 0.04168661560285075, 0.04168661560285075, 0.03751795404256568, 0.03751795404256568, 0.025011969361710452, 0.03751795404256568, 0.029180630921995528, 0.06964854467566094, 0.07374551789187628, 0.07784249110809163, 0.061454598243230234, 0.061454598243230234, 0.061454598243230234, 0.061454598243230234, 0.04916367859458418, 0.06555157145944558, 0.05326065181079954, 0.045066705378368835, 0.04916367859458418, 0.04096973216215349, 0.04096973216215349, 0.03687275894593814, 0.03687275894593814, 0.028678812513507443, 0.03277578572972279, 0.03277578572972279, 0.02458183929729209, 0.06406544725736157, 0.06406544725736157, 0.0896916261603062, 0.05979441744020413, 0.06406544725736157, 0.07260750689167644, 0.051252357805889256, 0.05979441744020413, 0.046981327988731815, 0.051252357805889256, 0.04271029817157438, 0.05552338762304669, 0.04271029817157438, 0.04271029817157438, 0.03843926835441694, 0.034168238537259504, 0.034168238537259504, 0.029897208720102066, 0.029897208720102066, 0.029897208720102066, 0.06698142342507118, 0.06279508446100424, 0.058608745496937294, 0.05442240653287034, 0.07116776238913813, 0.058608745496937294, 0.06279508446100424, 0.05023606756880339, 0.05442240653287034, 0.04186338964066949, 0.058608745496937294, 0.05023606756880339, 0.04604972860473644, 0.03767705067660254, 0.04186338964066949, 0.04186338964066949, 0.03767705067660254, 0.03349071171253559, 0.04186338964066949, 0.029304372748468647, 0.09314789193149677, 0.06884844186241065, 0.06479853351756297, 0.07694825855210602, 0.0566987168278676, 0.06479853351756297, 0.06074862517271528, 0.05264880848301991, 0.05264880848301991, 0.04454899179332454, 0.04454899179332454, 0.04049908344847685, 0.03644917510362917, 0.04049908344847685, 0.03644917510362917, 0.03644917510362917, 0.032399266758781485, 0.032399266758781485, 0.032399266758781485, 0.0283493584139338, 0.07142426050320368, 0.08332830392040429, 0.06348823155840326, 0.06745624603080347, 0.07539227497560387, 0.06348823155840326, 0.05555220261360286, 0.05555220261360286, 0.04761617366880245, 0.05555220261360286, 0.04364815919640225, 0.03968014472400204, 0.04364815919640225, 0.03968014472400204, 0.03968014472400204, 0.03174411577920163, 0.02777610130680143, 0.03571213025160184, 0.03174411577920163, 0.023808086834401224, 0.06527994183664043, 0.06935993820143045, 0.06935993820143045, 0.06527994183664043, 0.06935993820143045, 0.0611999454718504, 0.053039952742270344, 0.0611999454718504, 0.0611999454718504, 0.053039952742270344, 0.04895995637748032, 0.04895995637748032, 0.03671996728311024, 0.032639970918320216, 0.04487996001269029, 0.03671996728311024, 0.032639970918320216, 0.032639970918320216, 0.028559974553530185, 0.02447997818874016, 0.09082930655010382, 0.0660576774909846, 0.07018628233417114, 0.0660576774909846, 0.06192907264779806, 0.07018628233417114, 0.05367186296142499, 0.05367186296142499, 0.05367186296142499, 0.04954325811823845, 0.04541465327505191, 0.041286048431865376, 0.04954325811823845, 0.03715744358867884, 0.03715744358867884, 0.0330288387454923, 0.0330288387454923, 0.0330288387454923, 0.0330288387454923, 0.028900233902305762, 0.08109278491237056, 0.06081958868427792, 0.05676494943865939, 0.07703814566675203, 0.07703814566675203, 0.05676494943865939, 0.052710310193040864, 0.05676494943865939, 0.04054639245618528, 0.04865567094742233, 0.04460103170180381, 0.052710310193040864, 0.04460103170180381, 0.04054639245618528, 0.04054639245618528, 0.03243711396494822, 0.03649175321056675, 0.03243711396494822, 0.04054639245618528, 0.03243711396494822, 0.08753799384884836, 0.06252713846346311, 0.07503256615615574, 0.06669561436102732, 0.07503256615615574, 0.06252713846346311, 0.06252713846346311, 0.0541901866683347, 0.05002171077077049, 0.041684758975642076, 0.045853234873206285, 0.045853234873206285, 0.03334780718051366, 0.045853234873206285, 0.041684758975642076, 0.029179331282949455, 0.029179331282949455, 0.03751628307807787, 0.029179331282949455, 0.029179331282949455, 0.07055010784198788, 0.07838900871331987, 0.0627112069706559, 0.0627112069706559, 0.0627112069706559, 0.05487230609932391, 0.05487230609932391, 0.06663065740632189, 0.05487230609932391, 0.047033405227991924, 0.05095285566365792, 0.047033405227991924, 0.04311395479232593, 0.039194504356659936, 0.047033405227991924, 0.03527505392099394, 0.03527505392099394, 0.03527505392099394, 0.027436153049661956, 0.027436153049661956, 0.07201512922653802, 0.06354276108223943, 0.05507039293794084, 0.06777894515438873, 0.07625131329868731, 0.059306577010090136, 0.06777894515438873, 0.05083420886579155, 0.06354276108223943, 0.04659802479364225, 0.038125656649343656, 0.05083420886579155, 0.04659802479364225, 0.042361840721492953, 0.038125656649343656, 0.029653288505045068, 0.038125656649343656, 0.029653288505045068, 0.029653288505045068, 0.025417104432895774, 0.07766057665874454, 0.062128461326995635, 0.07377754782580731, 0.058245432494058405, 0.062128461326995635, 0.058245432494058405, 0.0698945189928701, 0.058245432494058405, 0.05047937482818395, 0.05436240366112118, 0.0427133171623095, 0.04659634599524673, 0.05047937482818395, 0.0427133171623095, 0.03494725949643505, 0.02718120183056059, 0.03494725949643505, 0.03494725949643505, 0.02718120183056059, 0.02718120183056059, 0.0750421258128989, 0.06253510484408242, 0.05836609785447693, 0.06670411183368792, 0.07087311882329342, 0.05836609785447693, 0.05836609785447693, 0.06670411183368792, 0.05002808387526594, 0.045859076885660446, 0.05002808387526594, 0.045859076885660446, 0.04169006989605495, 0.045859076885660446, 0.04169006989605495, 0.04169006989605495, 0.03335205591684396, 0.03335205591684396, 0.03335205591684396, 0.029183048927238466, 0.0685515564164152, 0.055698139588337345, 0.0685515564164152, 0.08997391779654494, 0.06426708414038924, 0.0685515564164152, 0.055698139588337345, 0.047129195036285446, 0.042844722760259496, 0.051413667312311395, 0.051413667312311395, 0.047129195036285446, 0.0342757782082076, 0.038560250484233546, 0.038560250484233546, 0.0342757782082076, 0.029991305932181647, 0.042844722760259496, 0.038560250484233546, 0.029991305932181647, 0.06428775151621079, 0.06428775151621079, 0.0763417049255003, 0.06026976704644761, 0.08035968939526347, 0.05223379810692126, 0.056251782576684436, 0.048215813637158086, 0.06026976704644761, 0.05223379810692126, 0.048215813637158086, 0.04419782916739491, 0.04419782916739491, 0.03616186022786857, 0.040179844697631736, 0.03616186022786857, 0.03214387575810539, 0.03616186022786857, 0.028125891288342218, 0.03214387575810539, 0.06729500397763459, 0.07570687947483892, 0.07991281722344108, 0.07150094172623675, 0.06308906622903243, 0.06308906622903243, 0.06308906622903243, 0.04626531523462378, 0.050471252983225946, 0.04205937748602162, 0.03785343973741946, 0.050471252983225946, 0.04205937748602162, 0.04205937748602162, 0.050471252983225946, 0.029441564240215136, 0.029441564240215136, 0.033647501988817295, 0.029441564240215136, 0.033647501988817295, 0.08040705617190351, 0.07236635055471316, 0.07236635055471316, 0.060305292128927636, 0.06432564493752281, 0.060305292128927636, 0.052264586511737286, 0.06834599774611799, 0.04824423370314211, 0.04422388089454693, 0.04824423370314211, 0.04824423370314211, 0.03618317527735658, 0.03618317527735658, 0.04824423370314211, 0.040203528085951755, 0.02814246966016623, 0.03618317527735658, 0.02814246966016623, 0.024122116851571056, 0.08816417955493608, 0.06011194060563824, 0.07614179143380843, 0.056104477898595684, 0.06411940331268079, 0.06011194060563824, 0.05209701519155314, 0.056104477898595684, 0.056104477898595684, 0.04408208977746804, 0.056104477898595684, 0.048089552484510585, 0.04408208977746804, 0.04007462707042549, 0.03606716436338294, 0.032059701656340395, 0.04007462707042549, 0.03606716436338294, 0.032059701656340395, 0.028052238949297842, 0.06489032731365411, 0.06083468185655073, 0.05677903639944735, 0.07705726368496425, 0.06083468185655073, 0.05677903639944735, 0.05677903639944735, 0.05272339094234397, 0.06083468185655073, 0.04866774548524058, 0.04866774548524058, 0.05272339094234397, 0.040556454571033816, 0.040556454571033816, 0.03650080911393044, 0.040556454571033816, 0.03244516365682706, 0.03650080911393044, 0.03650080911393044, 0.028389518199723674, 0.0688859811270784, 0.08914656381151322, 0.07699021420085232, 0.07699021420085232, 0.0688859811270784, 0.05267751497953054, 0.0688859811270784, 0.04457328190575661, 0.04457328190575661, 0.04457328190575661, 0.032416932295095714, 0.04457328190575661, 0.048625398442643575, 0.04052116536886965, 0.04052116536886965, 0.03646904883198268, 0.03646904883198268, 0.02836481575820875, 0.032416932295095714, 0.024312699221321787, 0.0684910943748144, 0.05640443066161186, 0.0684910943748144, 0.0684910943748144, 0.07251998227921524, 0.0684910943748144, 0.05640443066161186, 0.05640443066161186, 0.048346654852810166, 0.04431776694840932, 0.05640443066161186, 0.048346654852810166, 0.04431776694840932, 0.05237554275721101, 0.04028887904400847, 0.03625999113960762, 0.032231103235206775, 0.032231103235206775, 0.02820221533080593, 0.024173327426405083, 0.06786988334607828, 0.07635361876433806, 0.06362801563694839, 0.06362801563694839, 0.06362801563694839, 0.059386147927818496, 0.059386147927818496, 0.05090241250955871, 0.04241867709129893, 0.05090241250955871, 0.055144280218688606, 0.05090241250955871, 0.04241867709129893, 0.04666054480042882, 0.03817680938216903, 0.03817680938216903, 0.03817680938216903, 0.04241867709129893, 0.029693073963909248, 0.025451206254779354, 0.06349199057496727, 0.07195758931829624, 0.06772478994663175, 0.06349199057496727, 0.050793592459973815, 0.05925919120330279, 0.0550263918316383, 0.0550263918316383, 0.0550263918316383, 0.0550263918316383, 0.04656079308830933, 0.06349199057496727, 0.04232799371664485, 0.03809519434498036, 0.04232799371664485, 0.03809519434498036, 0.03809519434498036, 0.03809519434498036, 0.033862394973315876, 0.025396796229986907, 0.0753242980812006, 0.07113961485446724, 0.0627702484010005, 0.07950898130793396, 0.0627702484010005, 0.054400881947533765, 0.058585565174267135, 0.058585565174267135, 0.0627702484010005, 0.0376621490406003, 0.04184683226733366, 0.04603151549406703, 0.0376621490406003, 0.04184683226733366, 0.04184683226733366, 0.0376621490406003, 0.04184683226733366, 0.029292782587133567, 0.0376621490406003, 0.029292782587133567, 0.06340999060688796, 0.054955325192636226, 0.06340999060688796, 0.06763732331401381, 0.07186465602113967, 0.059182657899762084, 0.059182657899762084, 0.054955325192636226, 0.05072799248551036, 0.04227332707125863, 0.05072799248551036, 0.054955325192636226, 0.03804599436413277, 0.046500659778384495, 0.046500659778384495, 0.03804599436413277, 0.03381866165700691, 0.03804599436413277, 0.03804599436413277, 0.03381866165700691, 0.06832549752160533, 0.06832549752160533, 0.06832549752160533, 0.06832549752160533, 0.07636379134767654, 0.0562680567824985, 0.06832549752160533, 0.052248909869462894, 0.06028720369553411, 0.04421061604339168, 0.04421061604339168, 0.04421061604339168, 0.03617232221732047, 0.04421061604339168, 0.03617232221732047, 0.03617232221732047, 0.03617232221732047, 0.040191469130356074, 0.02813402839124925, 0.02813402839124925, 0.07490841288453565, 0.06658525589736503, 0.0582620989101944, 0.07074683439095034, 0.05410052041660908, 0.06242367740377971, 0.0582620989101944, 0.05410052041660908, 0.04577736342943845, 0.04577736342943845, 0.04577736342943845, 0.04993894192302377, 0.04577736342943845, 0.04161578493585314, 0.04993894192302377, 0.04577736342943845, 0.03329262794868251, 0.03745420644226782, 0.03329262794868251, 0.024969470961511885, 0.06674650858395045, 0.06674650858395045, 0.06674650858395045, 0.07091816537044736, 0.06674650858395045, 0.06674650858395045, 0.06257485179745356, 0.054231538224459745, 0.054231538224459745, 0.05005988143796285, 0.06257485179745356, 0.05005988143796285, 0.03754491107847213, 0.03754491107847213, 0.029201597505478325, 0.03337325429197523, 0.029201597505478325, 0.03337325429197523, 0.03337325429197523, 0.025029940718981424, 0.061502644428107746, 0.06970299701852212, 0.06970299701852212, 0.06970299701852212, 0.06970299701852212, 0.06560282072331493, 0.06560282072331493, 0.05330229183769338, 0.04510193924727902, 0.05330229183769338, 0.061502644428107746, 0.04510193924727902, 0.041001762952071835, 0.036901586656864646, 0.036901586656864646, 0.032801410361657464, 0.036901586656864646, 0.032801410361657464, 0.032801410361657464, 0.0246010577712431, 0.07400343690019447, 0.05755822870015125, 0.06989213485018367, 0.07400343690019447, 0.09044864510023769, 0.07400343690019447, 0.04933562460012965, 0.04933562460012965, 0.05344692665014045, 0.045224322550118844, 0.045224322550118844, 0.04111302050010804, 0.04111302050010804, 0.04111302050010804, 0.03289041640008643, 0.03700171845009723, 0.03289041640008643, 0.028779114350075626, 0.03289041640008643, 0.024667812300064824, 0.07597879245150489, 0.06798102482503068, 0.06798102482503068, 0.08797544389121618, 0.07197990863826778, 0.051985489572082294, 0.05598437338531939, 0.051985489572082294, 0.039988838132370995, 0.04398772194560809, 0.04798660575884519, 0.04798660575884519, 0.04398772194560809, 0.039988838132370995, 0.039988838132370995, 0.03598995431913389, 0.03199107050589679, 0.03199107050589679, 0.03199107050589679, 0.03199107050589679, 0.06604748276823368, 0.07017545044124829, 0.0866873211333067, 0.06191951509521907, 0.07017545044124829, 0.057791547422204465, 0.04953561207617526, 0.053663579749189866, 0.057791547422204465, 0.04953561207617526, 0.053663579749189866, 0.045407644403160655, 0.045407644403160655, 0.04127967673014605, 0.03302374138411684, 0.028895773711102232, 0.04127967673014605, 0.028895773711102232, 0.03302374138411684, 0.02476780603808763, 0.07689894944936322, 0.055538130157873435, 0.06408245787446935, 0.06408245787446935, 0.06408245787446935, 0.07689894944936322, 0.05981029401617139, 0.06835462173276731, 0.04699380244127752, 0.04272163858297957, 0.04272163858297957, 0.04699380244127752, 0.04272163858297957, 0.04272163858297957, 0.04272163858297957, 0.034177310866383655, 0.04272163858297957, 0.034177310866383655, 0.02563298314978774, 0.029905147008085695, 0.07592509520517499, 0.07592509520517499, 0.05594480699328683, 0.0799211528475526, 0.0639369222780421, 0.0639369222780421, 0.0519487493509092, 0.0519487493509092, 0.047952691708531565, 0.0519487493509092, 0.047952691708531565, 0.05994086463566446, 0.0399605764237763, 0.035964518781398676, 0.0399605764237763, 0.0399605764237763, 0.035964518781398676, 0.027972403496643414, 0.027972403496643414, 0.027972403496643414, 0.06560323072656238, 0.05740282688574209, 0.06560323072656238, 0.06560323072656238, 0.08610424032861312, 0.061503028806152236, 0.061503028806152236, 0.061503028806152236, 0.05330262496533194, 0.04920242304492179, 0.05330262496533194, 0.045102221124511635, 0.03690181728369134, 0.041002019204101486, 0.041002019204101486, 0.03280161536328119, 0.03280161536328119, 0.028701413442871044, 0.03280161536328119, 0.028701413442871044, 0.07280997431069137, 0.07280997431069137, 0.07685497288350757, 0.060674978592242815, 0.060674978592242815, 0.060674978592242815, 0.060674978592242815, 0.04044998572816187, 0.060674978592242815, 0.04853998287379425, 0.060674978592242815, 0.03640498715534569, 0.04449498430097806, 0.04044998572816187, 0.04044998572816187, 0.0323599885825295, 0.0323599885825295, 0.03640498715534569, 0.02831499000971331, 0.02831499000971331, 0.07133893188496314, 0.05944910990413595, 0.0753022058785722, 0.07133893188496314, 0.0753022058785722, 0.063412383897745, 0.05944910990413595, 0.05548583591052688, 0.043596013929699695, 0.043596013929699695, 0.05152256191691782, 0.047559287923308755, 0.03963273993609063, 0.043596013929699695, 0.03566946594248157, 0.03566946594248157, 0.0317061919488725, 0.043596013929699695, 0.02774291795526344, 0.023779643961654377, 0.07306660220793425, 0.06876856678393811, 0.06876856678393811, 0.06447053135994199, 0.060172495935945855, 0.05587446051194972, 0.060172495935945855, 0.05157642508795359, 0.05157642508795359, 0.047278389663957456, 0.05587446051194972, 0.05587446051194972, 0.04298035423996132, 0.04298035423996132, 0.04298035423996132, 0.034384283391969056, 0.04298035423996132, 0.030086247967972927, 0.025788212543976795, 0.025788212543976795, 0.06533780891337777, 0.08575587419880833, 0.06125419585629166, 0.06533780891337777, 0.06942142197046387, 0.06533780891337777, 0.04900335668503333, 0.04900335668503333, 0.04491974362794721, 0.05308696974211944, 0.04900335668503333, 0.04083613057086111, 0.05308696974211944, 0.04083613057086111, 0.04083613057086111, 0.04491974362794721, 0.03266890445668889, 0.028585291399602773, 0.03266890445668889, 0.024501678342516663, 0.06432289321210116, 0.06834307403785748, 0.06030271238634484, 0.06834307403785748, 0.07236325486361381, 0.06432289321210116, 0.07236325486361381, 0.06432289321210116, 0.04422198908331955, 0.052262350734832194, 0.04422198908331955, 0.04422198908331955, 0.040201808257563226, 0.036181627431806904, 0.040201808257563226, 0.03216144660605058, 0.03216144660605058, 0.03216144660605058, 0.036181627431806904, 0.02814126578029426, 0.0775422026923411, 0.07346103412958631, 0.06937986556683151, 0.08570453981785069, 0.06529869700407671, 0.06529869700407671, 0.057136359878567126, 0.053055191315812336, 0.04897402275305754, 0.04081168562754795, 0.04081168562754795, 0.04081168562754795, 0.036730517064793154, 0.04489285419030274, 0.036730517064793154, 0.03264934850203836, 0.036730517064793154, 0.03264934850203836, 0.028568179939283563, 0.028568179939283563, 0.06753608540407917, 0.05909407472856928, 0.06331508006632422, 0.05909407472856928, 0.06331508006632422, 0.06753608540407917, 0.06331508006632422, 0.05487306939081433, 0.04643105871530443, 0.05065206405305938, 0.04643105871530443, 0.04221005337754948, 0.05065206405305938, 0.04221005337754948, 0.04221005337754948, 0.04643105871530443, 0.033768042702039584, 0.03798904803979453, 0.033768042702039584, 0.02954703736428464, 0.06051783231536055, 0.0927940095502195, 0.0685868766240753, 0.06455235446971792, 0.06455235446971792, 0.06051783231536055, 0.06051783231536055, 0.05244878800664581, 0.05648331016100318, 0.04437974369793107, 0.04841426585228844, 0.04841426585228844, 0.03631069938921633, 0.04437974369793107, 0.04437974369793107, 0.03631069938921633, 0.02824165508050159, 0.03227617723485896, 0.02824165508050159, 0.02420713292614422, 0.06586008720929291, 0.061743831758712106, 0.061743831758712106, 0.06586008720929291, 0.06997634265987372, 0.06997634265987372, 0.061743831758712106, 0.05351132085755049, 0.04939506540696968, 0.05351132085755049, 0.04939506540696968, 0.045278809956388874, 0.045278809956388874, 0.045278809956388874, 0.04116255450580807, 0.02881378815406565, 0.02881378815406565, 0.03293004360464646, 0.02881378815406565, 0.04116255450580807, 0.08087033542720802, 0.06469626834176641, 0.06065275157040601, 0.06065275157040601, 0.06065275157040601, 0.06469626834176641, 0.06065275157040601, 0.056609234799045616, 0.04447868448496441, 0.04447868448496441, 0.04852220125632481, 0.06065275157040601, 0.04043516771360401, 0.04447868448496441, 0.04043516771360401, 0.032348134170883204, 0.032348134170883204, 0.03639165094224361, 0.028304617399522808, 0.032348134170883204, 0.06101236227777028, 0.07844446578570465, 0.06101236227777028, 0.06972841403173746, 0.09151854341665543, 0.0522963105238031, 0.06537038815475388, 0.047938284646819505, 0.047938284646819505, 0.043580258769835914, 0.047938284646819505, 0.047938284646819505, 0.047938284646819505, 0.03486420701586873, 0.03922223289285232, 0.03486420701586873, 0.03922223289285232, 0.03050618113888514, 0.03486420701586873, 0.021790129384917957, 0.07681589086201733, 0.06468706598906723, 0.06873000761338394, 0.06468706598906723, 0.07277294923770063, 0.06468706598906723, 0.056601182740433825, 0.056601182740433825, 0.048515299491800425, 0.056601182740433825, 0.06064412436475053, 0.03638647461885031, 0.04042941624316702, 0.04042941624316702, 0.03638647461885031, 0.032343532994533616, 0.032343532994533616, 0.03638647461885031, 0.03638647461885031, 0.024257649745900212, 0.05718331507499773, 0.05309879256964075, 0.06535236008571169, 0.06535236008571169, 0.07352140509642566, 0.061267837580354714, 0.06535236008571169, 0.04901427006428377, 0.05718331507499773, 0.05309879256964075, 0.061267837580354714, 0.05718331507499773, 0.03676070254821283, 0.04084522505356981, 0.03676070254821283, 0.03267618004285584, 0.03676070254821283, 0.03267618004285584, 0.03676070254821283, 0.024507135032141886, 0.07380606531248976, 0.06970572835068477, 0.07790640227429475, 0.05740471746526982, 0.05740471746526982, 0.05740471746526982, 0.0615050544270748, 0.04510370657985485, 0.04510370657985485, 0.05740471746526982, 0.04510370657985485, 0.04920404354165984, 0.041003369618049866, 0.041003369618049866, 0.04510370657985485, 0.032802695694439894, 0.032802695694439894, 0.032802695694439894, 0.041003369618049866, 0.020501684809024933, 0.07124554144387057, 0.058672798836128706, 0.08381828405161244, 0.06705462724128995, 0.06705462724128995, 0.06286371303870933, 0.0754364556464512, 0.04610005622838684, 0.05029097043096746, 0.04610005622838684, 0.04610005622838684, 0.04610005622838684, 0.0377182278232256, 0.04190914202580622, 0.0377182278232256, 0.0377182278232256, 0.0377182278232256, 0.029336399418064353, 0.029336399418064353, 0.029336399418064353, 0.05401817347920224, 0.06232866170677182, 0.08310488227569576, 0.07894963816191097, 0.06232866170677182, 0.06232866170677182, 0.05401817347920224, 0.05401817347920224, 0.04155244113784788, 0.04986292936541746, 0.04570768525163267, 0.04570768525163267, 0.04155244113784788, 0.04155244113784788, 0.04155244113784788, 0.04570768525163267, 0.03739719702406309, 0.03739719702406309, 0.0332419529102783, 0.029086708796493516, 0.0789565843106372, 0.0623341455083978, 0.05402292610727809, 0.0623341455083978, 0.07480097461007736, 0.0706453649095175, 0.05402292610727809, 0.0623341455083978, 0.04986731640671824, 0.05402292610727809, 0.045711706706158386, 0.045711706706158386, 0.04986731640671824, 0.04155609700559853, 0.04155609700559853, 0.04155609700559853, 0.02908926790391897, 0.03324487760447883, 0.02493365820335912, 0.02493365820335912, 0.06902962677847528, 0.0649690604973885, 0.0852718919028224, 0.0649690604973885, 0.06902962677847528, 0.06090849421630171, 0.05684792793521493, 0.05684792793521493, 0.04872679537304137, 0.04466622909195459, 0.04872679537304137, 0.04872679537304137, 0.04466622909195459, 0.03654509652978103, 0.03654509652978103, 0.03248453024869425, 0.03248453024869425, 0.03248453024869425, 0.03248453024869425, 0.03248453024869425, 0.07093513443965266, 0.07093513443965266, 0.06258982450557588, 0.06258982450557588, 0.07093513443965266, 0.06258982450557588, 0.058417169538537476, 0.058417169538537476, 0.0500718596044607, 0.04589920463742231, 0.06258982450557588, 0.04589920463742231, 0.037553894703345525, 0.04172654967038392, 0.04172654967038392, 0.03338123973630713, 0.029208584769268738, 0.03338123973630713, 0.03338123973630713, 0.02503592980223035, 0.06282027425468528, 0.06282027425468528, 0.06700829253833097, 0.06700829253833097, 0.07538432910562234, 0.07119631082197665, 0.05444423768739391, 0.05025621940374823, 0.06282027425468528, 0.04606820112010254, 0.04606820112010254, 0.04606820112010254, 0.04188018283645686, 0.03769216455281117, 0.03769216455281117, 0.03769216455281117, 0.03350414626916549, 0.03350414626916549, 0.03350414626916549, 0.0293161279855198, 0.07269084141659733, 0.08884436173139675, 0.06865246133789749, 0.06865246133789749, 0.06461408125919763, 0.06461408125919763, 0.056537321101797926, 0.048460560944398226, 0.048460560944398226, 0.05249894102309807, 0.04442218086569837, 0.04442218086569837, 0.04038380078699852, 0.04038380078699852, 0.036345420708298666, 0.036345420708298666, 0.028268660550898963, 0.03230704062959881, 0.03230704062959881, 0.028268660550898963, 0.08628233825391957, 0.05752155883594638, 0.05752155883594638, 0.06163024160994255, 0.07806497270592723, 0.05752155883594638, 0.06163024160994255, 0.05752155883594638, 0.04930419328795404, 0.053412876061950214, 0.04930419328795404, 0.05752155883594638, 0.04519551051395787, 0.03286946219196936, 0.036978144965965534, 0.036978144965965534, 0.02876077941797319, 0.03286946219196936, 0.03286946219196936, 0.02876077941797319, 0.06742579096263457, 0.07163990289779923, 0.07163990289779923, 0.05478345515714059, 0.06321167902746991, 0.05478345515714059, 0.06321167902746991, 0.05478345515714059, 0.05478345515714059, 0.050569343221975926, 0.05478345515714059, 0.05478345515714059, 0.042141119351646605, 0.037927007416481945, 0.033712895481317284, 0.029498783546152624, 0.033712895481317284, 0.033712895481317284, 0.042141119351646605, 0.029498783546152624, 0.09662243719429355, 0.0714165840131735, 0.05881365742261346, 0.0714165840131735, 0.05881365742261346, 0.06301463295280015, 0.06301463295280015, 0.05461268189242679, 0.042009755301866764, 0.04621073083205344, 0.042009755301866764, 0.05461268189242679, 0.03780877977168008, 0.04621073083205344, 0.03780877977168008, 0.03360780424149341, 0.03780877977168008, 0.03360780424149341, 0.02940682871130673, 0.025205853181120057, 0.06500307871495015, 0.06906577113463454, 0.06500307871495015, 0.06500307871495015, 0.06906577113463454, 0.056877693875581385, 0.056877693875581385, 0.056877693875581385, 0.04875230903621262, 0.052815001455897, 0.04468961661652823, 0.052815001455897, 0.04875230903621262, 0.03656423177715946, 0.04468961661652823, 0.028438846937790693, 0.04468961661652823, 0.03250153935747507, 0.03656423177715946, 0.028438846937790693, 0.07472243661756918, 0.05899139732965988, 0.05899139732965988, 0.08258795626152382, 0.07472243661756918, 0.05505863750768255, 0.05112587768570522, 0.05112587768570522, 0.05505863750768255, 0.06685691697361452, 0.05112587768570522, 0.043260358041750574, 0.03932759821977325, 0.03932759821977325, 0.035394838397795926, 0.035394838397795926, 0.0314620785758186, 0.035394838397795926, 0.0314620785758186, 0.0314620785758186, 0.0801256162212225, 0.06747420313366105, 0.06325706543780724, 0.0801256162212225, 0.06325706543780724, 0.059039927741953414, 0.059039927741953414, 0.059039927741953414, 0.050605652350245785, 0.04638851465439197, 0.050605652350245785, 0.04638851465439197, 0.03795423926268434, 0.03795423926268434, 0.03795423926268434, 0.029519963870976707, 0.033737101566830525, 0.042171376958538155, 0.029519963870976707, 0.029519963870976707, 0.07022048092393594, 0.054615929607505734, 0.07412161875304349, 0.06631934309482838, 0.058517067436613286, 0.06241820526572084, 0.06631934309482838, 0.04681365394929063, 0.04681365394929063, 0.054615929607505734, 0.04681365394929063, 0.054615929607505734, 0.039011378291075524, 0.04291251612018308, 0.04291251612018308, 0.039011378291075524, 0.04291251612018308, 0.03120910263286042, 0.03120910263286042, 0.027307964803752867, 0.0728394549295101, 0.07688609131448289, 0.06069954577459175, 0.07688609131448289, 0.06879281854453732, 0.06069954577459175, 0.06069954577459175, 0.04451300023470062, 0.0485596366196734, 0.05260627300464618, 0.0485596366196734, 0.0485596366196734, 0.04451300023470062, 0.05260627300464618, 0.04451300023470062, 0.028326454694809485, 0.028326454694809485, 0.028326454694809485, 0.028326454694809485, 0.028326454694809485, 0.06784061602169947, 0.08380311390915816, 0.05586874260610544, 0.07582186496542882, 0.07582186496542882, 0.05586874260610544, 0.06784061602169947, 0.05586874260610544, 0.047887493662376095, 0.047887493662376095, 0.047887493662376095, 0.03990624471864675, 0.04389686919051142, 0.03591562024678207, 0.03591562024678207, 0.0319249957749174, 0.03591562024678207, 0.0319249957749174, 0.0319249957749174, 0.02793437130305272, 0.06386512800430375, 0.06386512800430375, 0.06785669850457272, 0.06386512800430375, 0.0758398395051107, 0.051890416503496795, 0.05987355750403476, 0.06386512800430375, 0.04789884600322781, 0.04390727550295882, 0.05987355750403476, 0.051890416503496795, 0.039915705002689844, 0.039915705002689844, 0.039915705002689844, 0.03193256400215187, 0.03193256400215187, 0.02794099350188289, 0.03193256400215187, 0.03592413450242086, 0.07172479647890376, 0.05977066373241979, 0.07172479647890376, 0.06375537464791445, 0.0677400855634091, 0.055785952816925145, 0.05977066373241979, 0.05180124190143049, 0.05180124190143049, 0.05180124190143049, 0.05180124190143049, 0.047816530985935836, 0.047816530985935836, 0.03984710915494653, 0.03586239823945188, 0.03586239823945188, 0.04383182007044118, 0.031877687323957224, 0.027892976408462573, 0.027892976408462573, 0.06390389593927126, 0.07668467512712551, 0.06390389593927126, 0.06816415566855602, 0.06390389593927126, 0.059643636209986514, 0.06390389593927126, 0.051123116751417015, 0.04260259729284751, 0.06390389593927126, 0.04686285702213226, 0.04686285702213226, 0.04686285702213226, 0.04260259729284751, 0.04260259729284751, 0.03408207783427801, 0.038342337563562756, 0.029821818104993257, 0.03408207783427801, 0.029821818104993257, 0.05555686484911582, 0.09127199225211884, 0.06746190731678349, 0.07143025480600605, 0.07143025480600605, 0.05952521233833837, 0.05952521233833837, 0.05555686484911582, 0.05555686484911582, 0.051588517359893254, 0.051588517359893254, 0.0476201698706707, 0.03968347489222558, 0.043651822381448144, 0.03968347489222558, 0.02777843242455791, 0.031746779913780464, 0.02777843242455791, 0.02777843242455791, 0.031746779913780464, 0.0799068471165355, 0.06728997651918779, 0.07149560005163703, 0.06308435298673856, 0.05887872945428932, 0.05467310592184008, 0.05887872945428932, 0.06728997651918779, 0.04626185885694161, 0.05046748238939085, 0.042056235324492375, 0.042056235324492375, 0.03785061179204314, 0.042056235324492375, 0.042056235324492375, 0.033644988259593896, 0.03785061179204314, 0.033644988259593896, 0.03785061179204314, 0.02943936472714466, 0.06712415440045386, 0.05873363510039712, 0.08810045265059568, 0.06712415440045386, 0.06292889475042548, 0.06292889475042548, 0.05034311580034039, 0.05873363510039712, 0.041952596500283656, 0.05034311580034039, 0.04614785615031203, 0.054538375450368756, 0.041952596500283656, 0.041952596500283656, 0.03775733685025529, 0.03356207720022693, 0.03356207720022693, 0.02936681755019856, 0.03356207720022693, 0.02936681755019856, 0.06482754319228637, 0.07698270754084006, 0.07698270754084006, 0.06887926464180427, 0.06077582174276847, 0.05672410029325057, 0.05672410029325057, 0.06887926464180427, 0.04862065739421478, 0.04862065739421478, 0.04456893594469688, 0.05672410029325057, 0.04456893594469688, 0.04051721449517898, 0.03646549304566108, 0.02431032869710739, 0.032413771596143184, 0.032413771596143184, 0.032413771596143184, 0.028362050146625285, 0.06800584811072903, 0.0722562136176496, 0.0722562136176496, 0.06800584811072903, 0.059505117096887904, 0.05100438608304677, 0.06800584811072903, 0.05100438608304677, 0.05100438608304677, 0.04675402057612621, 0.04675402057612621, 0.04675402057612621, 0.04250365506920564, 0.03825328956228508, 0.03825328956228508, 0.034002924055364515, 0.04675402057612621, 0.04250365506920564, 0.029752558548443952, 0.029752558548443952, 0.07286263986949083, 0.09000679042701808, 0.0642905645907272, 0.07286263986949083, 0.06857660223010902, 0.055718489311963576, 0.055718489311963576, 0.05143245167258176, 0.05143245167258176, 0.04714641403319995, 0.042860376393818136, 0.04714641403319995, 0.03428830111505451, 0.04714641403319995, 0.03428830111505451, 0.03857433875443632, 0.030002263475672693, 0.03428830111505451, 0.030002263475672693, 0.02571622583629088, 0.07015534830935093, 0.07015534830935093, 0.06190177792001552, 0.06190177792001552, 0.06190177792001552, 0.06602856311468322, 0.06602856311468322, 0.05364820753068012, 0.04952142233601242, 0.05364820753068012, 0.045394637141344715, 0.05777499272534782, 0.045394637141344715, 0.037141066752009316, 0.037141066752009316, 0.037141066752009316, 0.03301428155734161, 0.03301428155734161, 0.02476071116800621, 0.02476071116800621, 0.06565353765542853, 0.05744684544849996, 0.06975688375889282, 0.05744684544849996, 0.06565353765542853, 0.06975688375889282, 0.07386022986235709, 0.0492401532415714, 0.053343499345035676, 0.036930114931178544, 0.0492401532415714, 0.04513680713810711, 0.04513680713810711, 0.04103346103464283, 0.04103346103464283, 0.032826768827714264, 0.036930114931178544, 0.036930114931178544, 0.036930114931178544, 0.0246200766207857, 0.06643057935488442, 0.07888631298392525, 0.06643057935488442, 0.05812675693552387, 0.06643057935488442, 0.04982293451616331, 0.05397484572584359, 0.05397484572584359, 0.04982293451616331, 0.04982293451616331, 0.04567102330648304, 0.041519112096802764, 0.05397484572584359, 0.05397484572584359, 0.04567102330648304, 0.03321528967744221, 0.029063378467761934, 0.03736720088712248, 0.03321528967744221, 0.029063378467761934, 0.07326779441946032, 0.06512692837285362, 0.07733822744276367, 0.06919736139615697, 0.061056495349550266, 0.07733822744276367, 0.048845196279640214, 0.052915629302943565, 0.04070433023303351, 0.048845196279640214, 0.048845196279640214, 0.04070433023303351, 0.044774763256336864, 0.04070433023303351, 0.03663389720973016, 0.04070433023303351, 0.03256346418642681, 0.03256346418642681, 0.028493031163123458, 0.03256346418642681, 0.07805947168224169, 0.06573429194294036, 0.06162589869650659, 0.07805947168224169, 0.06984268518937413, 0.07395107843580791, 0.053409112203639045, 0.049300718957205274, 0.049300718957205274, 0.049300718957205274, 0.0451923257107715, 0.041083932464337725, 0.0451923257107715, 0.041083932464337725, 0.036975539217903954, 0.03286714597147018, 0.028758752725036408, 0.0451923257107715, 0.03286714597147018, 0.024650359478602637, 0.06263209712807302, 0.07098304341181609, 0.07515851655368763, 0.09186040912117377, 0.07515851655368763, 0.06680757026994456, 0.05428115084432995, 0.04175473141871535, 0.05010567770245842, 0.045930204560586885, 0.05010567770245842, 0.045930204560586885, 0.037579258276843816, 0.03340378513497228, 0.04175473141871535, 0.03340378513497228, 0.03340378513497228, 0.029228311993100744, 0.03340378513497228, 0.029228311993100744, 0.06581362838643663, 0.0781536837088935, 0.07404033193474122, 0.06581362838643663, 0.06992698016058893, 0.0822670354830458, 0.05347357306397977, 0.05347357306397977, 0.04936022128982748, 0.045246869515675185, 0.045246869515675185, 0.05347357306397977, 0.03702016596737061, 0.03702016596737061, 0.032906814193218316, 0.028793462419066027, 0.03702016596737061, 0.032906814193218316, 0.032906814193218316, 0.02468011064491374, 0.06832920798909257, 0.07636793834075052, 0.06832920798909257, 0.0643098428132636, 0.06832920798909257, 0.06832920798909257, 0.052251747285776674, 0.052251747285776674, 0.04421301693411872, 0.05627111246160565, 0.048232382109947694, 0.06029047763743462, 0.04421301693411872, 0.04019365175828975, 0.036174286582460774, 0.036174286582460774, 0.0321549214066318, 0.028135556230802824, 0.028135556230802824, 0.024116191054973847, 0.06899161851797289, 0.0819275469900928, 0.06899161851797289, 0.06467964236059959, 0.06899161851797289, 0.060367666203226276, 0.060367666203226276, 0.04743173773110636, 0.04743173773110636, 0.05174371388847966, 0.043119761573733054, 0.04743173773110636, 0.043119761573733054, 0.034495809258986446, 0.03880778541635975, 0.030183833101613138, 0.04743173773110636, 0.034495809258986446, 0.030183833101613138, 0.02587185694423983, 0.0768263730678944, 0.06469589310980581, 0.0768263730678944, 0.06873938642916867, 0.06065239979044294, 0.06469589310980581, 0.056608906471080085, 0.05256541315171722, 0.048521919832354356, 0.048521919832354356, 0.06065239979044294, 0.04447842651299149, 0.04447842651299149, 0.04043493319362863, 0.04043493319362863, 0.03639143987426577, 0.024260959916177178, 0.024260959916177178, 0.032347946554902904, 0.032347946554902904, 0.06735406874128137, 0.07131607278488616, 0.08320208491570052, 0.06339206469767658, 0.06339206469767658, 0.06339206469767658, 0.06735406874128137, 0.04358204447965265, 0.05546805661046701, 0.05150605256686223, 0.04754404852325744, 0.04358204447965265, 0.03962004043604787, 0.03962004043604787, 0.03565803639244308, 0.03565803639244308, 0.03962004043604787, 0.03169603234883829, 0.027734028305233506, 0.027734028305233506, 0.07404755424896863, 0.07015031455165449, 0.08573927334091104, 0.06235583515702621, 0.07404755424896863, 0.06625307485434036, 0.054561355762397935, 0.054561355762397935, 0.050664116065083795, 0.04286963667045552, 0.035075157275827246, 0.04286963667045552, 0.04676687636776966, 0.031177917578513105, 0.04676687636776966, 0.038972396973141386, 0.035075157275827246, 0.027280677881198968, 0.031177917578513105, 0.031177917578513105, 0.06663899674560599, 0.06663899674560599, 0.07080393404220636, 0.07080393404220636, 0.07080393404220636, 0.062474059449005616, 0.06663899674560599, 0.05414418485580487, 0.04997924755920449, 0.04997924755920449, 0.041649372966003746, 0.045814310262604116, 0.045814310262604116, 0.041649372966003746, 0.03748443566940337, 0.02915456107620262, 0.02915456107620262, 0.041649372966003746, 0.02915456107620262, 0.02915456107620262, 0.08070714809180753, 0.07263643328262677, 0.07263643328262677, 0.06456571847344601, 0.0847425054963979, 0.06053036106885564, 0.05245964625967489, 0.05245964625967489, 0.05245964625967489, 0.044388931450494135, 0.04035357404590376, 0.04035357404590376, 0.044388931450494135, 0.04035357404590376, 0.032282859236723006, 0.036318216641313385, 0.032282859236723006, 0.036318216641313385, 0.024212144427542256, 0.032282859236723006, 0.06665432990349743, 0.06665432990349743, 0.06665432990349743, 0.0749861211414346, 0.06248843428452884, 0.05415664304659166, 0.05832253866556025, 0.04999074742762307, 0.04999074742762307, 0.04999074742762307, 0.04582485180865448, 0.05415664304659166, 0.04999074742762307, 0.05415664304659166, 0.0374930605707173, 0.0374930605707173, 0.033327164951748714, 0.033327164951748714, 0.029161269332780126, 0.024995373713811535, 0.07102571850285935, 0.08773765226823803, 0.06684773506151469, 0.07102571850285935, 0.06684773506151469, 0.06266975162017002, 0.06266975162017002, 0.04595781785479135, 0.04595781785479135, 0.04177983441344668, 0.04177983441344668, 0.054313784737480685, 0.04177983441344668, 0.04177983441344668, 0.04177983441344668, 0.029245884089412675, 0.033423867530757344, 0.033423867530757344, 0.033423867530757344, 0.029245884089412675, 0.06755963043700658, 0.05911467663238076, 0.07178210733931949, 0.06333715353469367, 0.06755963043700658, 0.08022706114394532, 0.06333715353469367, 0.050669722827754934, 0.05489219973006784, 0.05489219973006784, 0.050669722827754934, 0.04644724592544202, 0.04222476902312911, 0.0380022921208162, 0.0380022921208162, 0.0380022921208162, 0.02955733831619038, 0.02955733831619038, 0.02955733831619038, 0.025334861413877467, 0.06431412243139212, 0.07288933875557774, 0.06431412243139212, 0.06860173059348493, 0.06860173059348493, 0.07288933875557774, 0.060026514269299315, 0.051451297945113696, 0.051451297945113696, 0.04287608162092808, 0.047163689783020886, 0.060026514269299315, 0.034300865296742464, 0.04287608162092808, 0.03858847345883527, 0.034300865296742464, 0.034300865296742464, 0.030013257134649658, 0.034300865296742464, 0.034300865296742464, 0.07395242032960034, 0.06984395253351143, 0.07806088812568925, 0.06984395253351143, 0.06573548473742252, 0.04930161355306689, 0.06573548473742252, 0.04930161355306689, 0.045193145756977984, 0.045193145756977984, 0.0534100813491558, 0.04108467796088908, 0.045193145756977984, 0.045193145756977984, 0.03697621016480017, 0.045193145756977984, 0.03697621016480017, 0.028759274572622354, 0.03286774236871126, 0.024650806776533446, 0.06576176982156208, 0.0698718804354097, 0.07398199104925735, 0.05343143798001919, 0.06576176982156208, 0.06576176982156208, 0.05754154859386682, 0.0698718804354097, 0.04521121675232393, 0.049321327366171556, 0.049321327366171556, 0.049321327366171556, 0.0411011061384763, 0.04521121675232393, 0.0411011061384763, 0.03288088491078104, 0.03288088491078104, 0.03288088491078104, 0.02877077429693341, 0.02877077429693341, 0.06535748160067437, 0.06535748160067437, 0.07761200940080082, 0.07761200940080082, 0.06944232420071653, 0.06127263900063223, 0.05310295380054793, 0.057187796400590074, 0.04493326860046363, 0.04901811120050578, 0.05310295380054793, 0.04901811120050578, 0.04084842600042148, 0.036763583400379336, 0.03267874080033718, 0.036763583400379336, 0.028593898200295037, 0.03267874080033718, 0.03267874080033718, 0.036763583400379336, 0.08202929737684667, 0.061521973032635, 0.06972490277031966, 0.061521973032635, 0.06562343790147733, 0.061521973032635, 0.061521973032635, 0.05331904329495033, 0.061521973032635, 0.049217578426108, 0.05331904329495033, 0.045116113557265664, 0.036913183819581, 0.032811718950738665, 0.036913183819581, 0.036913183819581, 0.036913183819581, 0.032811718950738665, 0.028710254081896334, 0.028710254081896334, 0.0770131049295772, 0.06890646230541118, 0.06079981968124517, 0.06890646230541118, 0.06890646230541118, 0.05674649836916216, 0.05674649836916216, 0.06890646230541118, 0.05674649836916216, 0.044586534432913126, 0.044586534432913126, 0.05269317705707915, 0.044586534432913126, 0.04053321312083011, 0.04053321312083011, 0.0364798918087471, 0.0364798918087471, 0.03242657049666409, 0.02431992787249807, 0.02431992787249807, 0.07693385727234284, 0.056688105358568414, 0.06883555650683307, 0.0607372557413233, 0.06883555650683307, 0.0607372557413233, 0.056688105358568414, 0.05263895497581352, 0.04858980459305864, 0.04858980459305864, 0.0607372557413233, 0.04858980459305864, 0.04454065421030375, 0.04858980459305864, 0.04454065421030375, 0.03239320306203909, 0.03239320306203909, 0.028344052679284207, 0.028344052679284207, 0.028344052679284207, 0.06377312053082523, 0.06775894056400182, 0.06775894056400182, 0.0876880407298847, 0.07573058063035497, 0.05580148046447208, 0.05580148046447208, 0.06377312053082523, 0.047829840398118925, 0.05580148046447208, 0.04384402036494235, 0.051815660431295506, 0.0358723802985892, 0.03985820033176577, 0.03188656026541262, 0.03188656026541262, 0.0358723802985892, 0.03188656026541262, 0.02790074023223604, 0.03188656026541262, 0.07227332953255998, 0.07227332953255998, 0.07227332953255998, 0.06825814455852888, 0.06825814455852888, 0.06424295958449777, 0.060227774610466654, 0.05219740466240443, 0.04416703471434221, 0.05621258963643554, 0.04818221968837332, 0.0401518497403111, 0.0401518497403111, 0.03613666476627999, 0.03613666476627999, 0.032121479792248886, 0.03613666476627999, 0.032121479792248886, 0.02810629481821777, 0.03613666476627999, 0.08770945714086639, 0.07176228311525432, 0.06378869610244828, 0.06378869610244828, 0.06378869610244828, 0.06378869610244828, 0.07176228311525432, 0.043854728570433196, 0.04784152207683621, 0.04784152207683621, 0.04784152207683621, 0.043854728570433196, 0.04784152207683621, 0.03189434805122414, 0.043854728570433196, 0.03189434805122414, 0.03986793506403018, 0.027907554544821123, 0.027907554544821123, 0.027907554544821123, 0.07393708990405817, 0.06982947379827716, 0.06572185769249615, 0.06572185769249615, 0.06982947379827716, 0.0821523221156202, 0.06161424158671514, 0.05339900937515312, 0.0410761610578101, 0.04929139326937211, 0.04518377716359111, 0.04929139326937211, 0.04518377716359111, 0.0410761610578101, 0.032860928846248076, 0.032860928846248076, 0.036968544952029087, 0.032860928846248076, 0.032860928846248076, 0.02053808052890505, 0.07409701784807902, 0.05763101388183924, 0.06998051685651908, 0.07409701784807902, 0.06998051685651908, 0.05763101388183924, 0.06174751487339919, 0.04528151090715941, 0.053514512890279295, 0.06586401586495913, 0.049398011898719354, 0.049398011898719354, 0.03704850892403951, 0.04528151090715941, 0.032932007932479565, 0.02881550694091962, 0.04116500991559946, 0.03704850892403951, 0.032932007932479565, 0.024699005949359677, 0.06599545918529422, 0.07012017538437511, 0.061870742986213326, 0.08661904018069866, 0.061870742986213326, 0.06599545918529422, 0.061870742986213326, 0.045371878189889775, 0.04949659438897066, 0.05362131058805155, 0.05362131058805155, 0.045371878189889775, 0.037122445791728, 0.041247161990808884, 0.045371878189889775, 0.03299772959264711, 0.03299772959264711, 0.02887301339356622, 0.02887301339356622, 0.02887301339356622, 0.06789241862344082, 0.06364914245947578, 0.07213569478740588, 0.05940586629551073, 0.08910879944326609, 0.055162590131545675, 0.05091931396758062, 0.04667603780361557, 0.04667603780361557, 0.055162590131545675, 0.04243276163965052, 0.04667603780361557, 0.05091931396758062, 0.04243276163965052, 0.04243276163965052, 0.029702933147755364, 0.03394620931172041, 0.029702933147755364, 0.038189485475685465, 0.03394620931172041, 0.06466646081043592, 0.06062480700978368, 0.06466646081043592, 0.06062480700978368, 0.06870811461108817, 0.07679142221239266, 0.07679142221239266, 0.052541499408479186, 0.04041653800652245, 0.04849984560782694, 0.05658315320913143, 0.044458191807174696, 0.04849984560782694, 0.04041653800652245, 0.04041653800652245, 0.03233323040521796, 0.03233323040521796, 0.036374884205870206, 0.028291576604565716, 0.02424992280391347, 0.06010385029886541, 0.0681176970053808, 0.0681176970053808, 0.08013846706515389, 0.0681176970053808, 0.06411077365212312, 0.06411077365212312, 0.05609692694560772, 0.05609692694560772, 0.04407615688583464, 0.04808308023909233, 0.06010385029886541, 0.04407615688583464, 0.03606231017931925, 0.03606231017931925, 0.03205538682606156, 0.03205538682606156, 0.03205538682606156, 0.02804846347280386, 0.02804846347280386, 0.06661039353558187, 0.06661039353558187, 0.062447243939608016, 0.058284094343634144, 0.06661039353558187, 0.07077354313155575, 0.05412094474766028, 0.05412094474766028, 0.058284094343634144, 0.04579464555571254, 0.04579464555571254, 0.058284094343634144, 0.03746834636376481, 0.04579464555571254, 0.04579464555571254, 0.03330519676779094, 0.03330519676779094, 0.03330519676779094, 0.03330519676779094, 0.024978897575843206, 0.06542049966001037, 0.05315415597375842, 0.07359806211751166, 0.08995318703251426, 0.07768684334626232, 0.05724293720250907, 0.05724293720250907, 0.04906537474500778, 0.04906537474500778, 0.05315415597375842, 0.04088781228750648, 0.05315415597375842, 0.04906537474500778, 0.04088781228750648, 0.04088781228750648, 0.032710249830005186, 0.032710249830005186, 0.03679903105875583, 0.028621468601254536, 0.028621468601254536, 0.07263721212859209, 0.06860181145478142, 0.06860181145478142, 0.06456641078097075, 0.06456641078097075, 0.0564956094333494, 0.060531010107160076, 0.05246020875953873, 0.05246020875953873, 0.048424808085728056, 0.048424808085728056, 0.048424808085728056, 0.04438940741191739, 0.05246020875953873, 0.036318606064296044, 0.036318606064296044, 0.0282478047166747, 0.032283205390485376, 0.032283205390485376, 0.0282478047166747, 0.06836067081872711, 0.06836067081872711, 0.06836067081872711, 0.07238188674924048, 0.0603182389577004, 0.0603182389577004, 0.0603182389577004, 0.05629702302718704, 0.04825459116616032, 0.04825459116616032, 0.04825459116616032, 0.04423337523564696, 0.05227580709667368, 0.040212159305133596, 0.04423337523564696, 0.03619094337462024, 0.03619094337462024, 0.03216972744410688, 0.02814851151359352, 0.02814851151359352, 0.06630668609410481, 0.07873918973674945, 0.06630668609410481, 0.06216251821322326, 0.07045085397498636, 0.053874182451460154, 0.06216251821322326, 0.04973001457057861, 0.04973001457057861, 0.053874182451460154, 0.0414416788088155, 0.045585846689697056, 0.03729751092793396, 0.04973001457057861, 0.0414416788088155, 0.03729751092793396, 0.04973001457057861, 0.024865007285289304, 0.033153343047052405, 0.02072083940440775, 0.06739627389505282, 0.05897173965817122, 0.06739627389505282, 0.06739627389505282, 0.05054720542128962, 0.06318400677661203, 0.05475947253973042, 0.06739627389505282, 0.046334938302848815, 0.05475947253973042, 0.05475947253973042, 0.05054720542128962, 0.046334938302848815, 0.046334938302848815, 0.03369813694752641, 0.04212267118440802, 0.03369813694752641, 0.03369813694752641, 0.03369813694752641, 0.02948586982908561, 0.06703570190244976, 0.07097897848494679, 0.07097897848494679, 0.059149148737455665, 0.06703570190244976, 0.06703570190244976, 0.059149148737455665, 0.059149148737455665, 0.05126259557246157, 0.04731931898996453, 0.05126259557246157, 0.04731931898996453, 0.04337604240746749, 0.05126259557246157, 0.03943276582497044, 0.02760293607747931, 0.031546212659976355, 0.031546212659976355, 0.02760293607747931, 0.02760293607747931, 0.07536892659950675, 0.0714021409890064, 0.0714021409890064, 0.06346856976800569, 0.06346856976800569, 0.06346856976800569, 0.05553499854700497, 0.05553499854700497, 0.05156821293650462, 0.05156821293650462, 0.04760142732600426, 0.04363464171550391, 0.03966785610500355, 0.03966785610500355, 0.03966785610500355, 0.0357010704945032, 0.0357010704945032, 0.0357010704945032, 0.027767499273502486, 0.0357010704945032, 0.06770339932555389, 0.06770339932555389, 0.06770339932555389, 0.059240474409859654, 0.06347193686770677, 0.08039778669909524, 0.06347193686770677, 0.050777549494165415, 0.046546087036318295, 0.050777549494165415, 0.04231462457847118, 0.050777549494165415, 0.03808316212062406, 0.050777549494165415, 0.03808316212062406, 0.029620237204929827, 0.03808316212062406, 0.029620237204929827, 0.03385169966277694, 0.029620237204929827, 0.08050539867436339, 0.06842958887320888, 0.06440431893949071, 0.060379049005772546, 0.08453066860808156, 0.056353779072054376, 0.048303239204618036, 0.044277969270899865, 0.060379049005772546, 0.044277969270899865, 0.044277969270899865, 0.052328509138336206, 0.040252699337181695, 0.040252699337181695, 0.040252699337181695, 0.036227429403463525, 0.036227429403463525, 0.036227429403463525, 0.032202159469745355, 0.028176889536027188, 0.07098143356484106, 0.05915119463736755, 0.06703802058901656, 0.06309460761319205, 0.08281167249231457, 0.06309460761319205, 0.07492484654066556, 0.05520778166154305, 0.05126436868571854, 0.04337754273406954, 0.04337754273406954, 0.04732095570989404, 0.039434129758245035, 0.039434129758245035, 0.04337754273406954, 0.03549071678242053, 0.027603890830771523, 0.031547303806596025, 0.03549071678242053, 0.027603890830771523, 0.07216790180457118, 0.07641307249895772, 0.0636775604157981, 0.0636775604157981, 0.0636775604157981, 0.055187219027025024, 0.055187219027025024, 0.059432389721411565, 0.0424517069438654, 0.05094204833263848, 0.05094204833263848, 0.0424517069438654, 0.0424517069438654, 0.05094204833263848, 0.0424517069438654, 0.03396136555509232, 0.029716194860705782, 0.03396136555509232, 0.03396136555509232, 0.029716194860705782, 0.07050177103347084, 0.0622074450295331, 0.058060282027564226, 0.0622074450295331, 0.0622074450295331, 0.0622074450295331, 0.07464893403543972, 0.049765956023626476, 0.04147163001968873, 0.053913119025595355, 0.045618793021657604, 0.045618793021657604, 0.045618793021657604, 0.053913119025595355, 0.04147163001968873, 0.03732446701771986, 0.03732446701771986, 0.03732446701771986, 0.03317730401575099, 0.029030141013782113, 0.07438504245790796, 0.06612003774036262, 0.061987535381589964, 0.06612003774036262, 0.057855033022817304, 0.061987535381589964, 0.053722530664044636, 0.053722530664044636, 0.0702525400991353, 0.053722530664044636, 0.04545752594649931, 0.049590028305271976, 0.04132502358772665, 0.03306001887018131, 0.04132502358772665, 0.03719252122895398, 0.03719252122895398, 0.04132502358772665, 0.024795014152635988, 0.028927516511408652, 0.08004894416301621, 0.07162273951427767, 0.06740963718990839, 0.08004894416301621, 0.07583584183864694, 0.06319653486553911, 0.04634412556806202, 0.04634412556806202, 0.05055722789243129, 0.04213102324369274, 0.04213102324369274, 0.05055722789243129, 0.04213102324369274, 0.03791792091932347, 0.03791792091932347, 0.02949171627058492, 0.033704818594954195, 0.033704818594954195, 0.04213102324369274, 0.02949171627058492, 0.07594668029120094, 0.06328890024266745, 0.07172742027502312, 0.06328890024266745, 0.05906964022648962, 0.08016594030737877, 0.06328890024266745, 0.05485038021031179, 0.0421926001617783, 0.05063112019413396, 0.05063112019413396, 0.04641186017795613, 0.0421926001617783, 0.04641186017795613, 0.03375408012942264, 0.03797334014560047, 0.03375408012942264, 0.03375408012942264, 0.02953482011324481, 0.02531556009706698, 0.07246467538749524, 0.059676791495584317, 0.07246467538749524, 0.06820204742352494, 0.059676791495584317, 0.055414163531614005, 0.059676791495584317, 0.055414163531614005, 0.04688890760367339, 0.0511515355676437, 0.04688890760367339, 0.0511515355676437, 0.04688890760367339, 0.038363651675732774, 0.038363651675732774, 0.03410102371176247, 0.042626279639703085, 0.03410102371176247, 0.029838395747792158, 0.038363651675732774, 0.0671341295902037, 0.08391766198775463, 0.06293824649081597, 0.07133001268959144, 0.07552589578897917, 0.05874236339142824, 0.06293824649081597, 0.05035059719265278, 0.04615471409326505, 0.05035059719265278, 0.05035059719265278, 0.04615471409326505, 0.041958830993877316, 0.037762947894489585, 0.041958830993877316, 0.03356706479510185, 0.041958830993877316, 0.03356706479510185, 0.02517529859632639, 0.02517529859632639, 0.06983124671276981, 0.09036984868711388, 0.065723526317901, 0.06983124671276981, 0.061615805923032195, 0.061615805923032195, 0.05750808552816338, 0.049292644738425755, 0.04518492434355694, 0.049292644738425755, 0.04518492434355694, 0.04518492434355694, 0.04518492434355694, 0.04107720394868813, 0.04107720394868813, 0.0328617631589505, 0.04107720394868813, 0.02875404276408169, 0.0328617631589505, 0.024646322369212877, 0.06314423890734663, 0.08287681356589245, 0.06314423890734663, 0.06709075383905579, 0.05919772397563746, 0.06709075383905579, 0.05919772397563746, 0.051304694112219135, 0.051304694112219135, 0.04735817918050997, 0.051304694112219135, 0.04735817918050997, 0.03946514931709164, 0.04341166424880081, 0.04341166424880081, 0.03157211945367332, 0.03157211945367332, 0.03157211945367332, 0.03157211945367332, 0.03157211945367332, 0.08389828097676345, 0.07131353883024893, 0.06711862478141077, 0.07131353883024893, 0.06711862478141077, 0.05872879668373442, 0.054533882634896245, 0.05872879668373442, 0.046144054537219904, 0.037754226439543556, 0.046144054537219904, 0.050338968586058075, 0.037754226439543556, 0.037754226439543556, 0.033559312390705386, 0.033559312390705386, 0.04194914048838173, 0.033559312390705386, 0.04194914048838173, 0.02936439834186721, 0.07639153103428972, 0.06432971034466503, 0.07237092413774816, 0.06835031724120659, 0.06432971034466503, 0.056288496551581906, 0.060309103448123465, 0.04824728275849877, 0.04422667586195721, 0.056288496551581906, 0.04422667586195721, 0.05226788965504034, 0.04020606896541565, 0.04020606896541565, 0.03618546206887408, 0.03618546206887408, 0.03618546206887408, 0.032164855172332515, 0.032164855172332515, 0.03618546206887408, 0.058014139207744175, 0.07873347463908138, 0.062158006294011615, 0.062158006294011615, 0.06630187338027906, 0.053870272121476735, 0.058014139207744175, 0.062158006294011615, 0.058014139207744175, 0.04972640503520929, 0.04972640503520929, 0.04558253794894185, 0.03729480377640697, 0.04143867086267441, 0.04143867086267441, 0.04558253794894185, 0.029007069603872088, 0.03315093669013953, 0.029007069603872088, 0.029007069603872088, 0.07160794920908367, 0.07160794920908367, 0.050546787677000236, 0.07582018151550035, 0.0631834845962503, 0.05475901998341692, 0.06739571690266698, 0.050546787677000236, 0.050546787677000236, 0.04212232306416686, 0.050546787677000236, 0.050546787677000236, 0.05475901998341692, 0.037910090757750176, 0.04633455537058355, 0.029485626144916805, 0.03369785845133349, 0.029485626144916805, 0.03369785845133349, 0.025273393838500118, 0.06775541156393554, 0.05928598511844359, 0.0719901247866815, 0.06352069834118956, 0.06352069834118956, 0.0719901247866815, 0.06352069834118956, 0.05081655867295165, 0.06352069834118956, 0.05081655867295165, 0.05505127189569762, 0.042347132227459706, 0.04658184545020568, 0.042347132227459706, 0.03387770578196777, 0.029642992559221794, 0.03811241900471374, 0.029642992559221794, 0.029642992559221794, 0.029642992559221794, 0.0734500570633267, 0.06528893961184595, 0.061208380886105584, 0.06936949833758634, 0.06936949833758634, 0.06936949833758634, 0.05304726343462484, 0.05304726343462484, 0.05304726343462484, 0.04896670470888447, 0.04896670470888447, 0.044886145983144096, 0.03264446980592298, 0.05304726343462484, 0.04080558725740373, 0.03264446980592298, 0.03672502853166335, 0.028563911080182608, 0.04080558725740373, 0.028563911080182608, 0.06205162955091661, 0.06618840485431104, 0.07032518015770549, 0.06618840485431104, 0.08273550606788882, 0.05791485424752217, 0.06205162955091661, 0.04550452833733885, 0.06205162955091661, 0.04136775303394441, 0.05377807894412773, 0.05377807894412773, 0.037230977730549966, 0.037230977730549966, 0.03309420242715552, 0.037230977730549966, 0.037230977730549966, 0.03309420242715552, 0.028957427123761086, 0.028957427123761086, 0.06606712943665165, 0.06606712943665165, 0.07019632502644238, 0.06193793384686092, 0.05780873825707019, 0.06606712943665165, 0.053679542667279465, 0.04955034707748874, 0.053679542667279465, 0.04542115148769801, 0.04955034707748874, 0.05780873825707019, 0.037162760308116556, 0.04542115148769801, 0.04129195589790728, 0.037162760308116556, 0.04129195589790728, 0.037162760308116556, 0.037162760308116556, 0.028904369128535095, 0.06805518852591307, 0.05954828996017394, 0.0638017392430435, 0.05954828996017394, 0.07230863780878263, 0.05954828996017394, 0.05954828996017394, 0.046787942111565235, 0.0510413913944348, 0.05529484067730437, 0.0638017392430435, 0.046787942111565235, 0.046787942111565235, 0.04253449282869567, 0.046787942111565235, 0.0255206956972174, 0.02977414498008697, 0.03402759426295653, 0.0382810435458261, 0.02977414498008697, 0.06184220499249019, 0.06184220499249019, 0.09070190065565227, 0.07833345965715424, 0.06184220499249019, 0.06184220499249019, 0.06184220499249019, 0.053596577660158164, 0.053596577660158164, 0.053596577660158164, 0.037105322995494114, 0.04122813666166013, 0.045350950327826135, 0.04122813666166013, 0.0329825093293281, 0.0329825093293281, 0.037105322995494114, 0.0329825093293281, 0.02885969566316209, 0.02885969566316209, 0.06202797568365336, 0.07856876919929426, 0.07443357082038403, 0.06202797568365336, 0.0868391659571147, 0.06202797568365336, 0.06202797568365336, 0.04962238054692269, 0.04962238054692269, 0.05375757892583291, 0.045487182168012466, 0.04962238054692269, 0.045487182168012466, 0.037216785410192015, 0.037216785410192015, 0.03308158703128179, 0.03308158703128179, 0.028946388652371566, 0.028946388652371566, 0.028946388652371566, 0.058860518447097374, 0.07147348668576109, 0.06306484119331861, 0.07147348668576109, 0.05465619570087613, 0.06306484119331861, 0.04624755020843365, 0.05045187295465489, 0.058860518447097374, 0.05465619570087613, 0.05045187295465489, 0.04624755020843365, 0.04624755020843365, 0.04204322746221241, 0.03783890471599117, 0.03363458196976993, 0.03363458196976993, 0.04204322746221241, 0.03363458196976993, 0.03363458196976993, 0.08200504157841608, 0.05740352910489125, 0.06970428534165367, 0.06560403326273286, 0.06560403326273286, 0.05330327702597045, 0.06970428534165367, 0.049203024947049645, 0.05330327702597045, 0.05330327702597045, 0.049203024947049645, 0.04100252078920804, 0.036902268710287234, 0.049203024947049645, 0.036902268710287234, 0.036902268710287234, 0.04510277286812884, 0.028701764552445625, 0.03280201663136643, 0.024601512473524823, 0.07323703958949011, 0.06916831516785178, 0.07730576401112846, 0.06509959074621344, 0.06916831516785178, 0.0610308663245751, 0.052893417481298416, 0.04475596863802174, 0.04882469305966008, 0.0610308663245751, 0.04475596863802174, 0.04475596863802174, 0.040687244216383395, 0.040687244216383395, 0.03661851979474506, 0.03254979537310672, 0.03661851979474506, 0.028481070951468377, 0.028481070951468377, 0.028481070951468377, 0.06624425688926412, 0.07452478900042213, 0.0579637247781061, 0.08694558716715915, 0.07452478900042213, 0.06210399083368511, 0.0538234587225271, 0.045542926611369085, 0.045542926611369085, 0.04968319266694809, 0.045542926611369085, 0.045542926611369085, 0.041402660555790075, 0.03312212844463206, 0.045542926611369085, 0.037262394500211064, 0.037262394500211064, 0.02898186238905305, 0.02898186238905305, 0.03312212844463206, 0.06254882915940194, 0.06671875110336206, 0.06254882915940194, 0.058378907215441804, 0.07088867304732219, 0.05420898527148168, 0.06671875110336206, 0.05420898527148168, 0.050039063327521546, 0.050039063327521546, 0.05420898527148168, 0.06254882915940194, 0.03335937555168103, 0.04169921943960129, 0.04586914138356142, 0.03335937555168103, 0.03335937555168103, 0.03752929749564116, 0.029189453607720902, 0.029189453607720902, 0.06026907406772733, 0.07232288888127279, 0.07634082715245462, 0.07232288888127279, 0.07232288888127279, 0.05625113579654551, 0.05625113579654551, 0.048215259254181866, 0.048215259254181866, 0.052233197525363685, 0.06428701233890916, 0.04419732098300004, 0.04419732098300004, 0.036161444440636396, 0.04419732098300004, 0.028125567898272755, 0.028125567898272755, 0.036161444440636396, 0.028125567898272755, 0.024107629627090933, 0.060790201586856274, 0.06889556179843712, 0.07294824190422752, 0.0648428816926467, 0.0648428816926467, 0.0648428816926467, 0.05268484137527544, 0.05673752148106585, 0.05673752148106585, 0.05268484137527544, 0.060790201586856274, 0.05268484137527544, 0.044579481163694604, 0.04052680105790418, 0.03242144084632335, 0.028368760740532926, 0.03242144084632335, 0.03242144084632335, 0.04052680105790418, 0.028368760740532926, 0.0646939573771206, 0.07278070204926067, 0.07278070204926067, 0.0768240743853307, 0.07278070204926067, 0.06873732971319063, 0.06065058504105056, 0.05660721270498052, 0.04043372336070037, 0.044477095696770406, 0.044477095696770406, 0.04852046803284044, 0.036390351024630334, 0.044477095696770406, 0.036390351024630334, 0.02830360635249026, 0.0323469786885603, 0.0323469786885603, 0.02830360635249026, 0.036390351024630334, 0.06867550111737798, 0.06463576575753223, 0.07271523647722375, 0.07271523647722375, 0.060596030397686455, 0.08079470719691528, 0.056556295037840695, 0.04847682431814917, 0.04039735359845764, 0.056556295037840695, 0.04847682431814917, 0.0444370889583034, 0.0444370889583034, 0.04847682431814917, 0.036357618238611875, 0.032317882878766115, 0.028278147518920348, 0.032317882878766115, 0.032317882878766115, 0.032317882878766115, 0.07126458717551852, 0.06288051809604575, 0.07126458717551852, 0.07545662171525491, 0.06288051809604575, 0.06288051809604575, 0.05868848355630937, 0.06707255263578214, 0.04611237993710022, 0.04192034539736384, 0.05449644901657299, 0.05449644901657299, 0.04192034539736384, 0.037728310857627456, 0.03353627631789107, 0.03353627631789107, 0.029344241778154687, 0.037728310857627456, 0.03353627631789107, 0.029344241778154687, 0.07985161821974736, 0.06388129457579789, 0.06787387548678525, 0.07186645639777262, 0.07585903730875998, 0.06388129457579789, 0.07186645639777262, 0.05190355184283578, 0.055896132753823144, 0.03992580910987368, 0.03992580910987368, 0.04791097093184841, 0.03194064728789894, 0.03593322819888631, 0.03593322819888631, 0.03992580910987368, 0.03194064728789894, 0.03593322819888631, 0.027948066376911572, 0.027948066376911572, 0.07734319558457678, 0.06513111207122255, 0.08141389008902819, 0.06513111207122255, 0.06106041756677114, 0.06106041756677114, 0.05698972306231973, 0.05698972306231973, 0.06513111207122255, 0.04884833405341691, 0.044777639548965506, 0.044777639548965506, 0.040706945044514095, 0.036636250540062684, 0.044777639548965506, 0.040706945044514095, 0.03256555603561127, 0.028494861531159866, 0.028494861531159866, 0.024424167026708455, 0.07290922016557345, 0.0688587079341527, 0.060757683471311205, 0.06480819570273195, 0.0688587079341527, 0.06480819570273195, 0.05670717123989046, 0.05670717123989046, 0.05265665900846971, 0.04860614677704897, 0.04860614677704897, 0.05265665900846971, 0.03240409785136598, 0.05265665900846971, 0.03645461008278673, 0.03645461008278673, 0.03645461008278673, 0.03240409785136598, 0.02835358561994523, 0.024303073388524485, 0.06644454565802495, 0.08720846617615774, 0.058138977450771824, 0.06229176155439838, 0.06229176155439838, 0.07890289796890462, 0.06229176155439838, 0.04983340924351871, 0.04568062513989215, 0.04983340924351871, 0.04568062513989215, 0.04983340924351871, 0.03737505693263903, 0.04568062513989215, 0.04568062513989215, 0.033222272829012474, 0.029069488725385912, 0.033222272829012474, 0.033222272829012474, 0.029069488725385912, 0.06110298684299565, 0.06110298684299565, 0.05702945438679594, 0.06517651929919537, 0.06925005175539507, 0.06110298684299565, 0.06517651929919537, 0.05702945438679594, 0.05702945438679594, 0.04888238947439652, 0.0407353245619971, 0.05295592193059623, 0.03666179210579739, 0.04888238947439652, 0.04480885701819681, 0.04480885701819681, 0.0407353245619971, 0.03258825964959768, 0.03258825964959768, 0.02851472719339797, 0.05130688916152831, 0.07696033374229247, 0.06840918554870441, 0.06840918554870441, 0.06413361145191039, 0.05985803735511636, 0.055582463258322334, 0.06840918554870441, 0.042755740967940256, 0.05130688916152831, 0.047031315064734285, 0.047031315064734285, 0.042755740967940256, 0.047031315064734285, 0.034204592774352206, 0.038480166871146235, 0.038480166871146235, 0.034204592774352206, 0.038480166871146235, 0.025653444580764156, 0.06910309982350449, 0.07316798804841651, 0.06503821159859247, 0.06503821159859247, 0.06097332337368043, 0.06910309982350449, 0.06097332337368043, 0.0569084351487684, 0.05284354692385637, 0.05284354692385637, 0.04471377047403231, 0.04471377047403231, 0.036583994024208255, 0.040648882249120284, 0.03251910579929623, 0.04471377047403231, 0.03251910579929623, 0.03251910579929623, 0.036583994024208255, 0.0284542175743842, 0.07427176802625496, 0.06189314002187914, 0.06189314002187914, 0.07839797736104691, 0.06601934935667107, 0.06601934935667107, 0.04951451201750331, 0.06189314002187914, 0.045388302682711365, 0.05364072135229525, 0.045388302682711365, 0.04951451201750331, 0.04126209334791942, 0.045388302682711365, 0.03713588401312748, 0.045388302682711365, 0.033009674678335536, 0.03713588401312748, 0.028883465343543597, 0.024757256008751654, 0.06414164255723141, 0.06013278989740444, 0.07616820053671229, 0.09220361117602015, 0.06013278989740444, 0.06013278989740444, 0.06414164255723141, 0.056123937237577484, 0.048106231917923556, 0.05211508457775052, 0.04409737925809659, 0.048106231917923556, 0.04409737925809659, 0.03607967393844266, 0.04008852659826963, 0.032070821278615706, 0.03607967393844266, 0.032070821278615706, 0.028061968618788742, 0.024053115958961778, 0.08134178995489522, 0.0619746971084916, 0.06584811567777232, 0.07359495281633377, 0.07359495281633377, 0.0619746971084916, 0.054227859969930146, 0.05810127853921088, 0.054227859969930146, 0.05035444140064942, 0.05035444140064942, 0.04260760426208798, 0.04260760426208798, 0.03486076712352652, 0.05035444140064942, 0.0309873485542458, 0.0309873485542458, 0.027113929984965073, 0.027113929984965073, 0.0309873485542458, 0.07373556642072819, 0.06506079390064252, 0.06939818016068536, 0.06506079390064252, 0.06939818016068536, 0.06072340764059969, 0.06072340764059969, 0.047711248860471185, 0.05204863512051402, 0.05204863512051402, 0.047711248860471185, 0.047711248860471185, 0.03469909008034268, 0.03469909008034268, 0.04337386260042835, 0.030361703820299846, 0.03903647634038552, 0.03469909008034268, 0.03903647634038552, 0.030361703820299846, 0.06957030800312443, 0.061385565885109794, 0.07775505012113908, 0.06547793694411712, 0.06957030800312443, 0.061385565885109794, 0.07366267906213175, 0.049108452708087835, 0.05320082376709515, 0.05320082376709515, 0.05320082376709515, 0.04501608164908052, 0.040923710590073194, 0.036831339531065876, 0.03273896847205856, 0.03273896847205856, 0.03273896847205856, 0.03273896847205856, 0.02864659741305124, 0.02864659741305124, 0.06304873530536621, 0.07565848236643946, 0.06304873530536621, 0.06304873530536621, 0.07145523334608171, 0.06304873530536621, 0.05884548628500847, 0.05464223726465072, 0.05464223726465072, 0.04623573922393522, 0.05043898824429297, 0.05884548628500847, 0.04203249020357748, 0.03782924118321973, 0.04203249020357748, 0.03782924118321973, 0.029422743142504235, 0.03362599216286198, 0.025219494122146486, 0.025219494122146486, 0.06815275687845888, 0.06815275687845888, 0.07241230418336257, 0.06815275687845888, 0.05963366226865153, 0.05963366226865153, 0.05537411496374785, 0.046855020353940485, 0.046855020353940485, 0.046855020353940485, 0.051114567658844166, 0.03833592574413312, 0.046855020353940485, 0.042595473049036804, 0.046855020353940485, 0.03407637843922944, 0.042595473049036804, 0.03407637843922944, 0.029816831134325764, 0.03407637843922944, 0.06960980438595149, 0.06960980438595149, 0.07347701574072657, 0.06574259303117641, 0.058008170321626244, 0.06960980438595149, 0.061875381676401324, 0.058008170321626244, 0.06574259303117641, 0.04640653625730099, 0.04640653625730099, 0.04640653625730099, 0.04253932490252591, 0.04253932490252591, 0.03867211354775083, 0.030937690838200662, 0.034804902192975745, 0.030937690838200662, 0.030937690838200662, 0.023203268128650496, 0.06825857627516736, 0.0639924152579694, 0.0639924152579694, 0.059726254240771444, 0.0639924152579694, 0.0639924152579694, 0.059726254240771444, 0.05546009322357348, 0.05546009322357348, 0.051193932206375525, 0.05546009322357348, 0.04266161017197961, 0.04266161017197961, 0.051193932206375525, 0.04692777118917756, 0.029863127120385722, 0.029863127120385722, 0.03412928813758368, 0.029863127120385722, 0.025596966103187763, 0.07086994436540457, 0.07086994436540457, 0.058363483595039053, 0.07086994436540457, 0.06670112410861606, 0.06253230385182756, 0.05419466333825055, 0.06670112410861606, 0.04585702282467354, 0.06253230385182756, 0.04585702282467354, 0.04168820256788504, 0.04585702282467354, 0.037519382311096534, 0.037519382311096534, 0.03335056205430803, 0.03335056205430803, 0.037519382311096534, 0.029181741797519527, 0.025012921540731023, 0.06492925447268406, 0.07304541128176958, 0.06898733287722682, 0.06087117606814132, 0.06898733287722682, 0.06087117606814132, 0.06087117606814132, 0.04869694085451305, 0.052755019259055805, 0.052755019259055805, 0.0446388624499703, 0.0446388624499703, 0.040580784045427545, 0.04869694085451305, 0.040580784045427545, 0.02840654883179928, 0.03652270564088479, 0.040580784045427545, 0.03246462723634203, 0.02840654883179928, 0.08091329159781178, 0.07239610300856844, 0.06813750871394676, 0.0638789144193251, 0.05962032012470342, 0.08091329159781178, 0.051103131535460075, 0.05536172583008175, 0.042585942946216725, 0.042585942946216725, 0.042585942946216725, 0.051103131535460075, 0.03832734865159505, 0.03832734865159505, 0.03406875435697338, 0.03406875435697338, 0.03832734865159505, 0.03406875435697338, 0.03832734865159505, 0.025551565767730038, 0.06974520574075664, 0.061539887418314676, 0.07795052406319859, 0.06974520574075664, 0.06564254657953565, 0.061539887418314676, 0.04923190993465174, 0.0574372282570937, 0.04923190993465174, 0.04512925077343077, 0.04512925077343077, 0.053334569095872725, 0.04512925077343077, 0.04923190993465174, 0.041026591612209784, 0.041026591612209784, 0.02871861412854685, 0.02871861412854685, 0.041026591612209784, 0.02461595496732587, 0.06440207794632334, 0.060108606083235115, 0.06440207794632334, 0.07298902167249978, 0.06440207794632334, 0.06440207794632334, 0.06440207794632334, 0.05152166235705867, 0.05152166235705867, 0.04722819049397045, 0.038641246767794006, 0.04722819049397045, 0.042934718630882225, 0.060108606083235115, 0.038641246767794006, 0.030054303041617558, 0.038641246767794006, 0.038641246767794006, 0.030054303041617558, 0.025760831178529335, 0.07381931920435837, 0.06947700630998435, 0.06513469341561032, 0.06947700630998435, 0.060792380521236304, 0.060792380521236304, 0.05210775473248826, 0.05210775473248826, 0.047765441838114236, 0.047765441838114236, 0.05210775473248826, 0.043423128943740216, 0.056450067626862284, 0.039080816049366196, 0.039080816049366196, 0.034738503154992176, 0.039080816049366196, 0.034738503154992176, 0.034738503154992176, 0.02605387736624413, 0.07114354765186608, 0.06277371851635241, 0.07114354765186608, 0.058588803948595586, 0.09625303505840704, 0.06277371851635241, 0.05440388938083876, 0.05440388938083876, 0.050218974813081935, 0.04184914567756828, 0.0460340602453251, 0.050218974813081935, 0.03766423110981145, 0.03347931654205462, 0.04184914567756828, 0.029294401974297793, 0.03347931654205462, 0.03766423110981145, 0.025109487406540967, 0.029294401974297793, 0.06810697618764511, 0.06810697618764511, 0.06410068347072481, 0.09214473248916691, 0.06810697618764511, 0.06810697618764511, 0.05608809803688421, 0.0480755126030436, 0.0480755126030436, 0.05608809803688421, 0.05608809803688421, 0.044069219886123306, 0.044069219886123306, 0.0360566344522827, 0.0360566344522827, 0.0360566344522827, 0.028044049018442103, 0.032050341735362406, 0.0240377563015218, 0.0240377563015218, 0.07118372968919039, 0.07955828612321278, 0.05443461682114559, 0.058621895038156784, 0.06280917325516798, 0.06280917325516798, 0.06280917325516798, 0.06699645147217918, 0.04606006038712319, 0.05024733860413439, 0.05024733860413439, 0.05024733860413439, 0.03768550395310079, 0.03768550395310079, 0.04187278217011199, 0.03349822573608959, 0.03349822573608959, 0.03349822573608959, 0.03349822573608959, 0.029310947519078392, 0.07055743783037884, 0.06225656279151073, 0.07055743783037884, 0.06640700031094479, 0.07055743783037884, 0.05810612527207669, 0.05810612527207669, 0.04980525023320859, 0.04150437519434049, 0.04980525023320859, 0.04980525023320859, 0.06640700031094479, 0.04565481271377454, 0.03735393767490644, 0.04980525023320859, 0.04150437519434049, 0.029053062636038345, 0.029053062636038345, 0.029053062636038345, 0.024902625116604293, 0.06600156948061824, 0.09075215803585009, 0.06600156948061824, 0.05775137329554096, 0.05775137329554096, 0.0618764713880796, 0.06600156948061824, 0.04950117711046368, 0.04950117711046368, 0.0412509809253864, 0.04537607901792504, 0.05362627520300232, 0.04950117711046368, 0.04537607901792504, 0.03712588283284776, 0.03300078474030912, 0.03300078474030912, 0.0412509809253864, 0.02475058855523184, 0.02887568664777048, 0.05822025199030802, 0.06653743084606631, 0.06237884141818716, 0.0748546097018246, 0.07069602027394545, 0.06653743084606631, 0.06237884141818716, 0.05822025199030802, 0.06653743084606631, 0.05406166256242888, 0.04990307313454973, 0.05822025199030802, 0.0374273048509123, 0.033268715423033154, 0.033268715423033154, 0.02911012599515401, 0.02911012599515401, 0.033268715423033154, 0.02911012599515401, 0.024951536567274866, 0.0666545330157559, 0.05832271638878641, 0.07498634964272538, 0.07498634964272538, 0.05832271638878641, 0.0666545330157559, 0.05415680807530167, 0.05415680807530167, 0.04165908313484744, 0.04582499144833218, 0.04999089976181692, 0.05832271638878641, 0.03749317482136269, 0.04582499144833218, 0.04999089976181692, 0.03332726650787795, 0.03749317482136269, 0.03749317482136269, 0.029161358194393205, 0.02499544988090846, 0.07260304669143663, 0.06856954409746793, 0.07260304669143663, 0.06453604150349923, 0.06050253890953052, 0.06453604150349923, 0.06453604150349923, 0.06856954409746793, 0.04840203112762442, 0.05243553372159312, 0.04436852853365572, 0.04840203112762442, 0.04033502593968702, 0.04033502593968702, 0.04033502593968702, 0.02823451815778091, 0.032268020751749615, 0.032268020751749615, 0.032268020751749615, 0.02823451815778091, 0.06697004978105114, 0.06697004978105114, 0.06697004978105114, 0.06278442166973544, 0.06278442166973544, 0.06278442166973544, 0.07534130600368254, 0.05441316544710405, 0.05441316544710405, 0.046041909224472655, 0.046041909224472655, 0.046041909224472655, 0.03767065300184127, 0.04185628111315696, 0.046041909224472655, 0.03348502489052557, 0.029299396779209872, 0.03767065300184127, 0.03348502489052557, 0.029299396779209872, 0.07195082996096717, 0.07994536662329685, 0.06395629329863749, 0.05995902496747264, 0.08793990328562654, 0.0559617566363078, 0.0559617566363078, 0.051964488305142956, 0.04796721997397811, 0.04796721997397811, 0.04396995164281327, 0.051964488305142956, 0.039972683311648424, 0.039972683311648424, 0.031978146649318744, 0.031978146649318744, 0.03597541498048359, 0.03597541498048359, 0.03597541498048359, 0.0279808783181539, 0.06787592726558725, 0.05939143635738884, 0.06363368181148804, 0.06363368181148804, 0.06363368181148804, 0.07211817271968644, 0.05514919090328964, 0.05090694544919044, 0.05090694544919044, 0.046664699995091236, 0.04242245454099203, 0.05090694544919044, 0.05514919090328964, 0.046664699995091236, 0.04242245454099203, 0.033937963632793625, 0.02969571817869442, 0.033937963632793625, 0.033937963632793625, 0.02969571817869442, 0.07181621362492936, 0.05914276416170654, 0.05914276416170654, 0.06759173047052176, 0.06759173047052176, 0.05914276416170654, 0.05491828100729893, 0.05491828100729893, 0.046469314698483706, 0.05069379785289132, 0.046469314698483706, 0.05069379785289132, 0.05069379785289132, 0.046469314698483706, 0.05069379785289132, 0.03379586523526088, 0.02957138208085327, 0.03379586523526088, 0.03379586523526088, 0.02957138208085327, 0.09570750672222506, 0.07178063004166879, 0.06380500448148337, 0.06779281726157609, 0.06380500448148337, 0.05582937892129795, 0.05981719170139066, 0.04785375336111253, 0.04785375336111253, 0.03987812780092711, 0.05184156614120524, 0.03987812780092711, 0.04785375336111253, 0.03987812780092711, 0.043865940581019815, 0.035890315020834396, 0.035890315020834396, 0.03190250224074168, 0.027914689460648973, 0.03190250224074168, 0.07408432772165467, 0.0662859774351647, 0.07018515257840968, 0.08188267800814464, 0.07018515257840968, 0.054588452005429756, 0.07408432772165467, 0.04679010171893979, 0.038991751432449824, 0.05068927686218477, 0.04679010171893979, 0.054588452005429756, 0.038991751432449824, 0.04289092657569481, 0.04289092657569481, 0.027294226002714878, 0.03509257628920484, 0.027294226002714878, 0.03119340114595986, 0.03119340114595986, 0.06659494730321477, 0.0707571315096657, 0.07491931571611662, 0.06243276309676385, 0.06659494730321477, 0.054108394683862006, 0.0707571315096657, 0.054108394683862006, 0.04162184206450924, 0.045784026270960156, 0.04994621047741108, 0.04994621047741108, 0.03745965785805831, 0.03745965785805831, 0.04994621047741108, 0.03745965785805831, 0.03329747365160739, 0.03329747365160739, 0.029135289445156465, 0.029135289445156465, 0.06504405068122449, 0.06070778063580952, 0.06070778063580952, 0.06504405068122449, 0.06504405068122449, 0.06504405068122449, 0.05637151059039455, 0.05637151059039455, 0.047698970499564626, 0.052035240544979586, 0.052035240544979586, 0.047698970499564626, 0.04336270045414966, 0.04336270045414966, 0.04336270045414966, 0.04336270045414966, 0.03035389031790476, 0.034690160363319726, 0.034690160363319726, 0.026017620272489793, 0.06772122645198371, 0.06348864979873474, 0.08041895641173066, 0.06772122645198371, 0.06772122645198371, 0.06348864979873474, 0.050790919838987786, 0.04655834318573881, 0.04655834318573881, 0.04655834318573881, 0.042325766532489824, 0.04655834318573881, 0.042325766532489824, 0.04655834318573881, 0.03809318987924084, 0.033860613225991855, 0.042325766532489824, 0.03809318987924084, 0.029628036572742877, 0.033860613225991855, 0.06403889972333816, 0.07204376218875543, 0.06403889972333816, 0.0680413309560468, 0.07604619342146407, 0.056034037257920895, 0.06403889972333816, 0.05203160602521226, 0.060036468490629526, 0.04802917479250362, 0.04402674355979499, 0.056034037257920895, 0.04402674355979499, 0.03602188109437771, 0.04002431232708635, 0.03201944986166908, 0.03201944986166908, 0.03602188109437771, 0.03201944986166908, 0.028017018628960447, 0.06264352290393532, 0.06264352290393532, 0.07517222748472238, 0.06681975776419767, 0.06264352290393532, 0.06264352290393532, 0.05846728804367297, 0.06681975776419767, 0.05011481832314826, 0.05429105318341061, 0.0459385834628859, 0.041762348602623546, 0.05011481832314826, 0.0459385834628859, 0.03758611374236119, 0.033409878882098835, 0.033409878882098835, 0.02505740916157413, 0.03758611374236119, 0.02505740916157413, 0.06560258087493546, 0.06970274217961893, 0.061502419570251994, 0.061502419570251994, 0.07790306478898586, 0.061502419570251994, 0.05330209696088506, 0.05330209696088506, 0.049201935656201595, 0.04510177435151813, 0.049201935656201595, 0.05330209696088506, 0.04100161304683466, 0.04510177435151813, 0.04510177435151813, 0.049201935656201595, 0.04100161304683466, 0.028701129132784265, 0.03280129043746773, 0.024600967828100798, 0.07103785880927553, 0.07521655638629174, 0.07103785880927553, 0.07103785880927553, 0.07521655638629174, 0.05432306850121069, 0.0585017660782269, 0.05432306850121069, 0.05014437092419449, 0.05014437092419449, 0.05014437092419449, 0.045965673347178285, 0.04178697577016208, 0.03760827819314587, 0.03342958061612966, 0.02925088303911345, 0.02925088303911345, 0.03760827819314587, 0.04178697577016208, 0.02925088303911345, 0.07149588340739856, 0.06752388988476529, 0.06752388988476529, 0.06355189636213204, 0.06355189636213204, 0.059579902839498794, 0.06752388988476529, 0.05560790931686554, 0.04369192874896578, 0.05560790931686554, 0.03574794170369928, 0.04766392227159903, 0.03971993522633253, 0.04369192874896578, 0.03971993522633253, 0.03574794170369928, 0.04369192874896578, 0.03971993522633253, 0.02780395465843277, 0.02780395465843277, 0.06581901228091197, 0.07404638881602597, 0.061705324013354976, 0.08638745361869696, 0.061705324013354976, 0.06993270054846897, 0.061705324013354976, 0.04936425921068398, 0.04525057094312698, 0.04936425921068398, 0.04936425921068398, 0.04525057094312698, 0.04113688267556998, 0.03702319440801299, 0.04113688267556998, 0.032909506140455985, 0.028795817872898987, 0.032909506140455985, 0.032909506140455985, 0.028795817872898987, 0.07477447693914484, 0.0664662017236843, 0.08308275215460537, 0.06231206411595403, 0.06231206411595403, 0.05815792650822376, 0.05815792650822376, 0.049849651292763225, 0.045695513685032954, 0.049849651292763225, 0.05815792650822376, 0.045695513685032954, 0.04154137607730268, 0.03738723846957242, 0.04154137607730268, 0.03323310086184215, 0.02907896325411188, 0.02907896325411188, 0.04154137607730268, 0.02907896325411188, 0.07255675906229869, 0.06828871441157523, 0.0896289376651925, 0.06402066976085179, 0.06402066976085179, 0.059752625110128334, 0.059752625110128334, 0.05548458045940488, 0.04268044650723452, 0.05548458045940488, 0.046948491157957975, 0.04268044650723452, 0.04268044650723452, 0.046948491157957975, 0.04268044650723452, 0.034144357205787616, 0.029876312555064167, 0.034144357205787616, 0.029876312555064167, 0.029876312555064167, 0.0702545563937929, 0.08976971094762426, 0.07806061821532545, 0.07415758730455918, 0.06244849457226036, 0.05854546366149409, 0.05854546366149409, 0.05854546366149409, 0.04683637092919527, 0.039030309107662725, 0.042933340018428995, 0.039030309107662725, 0.03512727819689645, 0.04683637092919527, 0.039030309107662725, 0.03512727819689645, 0.03512727819689645, 0.03122424728613018, 0.03122424728613018, 0.03122424728613018, 0.07348785545457015, 0.06532253818184013, 0.06532253818184013, 0.06940519681820514, 0.06940519681820514, 0.057157220909110115, 0.05307456227274511, 0.05307456227274511, 0.05307456227274511, 0.05307456227274511, 0.0489919036363801, 0.04490924500001509, 0.057157220909110115, 0.036743927727285075, 0.04082658636365008, 0.03266126909092006, 0.036743927727285075, 0.03266126909092006, 0.03266126909092006, 0.028578610454555058, 0.06786807640118907, 0.06786807640118907, 0.07210983117626338, 0.06362632162611474, 0.06362632162611474, 0.05938456685104043, 0.0763515859513377, 0.0509010573008918, 0.03817579297566885, 0.0509010573008918, 0.0509010573008918, 0.0509010573008918, 0.04665930252581748, 0.033934038200594534, 0.033934038200594534, 0.03817579297566885, 0.033934038200594534, 0.029692283425520216, 0.029692283425520216, 0.029692283425520216, 0.06651179719097632, 0.06235480986654029, 0.07482577183984834, 0.06235480986654029, 0.06235480986654029, 0.07066878451541234, 0.054040835217668254, 0.058197822542104274, 0.04988384789323223, 0.06235480986654029, 0.04156987324436019, 0.04988384789323223, 0.03741288591992417, 0.04572686056879621, 0.03741288591992417, 0.04156987324436019, 0.03741288591992417, 0.03325589859548816, 0.03325589859548816, 0.024941923946616117, 0.06053398472808416, 0.09281877658306238, 0.072640781673701, 0.06456958370995644, 0.072640781673701, 0.05649838574621188, 0.05649838574621188, 0.05649838574621188, 0.05246278676433961, 0.05246278676433961, 0.05246278676433961, 0.04842718778246733, 0.04439158880059505, 0.0363203908368505, 0.0363203908368505, 0.02824919287310594, 0.03228479185497822, 0.02824919287310594, 0.03228479185497822, 0.02824919287310594, 0.07493617024331825, 0.0788801792034929, 0.05916013440261967, 0.0709921612831436, 0.0709921612831436, 0.06310414336279432, 0.05127211648227038, 0.05521612544244503, 0.043384098561921096, 0.05521612544244503, 0.047328107522095736, 0.043384098561921096, 0.05521612544244503, 0.043384098561921096, 0.03944008960174645, 0.027608062721222515, 0.0354960806415718, 0.027608062721222515, 0.027608062721222515, 0.03155207168139716, 0.06954541390058085, 0.07772722730064918, 0.07363632060061502, 0.07363632060061502, 0.06545450720054669, 0.049090880400410014, 0.049090880400410014, 0.05727269380047835, 0.06136360050051252, 0.049090880400410014, 0.044999973700375846, 0.049090880400410014, 0.03681816030030751, 0.04090906700034168, 0.03681816030030751, 0.03272725360027334, 0.03272725360027334, 0.044999973700375846, 0.024545440200205007, 0.028636346900239175, 0.07284518693056798, 0.06070432244213998, 0.06879823210109198, 0.06475127727161599, 0.06475127727161599, 0.05665736761266399, 0.05665736761266399, 0.05665736761266399, 0.05665736761266399, 0.04856345795371199, 0.04856345795371199, 0.04046954829475999, 0.04046954829475999, 0.03642259346528399, 0.05261041278318799, 0.03642259346528399, 0.04046954829475999, 0.028328683806331995, 0.032375638635807995, 0.028328683806331995, 0.06002886185519848, 0.06431663770199837, 0.06860441354879826, 0.07289218939559815, 0.06860441354879826, 0.06860441354879826, 0.06002886185519848, 0.051453310161598695, 0.051453310161598695, 0.051453310161598695, 0.051453310161598695, 0.04716553431479881, 0.038589982621199025, 0.051453310161598695, 0.03430220677439913, 0.038589982621199025, 0.03430220677439913, 0.03430220677439913, 0.03001443092759924, 0.025726655080799347, 0.09044715440161405, 0.057557280073754394, 0.06577974865571931, 0.06989098294670176, 0.06989098294670176, 0.06166851436473685, 0.057557280073754394, 0.053446045782771935, 0.041112342909824566, 0.04933481149178948, 0.045223577200807025, 0.04933481149178948, 0.045223577200807025, 0.04933481149178948, 0.028778640036877197, 0.03700110861884211, 0.032889874327859656, 0.032889874327859656, 0.028778640036877197, 0.032889874327859656, 0.07853455970655773, 0.05786757031009517, 0.07853455970655773, 0.06200096818938768, 0.0661343660686802, 0.06200096818938768, 0.06200096818938768, 0.04960077455151015, 0.04546737667221763, 0.04546737667221763, 0.04960077455151015, 0.04960077455151015, 0.04546737667221763, 0.04133397879292512, 0.04133397879292512, 0.04546737667221763, 0.0330671830343401, 0.03720058091363261, 0.028933785155047586, 0.0330671830343401, 0.07249081439302044, 0.06846354692674153, 0.08054534932557826, 0.0604090119941837, 0.06443627946046261, 0.08054534932557826, 0.05638174452790479, 0.05638174452790479, 0.04429994212906805, 0.04429994212906805, 0.04429994212906805, 0.04429994212906805, 0.04429994212906805, 0.04027267466278913, 0.04429994212906805, 0.032218139730231306, 0.032218139730231306, 0.032218139730231306, 0.032218139730231306, 0.02416360479767348, 0.07336518386136215, 0.0617812074621997, 0.06564253292858718, 0.0617812074621997, 0.06564253292858718, 0.06564253292858718, 0.05791988199581222, 0.05791988199581222, 0.054058556529424734, 0.054058556529424734, 0.046335905596649775, 0.05791988199581222, 0.04247458013026229, 0.03089060373109985, 0.04247458013026229, 0.03475192919748733, 0.03861325466387481, 0.03089060373109985, 0.03089060373109985, 0.027029278264712367, 0.07214624261992067, 0.06790234599521945, 0.08912182911872553, 0.059414552745817015, 0.059414552745817015, 0.06365844937051823, 0.06790234599521945, 0.0551706561211158, 0.042438966247012155, 0.046682862871713374, 0.042438966247012155, 0.046682862871713374, 0.042438966247012155, 0.046682862871713374, 0.033951172997609726, 0.03819506962231094, 0.033951172997609726, 0.033951172997609726, 0.029707276372908507, 0.025463379748207293, 0.06930874336817971, 0.06523175846416915, 0.07746271317620086, 0.08561668298422201, 0.061154773560158575, 0.06523175846416915, 0.057077788656148, 0.061154773560158575, 0.04484683394411629, 0.04892381884812686, 0.04892381884812686, 0.04892381884812686, 0.040769849040105714, 0.040769849040105714, 0.03669286413609515, 0.032615879232084574, 0.032615879232084574, 0.028538894328074, 0.028538894328074, 0.02446190942406343, 0.07623435793456292, 0.06776387371961148, 0.07199911582708719, 0.06776387371961148, 0.06352863161213576, 0.059293389504660045, 0.059293389504660045, 0.05082290528970861, 0.04658766318223289, 0.04658766318223289, 0.042352421074757174, 0.04658766318223289, 0.04658766318223289, 0.042352421074757174, 0.03811717896728146, 0.03388193685980574, 0.03388193685980574, 0.029646694752330022, 0.042352421074757174, 0.029646694752330022, 0.07735174153696427, 0.06920945295412592, 0.06920945295412592, 0.0732805972455451, 0.06920945295412592, 0.061067164371287584, 0.044782587205610895, 0.05292487578844924, 0.05292487578844924, 0.04071144291419172, 0.04885373149703007, 0.05292487578844924, 0.04071144291419172, 0.03664029862277255, 0.044782587205610895, 0.044782587205610895, 0.03664029862277255, 0.03256915433135338, 0.03256915433135338, 0.028498010039934206, 0.06672694658696733, 0.08635251911254596, 0.054951603071620156, 0.06672694658696733, 0.07457717559719879, 0.054951603071620156, 0.05887671757673588, 0.054951603071620156, 0.05887671757673588, 0.04317625955627298, 0.054951603071620156, 0.04317625955627298, 0.03925114505115725, 0.04317625955627298, 0.03925114505115725, 0.035326030546041526, 0.027475801535810078, 0.027475801535810078, 0.031400916040925805, 0.031400916040925805, 0.06406323812375414, 0.06406323812375414, 0.05552147304058692, 0.06833412066533774, 0.06833412066533774, 0.06406323812375414, 0.05552147304058692, 0.05125059049900331, 0.042708825415836094, 0.0469797079574197, 0.0469797079574197, 0.0469797079574197, 0.042708825415836094, 0.05125059049900331, 0.0469797079574197, 0.038437942874252486, 0.03416706033266887, 0.038437942874252486, 0.03416706033266887, 0.029896177791085263, 0.0732831848631457, 0.06921189681519316, 0.06921189681519316, 0.05699803267133554, 0.05699803267133554, 0.061069320719288084, 0.06921189681519316, 0.05699803267133554, 0.05699803267133554, 0.04478416852747793, 0.048855456575430466, 0.040712880479525385, 0.040712880479525385, 0.040712880479525385, 0.040712880479525385, 0.03664159243157285, 0.02849901633566777, 0.03257030438362031, 0.040712880479525385, 0.03257030438362031, 0.06629460629122085, 0.07043801918442215, 0.06629460629122085, 0.07458143207762345, 0.07872484497082476, 0.06215119339801955, 0.04972095471841564, 0.05386436761161694, 0.037290716038811726, 0.04972095471841564, 0.04972095471841564, 0.045577541825214334, 0.045577541825214334, 0.04143412893201303, 0.037290716038811726, 0.037290716038811726, 0.04143412893201303, 0.029003890252409122, 0.029003890252409122, 0.029003890252409122, 0.0666703476943688, 0.07083724442526686, 0.0583365542325727, 0.0666703476943688, 0.0583365542325727, 0.07083724442526686, 0.054169657501674656, 0.0583365542325727, 0.0500027607707766, 0.0500027607707766, 0.0500027607707766, 0.037502070578082455, 0.0583365542325727, 0.037502070578082455, 0.0416689673089805, 0.037502070578082455, 0.037502070578082455, 0.0333351738471844, 0.0333351738471844, 0.02916827711628635, 0.0672124744545699, 0.07141325410798052, 0.0672124744545699, 0.07141325410798052, 0.05461013549433804, 0.05881091514774866, 0.05881091514774866, 0.06301169480115928, 0.05040935584092742, 0.0462085761875168, 0.0462085761875168, 0.05040935584092742, 0.04200779653410618, 0.04200779653410618, 0.03360623722728495, 0.03360623722728495, 0.04200779653410618, 0.03360623722728495, 0.02520467792046371, 0.03360623722728495, 0.06832114618968387, 0.06430225523734953, 0.06832114618968387, 0.06430225523734953, 0.08439670999902125, 0.05626447333268084, 0.07234003714201821, 0.05224558238034649, 0.04822669142801214, 0.04822669142801214, 0.05224558238034649, 0.04822669142801214, 0.0442078004756778, 0.0442078004756778, 0.040188909523343454, 0.040188909523343454, 0.02411334571400607, 0.032151127618674766, 0.02813223666634042, 0.02411334571400607, 0.09022255610326783, 0.06151537916131898, 0.05741435388389771, 0.06151537916131898, 0.07791948027100404, 0.06561640443874024, 0.06561640443874024, 0.05331332860647645, 0.04511127805163392, 0.06151537916131898, 0.041010252774212656, 0.04511127805163392, 0.03690922749679139, 0.03690922749679139, 0.04511127805163392, 0.03280820221937012, 0.03280820221937012, 0.03280820221937012, 0.03690922749679139, 0.024606151664527592, 0.07313838956255525, 0.06907514569796885, 0.06907514569796885, 0.06094865796879605, 0.08532812115631447, 0.06094865796879605, 0.06501190183338244, 0.05688541410420964, 0.048758926375036836, 0.048758926375036836, 0.04469568251045043, 0.04469568251045043, 0.04063243864586403, 0.036569194781277625, 0.036569194781277625, 0.02844270705210482, 0.03250595091669122, 0.03250595091669122, 0.03250595091669122, 0.03250595091669122, 0.06933510057141803, 0.06933510057141803, 0.06933510057141803, 0.06525656524368757, 0.05709949458822662, 0.06525656524368757, 0.05709949458822662, 0.048942423932765676, 0.0448638886050352, 0.048942423932765676, 0.0448638886050352, 0.0448638886050352, 0.0448638886050352, 0.048942423932765676, 0.0448638886050352, 0.036706817949574255, 0.036706817949574255, 0.04078535327730473, 0.032628282621843786, 0.02854974729411331, 0.0697568478089874, 0.07386019179775137, 0.08617022376404326, 0.06565350382022343, 0.06565350382022343, 0.05744681584269551, 0.06155015983145947, 0.05334347185393155, 0.04103343988763965, 0.045136783876403616, 0.05744681584269551, 0.04103343988763965, 0.036930095898875685, 0.036930095898875685, 0.045136783876403616, 0.032826751910111716, 0.036930095898875685, 0.032826751910111716, 0.032826751910111716, 0.028723407921347754, 0.0613890029498606, 0.07366680353983272, 0.06957420334317535, 0.0613890029498606, 0.06548160314651798, 0.06548160314651798, 0.06548160314651798, 0.04911120235988848, 0.04501860216323111, 0.040926001966573734, 0.0613890029498606, 0.04911120235988848, 0.04501860216323111, 0.04501860216323111, 0.040926001966573734, 0.03274080157325899, 0.03683340176991636, 0.03274080157325899, 0.028648201376601613, 0.028648201376601613, 0.062471435568022815, 0.07080096031042586, 0.0791304850528289, 0.06663619793922433, 0.05830667319682129, 0.07080096031042586, 0.05414191082561977, 0.04581238608321673, 0.04581238608321673, 0.05830667319682129, 0.04581238608321673, 0.04581238608321673, 0.04164762371201521, 0.05414191082561977, 0.03748286134081369, 0.033318098969612166, 0.033318098969612166, 0.033318098969612166, 0.033318098969612166, 0.033318098969612166, 0.07205706435432693, 0.06805389411241987, 0.060047553628605775, 0.06405072387051282, 0.06405072387051282, 0.060047553628605775, 0.060047553628605775, 0.06405072387051282, 0.05604438338669872, 0.04803804290288462, 0.04803804290288462, 0.05604438338669872, 0.04403487266097757, 0.036028532177163466, 0.04403487266097757, 0.03202536193525641, 0.03202536193525641, 0.03202536193525641, 0.02802219169334936, 0.02802219169334936, 0.07082757856957676, 0.06666125041842519, 0.07916023487187991, 0.08749289117418306, 0.054162265964970466, 0.06666125041842519, 0.054162265964970466, 0.04582960966266732, 0.04999593781381889, 0.04999593781381889, 0.04999593781381889, 0.04582960966266732, 0.04166328151151574, 0.03749695336036417, 0.04999593781381889, 0.02916429705806102, 0.033330625209212594, 0.02916429705806102, 0.033330625209212594, 0.024997968906909446, 0.06558967395716016, 0.06558967395716016, 0.0860864470687727, 0.09428515631341773, 0.06149031933483765, 0.05739096471251514, 0.05329161009019263, 0.05739096471251514, 0.04919225546787012, 0.04919225546787012, 0.04509290084554761, 0.04919225546787012, 0.03279483697858008, 0.03689419160090259, 0.03689419160090259, 0.03689419160090259, 0.03279483697858008, 0.02869548235625757, 0.02869548235625757, 0.03279483697858008, 0.06432912519324303, 0.06432912519324303, 0.06432912519324303, 0.06834969551782072, 0.07237026584239842, 0.06030855486866535, 0.0763908361669761, 0.04824684389493228, 0.052267414219509964, 0.052267414219509964, 0.052267414219509964, 0.040205703245776894, 0.040205703245776894, 0.03618513292119921, 0.040205703245776894, 0.040205703245776894, 0.03216456259662152, 0.03618513292119921, 0.028143992272043828, 0.03216456259662152, 0.06816854903919006, 0.06816854903919006, 0.08420820763664653, 0.06014871974046181, 0.07217846368855418, 0.06415863438982594, 0.05613880509109769, 0.04410906114300533, 0.05613880509109769, 0.05613880509109769, 0.03608923184427709, 0.04811897579236945, 0.04410906114300533, 0.040099146493641206, 0.040099146493641206, 0.03608923184427709, 0.03608923184427709, 0.03608923184427709, 0.028069402545548846, 0.028069402545548846, 0.07088694185962865, 0.0625473016408488, 0.05420766142206897, 0.058377481531458886, 0.07922658207840849, 0.0625473016408488, 0.058377481531458886, 0.058377481531458886, 0.06671712175023872, 0.041698201093899205, 0.045868021203289124, 0.05003784131267905, 0.041698201093899205, 0.041698201093899205, 0.041698201093899205, 0.029188740765729443, 0.03335856087511936, 0.029188740765729443, 0.03335856087511936, 0.029188740765729443, 0.07153557645547998, 0.08345817253139332, 0.06358717907153776, 0.06358717907153776, 0.06358717907153776, 0.05961298037956665, 0.06358717907153776, 0.04769038430365332, 0.04769038430365332, 0.04769038430365332, 0.04371618561168221, 0.05563878168759554, 0.04371618561168221, 0.0397419869197111, 0.04371618561168221, 0.03576778822773999, 0.03179358953576888, 0.03576778822773999, 0.02781939084379777, 0.03179358953576888, 0.06316198614670675, 0.0757943833760481, 0.05474038799381252, 0.06737278522315387, 0.07158358429960099, 0.05895118707025964, 0.0757943833760481, 0.06316198614670675, 0.050529588917365406, 0.046318789840918285, 0.04210799076447117, 0.04210799076447117, 0.046318789840918285, 0.03789719168802405, 0.04210799076447117, 0.03368639261157694, 0.03368639261157694, 0.03789719168802405, 0.02947559353512982, 0.02947559353512982, 0.06061277636608409, 0.06465362812382303, 0.06869447988156197, 0.06869447988156197, 0.0727353316393009, 0.08081703515477878, 0.05657192460834515, 0.04849022109286727, 0.04849022109286727, 0.04849022109286727, 0.04444936933512833, 0.04444936933512833, 0.04444936933512833, 0.04040851757738939, 0.03636766581965045, 0.03232681406191151, 0.03636766581965045, 0.03636766581965045, 0.03232681406191151, 0.03232681406191151, 0.07726898712049791, 0.07320219832468224, 0.07320219832468224, 0.06506862073305088, 0.0610018319372352, 0.052868254345603834, 0.06506862073305088, 0.06506862073305088, 0.0406678879581568, 0.0406678879581568, 0.052868254345603834, 0.052868254345603834, 0.0406678879581568, 0.0406678879581568, 0.03660109916234112, 0.03253431036652544, 0.03660109916234112, 0.028467521570709756, 0.028467521570709756, 0.03253431036652544, 0.062222954000785584, 0.06637115093417129, 0.0746675448009427, 0.070519347867557, 0.07881574173432841, 0.058074757067399875, 0.07881574173432841, 0.058074757067399875, 0.05392656013401417, 0.05392656013401417, 0.04563016626724276, 0.03733377240047135, 0.03733377240047135, 0.03733377240047135, 0.03318557546708564, 0.03318557546708564, 0.029037378533699938, 0.029037378533699938, 0.03733377240047135, 0.024889181600314232, 0.07434740409629553, 0.06195617008024628, 0.07021699275761245, 0.0578257587415632, 0.06195617008024628, 0.05369534740288011, 0.06195617008024628, 0.0578257587415632, 0.04956493606419703, 0.041304113386830855, 0.05369534740288011, 0.04956493606419703, 0.0578257587415632, 0.04543452472551394, 0.04543452472551394, 0.037173702048147766, 0.033043290709464684, 0.033043290709464684, 0.0289128793707816, 0.024782468032098513, 0.06329237718391657, 0.07173136080843878, 0.07173136080843878, 0.07173136080843878, 0.059072885371655476, 0.054853393559394364, 0.054853393559394364, 0.06329237718391657, 0.046414409934872156, 0.046414409934872156, 0.054853393559394364, 0.04219491812261105, 0.059072885371655476, 0.04219491812261105, 0.04219491812261105, 0.03375593449808884, 0.03375593449808884, 0.029536442685827738, 0.03375593449808884, 0.02531695087356663, 0.06854525315804545, 0.06854525315804545, 0.06854525315804545, 0.06854525315804545, 0.0728293314804233, 0.05569301819091193, 0.06854525315804545, 0.05569301819091193, 0.042840783223778406, 0.042840783223778406, 0.03855670490140057, 0.042840783223778406, 0.051408939868534086, 0.05569301819091193, 0.034272626579022726, 0.034272626579022726, 0.03855670490140057, 0.029988548256644886, 0.034272626579022726, 0.029988548256644886, 0.07004089812196115, 0.07828100378336836, 0.04944063396844317, 0.07004089812196115, 0.06180079246055396, 0.07004089812196115, 0.06180079246055396, 0.05768073962985037, 0.045320581137739575, 0.04944063396844317, 0.04944063396844317, 0.045320581137739575, 0.04120052830703597, 0.03708047547633238, 0.045320581137739575, 0.03296042264562878, 0.04120052830703597, 0.028840369814925184, 0.03296042264562878, 0.028840369814925184, 0.06074934632609448, 0.07694917201305301, 0.07289921559131338, 0.05669938990435485, 0.07289921559131338, 0.052649433482615214, 0.06074934632609448, 0.05669938990435485, 0.044549520639135955, 0.044549520639135955, 0.05669938990435485, 0.048599477060875584, 0.04049956421739632, 0.044549520639135955, 0.044549520639135955, 0.03644960779565669, 0.03644960779565669, 0.03239965137391706, 0.024299738530437792, 0.03644960779565669, 0.07146239043606979, 0.06725872041041862, 0.07146239043606979, 0.06305505038476747, 0.06305505038476747, 0.05464771033346513, 0.05044404030781397, 0.05464771033346513, 0.0588513803591163, 0.04203670025651164, 0.04203670025651164, 0.05044404030781397, 0.04203670025651164, 0.0462403702821628, 0.04203670025651164, 0.03783303023086048, 0.03362936020520931, 0.03362936020520931, 0.04203670025651164, 0.02942569017955815, 0.07014158365253834, 0.07014158365253834, 0.08664548568842971, 0.0660156081435655, 0.07426755916151118, 0.06188963263459265, 0.05363768161664696, 0.041259755089728435, 0.05363768161664696, 0.05363768161664696, 0.03713377958075559, 0.0577636571256198, 0.041259755089728435, 0.03713377958075559, 0.03713377958075559, 0.0288818285628099, 0.03300780407178275, 0.0288818285628099, 0.03300780407178275, 0.0288818285628099, 0.0625787239916277, 0.07509446878995324, 0.07509446878995324, 0.0625787239916277, 0.0625787239916277, 0.05423489412607734, 0.0417191493277518, 0.05006297919330216, 0.05006297919330216, 0.05423489412607734, 0.04589106426052698, 0.05006297919330216, 0.0417191493277518, 0.04589106426052698, 0.04589106426052698, 0.03337531946220144, 0.03754723439497662, 0.03754723439497662, 0.03337531946220144, 0.03337531946220144, 0.06358171777650602, 0.07947714722063254, 0.06358171777650602, 0.06358171777650602, 0.06358171777650602, 0.06358171777650602, 0.0755032898596009, 0.051660145693411146, 0.04768628833237952, 0.05563400305444278, 0.043712430971347895, 0.051660145693411146, 0.03973857361031627, 0.03973857361031627, 0.03973857361031627, 0.03179085888825301, 0.03179085888825301, 0.03179085888825301, 0.03179085888825301, 0.02781700152722139, 0.0671220300166413, 0.06317367530978005, 0.0750187394303638, 0.08686380355094755, 0.0750187394303638, 0.05922532060291879, 0.05132861118919628, 0.05132861118919628, 0.04343190177547378, 0.05132861118919628, 0.05527696589605754, 0.04738025648233503, 0.04343190177547378, 0.04343190177547378, 0.035535192361751275, 0.031586837654890024, 0.031586837654890024, 0.02763848294802877, 0.02763848294802877, 0.02763848294802877, 0.06502313429424438, 0.06068825867462808, 0.06502313429424438, 0.07802776115309325, 0.06502313429424438, 0.047683631815779205, 0.06068825867462808, 0.052018507435395496, 0.047683631815779205, 0.052018507435395496, 0.052018507435395496, 0.039013880576546624, 0.043348756196162914, 0.039013880576546624, 0.047683631815779205, 0.03467900495693033, 0.03467900495693033, 0.039013880576546624, 0.03467900495693033, 0.03467900495693033, 0.0694570911961617, 0.0694570911961617, 0.06537137994932865, 0.0694570911961617, 0.0694570911961617, 0.0694570911961617, 0.04494282371516345, 0.057199957455662566, 0.04902853496199649, 0.05311424620882953, 0.04494282371516345, 0.04494282371516345, 0.0408571124683304, 0.04494282371516345, 0.0408571124683304, 0.036771401221497364, 0.028599978727831283, 0.036771401221497364, 0.028599978727831283, 0.036771401221497364, 0.06940671753917965, 0.06532396944863968, 0.053075725177019736, 0.05715847326755972, 0.08165496181079959, 0.07348946562971964, 0.0612412213580997, 0.040827480905399796, 0.0612412213580997, 0.04899297708647976, 0.04899297708647976, 0.04491022899593978, 0.040827480905399796, 0.04491022899593978, 0.040827480905399796, 0.03674473281485982, 0.03266198472431984, 0.03266198472431984, 0.03266198472431984, 0.02857923663377986, 0.06365873400098607, 0.07559474662617097, 0.0676374048760477, 0.07559474662617097, 0.08753075925135585, 0.05968006312592444, 0.05570139225086281, 0.05570139225086281, 0.043765379625677926, 0.043765379625677926, 0.04774405050073956, 0.05172272137580118, 0.039786708750616295, 0.035808037875554664, 0.043765379625677926, 0.03182936700049303, 0.027850696125431406, 0.035808037875554664, 0.03182936700049303, 0.027850696125431406, 0.06078423762652098, 0.07699336766025991, 0.07294108515182518, 0.06888880264339044, 0.07294108515182518, 0.06078423762652098, 0.05673195511808625, 0.06078423762652098, 0.052679672609651514, 0.04457510759278205, 0.04052282508434732, 0.04052282508434732, 0.04052282508434732, 0.04052282508434732, 0.04052282508434732, 0.04457510759278205, 0.032418260067477854, 0.03647054257591259, 0.032418260067477854, 0.028365977559043125, 0.07546519338561875, 0.07127268264197326, 0.07127268264197326, 0.05031012892374583, 0.07127268264197326, 0.054502639667391314, 0.054502639667391314, 0.046117618180100343, 0.046117618180100343, 0.05031012892374583, 0.06288766115468229, 0.05031012892374583, 0.04192510743645486, 0.046117618180100343, 0.05031012892374583, 0.03354008594916389, 0.03773259669280937, 0.025155064461872916, 0.03354008594916389, 0.0293475752055184, 0.07111383401773122, 0.058564333896955124, 0.06693066731080585, 0.06693066731080585, 0.06274750060388049, 0.058564333896955124, 0.058564333896955124, 0.058564333896955124, 0.05019800048310439, 0.04183166706925366, 0.058564333896955124, 0.04601483377617903, 0.05019800048310439, 0.04183166706925366, 0.05438116719002976, 0.029282166948477562, 0.037648500362328294, 0.033465333655402926, 0.029282166948477562, 0.025099000241552195, 0.06833761440223894, 0.05225817571935919, 0.07235747407295887, 0.08039719341439874, 0.07235747407295887, 0.064317754731519, 0.05225817571935919, 0.04823831604863925, 0.04823831604863925, 0.05225817571935919, 0.04823831604863925, 0.04823831604863925, 0.04019859670719937, 0.04019859670719937, 0.05225817571935919, 0.03617873703647943, 0.03617873703647943, 0.0321588773657595, 0.03617873703647943, 0.024119158024319624, 0.06930554298138562, 0.052998356397530186, 0.061151949689457905, 0.08968952621120493, 0.06522874633542176, 0.057075153043494045, 0.06930554298138562, 0.057075153043494045, 0.057075153043494045, 0.052998356397530186, 0.04484476310560247, 0.048921559751566326, 0.0407679664596386, 0.03669116981367474, 0.0407679664596386, 0.028537576521747023, 0.03261437316771088, 0.03261437316771088, 0.03261437316771088, 0.028537576521747023, 0.06775606459048646, 0.06775606459048646, 0.08369866802354209, 0.06377041373222254, 0.06775606459048646, 0.06377041373222254, 0.07572736630701427, 0.05181346115743082, 0.05181346115743082, 0.04782781029916691, 0.043842159440903, 0.043842159440903, 0.043842159440903, 0.03985650858263909, 0.03985650858263909, 0.03188520686611127, 0.03188520686611127, 0.035870857724375184, 0.027899556007847364, 0.027899556007847364, 0.06710374418110375, 0.06710374418110375, 0.07894558138953382, 0.07105102325058044, 0.06315646511162706, 0.07105102325058044, 0.05920918604215037, 0.05526190697267368, 0.043420069764243605, 0.043420069764243605, 0.03947279069476691, 0.05526190697267368, 0.043420069764243605, 0.03947279069476691, 0.03947279069476691, 0.03552551162529022, 0.03157823255581353, 0.03947279069476691, 0.02763095348633684, 0.02763095348633684, 0.07283105769618778, 0.06473871795216692, 0.06473871795216692, 0.07283105769618778, 0.056646378208146046, 0.06878488782417734, 0.06069254808015648, 0.06473871795216692, 0.048554038464125183, 0.048554038464125183, 0.04046169872010432, 0.06069254808015648, 0.048554038464125183, 0.04046169872010432, 0.03641552884809389, 0.03236935897608346, 0.03236935897608346, 0.03236935897608346, 0.03236935897608346, 0.024277019232062592, 0.06582736715044905, 0.05759894625664293, 0.06582736715044905, 0.05759894625664293, 0.06582736715044905, 0.06994157759735213, 0.05759894625664293, 0.053484735809739864, 0.053484735809739864, 0.06994157759735213, 0.04525631491593373, 0.04525631491593373, 0.04525631491593373, 0.03291368357522453, 0.04525631491593373, 0.03291368357522453, 0.03291368357522453, 0.0370278940221276, 0.03291368357522453, 0.028799473128321464, 0.0779679513383341, 0.06565722217964977, 0.07386437495210599, 0.06976079856587789, 0.06976079856587789, 0.05745006940719355, 0.06155364579342166, 0.04513934024850922, 0.04513934024850922, 0.04513934024850922, 0.04513934024850922, 0.05334649302096544, 0.04103576386228111, 0.05334649302096544, 0.04103576386228111, 0.032828611089824884, 0.032828611089824884, 0.036932187476052994, 0.028725034703596775, 0.028725034703596775, 0.06321818831932861, 0.06321818831932861, 0.06716932508928665, 0.07902273539916076, 0.06716932508928665, 0.05531591477941253, 0.05136477800945449, 0.05136477800945449, 0.043462504469538414, 0.05136477800945449, 0.05136477800945449, 0.05136477800945449, 0.04741364123949646, 0.03951136769958038, 0.03951136769958038, 0.03160909415966431, 0.043462504469538414, 0.03951136769958038, 0.03160909415966431, 0.03556023092962234, 0.0886646339207533, 0.06851358075694573, 0.06851358075694573, 0.06448337012418422, 0.0604531594914227, 0.0604531594914227, 0.06448337012418422, 0.0604531594914227, 0.04836252759313816, 0.040302106327615134, 0.052392738225899675, 0.052392738225899675, 0.03224168506209211, 0.036271895694853624, 0.040302106327615134, 0.03224168506209211, 0.03224168506209211, 0.03224168506209211, 0.036271895694853624, 0.02418126379656908, 0.07056940494809961, 0.07056940494809961, 0.07887168788317016, 0.07472054641563489, 0.058115980545493794, 0.062267122013029065, 0.04981369761042326, 0.07056940494809961, 0.04981369761042326, 0.041511414675352715, 0.04981369761042326, 0.04981369761042326, 0.045662556142887986, 0.03736027320781744, 0.03736027320781744, 0.029057990272746897, 0.03320913174028217, 0.03320913174028217, 0.029057990272746897, 0.02490684880521163, 0.07343064049557042, 0.05711272038544366, 0.06119220041297535, 0.06935116046803873, 0.05711272038544366, 0.06935116046803873, 0.07343064049557042, 0.05711272038544366, 0.053033240357911966, 0.053033240357911966, 0.04487428030284859, 0.04487428030284859, 0.04487428030284859, 0.03671532024778521, 0.03671532024778521, 0.032635840220253516, 0.03671532024778521, 0.032635840220253516, 0.02855636019272183, 0.02855636019272183, 0.08366057651070992, 0.07170906558060851, 0.06374139162720756, 0.06772522860390803, 0.06772522860390803, 0.06374139162720756, 0.04780604372040567, 0.055773717673806615, 0.06374139162720756, 0.05178988069710614, 0.0438222067437052, 0.0438222067437052, 0.03983836976700472, 0.035854532790304254, 0.0438222067437052, 0.035854532790304254, 0.035854532790304254, 0.03187069581360378, 0.03187069581360378, 0.027886858836903308, 0.07997136733936927, 0.0883894060066713, 0.07576234800571825, 0.07576234800571825, 0.054717251337463184, 0.06734430933841623, 0.042090193336510144, 0.05050823200381217, 0.054717251337463184, 0.046299212670161155, 0.046299212670161155, 0.046299212670161155, 0.046299212670161155, 0.046299212670161155, 0.033672154669208115, 0.0294631353355571, 0.0294631353355571, 0.033672154669208115, 0.0294631353355571, 0.0294631353355571, 0.06834951210314975, 0.07639063117410855, 0.07639063117410855, 0.06834951210314975, 0.07237007163862916, 0.08041119070958795, 0.05628783349671156, 0.052267273961232166, 0.04422615489027337, 0.04824671442575277, 0.040205595354793976, 0.04824671442575277, 0.03618503581931458, 0.04824671442575277, 0.03618503581931458, 0.02814391674835578, 0.032164476283835175, 0.03618503581931458, 0.032164476283835175, 0.024123357212876385, 0.07562115628595434, 0.07562115628595434, 0.06301763023829529, 0.06301763023829529, 0.0588164548890756, 0.06301763023829529, 0.06301763023829529, 0.0588164548890756, 0.05041410419063623, 0.04621292884141654, 0.05041410419063623, 0.04201175349219686, 0.04201175349219686, 0.03781057814297717, 0.03360940279375749, 0.03360940279375749, 0.03360940279375749, 0.04201175349219686, 0.0294082274445378, 0.0294082274445378, 0.05763604141610602, 0.07822034192185816, 0.05763604141610602, 0.06586976161840688, 0.07410348182070774, 0.05351918131495559, 0.05763604141610602, 0.045285461112654725, 0.045285461112654725, 0.05763604141610602, 0.05351918131495559, 0.06586976161840688, 0.0411686010115043, 0.0411686010115043, 0.0411686010115043, 0.03293488080920344, 0.03705174091035387, 0.03705174091035387, 0.03705174091035387, 0.024701160606902577, 0.07225037157711028, 0.07626428110917197, 0.06422255251298692, 0.06422255251298692, 0.0682364620450486, 0.06422255251298692, 0.06020864298092524, 0.06422255251298692, 0.04415300485267851, 0.05218082391680187, 0.04415300485267851, 0.040139095320616824, 0.03612518578855514, 0.04415300485267851, 0.03612518578855514, 0.03612518578855514, 0.03211127625649346, 0.028097366724431777, 0.03612518578855514, 0.028097366724431777, 0.06527648239163521, 0.0693562625411124, 0.081595602989544, 0.06119670224215801, 0.06119670224215801, 0.06527648239163521, 0.05711692209268081, 0.04895736179372641, 0.05303714194320361, 0.0448775816442492, 0.04895736179372641, 0.04895736179372641, 0.0367180213452948, 0.0448775816442492, 0.04895736179372641, 0.0367180213452948, 0.032638241195817604, 0.032638241195817604, 0.028558461046340404, 0.024478680896863204, 0.06162574934211569, 0.06984251592106445, 0.06573413263159007, 0.06573413263159007, 0.06573413263159007, 0.06573413263159007, 0.06573413263159007, 0.06162574934211569, 0.045192216184218176, 0.041083832894743794, 0.04930059947369256, 0.06162574934211569, 0.041083832894743794, 0.045192216184218176, 0.041083832894743794, 0.032867066315795036, 0.028758683026320658, 0.032867066315795036, 0.028758683026320658, 0.02465029973684628, 0.07043733839272279, 0.06629396554609203, 0.07458071123935354, 0.06629396554609203, 0.07043733839272279, 0.062150592699461285, 0.058007219852830534, 0.049720474159569025, 0.06629396554609203, 0.045577101312938274, 0.049720474159569025, 0.04143372846630752, 0.04143372846630752, 0.04143372846630752, 0.03729035561967677, 0.03729035561967677, 0.03729035561967677, 0.029003609926415267, 0.029003609926415267, 0.024860237079784513, 0.06942619339954453, 0.06942619339954453, 0.05640878213712994, 0.06508705631207301, 0.060747919224601474, 0.06508705631207301, 0.060747919224601474, 0.05640878213712994, 0.04339137087471534, 0.05206964504965841, 0.05640878213712994, 0.04773050796218687, 0.039052233787243804, 0.04773050796218687, 0.039052233787243804, 0.03471309669977227, 0.04339137087471534, 0.039052233787243804, 0.030373959612300737, 0.026034822524829204, 0.06103446006912626, 0.05696549606451784, 0.07324135208295152, 0.07324135208295152, 0.08544824409677676, 0.05696549606451784, 0.06103446006912626, 0.05289653205990942, 0.04882756805530101, 0.05696549606451784, 0.04882756805530101, 0.03662067604147576, 0.04475860405069259, 0.03662067604147576, 0.03662067604147576, 0.04068964004608417, 0.03662067604147576, 0.03255171203686734, 0.03255171203686734, 0.02848274803225892, 0.06874900951765056, 0.08088118766782418, 0.07279306890104177, 0.06874900951765056, 0.06066089075086814, 0.05661683136747693, 0.05661683136747693, 0.07279306890104177, 0.0444846532173033, 0.05257277198408572, 0.0444846532173033, 0.0444846532173033, 0.0444846532173033, 0.0444846532173033, 0.04044059383391209, 0.032352475067129675, 0.032352475067129675, 0.028308415683738465, 0.028308415683738465, 0.028308415683738465, 0.0860212468813782, 0.0573474979209188, 0.06553999762390719, 0.0696362474754014, 0.06553999762390719, 0.06553999762390719, 0.06553999762390719, 0.0573474979209188, 0.040962498514942, 0.06553999762390719, 0.045058748366436197, 0.0368662486634478, 0.0368662486634478, 0.040962498514942, 0.045058748366436197, 0.032769998811953596, 0.032769998811953596, 0.0368662486634478, 0.0368662486634478, 0.0286737489604594, 0.07056026633449758, 0.053957850726380506, 0.062259058530439046, 0.06640966243246832, 0.07471087023652685, 0.05810845462840977, 0.06640966243246832, 0.053957850726380506, 0.045656642922321966, 0.053957850726380506, 0.04980724682435123, 0.04980724682435123, 0.05810845462840977, 0.037355435118263426, 0.037355435118263426, 0.037355435118263426, 0.03320483121623416, 0.03320483121623416, 0.029054227314204886, 0.024903623412175616, 0.07295704259863803, 0.06485070453212269, 0.0770102116318957, 0.06485070453212269, 0.06890387356538036, 0.08106338066515337, 0.056744366465607356, 0.056744366465607356, 0.048638028399092016, 0.048638028399092016, 0.048638028399092016, 0.040531690332576684, 0.032425352266061344, 0.044584859365834346, 0.036478521299319014, 0.032425352266061344, 0.032425352266061344, 0.028372183232803678, 0.032425352266061344, 0.028372183232803678, 0.06314988288699733, 0.06314988288699733, 0.07156986727193031, 0.07998985165686329, 0.05893989069453084, 0.05893989069453084, 0.05893989069453084, 0.050519906309597865, 0.042099921924664885, 0.050519906309597865, 0.05472989850206435, 0.06314988288699733, 0.037889929732198395, 0.033679937539731905, 0.037889929732198395, 0.037889929732198395, 0.033679937539731905, 0.033679937539731905, 0.037889929732198395, 0.02946994534726542, 0.0803850143007251, 0.06769264362166325, 0.0719234338480172, 0.05500027294260138, 0.0719234338480172, 0.059231063168955336, 0.05076948271624743, 0.05076948271624743, 0.05500027294260138, 0.04653869248989348, 0.05076948271624743, 0.05076948271624743, 0.03807711203718557, 0.05076948271624743, 0.03807711203718557, 0.029615531584477668, 0.03384632181083162, 0.029615531584477668, 0.03384632181083162, 0.025384741358123714, 0.09448409976413406, 0.06572806940113674, 0.06572806940113674, 0.06572806940113674, 0.061620065063565696, 0.07394407807627883, 0.06572806940113674, 0.04929605205085255, 0.045188047713281505, 0.045188047713281505, 0.045188047713281505, 0.03286403470056837, 0.03697203903813941, 0.045188047713281505, 0.03697203903813941, 0.03697203903813941, 0.03286403470056837, 0.03286403470056837, 0.03286403470056837, 0.02875603036299732, 0.0663882670618533, 0.07468680044458496, 0.07053753375321914, 0.0788360671359508, 0.0663882670618533, 0.06223900037048747, 0.05808973367912164, 0.045641933605024144, 0.049791200296389976, 0.045641933605024144, 0.045641933605024144, 0.049791200296389976, 0.045641933605024144, 0.03734340022229248, 0.04149266691365831, 0.03734340022229248, 0.04149266691365831, 0.02904486683956082, 0.02904486683956082, 0.024895600148194988, 0.09346472091302403, 0.0731463033232362, 0.08127367035915134, 0.06908261980527863, 0.060955252769363505, 0.06501893628732107, 0.060955252769363505, 0.04063683517957567, 0.044700518697533234, 0.048764202215490805, 0.04063683517957567, 0.044700518697533234, 0.04063683517957567, 0.032509468143660535, 0.0365731516616181, 0.032509468143660535, 0.032509468143660535, 0.028445784625702967, 0.0365731516616181, 0.028445784625702967, 0.06721012990018385, 0.07981202925646833, 0.07141076301894535, 0.06721012990018385, 0.07141076301894535, 0.06721012990018385, 0.05460823054389938, 0.05460823054389938, 0.042006331187614904, 0.06300949678142236, 0.0462069643063764, 0.0462069643063764, 0.042006331187614904, 0.042006331187614904, 0.033605064950091926, 0.033605064950091926, 0.033605064950091926, 0.029404431831330433, 0.033605064950091926, 0.025203798712568944, 0.06597040308010159, 0.06597040308010159, 0.06597040308010159, 0.07421670346511428, 0.06597040308010159, 0.06184725288759523, 0.06184725288759523, 0.05360095250258254, 0.04535465211756984, 0.04947780231007619, 0.05772410269508889, 0.04947780231007619, 0.04123150192506349, 0.03710835173255714, 0.04123150192506349, 0.03710835173255714, 0.028862051347544444, 0.032985201540050794, 0.028862051347544444, 0.03710835173255714, 0.07256779791753006, 0.060473164931275046, 0.06450470926002672, 0.06853625358877839, 0.08869397523253673, 0.060473164931275046, 0.056441620602523376, 0.056441620602523376, 0.056441620602523376, 0.05241007627377171, 0.0403154432875167, 0.04434698761626837, 0.0403154432875167, 0.03628389895876503, 0.03628389895876503, 0.03225235463001336, 0.03628389895876503, 0.03225235463001336, 0.03225235463001336, 0.028220810301261688, 0.0790333018155351, 0.06322664145242808, 0.05532331127087457, 0.06717830654320484, 0.06322664145242808, 0.06717830654320484, 0.07508163672475834, 0.05927497636165133, 0.05137164618009781, 0.04741998108932106, 0.03951665090776755, 0.0434683159985443, 0.0434683159985443, 0.03951665090776755, 0.04741998108932106, 0.03161332072621404, 0.03161332072621404, 0.035564985816990795, 0.03951665090776755, 0.02370999054466053, 0.0671346409777678, 0.06293872591665732, 0.0671346409777678, 0.07133055603887829, 0.058742810855546834, 0.06293872591665732, 0.06293872591665732, 0.058742810855546834, 0.050350980733325855, 0.058742810855546834, 0.04615506567221537, 0.05454689579443634, 0.03776323554999439, 0.03776323554999439, 0.0335673204888839, 0.04615506567221537, 0.0335673204888839, 0.0335673204888839, 0.029371405427773417, 0.029371405427773417, 0.0835667484178906, 0.07162864150104908, 0.07560801047332959, 0.07560801047332959, 0.05969053458420757, 0.06366990355648808, 0.055711165611927065, 0.04775242766736606, 0.05173179663964656, 0.039793689722805045, 0.039793689722805045, 0.04377305869508555, 0.04377305869508555, 0.05173179663964656, 0.04377305869508555, 0.03581432075052454, 0.03183495177824404, 0.03581432075052454, 0.03183495177824404, 0.019896844861402523, 0.06756018513543618, 0.07550844221019339, 0.06756018513543618, 0.06358605659805759, 0.06358605659805759, 0.06358605659805759, 0.07153431367281479, 0.05166367098592179, 0.05166367098592179, 0.04768954244854319, 0.04371541391116459, 0.03974128537378599, 0.05166367098592179, 0.04371541391116459, 0.03974128537378599, 0.031793028299028796, 0.031793028299028796, 0.031793028299028796, 0.031793028299028796, 0.027818899761650195, 0.06333091313001013, 0.07599709575601216, 0.06755297400534414, 0.06333091313001013, 0.07177503488067816, 0.059108852254676124, 0.06333091313001013, 0.050664730504008104, 0.05488679137934212, 0.0464426696286741, 0.04222060875334009, 0.04222060875334009, 0.04222060875334009, 0.03799854787800608, 0.04222060875334009, 0.029554426127338062, 0.03377648700267207, 0.03799854787800608, 0.029554426127338062, 0.03799854787800608, 0.05907427372302818, 0.07173304666367708, 0.06329386470324448, 0.07173304666367708, 0.07595263764389337, 0.06329386470324448, 0.07173304666367708, 0.05485468274281188, 0.046415500782379285, 0.046415500782379285, 0.042195909802162986, 0.042195909802162986, 0.046415500782379285, 0.033756727841730394, 0.037976318821946686, 0.033756727841730394, 0.042195909802162986, 0.02953713686151409, 0.02953713686151409, 0.02953713686151409, 0.07448829742765861, 0.06621181993569655, 0.07448829742765861, 0.07035005868167758, 0.07448829742765861, 0.05793534244373448, 0.05793534244373448, 0.04138238745981034, 0.05793534244373448, 0.04552062620579138, 0.049658864951772415, 0.04138238745981034, 0.03310590996784828, 0.04552062620579138, 0.04138238745981034, 0.04138238745981034, 0.02896767122186724, 0.03310590996784828, 0.03310590996784828, 0.024829432475886207], \"Term\": [\"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1992a\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"1o\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"2c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5c\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"5i\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"6b\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"__\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"a_\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absence\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"absorbing\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"abstract_describe\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"academy_science\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acknowledgment_author\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"acoustic_speech\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"actuator\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"ad_hoc\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"adapts\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"addi_tional\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additionally\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"additive_gaussian\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"aggregate\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"ai_ai\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"allowing\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"alternate\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amherst\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"amir\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analysed\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"analyzer\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"ance\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"andrew_barto\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"annealed\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ansatz\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ant\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"ao\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"applying\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"appropriate\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximates\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arithmetic\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"artificially\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"ary\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"asynchronously\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attractor\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"attributed\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"automated\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"ave\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"averaged\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"az\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"back_propagating\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bartlett\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"bayesian_restoration\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"begun\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"bell_lab\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"belmont\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"ben\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"benchmark\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berger\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_ca\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berkeley_edu\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"berlin\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"bernhard\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"better_understanding\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"biological_cybernetics\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"bishop\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"boltzmann_machine\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bringing\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"bu\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"california_institute\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"caltech_edu\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"cam_bridge\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capability\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cardoso\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"categorization\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"centered_around\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"central\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changed\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"chapter_page\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"characterization\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"checking\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chemistry\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"chitecture\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"cij\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"circuitry\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"ck\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classi_fication\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classifi_cation\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closed\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"closer\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"co\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"coincide\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"colorado_edu\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"com_abstract\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"comp_physiol\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"compactly\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"comparable\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"compete\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tation\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"compu_tational\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"computer_society\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"con_tinuous\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"concatenation\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"conceptually\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concise\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"concluding_remark\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"conclusion_future\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"concrete\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"condi\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_density\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conditional_independence\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conf\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conference_acoustic\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conflicting\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"conjugate_gradient\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"consis\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constituent\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"constrained\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"construction\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contribute\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"contributing\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conveniently\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"conver_gence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"converter\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveyed\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"conveys\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copied\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"cor_responding\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"corpus\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"corrupted\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"cre\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"crl\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cse\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cubic\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decision_boundary\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"decomposed\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"default\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"degree_freedom\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demanding\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"denoted\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"depends_upon\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"dept_electrical\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"derivation\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"descriptive\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"designing\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"determines\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"deterministic_annealing\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"devel\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"developed\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"dif_ferent\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differentiate\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"differs\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digit_recognition\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"digitized\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discriminated\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"discrimination\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distant\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"distribu_tions\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"doubled\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"duce\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamic_programming\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"dynamical_system\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"ed_advance\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"edition\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"el\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elec\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"elicited\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"eliminated\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"embodies\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"emphasis\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"empirically\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"emulate\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encoded\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"encountered\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"englewood_cliff\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"english_text\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"ensure\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equipped\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"equiv\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"er\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"erlbaum\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"essential\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"etc\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"eval\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"ex_ample\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exceeding\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"exception\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"exp_exp\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expands\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"expected_reward\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experi_ments\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"experiment_conducted\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"exploration_microstructure\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"expressing\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"ez\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"fac\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"far_away\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"favorable\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ference\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"ff\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fi_fi\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"fig_4b\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"filling\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"finer\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"fitted\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"floating_point\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forced\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"forcing\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"formally\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fraction\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfill\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fulfilled\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fully\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"fundamentally\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gas\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gate_voltage\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaus_sian\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geiger\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"geman\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gen_eral\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"generafive\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"general_ization\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalised\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalize_well\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"generalizing\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"germany_abstract\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"ghahramani\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"girosi\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gl\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"gradient_ascent\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"graphic\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"gray_level\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"grossberg\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guaranteed_converge\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"guyon\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"ha_proven\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"hamilton\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_digit\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"handwritten_zip\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"harvard\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"haussler\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"hop_field\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"horizontal_vertical\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"hornik\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubbard\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hubel_wiesel\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"hy\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"i0\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ia\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ib\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"identically\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"iii\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"ill\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrated_fig\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"illustrative\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"imagine\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"implication\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"incorrect\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"independently\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indication\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indicative\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"indirectly\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"infor_mation\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"ingredient\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"inhibitory_connection\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"ini\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"initial_condition\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"institute_technology\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"int_conf\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"integrator\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"interested_reader\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"international_journal\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpolating\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"interpretable\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"introduced\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"investigated\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"involving\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"iris\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"irrelevant\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"isolation\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iter\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"itive\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"iw\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"ized\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jackel\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jensen_inequality\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"jerusalem_israel\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"john_denker\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"joint_conf\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"jp\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kinetic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kluwer_academic\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"kn\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"kr\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"ku\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kuhn\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"kung\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"l_\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"large_enough\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"lateral_geniculate\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"le\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"leaky\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"letting\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linguistic\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"listed_table\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"loading\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"local_maximum\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"located\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log2\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"log_log\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithm\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmic\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"logarithmically\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"look_like\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lq\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lu\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"macroscopic\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"main_reason\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"maintaining\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"make_sense\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"mason\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"massachusetts_amherst\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mated\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mation\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"mations\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"max_planck\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximally\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"maximized\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mcgraw_hill\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mdl\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"mech\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"metropolis\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"michael_mozer\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microelectronics\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"microstructure_cognition\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"millisecond\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"mn\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modification\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"modifies\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"motor_control\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_simpler\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"much_smaller\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"muscle\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"mutation\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"n2\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"named\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"nation\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"national_laboratory\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"necessity\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"neigh\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nervous_system\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"nettalk\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neu_ron\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuromorphic\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"nist\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"non_zero\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"notation\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"np\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"numerous\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obtaining\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"obviously\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"occurring\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oj\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"oja\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"ontario\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"oped\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"operate\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"opt_soc\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"optimum\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oriented\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"oscillatory\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"ously\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"overfit\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"p2\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"pa_rameters\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"page_springer\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paired\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_describe\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"paper_focus\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"par\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"parametric\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"park\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partial_derivative\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"partially\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"particu\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioned\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"partitioning\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalize\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"performs\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"physiology\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"picked\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"pittsburgh_pa\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"placing\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"platt\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"play_important\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"pointing\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"poisson\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"port\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"possibility\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"posteriori_probability\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"postsynaptic_potential\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"potentiation\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"pre_post\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"predefined\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"preset\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"previous\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"pro_gramming\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"processing_exploration\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"prog\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"projected_onto\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"proof_theorem\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"propagated\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"providing\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"psy\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purely\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"purpose\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"pyramid\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quarter\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"quiescent\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"ra\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"radius\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"ratio\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"recalled\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"receives\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recon\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"recovered\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"rectangular\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"refractory_period\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rejection\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"rela\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relatively_simple\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"reliable\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"rem\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"remained\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"removal\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacement\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"replacing\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"repre_sentations\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"represents\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"researcher\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"responding\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rev\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rewritten\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"richer\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"rij\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rodney\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"roughly\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"route\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"royal_society\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"rumelhart_mcclelland\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"ry\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"s0\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"salient\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"san_francisco\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"satisfies\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturated\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"saturating\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"school_computer\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"schwartz\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"scott\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"se_quence\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"secondly\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seek\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"seemingly\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"selec\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"self\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"semantics\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sending\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sens\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sensorimotor\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"sept\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"september\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequential_decision\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"sequentially\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"short_term\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"silverman\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"simulates\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sine_wave\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"sion\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"sition\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"slightly_different\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smaller\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"smc\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"sollich\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"solves\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sorted\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"sparsely\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"speaker_dependent\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_case\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"special_purpose\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specification\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"specified\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"spring\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"sr\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stabilization\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"standing\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stanford\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"steady_state\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stimulated\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"strongest\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"structuring\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sual\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"sub\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"substantially\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"subthreshold\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"successfully_applied\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suffers\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"suited\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sume\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"sur\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"survey\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"susceptible\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sutton_barto\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sv\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"sys\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"syst\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"systematic\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"table_summarizes\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"take_place\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"taken_account\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"taylor_series\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"technical_report\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"temporary\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"ter\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"terminated\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"termination\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"thomas\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"time_varying\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tit\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tolerant\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"topology\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"touretzky_hinton\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"tran\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transforms\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"treating\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"tri\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"triangular\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tune\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"tuple\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"two_dimensional\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unchanged\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"unclear\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"undergoes\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"unequal\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"uniform_distribution\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unifying\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"unique_solution\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"university_maryland\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unobserved\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unseen\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"untrained\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"updated\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"upper_lower\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"usefulness\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"valuable\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_chervonenkis\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vapnik_estimation\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"vari_ables\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"variate\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vation\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vg\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"vi\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"viewing\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"visited\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vlsi_implementation\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"vn\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"voiced\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"volume_page\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"vowel\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wagner\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"wang\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"well_suited\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"whilst\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiley\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"wiring\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"world_scientific\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"wright\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"writing\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wta\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"wx\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"x1\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xo\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"xp\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z0\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"z_\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zip_code\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\", \"zipset\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [10, 14, 9, 13, 1, 7, 3, 6, 12, 8, 17, 5, 18, 19, 20, 15, 4, 16, 11, 2]};\n","\n","function LDAvis_load_lib(url, callback){\n","  var s = document.createElement('script');\n","  s.src = url;\n","  s.async = true;\n","  s.onreadystatechange = s.onload = callback;\n","  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n","  document.getElementsByTagName(\"head\")[0].appendChild(s);\n","}\n","\n","if(typeof(LDAvis) !== \"undefined\"){\n","   // already loaded: just create the visualization\n","   !function(LDAvis){\n","       new LDAvis(\"#\" + \"ldavis_el1211396432301332489065514564\", ldavis_el1211396432301332489065514564_data);\n","   }(LDAvis);\n","}else if(typeof define === \"function\" && define.amd){\n","   // require.js is available: use it to load d3/LDAvis\n","   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n","   require([\"d3\"], function(d3){\n","      window.d3 = d3;\n","      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n","        new LDAvis(\"#\" + \"ldavis_el1211396432301332489065514564\", ldavis_el1211396432301332489065514564_data);\n","      });\n","    });\n","}else{\n","    // require.js not available: dynamically load d3 & LDAvis\n","    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n","         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n","                 new LDAvis(\"#\" + \"ldavis_el1211396432301332489065514564\", ldavis_el1211396432301332489065514564_data);\n","            })\n","         });\n","}\n","</script>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"1lOrOc8XR2w9","colab_type":"code","colab":{}},"source":["from gensim.models.ldamodel import LdaModel\n","def convertldaGenToldaMallet(mallet_model):\n","    model_gensim = LdaModel(\n","        id2word=mallet_model.id2word, num_topics=mallet_model.num_topics,\n","        alpha=mallet_model.alpha) # original function has 'eta=0' argument\n","    model_gensim.state.sstats[...] = mallet_model.wordtopics\n","    model_gensim.sync_state()\n","    return model_gensim"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4i2BKuppWuZj","colab_type":"code","colab":{}},"source":["new_lda = convertldaGenToldaMallet(opt_lda_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QPncZf6MWufA","colab_type":"code","outputId":"67b2ff43-c7de-4833-cb72-361cfd747fec","executionInfo":{"status":"ok","timestamp":1590838681149,"user_tz":-180,"elapsed":12323,"user":{"displayName":"Şenol Kurt","photoUrl":"","userId":"17783348337021535530"}},"colab":{"base_uri":"https://localhost:8080/","height":861}},"source":["vis_data = gensimvis.prepare(new_lda, bow_corpus, dictionary)\n","pyLDAvis.display(vis_data)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n","\n","\n","<div id=\"ldavis_el1211396432034480483565259572\"></div>\n","<script type=\"text/javascript\">\n","\n","var ldavis_el1211396432034480483565259572_data = {\"mdsDat\": {\"x\": [0.21622679706792122, 0.2031447390377533, -0.04649559067549921, 0.19875362866363824, -0.18241330459700802, 0.06064653393977317, -0.0684861216164837, -0.20035402256762525, 0.12640805010995004, 0.14889402313572725, 0.08022808417339265, -0.09300616142284028, 0.10746771225412145, -0.003994637450516309, -0.11003594827943901, -0.1290582598625202, -0.0429133373790176, -0.11277493069524794, -0.015826596964681926, -0.13641065687139797], \"y\": [0.06547905483721636, 0.08644726840810657, -0.15191251148123572, 0.14036711973986984, 0.18831307887377344, -0.218696451371223, -0.06810432365850017, 0.127196706653986, 0.12444701545378717, -0.055714077467755786, -0.07578958048816324, -0.06428536945145966, 0.06636756140157969, -0.05841495841829727, -0.002340830773418368, 0.05300834271967927, 0.009460489048837917, 0.05319606205625933, -0.2474253574954572, 0.028400761412415407], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [6.003699779510498, 5.867568492889404, 5.7869696617126465, 5.773654460906982, 5.68595027923584, 5.525237083435059, 5.320197582244873, 5.16851282119751, 5.140523910522461, 5.131361484527588, 4.894747734069824, 4.865540027618408, 4.820581912994385, 4.692460536956787, 4.499936580657959, 4.307872295379639, 4.2885236740112305, 4.232179164886475, 4.152657985687256, 3.8418266773223877]}, \"tinfo\": {\"Term\": [\"unit\", \"state\", \"image\", \"neuron\", \"vector\", \"training\", \"node\", \"signal\", \"cell\", \"control\", \"layer\", \"class\", \"pattern\", \"feature\", \"word\", \"object\", \"classification\", \"memory\", \"matrix\", \"sequence\", \"hidden_unit\", \"noise\", \"net\", \"classifier\", \"circuit\", \"recognition\", \"rule\", \"response\", \"distribution\", \"representation\", \"bayesian\", \"em\", \"em_algorithm\", \"variational\", \"belief\", \"gaussian_process\", \"posterior_distribution\", \"log_likelihood\", \"density_estimation\", \"neal\", \"generative_model\", \"hyperparameters\", \"logp\", \"graphical_model\", \"mixture_gaussians\", \"latent_variable\", \"factor_analysis\", \"marginal\", \"joint_distribution\", \"conditional_distribution\", \"gibbs_sampling\", \"conditional_density\", \"multinomial\", \"maximum_entropy\", \"mixture_density\", \"via_em\", \"expectation_maximization\", \"frey\", \"dempster_laird\", \"conditional_probability\", \"prior\", \"conditional\", \"posterior\", \"likelihood\", \"covariance\", \"mixture\", \"density\", \"maximum_likelihood\", \"probabilistic\", \"missing\", \"inference\", \"distribution\", \"gaussian\", \"posterior_probability\", \"mackay\", \"log\", \"mean_field\", \"probability\", \"entropy\", \"mutual_information\", \"variable\", \"gaussians\", \"approximation\", \"probability_distribution\", \"evidence\", \"sample\", \"estimate\", \"joint\", \"estimation\", \"hidden\", \"component\", \"step\", \"procedure\", \"theorem\", \"proof\", \"upper_bound\", \"lemma\", \"vc_dimension\", \"bound\", \"proposition\", \"boolean_function\", \"pac\", \"maass\", \"corollary\", \"dichotomy\", \"haussler\", \"theorem_let\", \"proof_theorem\", \"inf\", \"following_theorem\", \"sontag\", \"c_\", \"vapnik_chervonenkis\", \"rational\", \"union\", \"cn\", \"warmuth\", \"valiant\", \"threshold_circuit\", \"uniform_convergence\", \"venkatesh\", \"cardinality\", \"polynomial_degree\", \"kearns\", \"bounded\", \"polynomial\", \"vc\", \"loss\", \"definition\", \"lower_bound\", \"mistake\", \"sample_size\", \"boolean\", \"family\", \"inequality\", \"integer\", \"hypothesis\", \"exists\", \"prove\", \"concept\", \"complexity\", \"hold\", \"threshold\", \"define\", \"assume\", \"class\", \"rn\", \"xi\", \"probability\", \"theory\", \"size\", \"finite\", \"approximation\", \"arbitrary\", \"positive\", \"defined\", \"linear\", \"dimension\", \"constant\", \"distribution\", \"fixed\", \"sample\", \"property\", \"hidden_unit\", \"rumelhart_hinton\", \"cascade_correlation\", \"schmidhuber\", \"replication\", \"fahlman\", \"nettalk\", \"squashing_function\", \"sejnowski_rosenberg\", \"back_prop\", \"mozer_smolensky\", \"clamping\", \"david_rumelhart\", \"mcclelland_ed\", \"john_denker\", \"cognition_vol\", \"net\", \"back_propagation\", \"mcclelland_editor\", \"rumelhart\", \"institute_advanced\", \"rumelhart_et\", \"touretzky_editor\", \"bp\", \"becker\", \"archi_tecture\", \"internal_representation\", \"xor\", \"hidden_layer\", \"epoch\", \"unit\", \"parity\", \"backprop\", \"layer\", \"backpropagation\", \"feedforward_net\", \"activation\", \"architecture\", \"spiral\", \"layered\", \"hidden\", \"trained\", \"le_cun\", \"training\", \"hinton\", \"feed_forward\", \"connection\", \"sigmoid\", \"pattern\", \"learn\", \"net_work\", \"task\", \"required\", \"train\", \"generalization\", \"training_set\", \"simulation\", \"step\", \"procedure\", \"run\", \"information_processing\", \"neural_net\", \"table\", \"random\", \"descent\", \"positive_definite\", \"lagrange_multiplier\", \"stochastic_gradient\", \"perturbed\", \"lagrangian\", \"equilibrium_point\", \"eq_eq\", \"vv\", \"newton_method\", \"iterates\", \"lagrange\", \"prop\", \"functionals\", \"conver_gence\", \"fulfill\", \"jp_abstract\", \"con_vergence\", \"gra_dient\", \"co_co\", \"teukolsky\", \"newton\", \"faster_convergence\", \"jacobian\", \"perturbation\", \"stochas_tic\", \"aij\", \"convergence\", \"afosr_grant\", \"euler\", \"convergent\", \"wt\", \"gradient\", \"w2\", \"x0\", \"local_minimum\", \"optimization\", \"gradient_descent\", \"dt\", \"differential_equation\", \"equation\", \"quadratic\", \"operator\", \"optimization_problem\", \"converge\", \"energy\", \"constraint\", \"converges\", \"solution\", \"eq\", \"derivative\", \"iteration\", \"ui\", \"matrix\", \"minimum\", \"cost_function\", \"update\", \"minimization\", \"solving\", \"second_order\", \"formulation\", \"eigenvalue\", \"rate\", \"condition\", \"constant\", \"vector\", \"optimal\", \"fixed_point\", \"defined\", \"line\", \"ai\", \"nonlinear\", \"linear\", \"xi\", \"property\", \"dynamic\", \"rule\", \"firing\", \"oscillator\", \"firing_rate\", \"receptor\", \"spike_train\", \"membrane\", \"axon\", \"rat\", \"action_potential\", \"membrane_potential\", \"dendritic\", \"presynaptic\", \"dendrite\", \"hippocampal\", \"excitatory_inhibitory\", \"interneurons\", \"pyramidal_cell\", \"nerve\", \"spiking\", \"spiking_neuron\", \"synaptic_strength\", \"olfactory\", \"soma\", \"hippocampus\", \"physiol\", \"epsp\", \"calcium\", \"post_synaptic\", \"refractory_period\", \"dendritic_tree\", \"postsynaptic\", \"burst\", \"spike\", \"neuron\", \"synaptic\", \"neuronal\", \"excitatory\", \"fire\", \"oscillation\", \"synapsis\", \"conductance\", \"inhibitory\", \"fiber\", \"activity\", \"cell\", \"biological\", \"inhibition\", \"et_al\", \"threshold\", \"stimulus\", \"mechanism\", \"neural\", \"response\", \"current\", \"pattern\", \"connection\", \"active\", \"phase\", \"effect\", \"constant\", \"simulation\", \"fig\", \"margin\", \"support_vector\", \"svm\", \"boosting\", \"diagnosis\", \"decision_boundary\", \"bagging\", \"smola\", \"misclassification\", \"quinlan\", \"misclassified\", \"shavlik\", \"breast_cancer\", \"classifier\", \"cortes\", \"boost\", \"voting\", \"classi_fication\", \"university_wisconsin\", \"classifica_tion\", \"classifi_cation\", \"false_negative\", \"unlabeled\", \"ic_uci\", \"fold_cross\", \"linear_discriminant\", \"sification\", \"disagree\", \"classification\", \"uci_repository\", \"classifying\", \"nearest_neighbor\", \"classified\", \"burges\", \"std\", \"patient\", \"class\", \"committee\", \"class_label\", \"classify\", \"datasets\", \"exemplar\", \"ranking\", \"test\", \"training_set\", \"error_rate\", \"database\", \"member\", \"test_set\", \"machine\", \"training\", \"kernel\", \"table\", \"accuracy\", \"feature\", \"decision\", \"testing\", \"pattern\", \"sample\", \"experiment\", \"trained\", \"size\", \"technique\", \"comparison\", \"application\", \"measure\", \"selection\", \"item\", \"document\", \"symbolic\", \"semantic\", \"psychological\", \"smolensky\", \"shepard\", \"mouse\", \"categorization\", \"subjective\", \"judgment\", \"quarter\", \"pdp\", \"cognition\", \"linguistics\", \"story\", \"eighth\", \"prospect\", \"suspect\", \"psychology\", \"rule_extraction\", \"psych\", \"annual_conference\", \"domain_specific\", \"phrase\", \"men\", \"psychological_review\", \"hillsdale_nj\", \"stress\", \"hillsdale\", \"probe\", \"similarity\", \"linguistic\", \"user\", \"rule\", \"representation\", \"representational\", \"meaning\", \"connectionist\", \"paradigm\", \"knowledge\", \"human\", \"cue\", \"query\", \"role\", \"acquisition\", \"category\", \"task\", \"content\", \"target\", \"domain\", \"retrieval\", \"learned\", \"relevant\", \"subject\", \"concept\", \"specific\", \"structure\", \"group\", \"important\", \"context\", \"learn\", \"theory\", \"note\", \"level\", \"experiment\", \"generalization\", \"table\", \"receptive_field\", \"ocular_dominance\", \"spatial_frequency\", \"lgn\", \"v1\", \"surround\", \"attentional\", \"grating\", \"tuning_curve\", \"suppression\", \"center_surround\", \"primary_visual\", \"binocular\", \"stripe\", \"monocular\", \"orientation_preference\", \"segregation\", \"retinotopic\", \"rf\", \"orientation_selectivity\", \"topography\", \"hubel_wiesel\", \"lateral_connection\", \"orientation_tuning\", \"orientation_selective\", \"pouget\", \"striate_cortex\", \"linsker\", \"tanaka\", \"developmental\", \"contralateral\", \"parietal\", \"orientation\", \"visual_cortex\", \"lesion\", \"stimulus\", \"tuned\", \"cortical\", \"cortex\", \"bar\", \"cell\", \"spatial\", \"retinal\", \"response\", \"selective\", \"visual\", \"eye\", \"map\", \"contrast\", \"monkey\", \"activity\", \"development\", \"retina\", \"competition\", \"center\", \"area\", \"layer\", \"population\", \"pattern\", \"unit\", \"connection\", \"region\", \"effect\", \"mechanism\", \"property\", \"local\", \"correlation\", \"processing\", \"neural\", \"generalization_error\", \"student\", \"spin\", \"statistical_mechanic\", \"phys_rev\", \"replica\", \"exponent\", \"optimal_brain\", \"opper\", \"annealed\", \"jij\", \"saad\", \"sollich\", \"thermodynamic_limit\", \"power_law\", \"gardner\", \"spin_glass\", \"ansatz\", \"break_symmetry\", \"ob\", \"phys\", \"microscopic\", \"inverse_temperature\", \"phase_transition\", \"teacher\", \"department_physic\", \"breaking\", \"macroscopic\", \"regime\", \"noiseless\", \"tishby\", \"temperature\", \"ji\", \"curve\", \"ensemble\", \"noise\", \"limit\", \"asymptotic\", \"pruning\", \"overlap\", \"average\", \"correction\", \"correlation\", \"weight_decay\", \"eq\", \"theoretical\", \"symmetric\", \"decrease\", \"theory\", \"expression\", \"critical\", \"rate\", \"distribution\", \"equation\", \"increase\", \"size\", \"effect\", \"optimal\", \"generalization\", \"solution\", \"random\", \"find\", \"training\", \"obtained\", \"simulation\", \"line\", \"dynamic\", \"regression\", \"risk\", \"predictor\", \"spline\", \"hint\", \"moody\", \"gating_network\", \"bootstrap\", \"bias_variance\", \"linear_regression\", \"forecasting\", \"trading\", \"nonparametric\", \"ridge\", \"wahba\", \"financial\", \"regularizer\", \"rbfs\", \"kalman\", \"regularization\", \"smoother\", \"weigend\", \"regularization_term\", \"girosi\", \"locally_tuned\", \"stacked\", \"dilemma\", \"nonparametric_regression\", \"esti\", \"jacob_jordan\", \"outlier\", \"radial_basis\", \"mar\", \"confidence_interval\", \"least_square\", \"price\", \"prediction\", \"rbf\", \"mixture_expert\", \"time_series\", \"expert\", \"estimator\", \"cross_validation\", \"mse\", \"mean_squared\", \"estimate\", \"smoothing\", \"estimation\", \"basis_function\", \"nonlinear\", \"selection\", \"kernel\", \"fit\", \"variance\", \"linear\", \"training\", \"local\", \"sample\", \"estimated\", \"approximation\", \"bias\", \"procedure\", \"optimal\", \"technique\", \"variable\", \"noise\", \"policy\", \"reinforcement_learning\", \"reward\", \"td\", \"agent\", \"game\", \"sutton\", \"action\", \"mdp\", \"routing\", \"singh\", \"temporal_difference\", \"optimal_policy\", \"watkins\", \"packet\", \"markov_decision\", \"critic\", \"discounted\", \"mdps\", \"partially_observable\", \"bellman\", \"backgammon\", \"action_pair\", \"barto_sutton\", \"bertsekas\", \"littman\", \"discount_factor\", \"sutton_barto\", \"function_approximator\", \"bellman_equation\", \"scheduling\", \"exploration\", \"planning\", \"reinforcement\", \"barto\", \"dynamic_programming\", \"rl\", \"skill\", \"state\", \"schedule\", \"environment\", \"traffic\", \"goal\", \"trial\", \"step\", \"call\", \"optimal\", \"world\", \"cost\", \"evaluation\", \"task\", \"control\", \"strategy\", \"move\", \"machine\", \"transition\", \"current\", \"learn\", \"robot\", \"expected\", \"stochastic\", \"image\", \"pixel\", \"scene\", \"computer_vision\", \"pyramid\", \"facial\", \"face\", \"face_recognition\", \"stereo\", \"line_segment\", \"fusion\", \"object_recognition\", \"deformation\", \"occlusion\", \"visual_scene\", \"texture\", \"translation_rotation\", \"adelson\", \"clutter\", \"gray_scale\", \"im_age\", \"shadow\", \"rotation_translation\", \"lighting\", \"object\", \"imagery\", \"rendered\", \"mouth\", \"pentland\", \"pixel_intensity\", \"color\", \"contour\", \"illumination\", \"grouping\", \"pose\", \"early_vision\", \"view\", \"vision\", \"road\", \"discontinuity\", \"correspondence\", \"surface\", \"3d\", \"shape\", \"feature\", \"2d\", \"invariance\", \"scale\", \"region\", \"edge\", \"matching\", \"translation\", \"frame\", \"visual\", \"recognition\", \"part\", \"segment\", \"intensity\", \"local\", \"location\", \"rotation\", \"representation\", \"position\", \"line\", \"et_al\", \"clustering\", \"pca\", \"ica\", \"principal_component\", \"manifold\", \"subspace\", \"cluster\", \"kohonen\", \"independent_component\", \"oja\", \"blind_separation\", \"sanger\", \"distance_measure\", \"reconstruction_error\", \"bell_sejnowski\", \"singular_value\", \"visualization\", \"infomax\", \"cluster_center\", \"matlab\", \"blind_source\", \"mixing_matrix\", \"orthonormal\", \"blind_deconvolution\", \"source_separation\", \"analysis_pca\", \"projection_onto\", \"dimension_reduction\", \"component_analysis\", \"buhmann\", \"dot_product\", \"euclidean\", \"prototype\", \"tangent\", \"eigenvectors\", \"proximity\", \"vector_quantization\", \"vector\", \"unsupervised_learning\", \"distance\", \"transformation\", \"matrix\", \"dimensionality\", \"orthogonal\", \"basis\", \"self_organizing\", \"dimensional\", \"component\", \"unsupervised\", \"projection\", \"dimension\", \"mapping\", \"linear\", \"code\", \"decomposition\", \"map\", \"coefficient\", \"source\", \"coding\", \"representation\", \"structure\", \"coordinate\", \"column\", \"technique\", \"feature\", \"node\", \"tree\", \"message\", \"leaf\", \"genetic_algorithm\", \"genetic\", \"fitness\", \"graph\", \"tour\", \"error_correcting\", \"pointer\", \"mutation\", \"crossover\", \"root_node\", \"undirected\", \"combinatorial_optimization\", \"grown\", \"level_hierarchy\", \"merge\", \"branching\", \"hill_climbing\", \"wasted\", \"junction\", \"parent\", \"holland\", \"hierarchy\", \"merging\", \"vertex\", \"gas\", \"modulo\", \"assignment\", \"search\", \"ga\", \"neighbor\", \"link\", \"partition\", \"path\", \"decoding\", \"branch\", \"binding\", \"structure\", \"code\", \"arc\", \"level\", \"split\", \"block\", \"match\", \"edge\", \"variable\", \"local\", \"bit\", \"run\", \"solution\", \"constraint\", \"size\", \"stage\", \"instance\", \"element\", \"global\", \"procedure\", \"cost\", \"probability\", \"chip\", \"digital\", \"transistor\", \"analog_vlsi\", \"vlsi\", \"cmos\", \"wta\", \"charge\", \"resistor\", \"mead\", \"hardware\", \"fabricated\", \"device\", \"floating_gate\", \"silicon\", \"bus\", \"neuromorphic\", \"electronic\", \"murray\", \"bipolar\", \"drain\", \"lazzaro\", \"electron\", \"simd\", \"converter\", \"hardware_implementation\", \"low_power\", \"host\", \"high_speed\", \"vlsi_implementation\", \"processor\", \"capacitor\", \"circuit\", \"analog\", \"voltage\", \"winner_take\", \"amplifier\", \"resistive\", \"implementation\", \"array\", \"bit\", \"optical\", \"design\", \"current\", \"synapse\", \"pulse\", \"operation\", \"parallel\", \"implemented\", \"neuron\", \"computer\", \"computation\", \"neural\", \"connection\", \"architecture\", \"application\", \"element\", \"processing\", \"plant\", \"cmac\", \"leg\", \"fault\", \"cerebellar\", \"nat\", \"descending\", \"flight\", \"spring\", \"albus\", \"aggregate\", \"smc\", \"cerebellum\", \"sensed\", \"ogy\", \"stabilized\", \"frank\", \"figure_5a\", \"adapts\", \"organ\", \"food\", \"quired\", \"load\", \"insect\", \"behav\", \"fault_tolerance\", \"adaptation\", \"walter\", \"feedback\", \"switched\", \"control\", \"controller\", \"altered\", \"reflex\", \"command\", \"gain\", \"front\", \"behavior\", \"adaptive\", \"pressure\", \"controlled\", \"switching\", \"associative\", \"adapt\", \"failure\", \"mass\", \"artificial\", \"forward\", \"change\", \"loop\", \"element\", \"modeling\", \"animal\", \"neural\", \"produce\", \"dynamic\", \"desired\", \"region\", \"real\", \"level\", \"simulation\", \"experiment\", \"response\", \"effect\", \"range\", \"architecture\", \"condition\", \"curve\", \"attractor\", \"automaton\", \"bifurcation\", \"gene\", \"chaos\", \"basin\", \"demon\", \"fractal\", \"grammatical\", \"williams_zipser\", \"content_addressable\", \"neurodynamics\", \"recurrent\", \"mem_ory\", \"continually_running\", \"university_maryland\", \"collective_computational\", \"recalled\", \"recur_rent\", \"se_quences\", \"basin_attraction\", \"stack\", \"fully_recurrent\", \"associative_memory\", \"symbol\", \"decoder\", \"limit_cycle\", \"memory\", \"storing\", \"stored_memory\", \"giles\", \"protein\", \"recall\", \"sequence\", \"string\", \"dynamical_system\", \"state\", \"capacity\", \"transition\", \"hopfield\", \"module\", \"grammar\", \"dynamic\", \"finite_state\", \"sequential\", \"stored\", \"language\", \"fixed_point\", \"delay\", \"pattern\", \"length\", \"structure\", \"step\", \"activation\", \"continuous\", \"type\", \"context\", \"initial\", \"auditory\", \"eeg\", \"snr\", \"song\", \"power_spectrum\", \"artifact\", \"ear\", \"cochlea\", \"cochlear\", \"auditory_nerve\", \"white_noise\", \"sound_source\", \"periodicity\", \"hearing\", \"frequency_band\", \"noise_ratio\", \"db\", \"band_pas\", \"sound\", \"waveform\", \"sine_wave\", \"tone\", \"spectrum\", \"bird\", \"receiver\", \"audio\", \"low_pas\", \"filter\", \"low_frequency\", \"bandpass\", \"signal\", \"spectral\", \"channel\", \"frequency\", \"filtering\", \"wavelet\", \"detection\", \"amplitude\", \"onset\", \"source\", \"high_frequency\", \"temporal\", \"band\", \"event\", \"hz\", \"noise\", \"peak\", \"response\", \"recording\", \"rate\", \"correlation\", \"phase\", \"component\", \"processing\", \"stage\", \"delay\", \"subject\", \"measured\", \"change\", \"neural\", \"character\", \"hmm\", \"speaker\", \"speech_recognition\", \"phoneme\", \"vowel\", \"tdnn\", \"utterance\", \"recognizer\", \"phonetic\", \"phone\", \"continuous_speech\", \"viterbi\", \"waibel\", \"consonant\", \"spoken\", \"vocabulary\", \"speaker_independent\", \"handwritten\", \"writer\", \"formant\", \"dictionary\", \"hand_printed\", \"time_warping\", \"male_female\", \"large_vocabulary\", \"phonemic\", \"male\", \"speaker_dependent\", \"automatic_speech\", \"stroke\", \"acoustic_speech\", \"font\", \"word\", \"speech\", \"mlp\", \"letter\", \"hybrid\", \"recognition\", \"hmms\", \"token\", \"context_dependent\", \"acoustic\", \"frame\", \"segmentation\", \"hidden_markov\", \"context\", \"window\", \"training\", \"score\", \"segment\", \"trained\", \"feature\", \"sequence\", \"experiment\", \"architecture\", \"level\", \"probability\", \"state\", \"vector\", \"motion\", \"velocity\", \"movement\", \"arm\", \"saccade\", \"signature\", \"eye_movement\", \"motor_command\", \"kawato\", \"torque\", \"motor_control\", \"workspace\", \"saccadic\", \"gaze\", \"joint_angle\", \"desired_trajectory\", \"moving_object\", \"mt\", \"manipulator\", \"opposite_direction\", \"saccadic_eye\", \"robotic\", \"arm_movement\", \"positioning\", \"rotational\", \"direc\", \"head\", \"cone\", \"kinematics\", \"gun_kyoto\", \"finger\", \"muscle\", \"motor\", \"moving\", \"direction\", \"position\", \"acceleration\", \"trajectory\", \"autonomous\", \"field\", \"sensor\", \"location\", \"hand\", \"tracking\", \"robot\", \"move\", \"coordinate\", \"target\", \"light\", \"joint\", \"navigation\", \"rotation\", \"subject\", \"angle\", \"visual\", \"eye\", \"human\", \"speed\", \"et_al\", \"change\", \"environment\", \"task\"], \"Freq\": [9554.0, 10298.0, 7145.0, 8710.0, 7524.0, 9625.0, 4562.0, 4853.0, 5590.0, 4468.0, 5103.0, 5119.0, 7594.0, 6088.0, 3034.0, 3039.0, 3275.0, 3038.0, 3550.0, 3764.0, 2829.0, 4349.0, 2853.0, 2699.0, 2601.0, 2840.0, 4129.0, 4173.0, 4600.0, 4127.0, 1276.7418212890625, 764.812255859375, 524.518798828125, 290.493896484375, 256.0169982910156, 252.8827667236328, 238.25619506835938, 466.0125732421875, 216.31634521484375, 212.1373291015625, 212.1373291015625, 190.197509765625, 182.88421630859375, 174.52618408203125, 156.76536560058594, 153.631103515625, 147.36257934570312, 143.18356323242188, 137.9597930908203, 135.8702850341797, 127.51224517822266, 121.24372863769531, 118.10946655273438, 106.6171646118164, 104.52765655517578, 87.81159210205078, 86.76683807373047, 77.36405181884766, 76.31929016113281, 231.9876708984375, 1791.8056640625, 452.4307861328125, 721.9773559570312, 1003.0161743164062, 539.1453857421875, 1473.1556396484375, 1491.9613037109375, 490.0419006347656, 677.0529174804688, 433.62518310546875, 491.086669921875, 2736.263427734375, 1645.5401611328125, 295.7176513671875, 252.8827667236328, 1016.5980224609375, 486.90765380859375, 2212.841552734375, 558.9956665039062, 316.61273193359375, 1475.2451171875, 290.493896484375, 1023.9113159179688, 400.19305419921875, 507.8027648925781, 916.3015747070312, 944.510009765625, 457.654541015625, 558.9956665039062, 531.8320922851562, 661.3816528320312, 713.6193237304688, 501.5342102050781, 1416.62939453125, 905.2643432617188, 476.47998046875, 385.4294738769531, 365.31365966796875, 1736.364990234375, 213.91574096679688, 201.21102905273438, 176.8603057861328, 163.09686279296875, 150.39215087890625, 148.27468872070312, 134.51124572753906, 123.9239730834961, 122.86524200439453, 118.63034057617188, 111.21925354003906, 110.16053009033203, 110.16053009033203, 106.98434448242188, 99.57325744628906, 96.3970718383789, 188.50631713867188, 81.57489776611328, 74.16381072998047, 149.3334197998047, 68.87018585205078, 66.75271606445312, 64.63526916503906, 58.28290939331055, 99.57325744628906, 636.3477172851562, 752.8076782226562, 210.7395782470703, 853.38671875, 665.9920654296875, 351.5502014160156, 182.15394592285156, 212.8570098876953, 243.5601043701172, 387.54693603515625, 225.56173706054688, 284.8504333496094, 700.9300537109375, 435.1896667480469, 300.7313232421875, 563.2955932617188, 848.0930786132812, 435.1896667480469, 922.2039794921875, 604.5859375, 701.98876953125, 1295.9345703125, 361.0787658691406, 579.176513671875, 1019.6068725585938, 784.5695190429688, 883.031005859375, 492.3608703613281, 751.7489624023438, 441.5419921875, 519.8878173828125, 625.7604370117188, 784.5695190429688, 515.6528930664062, 568.5892333984375, 603.5272216796875, 487.0672302246094, 513.535400390625, 500.8306884765625, 2828.000732421875, 204.0771026611328, 113.63324737548828, 97.858154296875, 77.87637329101562, 71.56633758544922, 56.84291076660156, 49.481201171875, 46.32618713378906, 44.22283935546875, 43.171165466308594, 36.86112976074219, 35.80945587158203, 27.396076202392578, 26.344402313232422, 26.344402313232422, 2739.66015625, 1188.44287109375, 21.08603858947754, 215.6455078125, 28.44774627685547, 79.97972106933594, 77.87637329101562, 372.3447265625, 87.3414306640625, 35.80945587158203, 384.9648132324219, 219.85218811035156, 1086.4305419921875, 632.10791015625, 8365.0576171875, 230.36892700195312, 174.63026428222656, 3584.153564453125, 756.205322265625, 70.51466369628906, 1660.6439208984375, 2050.814697265625, 194.612060546875, 190.40536499023438, 893.9744262695312, 1521.8231201171875, 233.52395629882812, 3498.96826171875, 289.2625732421875, 325.0194396972656, 1144.272705078125, 318.70941162109375, 2134.948486328125, 707.8283081054688, 253.5057373046875, 1029.6402587890625, 514.320556640625, 390.2231750488281, 464.8919677734375, 510.1138610839844, 518.5272216796875, 614.2294311523438, 470.15032958984375, 396.5332336425781, 386.0164794921875, 356.56964111328125, 374.4480895996094, 376.5514221191406, 281.2661437988281, 134.8666229248047, 133.81338500976562, 113.80194091796875, 106.42930603027344, 76.93875122070312, 68.51287841796875, 66.40641021728516, 52.7143669128418, 44.28849411010742, 39.02232360839844, 37.969085693359375, 70.61934661865234, 34.809383392333984, 33.75615310668945, 32.70291519165039, 26.383512496948242, 25.330278396606445, 24.27704429626465, 47.44819259643555, 23.22381019592285, 82.20491790771484, 31.649682998657227, 95.89696502685547, 476.1144714355469, 26.383512496948242, 98.00343322753906, 1791.6038818359375, 22.170576095581055, 29.543212890625, 108.53577423095703, 310.7566833496094, 1432.4510498046875, 141.18603515625, 148.5586700439453, 427.6656799316406, 847.9061279296875, 663.5901489257812, 458.20947265625, 209.646240234375, 2591.008544921875, 462.4223937988281, 555.1069946289062, 164.357177734375, 437.1448059082031, 929.0050659179688, 1182.8345947265625, 320.2358093261719, 1679.9610595703125, 802.6170043945312, 519.2970581054688, 836.3204956054688, 211.75271606445312, 1451.4093017578125, 686.76123046875, 371.8442687988281, 719.4114990234375, 312.8631896972656, 326.5552062988281, 360.2586975097656, 287.5855407714844, 347.6199035644531, 1070.138427734375, 694.1339111328125, 752.061767578125, 1207.0589599609375, 684.65478515625, 380.2701416015625, 521.4035034179688, 522.4567260742188, 395.01544189453125, 455.0497741699219, 514.0308837890625, 421.3462829589844, 455.0497741699219, 457.1562194824219, 448.7303466796875, 1111.481201171875, 576.5021362304688, 563.0239868164062, 468.6769104003906, 451.0516052246094, 431.352783203125, 417.8746032714844, 394.0286560058594, 293.46087646484375, 287.2402038574219, 261.3206787109375, 231.25404357910156, 227.10691833496094, 222.9597930908203, 215.7023162841797, 206.3712921142578, 178.37818908691406, 175.26785278320312, 169.0471649169922, 163.86326599121094, 140.0172882080078, 137.9437255859375, 134.83338928222656, 132.75982666015625, 127.57592010498047, 123.42879486083984, 122.39201354980469, 113.06098937988281, 108.91386413574219, 105.80351257324219, 192.89312744140625, 331.82177734375, 1493.0167236328125, 7273.0712890625, 1465.0235595703125, 580.6492309570312, 478.0079040527344, 341.1528015136719, 498.7435302734375, 833.6238403320312, 398.17578125, 482.155029296875, 273.76202392578125, 1410.07421875, 2110.938232421875, 475.93438720703125, 344.2631530761719, 908.2720336914062, 694.6951904296875, 732.019287109375, 511.1849060058594, 721.6514892578125, 757.9387817382812, 674.996337890625, 817.0353393554688, 604.4951782226562, 461.41943359375, 503.92742919921875, 528.8102416992188, 522.5894775390625, 516.3688354492188, 493.55963134765625, 497.5449523925781, 466.6514587402344, 383.55865478515625, 298.3352355957031, 171.56539916992188, 138.54132080078125, 128.9536895751953, 120.43135070800781, 114.03958892822266, 89.53785705566406, 88.47256469726562, 79.95022583007812, 59.709659576416016, 2647.305908203125, 50.12202072143555, 43.73026657104492, 86.34197998046875, 37.33850860595703, 37.33850860595703, 34.14263153076172, 30.946752548217773, 30.946752548217773, 89.53785705566406, 27.750873565673828, 53.317901611328125, 51.18731689453125, 24.554996490478516, 37.33850860595703, 3078.749267578125, 32.01204299926758, 169.434814453125, 531.6343383789062, 306.8575744628906, 92.73373413085938, 107.64783477783203, 265.3111572265625, 3290.742431640625, 255.72354125976562, 122.56192779541016, 250.39706420898438, 152.39012145996094, 272.7681884765625, 133.21485900878906, 1563.903076171875, 1213.4217529296875, 481.5655822753906, 560.397216796875, 274.8987731933594, 590.2254028320312, 982.2531127929688, 2710.158203125, 698.8853149414062, 941.7720336914062, 641.3594970703125, 1889.882568359375, 516.7202758789062, 364.38336181640625, 1515.96484375, 830.9815673828125, 771.3251953125, 639.2288818359375, 643.4900512695312, 522.0466918945312, 411.2562561035156, 454.9332580566406, 398.4727478027344, 353.7304382324219, 422.33953857421875, 323.4837951660156, 293.3973083496094, 154.78440856933594, 130.0704803466797, 122.5488510131836, 116.10174560546875, 110.72914123535156, 97.83492279052734, 74.19551086425781, 55.92869186401367, 43.03446960449219, 41.95995330810547, 159.08248901367188, 36.58735656738281, 47.332542419433594, 27.9912109375, 22.618616104125977, 21.544095993041992, 170.90219116210938, 50.55610275268555, 38.736392974853516, 76.34455108642578, 53.77965545654297, 145.11373901367188, 38.736392974853516, 62.37580871582031, 40.885433197021484, 200.98870849609375, 30.140247344970703, 126.846923828125, 733.949951171875, 99.98396301269531, 535.1640014648438, 2769.088134765625, 2514.42724609375, 119.3252944946289, 197.76515197753906, 738.2479858398438, 305.2170104980469, 630.796142578125, 909.096435546875, 400.84918212890625, 439.5318298339844, 546.9837036132812, 141.8901824951172, 509.37554931640625, 2006.1800537109375, 174.12574768066406, 909.096435546875, 551.28173828125, 242.8949432373047, 566.3250122070312, 321.33477783203125, 467.46929931640625, 368.61358642578125, 432.0102233886719, 737.1735229492188, 428.7866516113281, 450.2770080566406, 448.12799072265625, 411.5943603515625, 482.5125732421875, 466.3948059082031, 511.5245666503906, 418.0414733886719, 378.2842712402344, 381.5078430175781, 1250.824462890625, 324.68072509765625, 296.037109375, 238.7498779296875, 228.14112854003906, 227.08026123046875, 184.645263671875, 170.85389709472656, 160.24513244628906, 134.78414916992188, 134.78414916992188, 134.78414916992188, 109.32315063476562, 106.1405258178711, 101.89703369140625, 100.83615112304688, 98.71440124511719, 94.47090148925781, 266.3326416015625, 84.92303466796875, 84.92303466796875, 84.92303466796875, 168.7321319580078, 77.49691009521484, 75.37516021728516, 72.19253540039062, 72.19253540039062, 219.6541290283203, 71.13166046142578, 70.0707778930664, 70.0707778930664, 70.0707778930664, 1010.005859375, 269.5152587890625, 171.91476440429688, 2200.307373046875, 376.66357421875, 943.1707763671875, 809.5005493164062, 402.1246032714844, 3311.043212890625, 1033.3450927734375, 293.91534423828125, 2156.8115234375, 242.99337768554688, 1477.8516845703125, 676.89111328125, 1489.521240234375, 784.03955078125, 236.6281280517578, 1311.29443359375, 523.0642700195312, 361.81134033203125, 328.9242248535156, 701.2913208007812, 579.2906494140625, 1151.102294921875, 431.8290710449219, 1207.32861328125, 1183.9893798828125, 707.6565551757812, 610.0560302734375, 522.0034790039062, 414.8551025390625, 497.6033020019531, 501.8468322753906, 429.7073059082031, 387.2723388671875, 392.57672119140625, 701.7073974609375, 524.6649169921875, 235.74893188476562, 179.2691192626953, 174.9245147705078, 162.9768524169922, 145.59844970703125, 116.27239227294922, 107.58318328857422, 97.8078384399414, 96.72168731689453, 91.29093170166016, 88.03248596191406, 69.56793212890625, 64.13717651367188, 52.18952178955078, 102.15243530273438, 45.6726188659668, 27.208065032958984, 336.7608947753906, 189.04446411132812, 78.25713348388672, 42.41416931152344, 144.51229858398438, 587.66162109375, 45.6726188659668, 52.18952178955078, 73.91252899169922, 307.434814453125, 44.58647155761719, 130.39234924316406, 534.4402465820312, 153.20150756835938, 1223.0595703125, 608.2984619140625, 2431.94482421875, 723.430419921875, 417.135986328125, 347.6224060058594, 379.1207580566406, 1218.7149658203125, 230.3181610107422, 795.1162719726562, 268.33343505859375, 603.953857421875, 422.5667419433594, 337.8470458984375, 433.42828369140625, 776.6517333984375, 376.9484558105469, 315.0378723144531, 858.113037109375, 852.6823120117188, 801.6331787109375, 565.9385986328125, 739.72265625, 655.0029296875, 673.4674682617188, 485.5634765625, 634.3660888671875, 523.5787353515625, 504.02801513671875, 581.1447143554688, 462.75433349609375, 459.4958801269531, 459.4958801269531, 446.4620361328125, 1216.4422607421875, 496.73681640625, 482.7305603027344, 428.86041259765625, 309.2686462402344, 262.9403076171875, 215.53456115722656, 215.53456115722656, 149.8129425048828, 146.5807342529297, 141.19371032714844, 121.80046081542969, 112.10382843017578, 111.02642059326172, 107.79420471191406, 107.79420471191406, 95.9427719116211, 92.71056365966797, 91.6331558227539, 519.3622436523438, 79.78172302246094, 140.11631774902344, 67.93028259277344, 64.69807434082031, 53.924034118652344, 53.924034118652344, 45.3048095703125, 37.76298522949219, 32.375972747802734, 78.70431518554688, 243.5470428466797, 459.0276794433594, 139.03890991210938, 94.86537170410156, 276.946533203125, 224.15377807617188, 2607.3701171875, 740.2299194335938, 200.45089721679688, 550.6069946289062, 912.614501953125, 593.703125, 484.8853759765625, 261.8628845214844, 204.76052856445312, 1699.119140625, 316.8104553222656, 913.69189453125, 405.15753173828125, 976.1813354492188, 663.7343139648438, 755.3135986328125, 540.9103393554688, 763.932861328125, 1193.8167724609375, 1401.7557373046875, 819.957763671875, 711.1400756835938, 424.55078125, 587.2387084960938, 436.4021911621094, 505.35601806640625, 496.73681640625, 474.1113586425781, 479.49835205078125, 437.4796142578125, 1305.8819580078125, 1046.0211181640625, 648.0753173828125, 611.107666015625, 589.3619995117188, 411.04754638671875, 405.6111145019531, 2398.601806640625, 283.8353271484375, 270.7879638671875, 256.6532897949219, 222.94747924804688, 200.11451721191406, 196.85267639160156, 190.3289794921875, 184.892578125, 163.1468963623047, 133.7902374267578, 122.91740417480469, 122.91740417480469, 109.87000274658203, 105.52086639404297, 94.64802551269531, 92.47346496582031, 84.86248016357422, 80.51334381103516, 80.51334381103516, 78.33877563476562, 75.07693481445312, 72.9023666381836, 135.96481323242188, 430.6186218261719, 340.37408447265625, 456.7134094238281, 192.50355529785156, 384.95269775390625, 512.1648559570312, 162.0596160888672, 4503.58251953125, 311.0174560546875, 875.317626953125, 231.645751953125, 701.3522338867188, 633.9406127929688, 1324.36572265625, 468.67352294921875, 886.1903686523438, 317.5411376953125, 539.3469848632812, 393.6510009765625, 882.9285278320312, 790.509521484375, 423.0076599121094, 402.3492431640625, 487.1573486328125, 411.04754638671875, 533.9105224609375, 425.1822204589844, 367.5561828613281, 382.77813720703125, 382.77813720703125, 7144.5302734375, 1606.434326171875, 548.390625, 240.42056274414062, 224.32456970214844, 157.79443359375, 1137.504150390625, 145.99070739746094, 139.55230712890625, 135.26004028320312, 123.45630645751953, 232.9091033935547, 99.84884643554688, 157.79443359375, 76.24137878417969, 468.98370361328125, 67.65684509277344, 63.36457824707031, 48.341644287109375, 45.12244415283203, 41.90324783325195, 40.83018112182617, 38.68404769897461, 37.61098098754883, 2956.35205078125, 36.53791427612305, 36.53791427612305, 103.06803894042969, 83.75284576416016, 39.75711441040039, 415.3304138183594, 612.774658203125, 143.84457397460938, 270.4664306640625, 261.88189697265625, 76.24137878417969, 1107.4583740234375, 532.2946166992188, 326.265869140625, 229.6898956298828, 286.5624084472656, 831.6802368164062, 232.9091033935547, 767.2962036132812, 2354.361572265625, 320.9005126953125, 308.02374267578125, 714.7159423828125, 948.6444702148438, 563.41357421875, 355.2386779785156, 256.51654052734375, 484.00665283203125, 684.6701049804688, 683.5969848632812, 576.2903442382812, 391.7229309082031, 324.1197204589844, 546.2445068359375, 451.81463623046875, 326.265869140625, 495.81036376953125, 354.16558837890625, 345.5810546875, 327.33892822265625, 1041.5421142578125, 587.8679809570312, 578.0528564453125, 514.8002319335938, 459.1814880371094, 418.8306884765625, 1467.9522705078125, 332.67620849609375, 316.3177795410156, 163.63897705078125, 162.54840087890625, 157.09559631347656, 138.55601501464844, 121.10701751708984, 117.8353271484375, 114.56364440917969, 105.8391342163086, 94.93350982666016, 93.84294128417969, 82.93731689453125, 81.84674835205078, 80.75618743896484, 73.12224578857422, 70.94112396240234, 68.7599868774414, 63.30717849731445, 55.67323303222656, 54.582672119140625, 51.31098556518555, 89.4806900024414, 190.90306091308594, 197.44642639160156, 459.1814880371094, 157.09559631347656, 277.0575256347656, 140.73715209960938, 159.27671813964844, 5151.87353515625, 259.6085205078125, 1374.163818359375, 954.297119140625, 2098.297607421875, 309.7743835449219, 294.5065002441406, 664.2073364257812, 308.683837890625, 817.9766845703125, 1316.364013671875, 298.8687744140625, 497.3511962890625, 705.6487426757812, 712.192138671875, 1177.862548828125, 587.8679809570312, 313.04608154296875, 766.7202758789062, 441.73248291015625, 575.8717651367188, 417.7401428222656, 785.2598266601562, 695.8336791992188, 388.294921875, 363.2120056152344, 404.6533508300781, 393.7477111816406, 4561.150390625, 1747.3294677734375, 412.4830322265625, 385.87481689453125, 230.66014099121094, 160.8135223388672, 137.53131103515625, 1494.5513916015625, 117.57513427734375, 113.14043426513672, 104.27102661132812, 69.90204620361328, 106.4883804321289, 51.05455017089844, 46.619842529296875, 43.2938117980957, 42.18513870239258, 53.27190017700195, 93.18425750732422, 102.05366516113281, 76.55410766601562, 24.44631576538086, 102.05366516113281, 292.7460021972656, 71.01072692871094, 287.2026062011719, 169.6829376220703, 243.96426391601562, 86.53219604492188, 27.772342681884766, 374.7880554199219, 1462.399658203125, 229.55145263671875, 299.3980407714844, 473.46026611328125, 435.7652587890625, 758.39013671875, 266.1377868652344, 253.9423370361328, 257.26837158203125, 1537.7896728515625, 687.4347534179688, 218.46470642089844, 1129.796630859375, 270.572509765625, 399.1789245605469, 406.9396667480469, 479.0036315917969, 812.7152099609375, 770.5855102539062, 512.2639770507812, 422.4611511230469, 638.653076171875, 463.4821472167969, 543.306884765625, 393.63555908203125, 369.2446594238281, 354.8318786621094, 311.593505859375, 324.8976135253906, 299.3980407714844, 299.3980407714844, 1848.64990234375, 590.7422485351562, 485.10821533203125, 403.1879577636719, 378.3962707519531, 305.09918212890625, 269.5285339355469, 246.89266967773438, 187.60826110839844, 172.51766967773438, 496.9651184082031, 153.11549377441406, 616.6118774414062, 142.3365020751953, 291.08648681640625, 135.86912536621094, 133.71331787109375, 268.45062255859375, 126.16802215576172, 124.0122299194336, 114.31114959716797, 108.9216537475586, 106.76585388183594, 106.76585388183594, 105.68795776367188, 104.61005401611328, 100.2984619140625, 94.90896606445312, 86.28578186035156, 80.89628601074219, 786.9197998046875, 195.15354919433594, 2423.169921875, 1406.71142578125, 1194.365478515625, 395.64263916015625, 191.9198455810547, 138.02491760253906, 822.4904174804688, 556.24951171875, 936.7477416992188, 266.2948303222656, 667.2730712890625, 1441.204345703125, 520.6788940429688, 391.3310546875, 556.24951171875, 576.7296142578125, 423.66802978515625, 1296.765869140625, 418.2785339355469, 537.92529296875, 578.8854370117188, 493.7314147949219, 477.5629577636719, 444.1481018066406, 413.9669494628906, 401.0321350097656, 321.17529296875, 152.1058349609375, 143.0282745361328, 285.99981689453125, 110.12212371826172, 99.90987396240234, 93.1017074584961, 93.1017074584961, 88.56292724609375, 76.08128356933594, 63.59963607788086, 62.46493911743164, 165.72216796875, 48.84860610961914, 45.444522857666016, 39.77104568481445, 37.50165939331055, 27.28940773010254, 59.060855865478516, 48.84860610961914, 166.8568572998047, 25.020015716552734, 376.775390625, 155.50991821289062, 31.82818603515625, 95.37109375, 857.8859252929688, 37.50165939331055, 1095.037109375, 59.060855865478516, 3412.083740234375, 901.0043334960938, 84.0241470336914, 111.25682067871094, 251.95895385742188, 949.7962036132812, 167.99156188964844, 1222.1229248046875, 1028.090087890625, 104.44864654541016, 313.2324523925781, 197.49362182617188, 265.5752868652344, 196.3589324951172, 217.91812133789062, 180.4731903076172, 262.17120361328125, 375.64068603515625, 1006.5309448242188, 272.38348388671875, 646.8327026367188, 441.4529724121094, 237.2079315185547, 642.2938842773438, 414.2203063964844, 590.0979614257812, 366.5631103515625, 503.86114501953125, 364.2937316894531, 515.2081298828125, 430.10601806640625, 460.7427673339844, 469.8203430175781, 421.0284729003906, 313.2324523925781, 334.7916564941406, 303.02020263671875, 281.4610290527344, 907.7477416992188, 376.388671875, 280.9013366699219, 213.4984893798828, 113.51762390136719, 112.39423370361328, 89.92662811279297, 88.8032455444336, 86.55648803710938, 55.10182189941406, 52.85506057739258, 42.74463653564453, 1805.3289794921875, 34.8809700012207, 34.8809700012207, 30.387447357177734, 58.471961975097656, 53.97844314575195, 24.77054214477539, 24.77054214477539, 112.39423370361328, 221.36216735839844, 56.22520065307617, 477.492919921875, 787.5459594726562, 225.85569763183594, 122.5046615600586, 2681.56591796875, 119.134521484375, 62.965484619140625, 109.02409362792969, 282.02471923828125, 432.5577087402344, 2528.786376953125, 587.584228515625, 307.8624572753906, 5436.09521484375, 717.8963623046875, 823.4942016601562, 495.467041015625, 851.5786743164062, 329.2066955566406, 1721.075439453125, 242.70640563964844, 328.08331298828125, 423.5706481933594, 452.778564453125, 407.8433532714844, 475.2461853027344, 1622.218017578125, 501.0839538574219, 524.6749267578125, 522.4281616210938, 426.9408264160156, 374.1419372558594, 368.5249938964844, 344.93402099609375, 322.4664001464844, 665.2559204101562, 429.1097717285156, 224.00628662109375, 216.24562072753906, 208.48495483398438, 202.9416046142578, 178.55091857910156, 163.02955627441406, 130.8782196044922, 86.53150177001953, 84.31417846679688, 80.98817443847656, 80.98817443847656, 76.55350494384766, 51.054141998291016, 151.94288635253906, 137.53021240234375, 42.1848030090332, 665.2559204101562, 375.89373779296875, 57.70615005493164, 181.8769073486328, 378.1110534667969, 74.33616638183594, 179.65957641601562, 83.20550537109375, 168.57290649414062, 1690.7735595703125, 95.40084838867188, 56.59748077392578, 4014.540771484375, 416.9144287109375, 1322.6959228515625, 1481.2353515625, 348.17706298828125, 255.04898071289062, 708.4940185546875, 502.2818298339844, 210.7022705078125, 985.660888671875, 140.85621643066406, 862.5987548828125, 235.0929718017578, 467.91314697265625, 275.0050048828125, 1161.9390869140625, 382.54571533203125, 788.3180541992188, 248.39697265625, 670.79931640625, 485.6518249511719, 495.62982177734375, 562.14990234375, 469.02178955078125, 379.2197265625, 327.11236572265625, 314.9170227050781, 283.87432861328125, 335.9817199707031, 311.5910339355469, 1162.1390380859375, 1012.013427734375, 781.8207397460938, 645.0396728515625, 623.910888671875, 543.8438720703125, 393.7182922363281, 352.57275390625, 291.41046142578125, 270.2817077636719, 249.1529083251953, 232.4722900390625, 193.5508270263672, 176.87020874023438, 172.4220428466797, 156.85345458984375, 154.62937927246094, 127.94038391113281, 126.82833862304688, 110.14771270751953, 103.4754638671875, 93.46708679199219, 93.46708679199219, 85.68280792236328, 80.12259674072266, 74.56239318847656, 72.33830261230469, 69.0021743774414, 69.0021743774414, 65.66605377197266, 165.74978637695312, 111.25975799560547, 100.13934326171875, 2766.81494140625, 1415.6844482421875, 709.5380249023438, 779.5967407226562, 524.939208984375, 2156.30419921875, 343.6764221191406, 213.5675811767578, 161.3016357421875, 438.199951171875, 761.8040161132812, 548.2920532226562, 355.9089050292969, 936.39453125, 415.9591064453125, 1427.9169921875, 342.56439208984375, 397.0544128417969, 640.591552734375, 739.563232421875, 615.0145874023438, 533.8355102539062, 462.66485595703125, 453.7685241699219, 379.26177978515625, 358.1329650878906, 353.6847839355469, 2212.475830078125, 1034.2576904296875, 1013.8458862304688, 550.0430297851562, 274.48284912109375, 252.93701171875, 251.8030242919922, 224.5872039794922, 165.6195831298828, 142.9397430419922, 140.6717529296875, 122.52787780761719, 121.39389038085938, 115.72393035888672, 105.51799774169922, 89.64210510253906, 73.76620483398438, 206.44332885742188, 133.86781311035156, 64.69426727294922, 62.4262809753418, 62.4262809753418, 61.29228973388672, 60.158294677734375, 55.6223258972168, 48.8183708190918, 504.68328857421875, 191.701416015625, 71.49821472167969, 27.27251625061035, 122.52787780761719, 399.2220153808594, 849.4169921875, 525.0951538085938, 1770.2188720703125, 1804.23876953125, 120.25989532470703, 1102.2972412109375, 155.4136505126953, 1348.3736572265625, 475.1994934082031, 1211.16064453125, 891.374755859375, 325.5124816894531, 561.3829345703125, 557.98095703125, 488.8074035644531, 1077.349365234375, 413.9638977050781, 403.7579650878906, 184.8974609375, 328.91448974609375, 491.07537841796875, 330.0484619140625, 557.98095703125, 376.54217529296875, 406.02593994140625, 368.6042175292969, 470.66351318359375, 425.3038330078125, 340.25439453125, 336.8524475097656], \"Total\": [9554.0, 10298.0, 7145.0, 8710.0, 7524.0, 9625.0, 4562.0, 4853.0, 5590.0, 4468.0, 5103.0, 5119.0, 7594.0, 6088.0, 3034.0, 3039.0, 3275.0, 3038.0, 3550.0, 3764.0, 2829.0, 4349.0, 2853.0, 2699.0, 2601.0, 2840.0, 4129.0, 4173.0, 4600.0, 4127.0, 1277.7725830078125, 765.843017578125, 525.549560546875, 291.5246276855469, 257.0477294921875, 253.9134979248047, 239.28692626953125, 468.1519775390625, 217.34707641601562, 213.16806030273438, 213.16806030273438, 191.22824096679688, 183.91494750976562, 175.55691528320312, 157.7960968017578, 154.66183471679688, 148.393310546875, 144.21429443359375, 138.9905242919922, 136.90101623535156, 128.54299926757812, 122.27447509765625, 119.14021301269531, 107.64791107177734, 105.55840301513672, 88.84233856201172, 87.7975845336914, 78.3947982788086, 77.35003662109375, 235.1217498779297, 1849.785888671875, 461.0068054199219, 756.9561157226562, 1066.3211669921875, 568.1692504882812, 1603.4521484375, 1690.200439453125, 529.859130859375, 766.2423095703125, 519.4026489257812, 603.5263061523438, 4600.7080078125, 2600.8828125, 337.8939208984375, 280.84857177734375, 1614.055908203125, 639.9994506835938, 4728.77001953125, 793.6804809570312, 379.4589538574219, 3736.588134765625, 343.8367004394531, 2983.046875, 611.199462890625, 972.43994140625, 3198.013427734375, 3486.492919921875, 862.3865356445312, 1670.1131591796875, 1596.110595703125, 3282.227783203125, 4646.6455078125, 2280.848876953125, 1417.659423828125, 906.29443359375, 477.510009765625, 386.4595031738281, 366.34368896484375, 1744.708251953125, 214.94577026367188, 202.24105834960938, 177.8903350830078, 164.12689208984375, 151.42218017578125, 149.30471801757812, 135.54127502441406, 124.95402526855469, 123.89529418945312, 119.66039276123047, 112.24930572509766, 111.19058227539062, 111.19058227539062, 108.01439666748047, 100.60330963134766, 97.4271240234375, 190.6269073486328, 82.60494995117188, 75.19386291503906, 151.41513061523438, 69.90023803710938, 67.78276824951172, 65.66532135009766, 59.31295394897461, 101.73800659179688, 673.6004638671875, 879.8939208984375, 231.15476989746094, 1056.413818359375, 840.4970092773438, 420.53179931640625, 206.82337951660156, 251.58404541015625, 300.96185302734375, 543.8746948242188, 275.0404968261719, 371.4923400878906, 1184.540283203125, 662.2196655273438, 406.6219177246094, 939.53759765625, 1697.5546875, 730.570068359375, 2578.019287109375, 1356.0841064453125, 1741.456787109375, 5119.935546875, 617.8746948242188, 1522.44091796875, 4728.77001953125, 2922.621826171875, 3740.22314453125, 1190.8118896484375, 2983.046875, 984.7503662109375, 1444.3321533203125, 2489.392822265625, 4487.28271484375, 1787.19775390625, 2811.66064453125, 4600.7080078125, 2050.8544921875, 3198.013427734375, 2788.514892578125, 2829.031005859375, 205.1074981689453, 114.66365051269531, 98.88855743408203, 78.90677642822266, 72.59674072265625, 57.87331008911133, 50.511600494384766, 47.35658645629883, 45.253238677978516, 44.20156478881836, 37.89152908325195, 36.8398551940918, 28.42647361755371, 27.374799728393555, 27.374799728393555, 2853.13134765625, 1241.7618408203125, 22.116436004638672, 231.47662353515625, 30.601524353027344, 86.3827133178711, 84.40734100341797, 403.78729248046875, 95.1163101196289, 39.08322525024414, 425.92681884765625, 247.83004760742188, 1226.12353515625, 716.947998046875, 9554.388671875, 271.63092041015625, 203.67315673828125, 5103.234375, 993.2587890625, 80.01488494873047, 2637.33056640625, 3715.311767578125, 253.89508056640625, 247.9155731201172, 1596.110595703125, 3172.264892578125, 338.912109375, 9625.0947265625, 445.9613342285156, 541.3404541015625, 3074.09033203125, 557.3455200195312, 7594.97216796875, 1847.917236328125, 417.0426025390625, 4950.51123046875, 1717.934814453125, 1024.64111328125, 1840.75, 2490.064453125, 2827.61181640625, 4646.6455078125, 2280.848876953125, 1424.6865234375, 1422.41552734375, 1279.277099609375, 2274.5830078125, 2557.057861328125, 282.2964172363281, 135.89694213867188, 134.8437042236328, 114.832275390625, 107.45964050292969, 77.96908569335938, 69.543212890625, 67.4367446899414, 53.744686126708984, 45.31881332397461, 40.052642822265625, 38.99940490722656, 72.70840454101562, 35.83970260620117, 34.78647232055664, 33.73323440551758, 27.41383171081543, 26.360597610473633, 25.307363510131836, 49.52326583862305, 24.25412940979004, 86.46746063232422, 33.738731384277344, 102.59725952148438, 512.2985229492188, 28.5258731842041, 106.66770935058594, 1956.81396484375, 24.273962020874023, 32.79088592529297, 122.00747680664062, 354.163818359375, 1685.1982421875, 159.17466735839844, 172.8809814453125, 537.0753173828125, 1125.5458984375, 866.60205078125, 588.10791015625, 255.25814819335938, 4142.58154296875, 657.9397583007812, 825.1964721679688, 206.4085235595703, 645.0018310546875, 1597.03515625, 2196.375732421875, 465.89154052734375, 3669.952392578125, 1492.654052734375, 896.0084228515625, 1686.1060791015625, 287.0175476074219, 3550.6826171875, 1361.102783203125, 607.4639892578125, 1525.7852783203125, 492.3218688964844, 543.8515625, 635.2589111328125, 458.1955261230469, 629.36669921875, 4068.8544921875, 2182.134765625, 2811.66064453125, 7524.642578125, 3265.709228515625, 789.0875854492188, 2489.392822265625, 2837.080078125, 983.2737426757812, 2093.365478515625, 4487.28271484375, 1522.44091796875, 2788.514892578125, 3857.883056640625, 4129.29150390625, 1112.5123291015625, 577.5333251953125, 564.05517578125, 469.7080078125, 452.08270263671875, 432.3838806152344, 418.90570068359375, 395.05975341796875, 294.4919738769531, 288.27130126953125, 262.3517761230469, 232.28518676757812, 228.1380615234375, 223.99093627929688, 216.73345947265625, 207.40243530273438, 179.40933227539062, 176.2989959716797, 170.07830810546875, 164.8944091796875, 141.04843139648438, 138.97486877441406, 135.86453247070312, 133.7909698486328, 128.60707092285156, 124.45994567871094, 123.42316436767578, 114.0921401977539, 109.94501495361328, 106.83466339111328, 195.05825805664062, 340.6371765136719, 1604.9146728515625, 8710.8955078125, 1705.0755615234375, 693.072265625, 563.9089965820312, 388.3882141113281, 605.3723754882812, 1111.6748046875, 474.6597900390625, 615.7955322265625, 312.6801452636719, 3073.74560546875, 5590.03369140625, 880.7485961914062, 539.4343872070312, 3820.31298828125, 2578.019287109375, 2933.3046875, 1375.8095703125, 3540.016845703125, 4173.7548828125, 3536.154052734375, 7594.97216796875, 3074.09033203125, 1324.1463623046875, 2123.024658203125, 3004.33056640625, 2811.66064453125, 2827.61181640625, 2085.250732421875, 498.57464599609375, 467.68115234375, 384.5883483886719, 299.36492919921875, 172.59510803222656, 139.57102966308594, 129.9833984375, 121.46107482910156, 115.0693130493164, 90.56758117675781, 89.50228881835938, 80.97994995117188, 60.739376068115234, 2699.428955078125, 51.151737213134766, 44.75998306274414, 88.43042755126953, 38.36822509765625, 38.36822509765625, 35.17234802246094, 31.976469039916992, 31.976469039916992, 92.74871063232422, 28.780590057373047, 55.47100067138672, 53.304317474365234, 25.584712982177734, 39.491607666015625, 3275.44482421875, 34.08651351928711, 180.472900390625, 627.5899658203125, 360.1532287597656, 102.6597900390625, 122.68379974365234, 329.1780700683594, 5119.935546875, 317.57763671875, 142.397216796875, 317.1229553222656, 191.03097534179688, 386.6408996582031, 165.40560913085938, 3280.78369140625, 2490.064453125, 802.7959594726562, 971.9375610351562, 400.2286071777344, 1137.6737060546875, 2295.644287109375, 9625.0947265625, 1508.232177734375, 2274.5830078125, 1348.588134765625, 6088.3232421875, 1174.3572998046875, 683.7560424804688, 7594.97216796875, 3198.013427734375, 3697.771484375, 3172.264892578125, 3740.22314453125, 2844.2783203125, 1536.3665771484375, 2762.45654296875, 2215.65673828125, 1225.759765625, 423.3687744140625, 324.5130310058594, 294.4265441894531, 155.81365966796875, 131.0997314453125, 123.57810974121094, 117.1310043334961, 111.7583999633789, 98.86418151855469, 75.22476959228516, 56.95794677734375, 44.063724517822266, 42.98920822143555, 163.33094787597656, 37.61661148071289, 49.44908142089844, 30.11102867126465, 24.725770950317383, 23.659500122070312, 187.84457397460938, 55.792049407958984, 42.948272705078125, 85.8436279296875, 61.35228729248047, 167.2717742919922, 45.185916900634766, 72.95293426513672, 48.224727630615234, 237.57228088378906, 35.66302490234375, 150.3437957763672, 895.2920532226562, 121.02996826171875, 702.0776977539062, 4129.29150390625, 4127.3271484375, 148.43907165527344, 261.6294250488281, 1156.18017578125, 436.5722351074219, 1015.2725219726562, 1565.0341796875, 617.2360229492188, 690.9237060546875, 908.4552612304688, 183.12612915039062, 884.7496948242188, 4950.51123046875, 252.75823974609375, 3010.14404296875, 1427.1834716796875, 429.20977783203125, 1727.7412109375, 707.2435302734375, 1380.4378662109375, 939.53759765625, 1335.1302490234375, 3989.48095703125, 1408.9075927734375, 1839.84912109375, 1877.9296875, 1847.917236328125, 2922.621826171875, 2752.885498046875, 3756.11572265625, 3697.771484375, 1840.75, 2274.5830078125, 1251.8543701171875, 325.7106628417969, 297.0670471191406, 239.77981567382812, 229.1710662841797, 228.11019897460938, 185.67520141601562, 171.8838348388672, 161.2750701904297, 135.8140869140625, 135.8140869140625, 135.8140869140625, 110.35308837890625, 107.17046356201172, 102.92697143554688, 101.8660888671875, 99.74433898925781, 95.50083923339844, 269.4800109863281, 85.95297241210938, 85.95297241210938, 85.95297241210938, 170.8854522705078, 78.52684783935547, 76.40509796142578, 73.22247314453125, 73.22247314453125, 222.79054260253906, 72.1615982055664, 71.10071563720703, 71.10071563720703, 71.10071563720703, 1239.5989990234375, 300.6118469238281, 185.4263458251953, 2933.3046875, 437.03436279296875, 1197.17529296875, 1014.7764282226562, 482.3345642089844, 5590.03369140625, 1485.247314453125, 368.65478515625, 4173.7548828125, 295.81829833984375, 2721.422119140625, 1056.5574951171875, 2847.537841796875, 1302.930419921875, 288.6316223144531, 3073.74560546875, 934.5820922851562, 547.2801513671875, 509.7808837890625, 1846.9710693359375, 1407.5362548828125, 5103.234375, 853.20458984375, 7594.97216796875, 9554.388671875, 3074.09033203125, 2870.5732421875, 3004.33056640625, 1375.8095703125, 2788.514892578125, 3409.512939453125, 1993.851318359375, 2251.865234375, 3540.016845703125, 702.736083984375, 525.693603515625, 236.77760314941406, 180.29779052734375, 175.95318603515625, 164.00552368164062, 146.6271209716797, 117.30107116699219, 108.61186218261719, 98.83651733398438, 97.7503662109375, 92.31961059570312, 89.06116485595703, 70.59661102294922, 65.16585540771484, 53.21820068359375, 104.2327880859375, 46.701297760009766, 28.236738204956055, 357.1189880371094, 200.78402709960938, 83.43293762207031, 45.66019821166992, 156.74195861816406, 638.1141967773438, 52.92198181152344, 61.07472229003906, 87.29840087890625, 366.81317138671875, 53.24909210205078, 158.3821563720703, 685.5687866210938, 187.02102661132812, 1916.6407470703125, 899.0823364257812, 4349.1044921875, 1108.7806396484375, 608.3839721679688, 543.7484741210938, 638.3753662109375, 2961.199951171875, 337.7456359863281, 1993.851318359375, 441.7176208496094, 1492.654052734375, 894.2161254882812, 661.46484375, 1059.7720947265625, 2922.621826171875, 864.0155029296875, 645.7506713867188, 4068.8544921875, 4600.7080078125, 4142.58154296875, 2081.9404296875, 3740.22314453125, 3004.33056640625, 3265.709228515625, 1840.75, 3669.952392578125, 2557.057861328125, 2363.46484375, 9625.0947265625, 2785.420654296875, 2827.61181640625, 2837.080078125, 3857.883056640625, 1217.4713134765625, 497.7658996582031, 483.7596435546875, 429.8894958496094, 310.2977294921875, 263.9693908691406, 216.5636749267578, 216.5636749267578, 150.84205627441406, 147.60984802246094, 142.2228240966797, 122.82957458496094, 113.13294219970703, 112.05553436279297, 108.82331848144531, 108.82331848144531, 96.97188568115234, 93.73967742919922, 92.66226959228516, 525.7255859375, 80.81083679199219, 142.19711303710938, 68.95939636230469, 65.72718811035156, 54.953147888183594, 54.953147888183594, 46.33392333984375, 38.79209899902344, 33.405086517333984, 81.94119262695312, 257.45294189453125, 492.30596923828125, 146.33656311035156, 99.1597671508789, 304.14910888671875, 243.84495544433594, 3259.776611328125, 863.5836181640625, 223.41986083984375, 679.7259521484375, 1217.732421875, 763.9823608398438, 609.1243286132812, 305.1495361328125, 234.14427185058594, 3486.492919921875, 426.9132385253906, 1670.1131591796875, 592.6728515625, 2093.365478515625, 1225.759765625, 1508.232177734375, 958.9278564453125, 1825.6182861328125, 4487.28271484375, 9625.0947265625, 3409.512939453125, 3198.013427734375, 1083.485595703125, 2983.046875, 1315.55615234375, 2280.848876953125, 3265.709228515625, 2844.2783203125, 3736.588134765625, 4349.1044921875, 1306.91064453125, 1047.0498046875, 649.1038818359375, 612.13623046875, 590.3905639648438, 412.0761413574219, 406.63970947265625, 2405.303955078125, 284.8639221191406, 271.8165588378906, 257.681884765625, 223.97610473632812, 201.1431427001953, 197.8813018798828, 191.35760498046875, 185.92120361328125, 164.17552185058594, 134.81886291503906, 123.9460220336914, 123.9460220336914, 110.89862060546875, 106.54948425292969, 95.67664337158203, 93.50208282470703, 85.89109802246094, 81.54196166992188, 81.54196166992188, 79.36739349365234, 76.10555267333984, 73.93098449707031, 138.0713348388672, 453.11077880859375, 360.6805419921875, 499.7257080078125, 206.01242065429688, 436.0231628417969, 674.7203369140625, 180.09812927246094, 10298.7275390625, 402.62420654296875, 1502.1767578125, 288.47381591796875, 1384.6158447265625, 1389.1300048828125, 4646.6455078125, 971.9575805664062, 3265.709228515625, 526.62353515625, 1477.2921142578125, 839.7687377929688, 4950.51123046875, 4468.5390625, 1123.36474609375, 1018.921875, 2295.644287109375, 1287.780029296875, 3536.154052734375, 1847.917236328125, 929.9110107421875, 1605.9344482421875, 1608.750732421875, 7145.56005859375, 1607.463623046875, 549.419921875, 241.4499053955078, 225.35391235351562, 158.8237762451172, 1144.9805908203125, 147.02005004882812, 140.58164978027344, 136.2893829345703, 124.48563385009766, 235.06182861328125, 100.878173828125, 159.8754425048828, 77.27070617675781, 475.3174133300781, 68.68617248535156, 64.39391326904297, 49.370975494384766, 46.15177536010742, 42.932579040527344, 41.85951232910156, 39.71337890625, 38.64031219482422, 3039.044677734375, 37.56724548339844, 37.56724548339844, 106.36676025390625, 86.8716812133789, 41.895111083984375, 447.2307434082031, 678.517333984375, 156.21383666992188, 302.27545166015625, 294.9913330078125, 81.5822982788086, 1516.629638671875, 692.5614013671875, 408.9950256347656, 276.999267578125, 360.5525817871094, 1249.0145263671875, 287.1607971191406, 1196.8912353515625, 6088.3232421875, 460.6172180175781, 444.6658630371094, 1724.987060546875, 2870.5732421875, 1294.0848388671875, 624.5131225585938, 373.9588623046875, 1264.8192138671875, 2721.422119140625, 2840.874755859375, 2371.6669921875, 1012.4879760742188, 639.7758178710938, 3409.512939453125, 2096.515380859375, 656.1529541015625, 4127.3271484375, 2721.529296875, 2837.080078125, 3820.31298828125, 1042.5704345703125, 588.8964233398438, 579.081298828125, 515.8286743164062, 460.2099304199219, 419.859130859375, 1472.16064453125, 333.70465087890625, 317.3462219238281, 164.6674346923828, 163.5768585205078, 158.12405395507812, 139.58447265625, 122.1354751586914, 118.86378479003906, 115.59210205078125, 106.86759185791016, 95.96196746826172, 94.87139892578125, 83.96577453613281, 82.87520599365234, 81.7846450805664, 74.15070343017578, 71.9695816040039, 69.78844451904297, 64.33563232421875, 56.701690673828125, 55.61112976074219, 52.33944320678711, 91.58221435546875, 197.27883911132812, 206.23561096191406, 489.3132019042969, 165.63551330566406, 299.150634765625, 149.33795166015625, 171.42559814453125, 7524.642578125, 292.4447937011719, 2105.0791015625, 1415.6954345703125, 3550.6826171875, 384.6451721191406, 378.2552490234375, 1069.6419677734375, 405.81243896484375, 1511.0797119140625, 3282.227783203125, 436.230712890625, 944.7434692382812, 1787.19775390625, 1816.7332763671875, 4487.28271484375, 1499.5889892578125, 504.80389404296875, 2847.537841796875, 992.389404296875, 1837.3697509765625, 907.1847534179688, 4127.3271484375, 3989.48095703125, 930.654296875, 914.8941040039062, 2844.2783203125, 6088.3232421875, 4562.177734375, 1748.35693359375, 413.5105895996094, 386.9023742675781, 231.6876983642578, 161.84107971191406, 138.55886840820312, 1507.5389404296875, 118.6026840209961, 114.16798400878906, 105.29857635498047, 70.92959594726562, 108.59382629394531, 52.08209991455078, 47.64739227294922, 44.32136154174805, 43.21268844604492, 55.42283248901367, 98.50407409667969, 110.33868408203125, 83.19856262207031, 26.585905075073242, 111.6873779296875, 327.2056884765625, 79.55990600585938, 324.68609619140625, 194.05999755859375, 279.1578674316406, 103.53913879394531, 33.29341506958008, 460.44049072265625, 1952.4957275390625, 298.1094665527344, 402.7729797363281, 687.5757446289062, 652.3555908203125, 1314.8594970703125, 379.9903564453125, 372.12615966796875, 388.8887939453125, 3989.48095703125, 1499.5889892578125, 329.63970947265625, 3756.11572265625, 460.8486328125, 895.4791259765625, 934.9603881835938, 1294.0848388671875, 3736.588134765625, 3409.512939453125, 1821.884521484375, 1424.6865234375, 3669.952392578125, 2196.375732421875, 3740.22314453125, 1505.71240234375, 1320.26318359375, 2220.780517578125, 1235.5732421875, 2280.848876953125, 1477.2921142578125, 4728.77001953125, 1849.6788330078125, 591.7713623046875, 486.1372985839844, 404.217041015625, 379.42535400390625, 306.1282653808594, 270.5576171875, 247.9217529296875, 188.63734436035156, 173.5467529296875, 500.2115478515625, 154.1445770263672, 621.0450439453125, 143.36558532714844, 293.2502746582031, 136.89820861816406, 134.74240112304688, 270.5542297363281, 127.19711303710938, 125.04132080078125, 115.34024047851562, 109.95074462890625, 107.7949447631836, 107.7949447631836, 106.71704864501953, 105.63914489746094, 101.32755279541016, 95.93805694580078, 87.31487274169922, 81.92537689208984, 809.0529174804688, 198.41468811035156, 2601.488525390625, 1514.4615478515625, 1447.332275390625, 444.4111022949219, 203.31674194335938, 145.8579559326172, 1335.674072265625, 891.7621459960938, 1821.884521484375, 348.78271484375, 1209.471435546875, 3536.154052734375, 915.6847534179688, 636.2720336914062, 1246.9716796875, 1476.4925537109375, 941.011962890625, 8710.8955078125, 1070.8238525390625, 1972.8218994140625, 3540.016845703125, 3074.09033203125, 3715.311767578125, 2762.45654296875, 2220.780517578125, 2251.865234375, 322.2015380859375, 153.132080078125, 144.0545196533203, 288.0777282714844, 111.14837646484375, 100.93612670898438, 94.12796020507812, 94.12796020507812, 89.58917999267578, 77.10753631591797, 64.62588500976562, 63.491188049316406, 168.8219757080078, 49.874855041503906, 46.47077178955078, 40.79729461669922, 38.52790832519531, 28.315658569335938, 62.240535736083984, 52.00543975830078, 179.84323120117188, 27.10499382019043, 414.5103454589844, 173.34033203125, 35.964778900146484, 108.25422668457031, 975.3931274414062, 42.874847412109375, 1258.1446533203125, 70.02719116210938, 4468.5390625, 1178.200439453125, 100.42215728759766, 136.09690856933594, 331.2306823730469, 1500.834228515625, 217.3072509765625, 2202.350341796875, 1899.0953369140625, 128.6116943359375, 500.4753112792969, 285.0664978027344, 422.3586730957031, 295.81646728515625, 346.01641845703125, 273.242431640625, 472.4601745605469, 804.3253173828125, 3504.890625, 527.9046020507812, 2220.780517578125, 1283.699462890625, 447.63836669921875, 3540.016845703125, 1641.8782958984375, 3857.883056640625, 1308.732421875, 2870.5732421875, 1419.02001953125, 3756.11572265625, 2827.61181640625, 3697.771484375, 4173.7548828125, 3004.33056640625, 1967.3531494140625, 3715.311767578125, 2182.134765625, 1916.6407470703125, 908.7745361328125, 377.41546630859375, 281.9281311035156, 214.5253143310547, 114.54444122314453, 113.42105102539062, 90.95344543457031, 89.83006286621094, 87.58330535888672, 56.128639221191406, 53.88187789916992, 43.771453857421875, 1848.8636474609375, 35.90778732299805, 35.90778732299805, 31.414262771606445, 60.63347244262695, 56.079776763916016, 25.7973575592041, 25.7973575592041, 117.63398742675781, 232.36700439453125, 59.42658233642578, 506.5450744628906, 861.6400146484375, 248.44503784179688, 138.1436309814453, 3038.06591796875, 135.25192260742188, 71.53421783447266, 124.77433013916016, 350.2178649902344, 556.4119262695312, 3764.31787109375, 842.8213500976562, 408.9465026855469, 10298.7275390625, 1097.4378662109375, 1287.780029296875, 717.8487548828125, 1377.635009765625, 480.9560546875, 3857.883056640625, 337.2395935058594, 508.2423095703125, 750.9049072265625, 893.5768432617188, 789.0875854492188, 1061.8223876953125, 7594.97216796875, 1586.203857421875, 3989.48095703125, 4646.6455078125, 2637.33056640625, 1825.262451171875, 2748.161376953125, 1877.9296875, 2135.73583984375, 666.283447265625, 430.1373291015625, 225.03384399414062, 217.27317810058594, 209.51251220703125, 203.9691619873047, 179.57847595214844, 164.05711364746094, 131.90577697753906, 87.55905151367188, 85.34172821044922, 82.0157241821289, 82.0157241821289, 77.5810546875, 52.08169174194336, 155.1339874267578, 140.70968627929688, 43.21235275268555, 688.5242919921875, 396.0965576171875, 60.88850784301758, 194.7241668701172, 411.0512390136719, 81.03719329833984, 197.0455780029297, 92.01734161376953, 187.9247283935547, 1894.610595703125, 107.83299255371094, 64.01996612548828, 4853.76708984375, 488.0006103515625, 1663.86669921875, 1978.9119873046875, 422.1731262207031, 301.88018798828125, 963.9942626953125, 749.19970703125, 265.4942932128906, 1837.3697509765625, 164.6929473876953, 1829.105224609375, 331.0425109863281, 889.2739868164062, 421.181884765625, 4349.1044921875, 820.4109497070312, 4173.7548828125, 399.7578125, 4068.8544921875, 1993.851318359375, 2123.024658203125, 3282.227783203125, 2251.865234375, 1505.71240234375, 1061.8223876953125, 1380.4378662109375, 1013.7672119140625, 3504.890625, 3540.016845703125, 1163.1663818359375, 1013.0408325195312, 782.84814453125, 646.0670776367188, 624.9382934570312, 544.8712768554688, 394.74566650390625, 353.6001281738281, 292.4378356933594, 271.30908203125, 250.1802978515625, 233.4996795654297, 194.57821655273438, 177.89759826660156, 173.44943237304688, 157.88084411621094, 155.65676879882812, 128.9677734375, 127.85572052001953, 111.17509460449219, 104.50284576416016, 94.49446868896484, 94.49446868896484, 86.71018981933594, 81.14997863769531, 75.58977508544922, 73.36568450927734, 70.02955627441406, 70.02955627441406, 66.69343566894531, 168.89892578125, 113.39581298828125, 102.21839904785156, 3034.32275390625, 1594.0804443359375, 818.3057250976562, 933.2529907226562, 610.0392456054688, 2840.874755859375, 403.21002197265625, 238.234375, 173.6689453125, 563.3981323242188, 1264.8192138671875, 842.2666015625, 485.4410400390625, 1877.9296875, 929.4664306640625, 9625.0947265625, 682.8455810546875, 1012.4879760742188, 3172.264892578125, 6088.3232421875, 3764.31787109375, 3697.771484375, 3715.311767578125, 3756.11572265625, 4728.77001953125, 10298.7275390625, 7524.642578125, 2213.502197265625, 1035.283935546875, 1014.8721923828125, 551.0693359375, 275.5091247558594, 253.9633026123047, 252.82931518554688, 225.61349487304688, 166.6458740234375, 143.96603393554688, 141.6980438232422, 123.55416107177734, 122.42017364501953, 116.75021362304688, 106.54428100585938, 90.66838836669922, 74.79248809814453, 209.61865234375, 135.95497131347656, 65.72055053710938, 63.45256423950195, 63.45256423950195, 62.318572998046875, 61.18457794189453, 56.64860916137695, 49.84465408325195, 517.47119140625, 196.97119140625, 73.56925201416016, 28.29880142211914, 128.0929412841797, 428.3132629394531, 985.3456420898438, 645.2440185546875, 2530.66796875, 2721.529296875, 134.33358764648438, 1725.933837890625, 181.15431213378906, 2268.80517578125, 686.176025390625, 2096.515380859375, 1506.2110595703125, 443.3289794921875, 929.9110107421875, 1018.921875, 930.654296875, 3010.14404296875, 742.834228515625, 862.3865356445312, 228.27731323242188, 656.1529541015625, 1380.4378662109375, 666.1495361328125, 2721.422119140625, 1056.5574951171875, 1565.0341796875, 1115.5157470703125, 3820.31298828125, 3504.890625, 1502.1767578125, 4950.51123046875], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.479300022125244, -4.991700172424316, -5.368899822235107, -5.959799766540527, -6.086100101470947, -6.098400115966797, -6.1579999923706055, -5.487100124359131, -6.2546000480651855, -6.274099826812744, -6.274099826812744, -6.383299827575684, -6.422500133514404, -6.469299793243408, -6.576600074768066, -6.596799850463867, -6.638500213623047, -6.667200088500977, -6.704400062561035, -6.719699859619141, -6.783100128173828, -6.833600044250488, -6.8597002029418945, -6.962100028991699, -6.981900215148926, -7.156199932098389, -7.168099880218506, -7.282800197601318, -7.29640007019043, -6.184700012207031, -4.140399932861328, -5.51669979095459, -5.0493998527526855, -4.720600128173828, -5.341400146484375, -4.33620023727417, -4.323500156402588, -5.4369001388549805, -5.11359977722168, -5.559199810028076, -5.434700012207031, -3.7170000076293945, -4.225500106811523, -5.941999912261963, -6.098400115966797, -4.707099914550781, -5.443299770355225, -3.92930006980896, -5.305200099945068, -5.873700141906738, -4.334799766540527, -5.959799766540527, -4.699999809265137, -5.639400005340576, -5.401299953460693, -4.810999870300293, -4.780700206756592, -5.505199909210205, -5.305200099945068, -5.355000019073486, -5.13700008392334, -5.060999870300293, -5.413700103759766, -4.352399826049805, -4.80019998550415, -5.441999912261963, -5.654099941253662, -5.707699775695801, -4.148900032043457, -6.242800235748291, -6.304100036621094, -6.43310022354126, -6.514100074768066, -6.595200061798096, -6.609399795532227, -6.7067999839782715, -6.78879976272583, -6.797299861907959, -6.832399845123291, -6.896900177001953, -6.906499862670898, -6.906499862670898, -6.935699939727783, -7.007500171661377, -7.039899826049805, -6.36929988861084, -7.206900119781494, -7.30210018157959, -6.602200031280518, -7.376200199127197, -7.407400131225586, -7.439700126647949, -7.543099880218506, -7.007500171661377, -5.152699947357178, -4.984600067138672, -6.257800102233887, -4.8592000007629395, -5.107100009918213, -5.746099948883057, -6.403600215911865, -6.247799873352051, -6.113100051879883, -5.648600101470947, -6.189799785614014, -5.956500053405762, -5.056000232696533, -5.532599925994873, -5.902200222015381, -5.274600028991699, -4.8653998374938965, -5.532599925994873, -4.781700134277344, -5.20389986038208, -5.054500102996826, -4.441400051116943, -5.719299793243408, -5.246799945831299, -4.68120002746582, -4.943299770355225, -4.825099945068359, -5.409200191497803, -4.986000061035156, -5.518099784851074, -5.354800224304199, -5.16949987411499, -4.943299770355225, -5.36299991607666, -5.2652997970581055, -5.205599784851074, -5.420000076293945, -5.367099761962891, -5.392199993133545, -3.6473000049591064, -6.276100158691406, -6.861599922180176, -7.011099815368652, -7.239500045776367, -7.323999881744385, -7.554299831390381, -7.692999839782715, -7.758900165557861, -7.8053998947143555, -7.829400062561035, -7.987400054931641, -8.016400337219238, -8.284199714660645, -8.3233003616333, -8.3233003616333, -3.678999900817871, -4.514200210571289, -8.545999526977539, -6.2210001945495605, -8.246500015258789, -7.212800025939941, -7.239500045776367, -5.674799919128418, -7.124800205230713, -8.016400337219238, -5.64139986038208, -6.201600074768066, -4.603899955749512, -5.145500183105469, -2.5627999305725098, -6.154900074005127, -6.4319000244140625, -3.4103000164031982, -4.966300010681152, -7.338799953460693, -4.179599761962891, -3.968600034713745, -6.323599815368652, -6.345399856567383, -4.798900127410889, -4.266900062561035, -6.141300201416016, -3.4344000816345215, -5.927299976348877, -5.810699939727783, -4.55210018157959, -5.8302998542785645, -3.9284000396728516, -5.032400131225586, -6.059199810028076, -4.657599925994873, -5.3516998291015625, -5.627900123596191, -5.4527997970581055, -5.360000133514404, -5.343599796295166, -5.174200057983398, -5.441500186920166, -5.611800193786621, -5.638700008392334, -5.718100070953369, -5.669099807739258, -5.66349983215332, -5.953000068664551, -6.688000202178955, -6.695799827575684, -6.857800006866455, -6.924799919128418, -7.249300003051758, -7.365300178527832, -7.396500110626221, -7.627399921417236, -7.801599979400635, -7.928199768066406, -7.95550012588501, -7.335000038146973, -8.042400360107422, -8.073100090026855, -8.1048002243042, -8.319499969482422, -8.360300064086914, -8.402799606323242, -7.732600212097168, -8.447099685668945, -7.18310022354126, -8.13759994506836, -7.0289998054504395, -5.426599979400635, -8.319499969482422, -7.007299900054932, -4.101399898529053, -8.493499755859375, -8.206399917602539, -6.905200004577637, -5.853300094604492, -4.325099945068359, -6.642199993133545, -6.591300010681152, -5.533899784088135, -4.8495001792907715, -5.094600200653076, -5.465000152587891, -6.2469000816345215, -3.7325000762939453, -5.4558000564575195, -5.273099899291992, -6.490200042724609, -5.51200008392334, -4.758200168609619, -4.516600131988525, -5.823200225830078, -4.165800094604492, -4.904399871826172, -5.339799880981445, -4.86329984664917, -6.2368998527526855, -4.311999797821045, -5.060299873352051, -5.673799991607666, -5.013899803161621, -5.846499919891357, -5.803699970245361, -5.70550012588501, -5.930799961090088, -5.741199970245361, -4.616700172424316, -5.049600124359131, -4.9695000648498535, -4.496300220489502, -5.063399791717529, -5.651400089263916, -5.3358001708984375, -5.333700180053711, -5.613399982452393, -5.47189998626709, -5.349999904632568, -5.548799991607666, -5.47189998626709, -5.467299938201904, -5.485899925231934, -4.563499927520752, -5.21999979019165, -5.24370002746582, -5.42710018157959, -5.465400218963623, -5.5100998878479, -5.541800022125244, -5.600599765777588, -5.895199775695801, -5.9166998863220215, -6.011199951171875, -6.133500099182129, -6.151599884033203, -6.170000076293945, -6.203100204467773, -6.247300148010254, -6.393099784851074, -6.410699844360352, -6.446800231933594, -6.47790002822876, -6.635200023651123, -6.650100231170654, -6.672900199890137, -6.688399791717529, -6.728300094604492, -6.761300086975098, -6.769800186157227, -6.849100112915039, -6.88640022277832, -6.91540002822876, -6.314799785614014, -5.77239990234375, -4.268400192260742, -2.684999942779541, -4.287399768829346, -5.212800025939941, -5.407400131225586, -5.74459981918335, -5.3649001121521, -4.851200103759766, -5.590099811553955, -5.39870023727417, -5.964700222015381, -4.3256001472473145, -3.922100067138672, -5.4116997718811035, -5.735599994659424, -4.765399932861328, -5.0335001945495605, -4.981200218200684, -5.340199947357178, -4.9953999519348145, -4.946400165557861, -5.062300205230713, -4.871300220489502, -5.172599792480469, -5.442699909210205, -5.354499816894531, -5.306399822235107, -5.31820011138916, -5.3302001953125, -5.37529993057251, -5.338600158691406, -5.402699947357178, -5.598800182342529, -5.850100040435791, -6.403299808502197, -6.617099761962891, -6.688899993896484, -6.757199764251709, -6.811800003051758, -7.053599834442139, -7.0655999183654785, -7.166900157928467, -7.458799839019775, -3.6670000553131104, -7.633800029754639, -7.770299911499023, -7.090000152587891, -7.928299903869629, -7.928299903869629, -8.017800331115723, -8.116000175476074, -8.116000175476074, -7.053599834442139, -8.225000381469727, -7.572000026702881, -7.612800121307373, -8.347399711608887, -7.928299903869629, -3.5160000324249268, -8.082200050354004, -6.415800094604492, -5.27239990234375, -5.821899890899658, -7.018599987030029, -6.8694000244140625, -5.967400074005127, -3.449399948120117, -6.004199981689453, -6.739699840545654, -6.025300025939941, -6.521900177001953, -5.939700126647949, -6.656300067901611, -4.193399906158447, -4.4471001625061035, -5.371300220489502, -5.219699859619141, -5.9319000244140625, -5.167799949645996, -4.6585001945495605, -3.6435000896453857, -4.998799800872803, -4.700500011444092, -5.084700107574463, -4.004000186920166, -5.30079984664917, -5.650100231170654, -4.2245001792907715, -4.825699806213379, -4.900199890136719, -5.0879998207092285, -5.081399917602539, -5.290599822998047, -5.529099941253662, -5.428199768066406, -5.560699939727783, -5.679800033569336, -5.464700222015381, -5.731299877166748, -5.828999996185303, -6.468500137329102, -6.642399787902832, -6.702000141143799, -6.75600004196167, -6.803400039672852, -6.927199840545654, -7.203800201416016, -7.486400127410889, -7.748499870300293, -7.773799896240234, -6.441100120544434, -7.910799980163574, -7.653299808502197, -8.178600311279297, -8.39169979095459, -8.440400123596191, -6.3694000244140625, -7.587399959564209, -7.853700160980225, -7.17519998550415, -7.525599956512451, -6.5329999923706055, -7.853700160980225, -7.377299785614014, -7.799699783325195, -6.207200050354004, -8.104599952697754, -6.667500019073486, -4.912099838256836, -6.9054999351501465, -5.22790002822876, -3.584199905395508, -3.6807000637054443, -6.728600025177002, -6.223400115966797, -4.906199932098389, -5.7895002365112305, -5.063499927520752, -4.697999954223633, -5.516900062561035, -5.424799919128418, -5.206099987030029, -6.5553998947143555, -5.277299880981445, -3.9065001010894775, -6.3506999015808105, -4.697999954223633, -5.198200225830078, -6.017899990081787, -5.171299934387207, -5.73799991607666, -5.3632001876831055, -5.6006999015808105, -5.441999912261963, -4.907700061798096, -5.44950008392334, -5.400599956512451, -5.405399799346924, -5.490499973297119, -5.331500053405762, -5.365499973297119, -5.273099899291992, -5.474899768829346, -5.57480001449585, -5.566400051116943, -4.349999904632568, -5.698699951171875, -5.791100025177002, -6.006199836730957, -6.051599979400635, -6.056300163269043, -6.2631001472473145, -6.340799808502197, -6.404900074005127, -6.577899932861328, -6.577899932861328, -6.577899932861328, -6.787300109863281, -6.816800117492676, -6.857600212097168, -6.868100166320801, -6.8892998695373535, -6.933300018310547, -5.8968000411987305, -7.03980016708374, -7.03980016708374, -7.03980016708374, -6.353300094604492, -7.13129997253418, -7.15910005569458, -7.202199935913086, -7.202199935913086, -6.0894999504089355, -7.2170000076293945, -7.232100009918213, -7.232100009918213, -7.232100009918213, -4.563899993896484, -5.884900093078613, -6.33459997177124, -3.7852001190185547, -5.55019998550415, -4.632299900054932, -4.785099983215332, -5.484799861907959, -3.376499891281128, -4.540999889373779, -5.798299789428711, -3.8052000999450684, -5.988500118255615, -4.183199882507324, -4.964099884033203, -4.1753997802734375, -4.8171000480651855, -6.015100002288818, -4.302800178527832, -5.22189998626709, -5.590400218963623, -5.685699939727783, -4.928599834442139, -5.119800090789795, -4.43310022354126, -5.41349983215332, -4.38539981842041, -4.404900074005127, -4.919600009918213, -5.067999839782715, -5.223899841308594, -5.45359992980957, -5.2718000411987305, -5.263299942016602, -5.418499946594238, -5.52239990234375, -5.508800029754639, -4.922599792480469, -5.213399887084961, -6.013400077819824, -6.287199974060059, -6.311800003051758, -6.382500171661377, -6.495299816131592, -6.720200061798096, -6.797900199890137, -6.893099784851074, -6.904300212860107, -6.962100028991699, -6.9984002113342285, -7.233799934387207, -7.315100193023682, -7.521299839019775, -6.849699974060059, -7.654600143432617, -8.172599792480469, -5.656799793243408, -6.2342000007629395, -7.116099834442139, -7.728700160980225, -6.502799987792969, -5.099999904632568, -7.654600143432617, -7.521299839019775, -7.173299789428711, -5.747900009155273, -7.678699970245361, -6.605599880218506, -5.194900035858154, -6.444399833679199, -4.367000102996826, -5.065499782562256, -3.6796998977661133, -4.892099857330322, -5.442699909210205, -5.625, -5.538300037384033, -4.37060022354126, -6.0366997718811035, -4.797599792480469, -5.883900165557861, -5.0725998878479, -5.429800033569336, -5.653500080108643, -5.404399871826172, -4.821100234985352, -5.544000148773193, -5.723400115966797, -4.721399784088135, -4.727799892425537, -4.7895002365112305, -5.137700080871582, -4.869900226593018, -4.991499900817871, -4.963699817657471, -5.290800094604492, -5.023499965667725, -5.215400218963623, -5.253499984741211, -5.111100196838379, -5.338900089263916, -5.3460001945495605, -5.3460001945495605, -5.374800205230713, -4.370699882507324, -5.266300201416016, -5.294899940490723, -5.4131999015808105, -5.740099906921387, -5.902400016784668, -6.101200103759766, -6.101200103759766, -6.465000152587891, -6.486800193786621, -6.524199962615967, -6.671999931335449, -6.754899978637695, -6.764599800109863, -6.794099807739258, -6.794099807739258, -6.910600185394287, -6.944900035858154, -6.956600189208984, -5.221799850463867, -7.095099925994873, -6.531899929046631, -7.255899906158447, -7.304599761962891, -7.486800193786621, -7.486800193786621, -7.660900115966797, -7.8429999351501465, -7.9969000816345215, -7.108699798583984, -5.979000091552734, -5.345200061798096, -6.539599895477295, -6.921899795532227, -5.850500106811523, -6.061999797821045, -3.608299970626831, -4.867400169372559, -6.173799991607666, -5.163300037384033, -4.6579999923706055, -5.0879998207092285, -5.29040002822876, -5.906499862670898, -6.152500152587891, -4.036499977111816, -5.716000080108643, -4.656899929046631, -5.470099925994873, -4.590700149536133, -4.976500034332275, -4.8471999168396, -5.181099891662598, -4.835899829864502, -4.389400005340576, -4.228899955749512, -4.765100002288818, -4.90749979019165, -5.423299789428711, -5.098899841308594, -5.3958001136779785, -5.249100208282471, -5.266300201416016, -5.312900066375732, -5.301599979400635, -5.3933000564575195, -4.252500057220459, -4.474400043487549, -4.953100204467773, -5.011899948120117, -5.048099994659424, -5.408400058746338, -5.421800136566162, -3.6445000171661377, -5.778800010681152, -5.825799942016602, -5.87939977645874, -6.020199775695801, -6.128300189971924, -6.144700050354004, -6.178400039672852, -6.207399845123291, -6.332499980926514, -6.530900001525879, -6.615600109100342, -6.615600109100342, -6.727799892425537, -6.768199920654297, -6.876999855041504, -6.900199890136719, -6.986100196838379, -7.038700103759766, -7.038700103759766, -7.066100120544434, -7.10860013961792, -7.138000011444092, -6.514699935913086, -5.3618998527526855, -5.597099781036377, -5.303100109100342, -6.166999816894531, -5.473999977111816, -5.188499927520752, -6.339200019836426, -3.0144999027252197, -5.687300205230713, -4.652599811553955, -5.981900215148926, -4.874100208282471, -4.975200176239014, -4.238500118255615, -5.277200222015381, -4.640200138092041, -5.666500091552734, -5.1367998123168945, -5.451700210571289, -4.643899917602539, -4.754499912261963, -5.379799842834473, -5.429800033569336, -5.23859977722168, -5.408400058746338, -5.146900177001953, -5.374599933624268, -5.520299911499023, -5.479700088500977, -5.479700088500977, -2.547100067138672, -4.039400100708008, -5.114200115203857, -5.938799858093262, -6.0081000328063965, -6.359899997711182, -4.3846001625061035, -6.437600135803223, -6.482699871063232, -6.513999938964844, -6.605299949645996, -5.9704999923706055, -6.817500114440918, -6.359899997711182, -7.087299823760986, -5.270599842071533, -7.206699848175049, -7.272299766540527, -7.542900085449219, -7.611800193786621, -7.685800075531006, -7.711699962615967, -7.765699863433838, -7.793900012969971, -3.4293999671936035, -7.822800159454346, -7.822800159454346, -6.785799980163574, -6.993299961090088, -7.738399982452393, -5.392099857330322, -5.003200054168701, -6.452400207519531, -5.821000099182129, -5.853300094604492, -7.087299823760986, -4.411300182342529, -5.144000053405762, -5.633399963378906, -5.984399795532227, -5.763199806213379, -4.697700023651123, -5.9704999923706055, -4.778299808502197, -3.657099962234497, -5.650000095367432, -5.690999984741211, -4.849299907684326, -4.566100120544434, -5.087100028991699, -5.548399925231934, -5.874000072479248, -5.239099979400635, -4.892199993133545, -4.893799781799316, -5.064499855041504, -5.4506001472473145, -5.639999866485596, -5.118100166320801, -5.3078999519348145, -5.633399963378906, -5.215000152587891, -5.551400184631348, -5.575900077819824, -5.630199909210205, -4.463399887084961, -5.035399913787842, -5.052199840545654, -5.168099880218506, -5.282400131225586, -5.3744001388549805, -4.120299816131592, -5.604700088500977, -5.655099868774414, -6.314199924468994, -6.320899963378906, -6.355000019073486, -6.480599880218506, -6.615200042724609, -6.642600059509277, -6.6707000732421875, -6.75, -6.858699798583984, -6.870299816131592, -6.993800163269043, -7.006999969482422, -7.020400047302246, -7.119699954986572, -7.150000095367432, -7.181300163269043, -7.263899803161621, -7.392399787902832, -7.412199974060059, -7.473999977111816, -6.917900085449219, -6.160099983215332, -6.126399993896484, -5.282400131225586, -6.355000019073486, -5.787700176239014, -6.465000152587891, -6.34119987487793, -2.864799976348877, -5.852700233459473, -4.186299800872803, -4.550899982452393, -3.763000011444092, -5.676000118255615, -5.726600170135498, -4.913300037384033, -5.679599761962891, -4.704999923706055, -4.2291998863220215, -5.711900234222412, -5.202600002288818, -4.852799892425537, -4.843500137329102, -4.340400218963623, -5.035399913787842, -5.665500164031982, -4.769800186157227, -5.321199893951416, -5.056000232696533, -5.376999855041504, -4.7459001541137695, -4.866799831390381, -5.450099945068359, -5.516900062561035, -5.40880012512207, -5.436200141906738, -2.9595999717712402, -3.919100046157837, -5.36269998550415, -5.4293999671936035, -5.943999767303467, -6.304699897766113, -6.461100101470947, -4.075399875640869, -6.6178998947143555, -6.656300067901611, -6.7378997802734375, -7.137800216674805, -6.716899871826172, -7.452000141143799, -7.542900085449219, -7.6168999671936035, -7.642899990081787, -7.4095001220703125, -6.850399971008301, -6.759399890899658, -7.046899795532227, -8.18850040435791, -6.759399890899658, -5.705599784851074, -7.122099876403809, -5.724800109863281, -6.250999927520752, -5.887899875640869, -6.9243998527526855, -8.06089973449707, -5.458600044250488, -4.097099781036377, -5.948800086975098, -5.683199882507324, -5.224899768829346, -5.307799816131592, -4.753699779510498, -5.800899982452393, -5.847799777984619, -5.834799766540527, -4.046800136566162, -4.8520002365112305, -5.998300075531006, -4.355100154876709, -5.78439998626709, -5.395500183105469, -5.376299858093262, -5.213200092315674, -4.684599876403809, -4.737800121307373, -5.146100044250488, -5.338799953460693, -4.925600051879883, -5.246200084686279, -5.087299823760986, -5.4095001220703125, -5.473499774932861, -5.513299942016602, -5.643199920654297, -5.601399898529053, -5.683199882507324, -5.683199882507324, -3.8208000659942627, -4.961699962615967, -5.158699989318848, -5.343599796295166, -5.407100200653076, -5.622399806976318, -5.746399879455566, -5.834099769592285, -6.108699798583984, -6.192500114440918, -5.134500026702881, -6.3119001388549805, -4.918799877166748, -6.384900093078613, -5.669400215148926, -6.431399822235107, -6.447299957275391, -5.750400066375732, -6.50540018081665, -6.52269983291626, -6.604100227355957, -6.652400016784668, -6.672399997711182, -6.672399997711182, -6.682600021362305, -6.692800045013428, -6.734899997711182, -6.79010009765625, -6.88539981842041, -6.949900150299072, -4.674900054931641, -6.069300174713135, -3.5501999855041504, -4.093999862670898, -4.257699966430664, -5.362500190734863, -6.085999965667725, -6.415599822998047, -4.63070011138916, -5.0218000411987305, -4.5005998611450195, -5.758399963378906, -4.839799880981445, -4.069799900054932, -5.087900161743164, -5.373499870300293, -5.0218000411987305, -4.9857001304626465, -5.294099807739258, -4.1753997802734375, -5.3069000244140625, -5.055300235748291, -4.981900215148926, -5.14109992980957, -5.174300193786621, -5.2469000816345215, -5.317299842834473, -5.348999977111816, -5.527400016784668, -6.274799823760986, -6.336400032043457, -5.643400192260742, -6.597799777984619, -6.695199966430664, -6.765699863433838, -6.765699863433838, -6.815700054168701, -6.967599868774414, -7.1468000411987305, -7.16480016708374, -6.1890997886657715, -7.410699844360352, -7.482900142669678, -7.616300106048584, -7.675000190734863, -7.9928998947143555, -7.220900058746338, -7.410699844360352, -6.182300090789795, -8.07979965209961, -5.367800235748291, -6.252699851989746, -7.839099884033203, -6.741700172424316, -4.545000076293945, -7.675000190734863, -4.300899982452393, -7.220900058746338, -3.164299964904785, -4.4959001541137695, -6.868299961090088, -6.587600231170654, -5.770199775695801, -4.44320011138916, -6.17549991607666, -4.191100120544434, -4.363999843597412, -6.650700092315674, -5.552499771118164, -6.013700008392334, -5.71750020980835, -6.019499778747559, -5.915299892425537, -6.103799819946289, -5.730400085449219, -5.370800018310547, -4.385200023651123, -5.692200183868408, -4.827300071716309, -5.209400177001953, -5.83050012588501, -4.834400177001953, -5.2729997634887695, -4.919099807739258, -5.395299911499023, -5.077099800109863, -5.401500225067139, -5.054900169372559, -5.235400199890137, -5.166600227355957, -5.14709997177124, -5.256700038909912, -5.552499771118164, -5.485899925231934, -5.585599899291992, -5.65939998626709, -4.484000205993652, -5.364299774169922, -5.656899929046631, -5.931300163269043, -6.563000202178955, -6.57289981842041, -6.795899868011475, -6.808499813079834, -6.834099769592285, -7.285699844360352, -7.327400207519531, -7.539700031280518, -3.7964000701904297, -7.743000030517578, -7.743000030517578, -7.880899906158447, -7.226399898529053, -7.306300163269043, -8.08530044555664, -8.08530044555664, -6.57289981842041, -5.895100116729736, -7.265600204467773, -5.126399993896484, -4.625999927520752, -5.875, -6.486800193786621, -3.4007999897003174, -6.514699935913086, -7.152299880981445, -6.603400230407715, -5.652900218963623, -5.225200176239014, -3.459399938583374, -4.918900012969971, -5.565299987792969, -2.6940999031066895, -4.718599796295166, -4.581399917602539, -5.089399814605713, -4.547800064086914, -5.498199939727783, -3.8441998958587646, -5.803100109100342, -5.501699924468994, -5.246200084686279, -5.179500102996826, -5.283999919891357, -5.131100177764893, -3.90339994430542, -5.078199863433838, -5.032100200653076, -5.036399841308594, -5.23829984664917, -5.370299816131592, -5.38539981842041, -5.451600074768066, -5.518899917602539, -4.781499862670898, -5.21999979019165, -5.869999885559082, -5.905300140380859, -5.941800117492676, -5.968800067901611, -6.096799850463867, -6.18779993057251, -6.407400131225586, -6.821199893951416, -6.847099781036377, -6.887400150299072, -6.887400150299072, -6.943699836730957, -7.348800182342529, -6.258200168609619, -6.357900142669678, -7.539599895477295, -4.781499862670898, -5.352399826049805, -7.22629976272583, -6.078400135040283, -5.346499919891357, -6.973100185394287, -6.09060001373291, -6.860400199890137, -6.154300212860107, -3.84879994392395, -6.723599910736084, -7.245699882507324, -2.9839999675750732, -5.248799800872803, -4.094299793243408, -3.981100082397461, -5.428999900817871, -5.740200042724609, -4.718599796295166, -5.0625, -5.931300163269043, -4.388400077819824, -6.334000110626221, -4.521699905395508, -5.821700096130371, -5.133399963378906, -5.664899826049805, -4.223899841308594, -5.33489990234375, -4.611800193786621, -5.76669979095459, -4.773200035095215, -5.096199989318848, -5.075900077819824, -4.949900150299072, -5.13100004196167, -5.343599796295166, -5.491399765014648, -5.529399871826172, -5.633200168609619, -5.464600086212158, -5.539999961853027, -4.204699993133545, -4.3429999351501465, -4.601099967956543, -4.793399810791016, -4.826700210571289, -4.964099884033203, -5.287099838256836, -5.397500038146973, -5.5879998207092285, -5.663300037384033, -5.744699954986572, -5.814000129699707, -5.997200012207031, -6.087299823760986, -6.112800121307373, -6.207399845123291, -6.221700191497803, -6.411200046539307, -6.419899940490723, -6.5609002113342285, -6.6234002113342285, -6.725100040435791, -6.725100040435791, -6.812099933624268, -6.879199981689453, -6.951099872589111, -6.981400012969971, -7.028600215911865, -7.028600215911865, -7.078100204467773, -6.152299880981445, -6.550899982452393, -6.656199932098389, -3.3373000621795654, -4.007400035858154, -4.6981000900268555, -4.604000091552734, -4.9994001388549805, -3.5866000652313232, -5.422999858856201, -5.898799896240234, -6.179500102996826, -5.180099964141846, -4.626999855041504, -4.955900192260742, -5.3881001472473145, -4.4207000732421875, -5.232100009918213, -3.998800039291382, -5.426300048828125, -5.27869987487793, -4.800300121307373, -4.656700134277344, -4.841100215911865, -4.982600212097168, -5.125699996948242, -5.145100116729736, -5.32450008392334, -5.381800174713135, -5.3942999839782715, -3.483099937438965, -4.243500232696533, -4.263400077819824, -4.874899864196777, -5.570000171661377, -5.651800155639648, -5.656300067901611, -5.770699977874756, -6.075200080871582, -6.222499847412109, -6.238500118255615, -6.3765997886657715, -6.385900020599365, -6.433700084686279, -6.526000022888184, -6.6890997886657715, -6.883999824523926, -5.854899883270264, -6.288099765777588, -7.0152997970581055, -7.050899982452393, -7.050899982452393, -7.069300174713135, -7.0879998207092285, -7.166299819946289, -7.296800136566162, -4.960999965667725, -5.928999900817871, -6.915299892425537, -7.8790998458862305, -6.3765997886657715, -5.195400238037109, -4.440400123596191, -4.92140007019043, -3.7060999870300293, -3.687000036239624, -6.395299911499023, -4.179800033569336, -6.138800144195557, -3.978300094604492, -5.021200180053711, -4.085599899291992, -4.392199993133545, -5.399499893188477, -4.854499816894531, -4.860599994659424, -4.993000030517578, -4.202700138092041, -5.159200191497803, -5.184100151062012, -5.965099811553955, -5.389100074768066, -4.98829984664917, -5.385700225830078, -4.860599994659424, -5.253900051116943, -5.178500175476074, -5.275199890136719, -5.030799865722656, -5.1321001052856445, -5.355199813842773, -5.365300178527832], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.812000036239624, 2.8113999366760254, 2.810800075531006, 2.809299945831299, 2.808799982070923, 2.8087000846862793, 2.808500051498413, 2.808199882507324, 2.808000087738037, 2.8078999519348145, 2.8078999519348145, 2.8073999881744385, 2.8071999549865723, 2.8069000244140625, 2.8062000274658203, 2.8060998916625977, 2.805799961090088, 2.8055999279022217, 2.8053998947143555, 2.8052000999450684, 2.8046998977661133, 2.80430006980896, 2.8041000366210938, 2.8032000064849854, 2.802999973297119, 2.801100015640259, 2.8010001182556152, 2.7995998859405518, 2.7994000911712646, 2.7994000911712646, 2.780900001525879, 2.7939999103546143, 2.765500068664551, 2.7516000270843506, 2.7604000568389893, 2.7279999256134033, 2.687999963760376, 2.7346999645233154, 2.688999891281128, 2.6322999000549316, 2.606600046157837, 2.2932000160217285, 2.3550000190734863, 2.679500102996826, 2.707900047302246, 2.3505001068115234, 2.539400100708008, 2.0534000396728516, 2.4623000621795654, 2.631700038909912, 1.8833999633789062, 2.644200086593628, 1.743499994277954, 2.3893001079559326, 2.163100004196167, 1.5628999471664429, 1.5068000555038452, 2.1791999340057373, 1.7182999849319458, 1.7137999534606934, 1.210800051689148, 0.9391999840736389, 1.298200011253357, 2.8350000381469727, 2.8345999717712402, 2.8336000442504883, 2.8331000804901123, 2.832900047302246, 2.830899953842163, 2.830899953842163, 2.8306000232696533, 2.829900026321411, 2.829400062561035, 2.828900098800659, 2.8287999629974365, 2.8280999660491943, 2.827500104904175, 2.827399969100952, 2.8271000385284424, 2.8264999389648438, 2.8264000415802, 2.8264000415802, 2.8261001110076904, 2.8254001140594482, 2.8250999450683594, 2.82450008392334, 2.823199987411499, 2.821899890899658, 2.821899890899658, 2.8208999633789062, 2.8203999996185303, 2.8199000358581543, 2.81820011138916, 2.814199924468994, 2.7788000106811523, 2.6796998977661133, 2.743299961090088, 2.622299909591675, 2.6029999256134033, 2.656599998474121, 2.708699941635132, 2.668600082397461, 2.6240999698638916, 2.496799945831299, 2.637399911880493, 2.570199966430664, 2.311000108718872, 2.4158999919891357, 2.53410005569458, 2.3241000175476074, 2.1417999267578125, 2.317699909210205, 1.8077000379562378, 2.027899980545044, 1.9271999597549438, 1.4617999792099, 2.2985000610351562, 1.8693000078201294, 1.3014999628067017, 1.5205999612808228, 1.392199993133545, 1.9526000022888184, 1.4573999643325806, 2.033600091934204, 1.8138999938964844, 1.4549000263214111, 1.0918999910354614, 1.5928000211715698, 1.2374000549316406, 0.8046000003814697, 1.3981000185012817, 1.0068000555038452, 1.1187000274658203, 2.8492000102996826, 2.8445000648498535, 2.8405001163482666, 2.839099884033203, 2.836400032043457, 2.8352999687194824, 2.8315999507904053, 2.8289999961853027, 2.8276000022888184, 2.8264999389648438, 2.8259999752044678, 2.822000026702881, 2.821199893951416, 2.8125998973846436, 2.811199903488159, 2.811199903488159, 2.809000015258789, 2.8057000637054443, 2.8018999099731445, 2.778700113296509, 2.776599884033203, 2.7725000381469727, 2.7690000534057617, 2.7685000896453857, 2.7643001079559326, 2.7620999813079834, 2.7483999729156494, 2.72979998588562, 2.728600025177002, 2.723599910736084, 2.716599941253662, 2.684799909591675, 2.69569993019104, 2.4962000846862793, 2.576900005340576, 2.7232000827789307, 2.38700008392334, 2.2553000450134277, 2.5836000442504883, 2.585599899291992, 2.26990008354187, 2.115000009536743, 2.477099895477295, 1.8377000093460083, 2.4166998863220215, 2.339400053024292, 1.861299991607666, 2.2906999588012695, 1.5805000066757202, 1.8898999691009521, 2.351799964904785, 1.2792999744415283, 1.6434999704360962, 1.8841999769210815, 1.4733999967575073, 1.2640999555587769, 1.15339994430542, 0.8259999752044678, 1.270300030708313, 1.5706000328063965, 1.545300006866455, 1.5720000267028809, 1.0455000400543213, 0.9340000152587891, 2.8482000827789307, 2.8443000316619873, 2.8441998958587646, 2.842900037765503, 2.8422000408172607, 2.838599920272827, 2.836899995803833, 2.8364999294281006, 2.8324999809265137, 2.828900098800659, 2.8257999420166016, 2.8250999450683594, 2.822700023651123, 2.822700023651123, 2.8217999935150146, 2.8208000659942627, 2.8136000633239746, 2.812000036239624, 2.81030011177063, 2.8090999126434326, 2.808500051498413, 2.801300048828125, 2.787899971008301, 2.7843000888824463, 2.778599977493286, 2.7737998962402344, 2.7671000957489014, 2.763700008392334, 2.761199951171875, 2.7476000785827637, 2.7348999977111816, 2.721100091934204, 2.6893999576568604, 2.7318999767303467, 2.700200080871582, 2.6240999698638916, 2.5685999393463135, 2.58489990234375, 2.602299928665161, 2.6549999713897705, 2.3826000690460205, 2.4992001056671143, 2.455399990081787, 2.624000072479248, 2.462899923324585, 2.3101000785827637, 2.2330000400543213, 2.4769999980926514, 2.070499897003174, 2.2314000129699707, 2.3064000606536865, 2.150700092315674, 2.5476999282836914, 1.9572999477386475, 2.167799949645996, 2.3610000610351562, 2.0999999046325684, 2.3984999656677246, 2.3417999744415283, 2.2846999168395996, 2.3861000537872314, 2.2583000659942627, 1.5162999629974365, 1.7065000534057617, 1.5332000255584717, 1.0219000577926636, 1.2894999980926514, 2.1219000816345215, 1.288599967956543, 1.1598999500274658, 1.9399000406265259, 1.325700044631958, 0.6851000189781189, 1.567199945449829, 1.0390000343322754, 0.718999981880188, 0.6323999762535095, 2.8661999702453613, 2.8654000759124756, 2.865299940109253, 2.865000009536743, 2.8649001121520996, 2.864799976348877, 2.8647000789642334, 2.8645999431610107, 2.8636999130249023, 2.863600015640259, 2.8631999492645264, 2.8626999855041504, 2.862600088119507, 2.862600088119507, 2.8624000549316406, 2.8622000217437744, 2.8613998889923096, 2.861299991607666, 2.8610999584198, 2.8608999252319336, 2.859800100326538, 2.8596999645233154, 2.859600067138672, 2.8594000339508057, 2.859100103378296, 2.8589000701904297, 2.858799934387207, 2.858099937438965, 2.8577001094818115, 2.8575000762939453, 2.8559999465942383, 2.8410000801086426, 2.7948999404907227, 2.686800003051758, 2.715399980545044, 2.690200090408325, 2.701900005340576, 2.737499952316284, 2.6733999252319336, 2.5792999267578125, 2.691499948501587, 2.622499942779541, 2.734299898147583, 2.087899923324585, 1.8933000564575195, 2.251699924468994, 2.418100118637085, 1.4306000471115112, 1.555899977684021, 1.479099988937378, 1.8770999908447266, 1.2768000364303589, 1.1612000465393066, 1.2110999822616577, 0.6376000046730042, 1.2408000230789185, 1.812999963760376, 1.4290000200271606, 1.1299999952316284, 1.184399962425232, 1.1668000221252441, 1.426200032234192, 2.8938000202178955, 2.8935999870300293, 2.893199920654297, 2.892400026321411, 2.889899969100952, 2.888400077819824, 2.8879001140594482, 2.8873000144958496, 2.886899948120117, 2.884399890899658, 2.8842999935150146, 2.882999897003174, 2.878700017929077, 2.876300096511841, 2.875499963760376, 2.8726000785827637, 2.8719000816345215, 2.8685998916625977, 2.8685998916625977, 2.8661000728607178, 2.863100051879883, 2.863100051879883, 2.860599994659424, 2.8594000339508057, 2.856300115585327, 2.855299949645996, 2.85479998588562, 2.8397998809814453, 2.833899974822998, 2.8331000804901123, 2.83270001411438, 2.7298998832702637, 2.7356998920440674, 2.7941999435424805, 2.7651000022888184, 2.6800999641418457, 2.4537999629974365, 2.6791999340057373, 2.745800018310547, 2.659600019454956, 2.6698999404907227, 2.546999931335449, 2.6793999671936035, 2.154900074005127, 2.177000045776367, 2.3847999572753906, 2.3452000617980957, 2.52020001411438, 2.2395999431610107, 2.0469000339508057, 1.628499984741211, 2.1266000270843506, 2.0141000747680664, 2.152600049972534, 1.7259999513626099, 2.074899911880493, 2.266400098800659, 1.2843999862670898, 1.548200011253357, 1.3285000324249268, 1.2939000129699707, 1.1359000205993652, 1.2005000114440918, 1.5779000520706177, 1.0921000242233276, 1.1801999807357788, 1.6531000137329102, 2.9312000274658203, 2.930500030517578, 2.9302000999450684, 2.927000045776367, 2.925800085067749, 2.925299882888794, 2.924799919128418, 2.9244000911712646, 2.9231998920440674, 2.9198999404907227, 2.9154000282287598, 2.9100000858306885, 2.90939998626709, 2.9072999954223633, 2.905900001525879, 2.889899969100952, 2.8606998920440674, 2.844599962234497, 2.8399999141693115, 2.839099884033203, 2.835099935531616, 2.830399990081787, 2.8164000511169434, 2.8018999099731445, 2.791599988937378, 2.7797000408172607, 2.7769999504089355, 2.7685999870300293, 2.766400098800659, 2.765399932861328, 2.763700008392334, 2.734999895095825, 2.7425999641418457, 2.6621999740600586, 2.53410005569458, 2.4381000995635986, 2.7153000831604004, 2.6538000106811523, 2.485100030899048, 2.575700044631958, 2.45770001411438, 2.390399932861328, 2.502000093460083, 2.481300115585327, 2.426300048828125, 2.678499937057495, 2.381500005722046, 2.030400037765503, 2.561000108718872, 1.7364000082015991, 1.9823999404907227, 2.364300012588501, 1.8183000087738037, 2.1447999477386475, 1.8508000373840332, 1.9980000257492065, 1.805299997329712, 1.2451000213623047, 1.74399995803833, 1.5261000394821167, 1.5008000135421753, 1.4319000244140625, 1.1324000358581543, 1.1583000421524048, 0.9398999810218811, 0.7537999749183655, 1.3514000177383423, 1.1482000350952148, 2.9618000984191895, 2.959399938583374, 2.9591000080108643, 2.9583001136779785, 2.9581000804901123, 2.9581000804901123, 2.9570000171661377, 2.9565999507904053, 2.956199884414673, 2.9549999237060547, 2.9549999237060547, 2.9549999237060547, 2.953200101852417, 2.952899932861328, 2.952500104904175, 2.952399969100952, 2.952199935913086, 2.95169997215271, 2.9507999420166016, 2.950500011444092, 2.950500011444092, 2.950500011444092, 2.949899911880493, 2.949399948120117, 2.9489998817443848, 2.9484000205993652, 2.9484000205993652, 2.9484000205993652, 2.948199987411499, 2.947999954223633, 2.947999954223633, 2.947999954223633, 2.7578001022338867, 2.8533999919891357, 2.886899948120117, 2.675100088119507, 2.8138999938964844, 2.724100112915039, 2.736599922180176, 2.7806999683380127, 2.4388999938964844, 2.5998001098632812, 2.7360000610351562, 2.3024001121520996, 2.765899896621704, 2.3519999980926514, 2.5172998905181885, 2.3145999908447266, 2.454699993133545, 2.7639000415802, 2.1106998920440674, 2.382200002670288, 2.5487000942230225, 2.524399995803833, 1.9941999912261963, 2.0748000144958496, 1.4733999967575073, 2.281599998474121, 1.1234999895095825, 0.8744999766349792, 1.4938000440597534, 1.4139000177383423, 1.212399959564209, 1.763700008392334, 1.2390999794006348, 1.0465999841690063, 1.427899956703186, 1.2022000551223755, 0.7634000182151794, 2.966599941253662, 2.966099977493286, 2.96370005607605, 2.9623000621795654, 2.9621999263763428, 2.961699962615967, 2.9609999656677246, 2.959199905395508, 2.9584999084472656, 2.9576001167297363, 2.95740008354187, 2.9567999839782715, 2.956399917602539, 2.9532999992370605, 2.9521000385284424, 2.948499917984009, 2.9479000568389893, 2.94569993019104, 2.9309000968933105, 2.9093000888824463, 2.9077999591827393, 2.9040000438690186, 2.8942999839782715, 2.8868000507354736, 2.8856000900268555, 2.82069993019104, 2.810800075531006, 2.8015999794006348, 2.7913999557495117, 2.7904999256134033, 2.7736001014709473, 2.7190001010894775, 2.7685000896453857, 2.5188000202178955, 2.5773000717163086, 2.386699914932251, 2.5409998893737793, 2.59060001373291, 2.5206000804901123, 2.446899890899658, 2.080199956893921, 2.585200071334839, 2.0487000942230225, 2.469599962234497, 2.063199996948242, 2.218400001525879, 2.2962000370025635, 2.073899984359741, 1.642799973487854, 2.1384999752044678, 2.2502999305725098, 1.4115999937057495, 1.2824000120162964, 1.325600028038025, 1.6654000282287598, 1.3473999500274658, 1.4448000192642212, 1.38919997215271, 1.6354000568389893, 1.2127000093460083, 1.382099986076355, 1.4227999448776245, 0.16089999675750732, 1.1730999946594238, 1.1510000228881836, 1.1476000547409058, 0.8115000128746033, 2.9690001010894775, 2.9677000045776367, 2.9677000045776367, 2.967400074005127, 2.9665000438690186, 2.96589994430542, 2.9649999141693115, 2.9649999141693115, 2.9630000591278076, 2.9628000259399414, 2.9625000953674316, 2.961400032043457, 2.960700035095215, 2.960599899291992, 2.9602999687194824, 2.9602999687194824, 2.9591000080108643, 2.9588000774383545, 2.9586000442504883, 2.9576001167297363, 2.9570000171661377, 2.9551000595092773, 2.9547998905181885, 2.9539999961853027, 2.950900077819824, 2.950900077819824, 2.9472999572753906, 2.9428999423980713, 2.938499927520752, 2.929500102996826, 2.914299964904785, 2.8998000621795654, 2.918600082397461, 2.92549991607666, 2.8761000633239746, 2.8856000900268555, 2.746500015258789, 2.815700054168701, 2.861299991607666, 2.7590999603271484, 2.6814000606536865, 2.717600107192993, 2.7416999340057373, 2.816800117492676, 2.835700035095215, 2.250999927520752, 2.6714999675750732, 2.3666000366210938, 2.589400053024292, 2.206899881362915, 2.3564000129699707, 2.2781999111175537, 2.397200107574463, 2.098599910736084, 1.6456999778747559, 1.0432000160217285, 1.544700026512146, 1.4664000272750854, 2.032900094985962, 1.344499945640564, 1.8662999868392944, 1.4628000259399414, 1.0865999460220337, 1.1782000064849854, 0.9165999889373779, 0.6730999946594238, 3.016200065612793, 3.0160000324249268, 3.015399932861328, 3.0153000354766846, 3.0153000354766846, 3.0144999027252197, 3.0144999027252197, 3.01419997215271, 3.013400077819824, 3.013200044631958, 3.013000011444092, 3.012399911880493, 3.011899948120117, 3.0118000507354736, 3.0116000175476074, 3.0114998817443848, 3.010699987411499, 3.0092999935150146, 3.008699893951416, 3.008699893951416, 3.007699966430664, 3.0072999000549316, 3.006200075149536, 3.0058999061584473, 3.005000114440918, 3.004300117492676, 3.004300117492676, 3.003999948501587, 3.0034000873565674, 3.003000020980835, 3.0016000270843506, 2.966099977493286, 2.9591000080108643, 2.927000045776367, 2.949199914932251, 2.892400026321411, 2.7414000034332275, 2.9114999771118164, 2.1898999214172363, 2.7588999271392822, 2.476900100708008, 2.797600030899048, 2.3368000984191895, 2.2325000762939453, 1.7618000507354736, 2.287600040435791, 1.7127000093460083, 2.5111000537872314, 2.009399890899658, 2.2592999935150146, 1.2929999828338623, 1.2848999500274658, 2.040299892425537, 2.0878000259399414, 1.4667999744415283, 1.875, 1.1263999938964844, 1.547700047492981, 2.0887999534606934, 1.5829999446868896, 1.5812000036239624, 3.0227999687194824, 3.02239990234375, 3.0211000442504883, 3.018699884414673, 3.018399953842163, 3.0164999961853027, 3.016400098800659, 3.0160000324249268, 3.0155999660491943, 3.015399932861328, 3.014699935913086, 3.0137999057769775, 3.012700080871582, 3.0099000930786133, 3.0095999240875244, 3.0095999240875244, 3.0078999996185303, 3.0069000720977783, 3.0018999576568604, 3.0004000663757324, 2.998699903488159, 2.9981000423431396, 2.9967000484466553, 2.996000051498413, 2.9953999519348145, 2.9951999187469482, 2.9951999187469482, 2.991499900817871, 2.9863998889923096, 2.970599889755249, 2.9489998817443848, 2.921099901199341, 2.940500020980835, 2.911799907684326, 2.903899908065796, 2.9553000926971436, 2.7086000442504883, 2.7597999572753906, 2.796999931335449, 2.835700035095215, 2.793299913406372, 2.616300106048584, 2.8136000633239746, 2.578399896621704, 2.0729000568389893, 2.661600112915039, 2.6558001041412354, 2.141900062561035, 1.9157999753952026, 2.1914000511169434, 2.4588000774383545, 2.6459999084472656, 2.0624001026153564, 1.6430000066757202, 1.5985000133514404, 1.608299970626831, 2.0734000205993652, 2.3429999351501465, 1.1916999816894531, 1.4881999492645264, 2.3243000507354736, 0.9038000106811523, 0.9837999939918518, 0.9176999926567078, 0.5659000277519226, 3.0313000679016113, 3.0304999351501465, 3.0304999351501465, 3.0302999019622803, 3.0299999713897705, 3.0297999382019043, 3.029400110244751, 3.0292000770568848, 3.0290000438690186, 3.0260000228881836, 3.0260000228881836, 3.0257999897003174, 3.024899959564209, 3.0237998962402344, 3.0236001014709473, 3.0232999324798584, 3.022599935531616, 3.0215001106262207, 3.021399974822998, 3.0199999809265137, 3.0197999477386475, 3.0195999145507812, 3.0183000564575195, 3.017899990081787, 3.017400026321411, 3.016200065612793, 3.0139999389648438, 3.0136001110076904, 3.012399911880493, 3.0090999603271484, 2.9993999004364014, 2.9886999130249023, 2.9686999320983887, 2.979300022125244, 2.9556000232696533, 2.9730000495910645, 2.9588000774383545, 2.6535000801086426, 2.9131999015808105, 2.605799913406372, 2.6379001140594482, 2.5062999725341797, 2.8157999515533447, 2.7820000648498535, 2.555799961090088, 2.758699893951416, 2.4184999465942383, 2.1185998916625977, 2.654099941253662, 2.390700101852417, 2.1029999256134033, 2.0957999229431152, 1.694700002670288, 2.0957999229431152, 2.554500102996826, 1.7201999425888062, 2.222899913787842, 1.8720999956130981, 2.2567999362945557, 1.3729000091552734, 1.2860000133514404, 2.1582000255584717, 2.1085000038146973, 1.082200050354004, 0.2939000129699707, 3.059000015258789, 3.0585999488830566, 3.0566999912261963, 3.0566000938415527, 3.054800033569336, 3.052799940109253, 3.051800012588501, 3.050600051879883, 3.05049991607666, 3.0501999855041504, 3.0494000911712646, 3.044600009918213, 3.039599895477295, 3.039299964904785, 3.037400007247925, 3.035799980163574, 3.035099983215332, 3.0195999145507812, 3.003700017929077, 2.9811999797821045, 2.9760000705718994, 2.9753000736236572, 2.9690001010894775, 2.9479000568389893, 2.945499897003174, 2.936500072479248, 2.924999952316284, 2.924499988555908, 2.8798000812530518, 2.8778998851776123, 2.8533999919891357, 2.77020001411438, 2.7978999614715576, 2.7625999450683594, 2.6861000061035156, 2.6556999683380127, 2.5088999271392822, 2.7030999660491943, 2.6770999431610107, 2.6459999084472656, 2.1059000492095947, 2.2792000770568848, 2.6477999687194824, 1.8579000234603882, 2.526700019836426, 2.251300096511841, 2.227400064468384, 2.0653998851776123, 1.5336999893188477, 1.5720000267028809, 1.7904000282287598, 1.8436000347137451, 1.3106000423431396, 1.5033999681472778, 1.1299999952316284, 1.7175999879837036, 1.785099983215332, 1.2252000570297241, 1.6815999746322632, 1.1103999614715576, 1.4630000591278076, 0.2996000051498413, 3.100600004196167, 3.099400043487549, 3.0989999771118164, 3.098599910736084, 3.098400115966797, 3.0977001190185547, 3.0973000526428223, 3.09689998626709, 3.095599889755249, 3.0952000617980957, 3.094599962234497, 3.094399929046631, 3.093899965286255, 3.093899965286255, 3.0936999320983887, 3.093600034713745, 3.093400001525879, 3.0933001041412354, 3.0929999351501465, 3.0927999019622803, 3.092099905014038, 3.0917000770568848, 3.0915000438690186, 3.0915000438690186, 3.091399908065796, 3.0913000106811523, 3.09089994430542, 3.0903000831604004, 3.0892999172210693, 3.0885000228881836, 3.0734000205993652, 3.0845000743865967, 3.030100107192993, 3.0272998809814453, 2.9089999198913574, 2.9848999977111816, 3.0434000492095947, 3.0459001064300537, 2.616300106048584, 2.6291000843048096, 2.4358999729156494, 2.8313000202178955, 2.5064001083374023, 2.2035000324249268, 2.536600112915039, 2.615000009536743, 2.2939000129699707, 2.161099910736084, 2.303100109100342, 1.1964000463485718, 2.161099910736084, 1.8015999794006348, 1.2903000116348267, 1.2723000049591064, 1.0496000051498413, 1.2733999490737915, 1.4213000535964966, 1.375599980354309, 3.1414999961853027, 3.138000011444092, 3.1375999450683594, 3.137500047683716, 3.135499954223633, 3.134500026702881, 3.1338000297546387, 3.1338000297546387, 3.13319993019104, 3.1312999725341797, 3.128700017929077, 3.1284000873565674, 3.126199960708618, 3.1238999366760254, 3.1224000453948975, 3.1191999912261963, 3.1177000999450684, 3.107800006866455, 3.0922999382019043, 3.0820999145507812, 3.0697999000549316, 3.06469988822937, 3.049299955368042, 3.0362000465393066, 3.0225000381469727, 3.0179998874664307, 3.016400098800659, 3.0107998847961426, 3.0058999061584473, 2.974400043487549, 2.875, 2.876499891281128, 2.966399908065796, 2.94320011138916, 2.8712000846862793, 2.6872000694274902, 2.8873000144958496, 2.555799961090088, 2.531100034713745, 2.9365999698638916, 2.676100015640259, 2.7776999473571777, 2.680799961090088, 2.7348999977111816, 2.6823999881744385, 2.7298998832702637, 2.555799961090088, 2.3833999633789062, 1.8970999717712402, 2.4830000400543213, 1.9112000465393066, 2.0773000717163086, 2.509700059890747, 1.4378999471664429, 1.7675000429153442, 1.2670999765396118, 1.8720999956130981, 1.4048000574111938, 1.784999966621399, 1.1582000255584717, 1.2616000175476074, 1.0621000528335571, 0.9605000019073486, 1.1796000003814697, 1.3071999549865723, 0.7379999756813049, 1.1705000400543213, 1.2264000177383423, 3.148099899291992, 3.1465001106262207, 3.1456000804901123, 3.144399881362915, 3.140199899673462, 3.1401000022888184, 3.1379001140594482, 3.137700080871582, 3.137399911880493, 3.1308000087738037, 3.130000114440918, 3.125499963760376, 3.1254000663757324, 3.1201999187469482, 3.1201999187469482, 3.115999937057495, 3.1129000186920166, 3.1110000610351562, 3.108599901199341, 3.108599901199341, 3.1036999225616455, 3.1006999015808105, 3.093899965286255, 3.0901999473571777, 3.059299945831299, 3.0539000034332275, 3.029099941253662, 3.024399995803833, 3.0223000049591064, 3.0216000080108643, 3.0143001079559326, 2.9326999187469482, 2.89739990234375, 2.7513999938964844, 2.7885000705718994, 2.865299940109253, 2.5102999210357666, 2.7248001098632812, 2.7021000385284424, 2.7785000801086426, 2.6682000160217285, 2.7701001167297363, 2.342099905014038, 2.8203001022338867, 2.7114999294281006, 2.57669997215271, 2.469399929046631, 2.4892001152038574, 2.3452999591827393, 1.6054999828338623, 1.996899962425232, 1.1205999851226807, 0.9638000130653381, 1.3284000158309937, 1.5643999576568604, 1.1399999856948853, 1.454699993133545, 1.2587000131607056, 3.160900115966797, 3.160099983215332, 3.157900094985962, 3.1577000617980957, 3.1575000286102295, 3.157399892807007, 3.1566998958587646, 3.1561999320983887, 3.154599905014038, 3.150599956512451, 3.1503000259399414, 3.1498000621795654, 3.1498000621795654, 3.1491000652313232, 3.1424999237060547, 3.141700029373169, 3.1396000385284424, 3.138400077819824, 3.1280999183654785, 3.110100030899048, 3.108799934387207, 3.0941998958587646, 3.078900098800659, 3.0761001110076904, 3.0701000690460205, 3.061800003051758, 3.053800106048584, 3.0485999584198, 3.0399999618530273, 3.0392000675201416, 2.972599983215332, 3.005000114440918, 2.933000087738037, 2.87280011177063, 2.9697000980377197, 2.9939000606536865, 2.8545000553131104, 2.7625999450683594, 2.931299924850464, 2.5397000312805176, 3.0060999393463135, 2.410799980163574, 2.820199966430664, 2.5202999114990234, 2.7362000942230225, 1.8425999879837036, 2.3994998931884766, 1.4958000183105469, 2.6865999698638916, 1.3597999811172485, 1.750100016593933, 1.7077000141143799, 1.3978999853134155, 1.5936000347137451, 1.7834999561309814, 1.9850000143051147, 1.6845999956130981, 1.8896000385284424, 0.8176000118255615, 0.7322999835014343, 3.180500030517578, 3.1803998947143555, 3.1800999641418457, 3.179800033569336, 3.179800033569336, 3.179500102996826, 3.178800106048584, 3.178499937057495, 3.1779000759124756, 3.1775999069213867, 3.177299976348877, 3.177000045776367, 3.176100015640259, 3.175600051879883, 3.17549991607666, 3.1749000549316406, 3.174799919128418, 3.1733999252319336, 3.1733999252319336, 3.172100067138672, 3.1714999675750732, 3.1705000400543213, 3.1705000400543213, 3.1695001125335693, 3.1686999797821045, 3.1677000522613525, 3.16729998588562, 3.166599988937378, 3.166599988937378, 3.1658999919891357, 3.162600040435791, 3.162400007247925, 3.160900115966797, 3.089099884033203, 3.062700033187866, 3.038800001144409, 3.001499891281128, 3.0311999320983887, 2.9056999683380127, 3.021699905395508, 3.0720999240875244, 3.1075000762939453, 2.9300999641418457, 2.6744000911712646, 2.7520999908447266, 2.871000051498413, 2.4855000972747803, 2.3773999214172363, 1.273300051689148, 2.4916000366210938, 2.245300054550171, 1.5815999507904053, 1.0734000205993652, 1.3696999549865723, 1.246000051498413, 1.0981999635696411, 1.0678999423980713, 0.6582000255584717, -0.17739999294281006, 0.12389999628067017, 3.2588000297546387, 3.25819993019104, 3.25819993019104, 3.2574000358581543, 3.255500078201294, 3.255199909210205, 3.255199909210205, 3.254699945449829, 3.253000020980835, 3.2520999908447266, 3.252000093460083, 3.2509000301361084, 3.2507998943328857, 3.2504000663757324, 3.249500036239624, 3.24780011177063, 3.2453999519348145, 3.24399995803833, 3.243799924850464, 3.243499994277954, 3.2428998947143555, 3.2428998947143555, 3.2425999641418457, 3.242300033569336, 3.2409000396728516, 3.2383999824523926, 3.2342000007629395, 3.232100009918213, 3.2307000160217285, 3.2223000526428223, 3.2147998809814453, 3.1888999938964844, 3.11080002784729, 3.0532000064849854, 2.9017999172210693, 2.8482000827789307, 3.1486001014709473, 2.8108999729156494, 3.1059999465942383, 2.7388999462127686, 2.8917999267578125, 2.7105000019073486, 2.734600067138672, 2.9502999782562256, 2.754499912261963, 2.6570000648498535, 2.615299940109253, 2.2316999435424805, 2.674499988555908, 2.5002999305725098, 3.0485000610351562, 2.5685999393463135, 2.2256999015808105, 2.5569000244140625, 1.6746000051498413, 2.2274999618530273, 1.909999966621399, 2.151900053024292, 1.1653000116348267, 1.1500999927520752, 1.7741999626159668, 0.5716000199317932]}, \"token.table\": {\"Topic\": [2, 12, 13, 16, 20, 2, 12, 20, 11, 20, 2, 6, 7, 10, 12, 15, 18, 19, 20, 18, 19, 18, 19, 7, 15, 20, 11, 16, 11, 5, 2, 3, 5, 7, 8, 17, 3, 5, 7, 8, 10, 12, 15, 16, 17, 3, 5, 7, 8, 17, 1, 10, 13, 16, 18, 16, 18, 19, 3, 4, 10, 11, 13, 16, 18, 1, 14, 16, 12, 4, 12, 11, 16, 1, 2, 4, 5, 6, 7, 11, 12, 17, 4, 13, 16, 3, 12, 16, 18, 5, 15, 5, 17, 18, 15, 17, 15, 13, 7, 8, 12, 13, 18, 20, 5, 8, 16, 9, 2, 7, 9, 1, 2, 4, 6, 10, 11, 12, 13, 14, 15, 16, 17, 19, 1, 2, 4, 9, 10, 11, 2, 3, 4, 9, 11, 13, 14, 16, 6, 10, 11, 14, 3, 14, 16, 3, 8, 10, 11, 15, 16, 17, 19, 20, 5, 6, 7, 8, 11, 12, 14, 15, 16, 20, 20, 20, 8, 12, 15, 18, 20, 18, 5, 6, 8, 10, 13, 14, 16, 17, 19, 20, 7, 12, 13, 14, 5, 7, 15, 16, 15, 17, 1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 20, 2, 4, 9, 8, 17, 18, 19, 18, 18, 19, 17, 17, 20, 1, 3, 5, 6, 9, 10, 11, 18, 19, 5, 3, 3, 16, 19, 11, 3, 10, 3, 4, 15, 6, 8, 10, 12, 18, 20, 18, 8, 13, 18, 1, 8, 11, 13, 11, 16, 20, 11, 17, 4, 17, 7, 8, 10, 11, 13, 16, 10, 13, 1, 3, 7, 20, 5, 16, 2, 5, 7, 9, 11, 15, 16, 17, 1, 13, 11, 11, 11, 1, 3, 7, 9, 10, 15, 10, 17, 5, 12, 14, 8, 1, 5, 8, 15, 16, 17, 20, 15, 16, 18, 2, 3, 13, 14, 15, 13, 13, 13, 12, 13, 14, 15, 17, 20, 2, 7, 14, 2, 6, 6, 10, 1, 2, 2, 11, 14, 3, 9, 5, 14, 5, 14, 9, 7, 9, 19, 20, 6, 12, 13, 6, 19, 5, 19, 15, 2, 5, 2, 4, 6, 8, 11, 14, 16, 17, 19, 20, 15, 17, 18, 2, 5, 6, 16, 17, 2, 3, 7, 3, 6, 7, 9, 19, 5, 8, 15, 3, 6, 8, 10, 12, 13, 16, 18, 19, 20, 8, 16, 5, 16, 3, 4, 5, 6, 8, 9, 11, 12, 15, 16, 17, 18, 20, 5, 15, 18, 17, 19, 15, 15, 5, 15, 3, 1, 2, 6, 19, 1, 6, 6, 6, 6, 2, 6, 19, 6, 19, 3, 6, 19, 3, 6, 19, 6, 19, 5, 6, 13, 15, 13, 13, 12, 16, 15, 2, 13, 1, 4, 18, 18, 5, 13, 14, 15, 18, 1, 5, 7, 13, 14, 17, 18, 1, 2, 4, 5, 10, 13, 19, 7, 12, 3, 16, 17, 8, 11, 12, 4, 8, 13, 14, 15, 17, 14, 16, 20, 6, 9, 1, 3, 4, 6, 7, 8, 9, 10, 13, 14, 15, 18, 19, 8, 10, 17, 1, 2, 3, 6, 9, 10, 13, 14, 15, 1, 4, 5, 7, 11, 13, 16, 18, 20, 13, 2, 4, 5, 8, 10, 11, 12, 13, 14, 15, 17, 18, 20, 2, 6, 7, 11, 12, 14, 15, 16, 18, 19, 12, 4, 2, 7, 18, 2, 4, 5, 7, 8, 11, 16, 18, 1, 15, 1, 1, 1, 3, 5, 15, 8, 20, 6, 10, 16, 3, 5, 8, 14, 15, 18, 3, 7, 11, 14, 19, 19, 2, 4, 5, 10, 12, 15, 16, 18, 20, 4, 7, 12, 14, 20, 7, 10, 15, 17, 1, 2, 7, 8, 11, 17, 19, 19, 20, 17, 1, 2, 3, 4, 9, 11, 15, 16, 17, 19, 19, 8, 12, 8, 1, 2, 3, 5, 8, 9, 10, 12, 13, 14, 20, 11, 15, 16, 20, 4, 9, 11, 15, 16, 11, 16, 4, 2, 3, 4, 11, 17, 4, 11, 4, 5, 1, 2, 4, 11, 15, 12, 13, 20, 2, 9, 13, 14, 18, 20, 8, 9, 13, 18, 4, 7, 12, 6, 5, 8, 5, 8, 4, 6, 10, 11, 13, 14, 15, 4, 6, 9, 10, 16, 1, 5, 11, 5, 7, 9, 14, 16, 6, 10, 19, 14, 15, 7, 8, 1, 3, 5, 7, 10, 11, 14, 15, 17, 19, 20, 5, 9, 11, 12, 14, 15, 16, 19, 2, 6, 12, 19, 1, 6, 3, 7, 10, 18, 6, 11, 14, 18, 19, 6, 3, 14, 15, 17, 13, 14, 18, 19, 7, 10, 11, 13, 3, 5, 6, 8, 9, 14, 15, 16, 20, 1, 2, 4, 9, 11, 12, 13, 17, 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 13, 14, 17, 18, 19, 20, 2, 9, 13, 17, 12, 5, 16, 17, 18, 17, 1, 5, 5, 5, 1, 5, 8, 9, 13, 15, 1, 5, 9, 1, 3, 4, 10, 18, 20, 16, 4, 3, 4, 6, 10, 14, 15, 16, 19, 2, 3, 4, 6, 11, 14, 15, 16, 17, 20, 20, 12, 18, 19, 6, 7, 8, 10, 14, 15, 19, 8, 15, 16, 6, 2, 19, 4, 5, 15, 10, 1, 2, 8, 9, 10, 11, 12, 13, 14, 13, 1, 2, 4, 9, 10, 12, 13, 20, 6, 13, 19, 20, 4, 6, 8, 11, 13, 14, 20, 6, 17, 4, 8, 12, 20, 11, 11, 6, 8, 11, 12, 13, 15, 19, 20, 13, 1, 2, 5, 8, 9, 13, 18, 7, 2, 6, 7, 9, 10, 11, 12, 14, 18, 7, 13, 6, 9, 13, 15, 2, 4, 5, 4, 5, 9, 11, 15, 16, 17, 20, 11, 19, 4, 17, 18, 12, 15, 8, 11, 12, 14, 15, 18, 3, 5, 7, 8, 9, 10, 11, 15, 16, 18, 19, 4, 9, 13, 4, 13, 7, 13, 15, 7, 15, 2, 3, 7, 8, 12, 13, 14, 15, 16, 17, 1, 1, 4, 8, 12, 16, 18, 19, 20, 6, 8, 9, 1, 2, 7, 9, 13, 7, 11, 15, 16, 20, 3, 11, 18, 5, 4, 9, 13, 20, 4, 1, 4, 5, 8, 9, 11, 15, 20, 4, 14, 1, 6, 19, 10, 1, 2, 10, 11, 18, 19, 20, 1, 10, 11, 18, 19, 20, 1, 4, 10, 18, 19, 1, 10, 1, 2, 4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 16, 17, 19, 20, 13, 14, 4, 14, 1, 6, 10, 11, 15, 19, 2, 3, 5, 11, 17, 18, 1, 5, 7, 8, 20, 5, 8, 5, 6, 7, 14, 2, 4, 7, 8, 13, 14, 16, 1, 1, 2, 3, 6, 7, 9, 10, 11, 14, 15, 16, 18, 20, 1, 5, 6, 7, 8, 10, 11, 12, 13, 14, 16, 18, 19, 20, 7, 10, 3, 11, 20, 9, 1, 2, 4, 5, 7, 9, 13, 8, 12, 15, 20, 20, 15, 7, 12, 12, 12, 1, 3, 3, 11, 16, 6, 1, 2, 7, 10, 2, 4, 3, 16, 15, 16, 6, 7, 8, 12, 13, 19, 3, 8, 16, 19, 5, 15, 16, 2, 3, 5, 12, 18, 20, 8, 9, 12, 14, 20, 5, 8, 9, 12, 13, 14, 15, 16, 18, 20, 16, 12, 18, 12, 18, 10, 1, 2, 3, 4, 6, 9, 11, 12, 13, 14, 16, 20, 1, 2, 4, 9, 11, 17, 11, 17, 5, 7, 5, 5, 1, 6, 8, 10, 12, 13, 15, 16, 14, 1, 2, 3, 5, 9, 10, 11, 14, 15, 16, 17, 18, 19, 20, 4, 17, 16, 15, 6, 17, 2, 3, 19, 11, 16, 10, 19, 1, 4, 8, 10, 11, 13, 14, 17, 4, 15, 16, 17, 19, 20, 17, 8, 12, 19, 16, 2, 5, 18, 19, 18, 1, 7, 12, 16, 4, 11, 17, 11, 4, 12, 5, 12, 14, 16, 8, 9, 15, 16, 18, 11, 9, 6, 14, 10, 1, 8, 9, 10, 12, 13, 18, 1, 1, 8, 12, 20, 17, 2, 3, 6, 7, 9, 11, 9, 1, 14, 14, 1, 3, 17, 10, 4, 8, 10, 13, 14, 15, 17, 20, 1, 3, 6, 7, 10, 11, 13, 16, 20, 4, 3, 4, 11, 12, 3, 4, 19, 7, 17, 19, 17, 11, 14, 1, 8, 12, 3, 5, 6, 7, 10, 12, 13, 20, 8, 12, 14, 19, 14, 20, 2, 3, 6, 7, 9, 10, 11, 12, 19, 20, 19, 19, 14, 15, 15, 2, 6, 18, 20, 18, 1, 3, 11, 15, 17, 19, 2, 3, 19, 1, 19, 3, 1, 12, 14, 5, 18, 15, 14, 17, 7, 17, 3, 7, 10, 1, 3, 5, 5, 19, 1, 19, 1, 2, 4, 9, 11, 15, 17, 7, 14, 4, 5, 14, 15, 17, 15, 8, 7, 8, 12, 20, 10, 15, 19, 1, 2, 5, 6, 7, 8, 19, 20, 5, 18, 6, 13, 12, 20, 12, 12, 12, 1, 3, 4, 5, 6, 10, 11, 12, 14, 15, 16, 3, 10, 12, 14, 15, 16, 17, 18, 19, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 16, 17, 19, 20, 2, 3, 5, 6, 9, 11, 14, 15, 16, 18, 19, 20, 13, 2, 4, 2, 1, 7, 17, 13, 1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 17, 19, 20, 5, 8, 5, 8, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 17, 19, 20, 5, 16, 18, 2, 6, 7, 11, 12, 14, 19, 3, 17, 2, 3, 11, 15, 5, 12, 15, 18, 3, 7, 20, 5, 6, 12, 13, 9, 14, 7, 4, 1, 2, 3, 4, 11, 12, 13, 17, 19, 10, 12, 16, 4, 20, 2, 4, 9, 11, 9, 3, 1, 20, 20, 1, 4, 7, 8, 14, 15, 10, 20, 2, 16, 6, 7, 8, 10, 1, 20, 1, 6, 7, 10, 11, 18, 19, 13, 4, 4, 4, 7, 17, 19, 19, 1, 8, 17, 3, 5, 8, 15, 18, 19, 3, 9, 15, 3, 12, 19, 14, 2, 3, 6, 7, 11, 13, 17, 3, 7, 11, 12, 13, 17, 10, 13, 16, 2, 1, 2, 5, 9, 11, 13, 14, 15, 17, 18, 19, 8, 16, 12, 17, 19, 1, 5, 7, 9, 10, 12, 14, 15, 16, 18, 19, 14, 17, 8, 5, 7, 12, 20, 12, 1, 19, 2, 4, 9, 14, 15, 17, 18, 5, 17, 20, 4, 6, 8, 9, 11, 12, 13, 14, 15, 18, 19, 20, 12, 2, 4, 6, 8, 9, 10, 12, 13, 15, 18, 6, 11, 10, 7, 19, 7, 2, 11, 14, 16, 19, 4, 8, 11, 3, 11, 15, 16, 4, 5, 8, 10, 11, 12, 13, 14, 17, 19, 3, 4, 14, 10, 8, 11, 12, 19, 20, 1, 2, 10, 15, 18, 19, 1, 18, 1, 4, 7, 14, 15, 16, 17, 1, 2, 6, 11, 18, 5, 18, 15, 18, 15, 1, 2, 11, 2, 2, 6, 9, 11, 14, 15, 17, 1, 10, 9, 17, 19, 19, 13, 8, 20, 1, 2, 6, 8, 11, 12, 13, 17, 3, 7, 10, 11, 13, 14, 16, 17, 20, 1, 10, 6, 1, 11, 1, 12, 16, 2, 7, 8, 12, 14, 18, 7, 12, 14, 15, 16, 18, 13, 4, 13, 1, 1, 10, 3, 3, 11, 11, 15, 1, 9, 10, 13, 2, 4, 7, 8, 9, 17, 1, 2, 6, 7, 9, 10, 12, 13, 14, 18, 20, 5, 6, 8, 9, 10, 13, 15, 18, 20, 5, 7, 8, 16, 20, 17, 4, 6, 7, 10, 11, 14, 5, 5, 11, 15, 17, 7, 14, 15, 12, 14, 14, 18, 19, 14, 5, 9, 4, 5, 10, 13, 3, 4, 6, 9, 10, 13, 15, 17, 20, 6, 6, 1, 6, 12, 17, 2, 7, 13, 1, 13, 18, 1, 1, 10, 1, 10, 19, 1, 5, 7, 10, 16, 19, 12, 15, 17, 19, 20, 14, 17, 8, 15, 20, 8, 10, 20, 5, 16, 20, 20, 20, 7, 12, 16, 3, 11, 14, 20, 20, 8, 15, 18, 20, 20, 3, 10, 19, 7, 20, 1, 15, 5, 18, 20, 14, 1, 2, 3, 13, 18, 16, 5, 11, 20, 1, 5, 6, 12, 13, 15, 8, 11, 12, 14, 17, 5, 3, 17, 19, 2, 3, 4, 5, 9, 15, 17, 3, 1, 3, 4, 5, 8, 9, 10, 13, 15, 16, 17, 18, 19, 2, 3, 6, 15, 17, 19, 17, 15, 4, 5, 15, 5, 8, 4, 10, 4, 14, 1, 9, 10, 12, 15, 18, 9, 10, 18, 9, 13, 4, 5, 10, 13, 15, 16, 10, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 15, 18, 9, 11, 12, 7, 12, 12, 17, 1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 18, 19, 3, 12, 8, 16, 13, 5, 5, 9, 18, 10, 12, 13, 14, 15, 16, 17, 20, 2, 4, 6, 12, 13, 9, 20, 8, 12, 15, 18, 2, 4, 6, 9, 10, 11, 13, 18, 9, 11, 3, 4, 6, 10, 12, 14, 4, 14, 6, 16, 8, 12, 8, 8, 8, 8, 2, 8, 13, 13, 5, 17, 5, 10, 12, 3, 5, 9, 17, 18, 19, 2, 11, 5, 7, 19, 20, 2, 3, 4, 7, 8, 14, 15, 16, 20, 1, 14, 8, 2, 3, 1, 2, 3, 5, 7, 8, 10, 11, 12, 14, 15, 16, 18, 19, 20, 11, 1, 2, 9, 10, 13, 14, 11, 14, 15, 19, 20, 6, 7, 8, 3, 5, 6, 7, 8, 17, 18, 20, 13, 7, 1, 3, 5, 8, 9, 15, 18, 1, 12, 18, 4, 20, 4, 5, 9, 11, 12, 13, 16, 17, 18, 9, 13, 17, 19, 19, 19, 19, 7, 19, 8, 9, 15, 9, 5, 12, 12, 18, 11, 20, 16, 14, 11, 2, 10, 2, 5, 6, 8, 20, 1, 2, 11, 12, 14, 7, 8, 11, 12, 15, 19, 20, 20, 2, 3, 4, 5, 7, 8, 9, 12, 15, 20, 4, 5, 1, 8, 1, 1, 19, 5, 20, 8, 9, 18, 7, 8, 9, 10, 17, 10, 3, 16, 5, 5, 10, 8, 13, 1, 7, 1, 2, 7, 1, 2, 5, 6, 7, 11, 14, 17, 19, 1, 2, 9, 14, 7, 17, 1, 3, 6, 9, 10, 14, 19, 3, 5, 7, 8, 12, 14, 15, 18, 19, 11, 15, 19, 3, 5, 6, 7, 10, 11, 12, 13, 14, 16, 18, 19, 20, 8, 10, 12, 13, 16, 13, 2, 2, 2, 4, 2, 4, 5, 7, 8, 9, 12, 13, 16, 18, 20, 2, 7, 15, 5, 7, 17, 13, 15, 2, 4, 11, 19, 8, 13, 20, 6, 9, 11, 14, 7, 8, 7, 7, 8, 7, 8, 5, 15, 18, 12, 5, 2, 4, 10, 7, 2, 7, 10, 6, 2, 16, 10, 19, 1, 2, 3, 5, 6, 8, 9, 11, 12, 14, 17, 18, 2, 3, 5, 9, 11, 12, 13, 15, 16, 17, 18, 20, 6, 7, 5, 4, 5, 6, 7, 9, 11, 18, 19, 20, 2, 10, 19, 10, 1, 2, 4, 5, 6, 9, 11, 12, 16, 17, 18, 2, 7, 17, 7, 17, 13, 18, 8, 5, 12, 19, 19, 13, 5, 18, 17, 5, 17, 16, 20, 5, 5, 8, 9, 15, 2, 8, 10, 11, 12, 14, 16, 17, 10, 6, 10, 12, 10, 10, 11, 16, 11, 2, 5, 6, 7, 9, 10, 11, 13, 18, 20, 12, 9, 3, 7, 8, 12, 13, 14, 18, 19, 7, 17, 2, 3, 4, 6, 11, 12, 13, 14, 15, 16, 17, 19, 20, 15, 20, 15, 5, 8, 16, 18, 5, 8, 12, 20, 8, 20, 8, 7, 11, 17, 11, 2, 8, 10, 10, 4, 9, 11, 15, 18, 2, 4, 5, 9, 11, 13, 14, 2, 3, 4, 6, 12, 15, 16, 11, 20, 20, 5, 7, 8, 9, 16, 14, 12, 20, 12, 20, 11, 4, 5, 7, 9, 13, 17, 3, 7, 3, 4, 18, 3, 7, 3, 1, 3, 6, 10, 11, 14, 15, 16, 20, 9, 20, 20, 20, 1, 2, 6, 10, 18, 19, 2, 6, 10, 13, 1, 4, 5, 9, 11, 12, 15, 17, 18, 12, 4, 11, 11, 15, 3, 6, 7, 13, 19, 17, 6, 11, 12, 14, 19, 4, 5, 13, 17, 18, 5, 12, 14, 18, 19, 12, 19, 8, 3, 6, 8, 10, 11, 14, 20, 5, 7, 8, 17, 8, 13, 14, 7, 16, 11, 15, 16, 18, 20, 2, 11, 12, 17, 19, 5, 6, 10, 14, 16, 17, 12, 1, 4, 5, 8, 9, 10, 12, 18, 6, 7, 6, 2, 3, 4, 5, 15, 18, 19, 5, 15, 16, 18, 20, 20, 15, 16, 15, 7, 13, 3, 4, 5, 8, 9, 11, 15, 16, 17, 10, 18, 11, 13, 2, 3, 6, 8, 9, 12, 14, 15, 19, 11, 20, 16, 6, 7, 10, 1, 10, 12, 15, 18, 19, 18, 9, 3, 4, 6, 9, 11, 13, 14, 15, 2, 4, 6, 9, 11, 12, 15, 5, 18, 2, 18, 19, 18, 13, 15, 18, 13, 5, 8, 12, 18, 20, 8, 19, 19, 19, 2, 4, 5, 7, 8, 9, 10, 12, 13, 14, 17, 19, 20, 18, 19, 9, 18, 19, 13, 18, 19, 19, 3, 4, 6, 11, 14, 15, 19, 20, 5, 18, 5, 5, 5, 9, 3, 9, 1, 3, 5, 20, 10, 2, 6, 10, 14, 19, 16, 3, 16, 17, 18, 10, 3, 8, 14, 15, 18, 19, 20, 11, 17, 19, 9, 6, 10, 1, 3, 4, 5, 7, 9, 10, 11, 14, 15, 17, 18, 20, 12, 5, 8, 4, 19, 1, 2, 4, 5, 9, 11, 17, 4, 6, 12, 15, 17, 7, 15, 17, 15, 17, 7, 11, 1, 7, 10, 11, 12, 16, 19, 7, 10, 8, 6, 14, 17, 19, 8, 8, 19, 1, 5, 7, 8, 13, 14, 17, 18, 9, 4, 6, 7, 18, 20, 7, 13, 6, 8, 4, 7, 8, 9, 10, 12, 16, 8, 7, 9, 11, 11, 6, 15, 16, 19, 11, 15, 16, 17, 7, 17, 7, 2, 4, 9, 17, 5, 15, 5, 15, 5, 8, 15, 5, 3, 6, 7, 10, 11, 15, 19, 8, 12, 13, 2, 3, 7, 9, 10, 19, 20, 3, 6, 7, 9, 11, 12, 13, 16, 19, 20, 11, 19, 7, 9, 20, 2, 3, 4, 6, 7, 10, 11, 12, 13, 14, 15, 18, 19, 9, 11, 14, 15, 16, 5, 8, 11, 17, 18, 19, 20, 11, 1, 3, 6, 7, 8, 9, 10, 15, 16, 18, 19, 20, 6, 10, 19, 3, 6, 7, 10, 15, 16, 19, 4, 8, 12, 2, 2, 2, 4, 5, 8, 9, 11, 16, 18, 20, 1, 2, 4, 7, 8, 9, 16, 9, 2, 3, 5, 6, 12, 15, 18, 2, 3, 1, 10, 17, 18, 19, 9, 17, 7, 19, 7, 18, 8, 20, 14, 3, 11, 18, 12, 16, 20, 10, 11, 12, 3, 5, 6, 10, 13, 16, 19, 3, 6, 7, 10, 19, 20, 3, 6, 7, 9, 10, 11, 19, 3, 6, 9, 10, 19, 4, 11, 17, 20, 2, 6, 12, 13, 19, 20, 15, 11, 17, 19, 7, 12, 19, 12, 14, 5, 6, 7, 11, 16, 17, 18, 20, 6, 8, 14, 18, 20, 8, 1, 2, 3, 5, 7, 8, 10, 11, 12, 13, 14, 15, 17, 18, 1, 6, 1, 2, 4, 17, 19, 14, 2, 2, 3, 8, 13, 17, 6, 6, 13, 1, 3, 12, 13, 3, 8, 13, 1, 3, 4, 11, 14, 15, 2, 7, 15, 19, 19, 8, 2, 2, 1, 2, 4, 5, 9, 10, 11, 13, 14, 17, 18, 1, 9, 10, 13, 1, 2, 8, 15, 2, 2, 4, 6, 9, 13, 18, 19, 13, 19, 20, 2, 2, 12, 14, 1, 1, 2, 3, 5, 7, 12, 15, 20, 8, 12, 20, 8, 12, 20, 5, 8, 12, 13, 19, 15, 15, 19, 5, 15, 2, 6, 19, 4, 2, 4, 10, 10, 19, 7, 16, 17, 2, 14, 19, 11, 6, 18, 13, 18, 3, 10, 3, 9, 10, 18, 17, 2, 5, 12, 13, 17, 18, 19, 20, 8, 15, 7, 19, 20, 7, 8, 10, 11, 12, 17, 18, 19, 4, 9, 11, 15, 2, 4, 1, 2, 4, 10, 13, 3, 15], \"Freq\": [0.04993300139904022, 0.6968910098075867, 0.0781560018658638, 0.09552400559186935, 0.07598499953746796, 0.0034823694732040167, 0.811392068862915, 0.18108321726322174, 0.09677401185035706, 0.8932985663414001, 0.031143682077527046, 0.47531190514564514, 0.0459740050137043, 0.11271046847105026, 0.013347292318940163, 0.04745703935623169, 0.013347292318940163, 0.2269039750099182, 0.032626714557409286, 0.2200930267572403, 0.7774253487586975, 0.008818667382001877, 0.9788721203804016, 0.7754218578338623, 0.06552860885858536, 0.1529000848531723, 0.9973791241645813, 0.002494487212970853, 0.9929277896881104, 0.9949337244033813, 0.020096078515052795, 0.629803478717804, 0.10389292985200882, 0.07545508444309235, 0.008720939047634602, 0.1619061380624771, 0.10648369789123535, 0.34814882278442383, 0.06947872787714005, 0.02492171712219715, 0.20541535317897797, 0.11479093879461288, 0.074765145778656, 0.04002578556537628, 0.01434886734932661, 0.08068332076072693, 0.4587237238883972, 0.009109406732022762, 0.42651546001434326, 0.024400196969509125, 0.10141423344612122, 0.023663321509957314, 0.08451186120510101, 0.662572979927063, 0.12507756054401398, 0.8796453475952148, 0.09534616768360138, 0.023580236360430717, 0.03422682359814644, 0.17271381616592407, 0.1111055314540863, 0.07108648121356964, 0.022642359137535095, 0.5413103699684143, 0.04686442017555237, 0.016066700220108032, 0.016066700220108032, 0.9479352831840515, 0.9783533215522766, 0.9063209295272827, 0.04119640588760376, 0.9976446628570557, 0.9903152585029602, 0.08847993612289429, 0.16170471906661987, 0.40171927213668823, 0.008136086165904999, 0.06305466592311859, 0.06305466592311859, 0.05796961486339569, 0.04881651699542999, 0.10678613185882568, 0.9187410473823547, 0.07499926537275314, 0.9856364727020264, 0.019915923476219177, 0.009957961738109589, 0.8364688158035278, 0.11949554085731506, 0.04918434098362923, 0.9443393349647522, 0.32568085193634033, 0.002669515088200569, 0.6700482964515686, 0.9290430545806885, 0.07065217196941376, 0.9969891309738159, 0.9792396426200867, 0.05704424902796745, 0.16062459349632263, 0.2506944537162781, 0.03302561864256859, 0.001501164399087429, 0.4953842759132385, 0.2658395767211914, 0.2010551542043686, 0.529445230960846, 0.9915363788604736, 0.10484179109334946, 0.8853307366371155, 0.9849833250045776, 0.0723993256688118, 0.0488695465028286, 0.0217197984457016, 0.1647084802389145, 0.1277848184108734, 0.0687793642282486, 0.03402768447995186, 0.07819127291440964, 0.0615394301712513, 0.16072651743888855, 0.07095134258270264, 0.015203858725726604, 0.07493330538272858, 0.34327319264411926, 0.25209125876426697, 0.07073304802179337, 0.07743760198354721, 0.19677866995334625, 0.05933530628681183, 0.4488447308540344, 0.14521446824073792, 0.027418116107583046, 0.0792078897356987, 0.03858846053481102, 0.11881183832883835, 0.1309976726770401, 0.010154858231544495, 0.2639245092868805, 0.012134460732340813, 0.057638686150312424, 0.6613280773162842, 0.9211112856864929, 0.02558642439544201, 0.02558642439544201, 0.5520398020744324, 0.019648417830467224, 0.0067289103753864765, 0.013996133580803871, 0.1286567598581314, 0.09016739577054977, 0.055984534323215485, 0.12461942434310913, 0.008343849331140518, 0.03552306443452835, 0.08525535464286804, 0.06962520629167557, 0.4113570749759674, 0.02841845154762268, 0.07033566385507584, 0.04333813861012459, 0.17264209687709808, 0.039075370877981186, 0.044048599898815155, 0.9980595111846924, 0.9788414239883423, 0.2209109216928482, 0.008971001952886581, 0.6234846115112305, 0.037005383521318436, 0.10765202343463898, 0.9952484965324402, 0.004233160987496376, 0.1608601212501526, 0.002116580493748188, 0.10582902282476425, 0.0804300606250763, 0.004233160987496376, 0.5545440912246704, 0.04233160987496376, 0.01058290246874094, 0.03386528789997101, 0.05429583415389061, 0.008687333203852177, 0.12162266671657562, 0.8144375085830688, 0.3527807295322418, 0.004735311958938837, 0.009470623917877674, 0.6297964453697205, 0.05527642369270325, 0.9416733384132385, 0.14930029213428497, 0.4031107723712921, 0.0022969276178628206, 0.09647095203399658, 0.041344694793224335, 0.08785747736692429, 0.047087013721466064, 0.07005628943443298, 0.010336173698306084, 0.047087013721466064, 0.04479008540511131, 0.20217494666576385, 0.11012782156467438, 0.685422420501709, 0.9963635206222534, 0.9991477131843567, 0.9020038843154907, 0.08694013208150864, 0.9980736970901489, 0.9936151504516602, 0.9896026253700256, 0.9962495565414429, 0.1380038857460022, 0.8556241393089294, 0.03680940344929695, 0.10164798051118851, 0.06855329126119614, 0.11887072771787643, 0.411657452583313, 0.09253005683422089, 0.05639605596661568, 0.08678913861513138, 0.02667837403714657, 0.9978379607200623, 0.9723060727119446, 0.9567052125930786, 0.000805307412520051, 0.04107067734003067, 0.9948429465293884, 0.8592197299003601, 0.1374751627445221, 0.7611309289932251, 0.23357456922531128, 0.004027147777378559, 0.9924344420433044, 0.1872871220111847, 0.0030207601375877857, 0.09062280505895615, 0.7098786234855652, 0.0060415202751755714, 0.971944272518158, 0.07810063660144806, 0.015620127320289612, 0.8903472423553467, 0.06841724365949631, 0.8334463834762573, 0.03939174488186836, 0.05597774311900139, 0.9368367195129395, 0.048540763556957245, 0.009708152152597904, 0.9839352965354919, 0.9874709844589233, 0.03400377929210663, 0.9521057605743408, 0.08974965661764145, 0.08787987381219864, 0.07198670506477356, 0.03646079823374748, 0.6207684874534607, 0.09255433082580566, 0.6833449602127075, 0.315519779920578, 0.9993953704833984, 0.9146696329116821, 0.010513444431126118, 0.06308066099882126, 0.0834149420261383, 0.8897594213485718, 0.009535267949104309, 0.1017095223069191, 0.038141071796417236, 0.11351509392261505, 0.038141071796417236, 0.012713690288364887, 0.5548617839813232, 0.13122344017028809, 0.9959239959716797, 0.9927330017089844, 0.99189692735672, 0.987407386302948, 0.9896252751350403, 0.050929032266139984, 0.26300662755966187, 0.020523639395833015, 0.184712752699852, 0.3314187824726105, 0.1482262909412384, 0.9944176077842712, 0.9967079162597656, 0.18257147073745728, 0.1542857587337494, 0.6608573198318481, 0.9877385497093201, 0.01703096739947796, 0.540449321269989, 0.1385185271501541, 0.15100790560245514, 0.0011353978188708425, 0.0442805141210556, 0.10672739148139954, 0.991672158241272, 0.07404007762670517, 0.9131609201431274, 0.08672338724136353, 0.09111444652080536, 0.026346346363425255, 0.2810277044773102, 0.5143026113510132, 0.9865279197692871, 0.996473491191864, 0.9894394874572754, 0.12395598739385605, 0.0558360293507576, 0.44557151198387146, 0.18425890803337097, 0.1853756159543991, 0.003350161947309971, 0.81073397397995, 0.016613401472568512, 0.16945669054985046, 0.9938634634017944, 0.9830209016799927, 0.9954406023025513, 0.9973971843719482, 0.004012132063508034, 0.9950087666511536, 0.944179892539978, 0.028206631541252136, 0.026722071692347527, 0.9212771058082581, 0.07429654151201248, 0.3144094944000244, 0.6825641989707947, 0.06344103068113327, 0.9244264960289001, 0.9562010765075684, 0.01637338660657406, 0.8514160513877869, 0.01637338660657406, 0.09824031591415405, 0.9878270626068115, 0.010919150896370411, 0.9718044400215149, 0.9059048295021057, 0.08766821026802063, 0.9746440649032593, 0.023485399782657623, 0.9934388399124146, 0.9892924427986145, 0.9884692430496216, 0.20165489614009857, 0.0020577029790729284, 0.11523136496543884, 0.022634733468294144, 0.48253133893013, 0.09671203792095184, 0.0010288514895364642, 0.025721287354826927, 0.05041372403502464, 0.0010288514895364642, 0.9827901721000671, 0.005039949435740709, 0.005039949435740709, 0.2086678445339203, 0.050116732716560364, 0.08565405011177063, 0.0009112133411690593, 0.6542511582374573, 0.9898679852485657, 0.9942122101783752, 0.9912589192390442, 0.0180842112749815, 0.24413684010505676, 0.5753039717674255, 0.0011302632046863437, 0.15936711430549622, 0.3776363730430603, 0.5923041105270386, 0.029874596744775772, 0.04872842878103256, 0.06876122951507568, 0.3795403242111206, 0.14510244131088257, 0.073634073138237, 0.03898274526000023, 0.0032485618721693754, 0.020032798871397972, 0.11315824091434479, 0.10828539729118347, 0.9940058588981628, 0.989668071269989, 0.011846798472106457, 0.9832842946052551, 0.09130099415779114, 0.06334006786346436, 0.08645062893629074, 0.001141262473538518, 0.06162817031145096, 0.036235082894563675, 0.08359747380018234, 0.035664450377225876, 0.023966511711478233, 0.2873128056526184, 0.012268571183085442, 0.09586604684591293, 0.12125913053750992, 0.12140395492315292, 0.08293933421373367, 0.7951357960700989, 0.9952468872070312, 0.9989972114562988, 0.9962821006774902, 0.9996330142021179, 0.0680379718542099, 0.9313898682594299, 0.9764715433120728, 0.03496137633919716, 0.25312820076942444, 0.6427815556526184, 0.06914149224758148, 0.13342957198619843, 0.8637809157371521, 0.9643396139144897, 0.9694628715515137, 0.9666684865951538, 0.0003053020336665213, 0.9400249719619751, 0.059533897787332535, 0.8524149656295776, 0.14438298344612122, 0.00037044871714897454, 0.9805777668952942, 0.018522435799241066, 0.05676031857728958, 0.788337767124176, 0.15136085450649261, 0.9364286661148071, 0.055409982800483704, 0.0006792736821807921, 0.0006792736821807921, 0.9971737861633301, 0.0006792736821807921, 0.9908149242401123, 0.9994528293609619, 0.9722311496734619, 0.9926071763038635, 0.9963144063949585, 0.9914655089378357, 0.005245849024504423, 0.020192530006170273, 0.9490488767623901, 0.9935564398765564, 0.9931331276893616, 0.05734904855489731, 0.39210742712020874, 0.4581255316734314, 0.030675072222948074, 0.060683295130729675, 0.0011023113038390875, 0.23038305342197418, 0.028660094365477562, 0.46076610684394836, 0.0088184904307127, 0.040785517543554306, 0.22817844152450562, 0.0725521668791771, 0.15115034580230713, 0.07960584759712219, 0.012092027813196182, 0.13905832171440125, 0.44538968801498413, 0.09875155985355377, 0.9734836220741272, 0.018367614597082138, 0.9497786164283752, 0.016492540016770363, 0.9565673470497131, 0.0603715218603611, 0.008943929336965084, 0.927932620048523, 0.033883702009916306, 0.2798137962818146, 0.3967672288417816, 0.07432553917169571, 0.11039528995752335, 0.10493017733097076, 0.9701868295669556, 0.7607991099357605, 0.23548543453216553, 0.8061020970344543, 0.19207902252674103, 0.013668612577021122, 0.07094661146402359, 0.04556204378604889, 0.26751428842544556, 0.13017725944519043, 0.0299407709389925, 0.09177497029304504, 0.11195244640111923, 0.04556204378604889, 0.02343190833926201, 0.06769217550754547, 0.0670412927865982, 0.0344969742000103, 0.6453753113746643, 0.10004298388957977, 0.25304991006851196, 0.013548900373280048, 0.49954208731651306, 0.09660955518484116, 0.11546019464731216, 0.007658074144273996, 0.1078021228313446, 0.04005761817097664, 0.06833358854055405, 0.05007202550768852, 0.20138761401176453, 0.06702764332294464, 0.021631648764014244, 0.05575481429696083, 0.004874737933278084, 0.4009471833705902, 0.02985776960849762, 0.1712251603603363, 0.04691935330629349, 0.9744085073471069, 0.21035857498645782, 0.048661261796951294, 0.048661261796951294, 0.013179091736674309, 0.026865070685744286, 0.01976863667368889, 0.11252916604280472, 0.030920175835490227, 0.0947880819439888, 0.2727058231830597, 0.0603196881711483, 0.014699756167829037, 0.046126820147037506, 0.04202371835708618, 0.0943199023604393, 0.0821797177195549, 0.002801581285893917, 0.06163478642702103, 0.09245218336582184, 0.39035364985466003, 0.16622714698314667, 0.02614809200167656, 0.04202371835708618, 0.9939950108528137, 0.9483851790428162, 0.599230945110321, 0.3927463889122009, 0.007450473494827747, 0.1952216774225235, 0.3180371820926666, 0.03711961209774017, 0.13473044335842133, 0.07148962467908859, 0.05453375354409218, 0.13885484635829926, 0.049951083958148956, 0.9804627299308777, 0.017353322356939316, 0.9895769357681274, 0.9934184551239014, 0.9867228269577026, 0.008506231009960175, 0.8384953141212463, 0.16011467576026917, 0.02030753903090954, 0.9747618436813354, 0.020169470459222794, 0.9580498337745667, 0.010084735229611397, 0.37214261293411255, 0.1964808851480484, 0.2303120344877243, 0.03968653827905655, 0.16069795191287994, 0.00032529947930015624, 0.19893093407154083, 0.6383088231086731, 0.008649171330034733, 0.0034596684854477644, 0.14876574277877808, 0.991643488407135, 0.2023715078830719, 0.26745760440826416, 0.18601106107234955, 0.0775342509150505, 0.03094256855547428, 0.04872565343976021, 0.08037954568862915, 0.039478447288274765, 0.0672200620174408, 0.5386145710945129, 0.08422967046499252, 0.13431216776371002, 0.21080182492733002, 0.031415391713380814, 0.6884048581123352, 0.04747619852423668, 0.2611190974712372, 0.9836331009864807, 0.03141757845878601, 0.044730108231306076, 0.23856058716773987, 0.0005325013189576566, 0.0021300052758306265, 0.1837129443883896, 0.49842122197151184, 0.9270511865615845, 0.06333889812231064, 0.9747189283370972, 0.11231261491775513, 0.13861019909381866, 0.04437717795372009, 0.22133803367614746, 0.04985584318637848, 0.09697235375642776, 0.013148793950676918, 0.06300463527441025, 0.2049020379781723, 0.05478664115071297, 0.9935773611068726, 0.0957971140742302, 0.9034404158592224, 0.9845189452171326, 0.04912004619836807, 0.03300252929329872, 0.03760753571987152, 0.043747540563344955, 0.6017205715179443, 0.04835254326462746, 0.05065504461526871, 0.06600505858659744, 0.024560023099184036, 0.029932526871562004, 0.014582512900233269, 0.17701534926891327, 0.006489817053079605, 0.7635605335235596, 0.052813682705163956, 0.06393922120332718, 0.015984805300831795, 0.0979069247841835, 0.195813849568367, 0.625405490398407, 0.23425555229187012, 0.7647255659103394, 0.9773914217948914, 0.07906953990459442, 0.05581379681825638, 0.6775174736976624, 0.15503832697868347, 0.031007664278149605, 0.9157743453979492, 0.08380970358848572, 0.8933878540992737, 0.09835463017225266, 0.0021464223973453045, 0.0815640464425087, 0.686855137348175, 0.22752076387405396, 0.993280827999115, 0.056949179619550705, 0.41691097617149353, 0.5254367589950562, 0.9906078577041626, 0.6809858679771423, 0.15396201610565186, 0.0029608081094920635, 0.047372929751873016, 0.10954989492893219, 0.21566301584243774, 0.39872580766677856, 0.14193636178970337, 0.24374936521053314, 0.013867602683603764, 0.1885993927717209, 0.7960004210472107, 0.9774839282035828, 0.20102950930595398, 0.7982053756713867, 0.21133078634738922, 0.7876874804496765, 0.026399653404951096, 0.12252146750688553, 0.06566067785024643, 0.3648567497730255, 0.14892111718654633, 0.20239734649658203, 0.06769142299890518, 0.6123819947242737, 0.1514493077993393, 0.14321836829185486, 0.09054034948348999, 0.0016461880877614021, 0.9486609697341919, 0.04928109049797058, 0.9928398728370667, 0.11304672807455063, 0.11459530889987946, 0.4878043830394745, 0.20596183836460114, 0.07742926478385925, 0.17566201090812683, 0.7962249517440796, 0.027908915653824806, 0.9761144518852234, 0.009208627045154572, 0.649670422077179, 0.34832704067230225, 0.016402000561356544, 0.000565586204174906, 0.19088535010814667, 0.0308244489133358, 0.013291276060044765, 0.15101152658462524, 0.04779203608632088, 0.4075048565864563, 0.04835762083530426, 0.027430931106209755, 0.06617359071969986, 0.049565888941287994, 0.6380955576896667, 0.01878286339342594, 0.03652223199605942, 0.03495699539780617, 0.05217462033033371, 0.14661067724227905, 0.022435085847973824, 0.0010288726771250367, 0.5761687159538269, 0.15124428272247314, 0.2695646286010742, 0.1989206224679947, 0.7956824898719788, 0.9772025346755981, 0.007106831297278404, 0.007106831297278404, 0.9807426929473877, 0.4402408003807068, 0.28100475668907166, 0.22906146943569183, 0.020436709746718407, 0.028100477531552315, 0.9959086775779724, 0.004025035072118044, 0.004025035072118044, 0.07647566497325897, 0.9096579551696777, 0.06842278689146042, 0.7000177502632141, 0.07631772756576538, 0.15263545513153076, 0.1683822125196457, 0.15451544523239136, 0.05546708405017853, 0.6200427412986755, 0.04623635485768318, 0.10002150386571884, 0.05661594495177269, 0.0868111178278923, 0.40857842564582825, 0.0858675166964531, 0.03491316735744476, 0.16984784603118896, 0.009435990825295448, 0.1179867833852768, 0.44613751769065857, 0.2005775272846222, 0.03244636580348015, 0.053831469267606735, 0.03465861827135086, 0.07374174147844315, 0.04055795818567276, 0.04900793358683586, 0.25146692991256714, 0.02892271615564823, 0.20928798615932465, 0.01044431421905756, 0.004017043858766556, 0.009239200502634048, 0.09199029952287674, 0.06427270174026489, 0.019281810149550438, 0.10444314032793045, 0.07069996744394302, 0.052221570163965225, 0.011649427004158497, 0.008034087717533112, 0.014461358077824116, 0.7923883199691772, 0.04997043311595917, 0.05710906535387039, 0.09875109791755676, 0.9912946820259094, 0.22885183990001678, 0.015068433247506618, 0.4473441243171692, 0.307961106300354, 0.9895172119140625, 0.9825463891029358, 0.9950115084648132, 0.9948474764823914, 0.9921873211860657, 0.882735550403595, 0.003549874760210514, 0.028990644961595535, 0.04496508091688156, 0.02070760354399681, 0.01834101974964142, 0.9938021898269653, 0.11337444186210632, 0.869204044342041, 0.058035168796777725, 0.14843609929084778, 0.5792356133460999, 0.04464243724942207, 0.04352637752890587, 0.12499882280826569, 0.9880167245864868, 0.9954075813293457, 0.0008268074598163366, 0.052088867872953415, 0.10252412408590317, 0.06779821217060089, 0.04382079467177391, 0.5514805912971497, 0.10417773574590683, 0.0760662853717804, 0.04813818261027336, 0.251388281583786, 0.1489991396665573, 0.06571243703365326, 0.021394748240709305, 0.041261300444602966, 0.013753766193985939, 0.280424028635025, 0.030563924461603165, 0.09704045951366425, 0.9926282167434692, 0.2593376338481903, 0.7344442009925842, 0.005186752881854773, 0.0010699969716370106, 0.14658957719802856, 0.5596083998680115, 0.07275979220867157, 0.04921986162662506, 0.09201973676681519, 0.07810977846384048, 0.9845189452171326, 0.9934867024421692, 0.004830567631870508, 0.9965519905090332, 0.99126136302948, 0.9841845631599426, 0.8226965665817261, 0.17629212141036987, 0.998696506023407, 0.9712106585502625, 0.058751191943883896, 0.28872016072273254, 0.07721585035324097, 0.03748885542154312, 0.09624005109071732, 0.020143266767263412, 0.012869308702647686, 0.3950318396091461, 0.013428844511508942, 0.9890106320381165, 0.052280496805906296, 0.056912947446107864, 0.09595787525177002, 0.06154539808630943, 0.14757660031318665, 0.020515132695436478, 0.5413347482681274, 0.02250046841800213, 0.10659174621105194, 0.8059375882148743, 0.08579335361719131, 0.9830542802810669, 0.07942567020654678, 0.014225493185222149, 0.056506820023059845, 0.009878815151751041, 0.10234452039003372, 0.03753949701786041, 0.6994200944900513, 0.936907947063446, 0.02532183565199375, 0.03610117733478546, 0.09386306256055832, 0.8303270936012268, 0.03971129655838013, 0.9933536052703857, 0.9939262270927429, 0.06318052113056183, 0.07743176817893982, 0.010925955139100552, 0.03372794762253761, 0.652707040309906, 0.009975872002542019, 0.01377620454877615, 0.13776203989982605, 0.9958127737045288, 0.5946910977363586, 0.13128414750099182, 0.01934484951198101, 0.008694314397871494, 0.18540625274181366, 0.04934023320674896, 0.011085250414907932, 0.9953375458717346, 0.11491164565086365, 0.04974833503365517, 0.38607510924339294, 0.0014013615436851978, 0.0539524182677269, 0.1485443264245987, 0.05815650150179863, 0.0539524182677269, 0.13312934339046478, 0.88016277551651, 0.11409517377614975, 0.020275870338082314, 0.005068967584520578, 0.9681727886199951, 0.9883801341056824, 0.02550552412867546, 0.7787686586380005, 0.19384197890758514, 0.11845874786376953, 0.0515826940536499, 0.11560744047164917, 0.035252492874860764, 0.0002592095115687698, 0.15293361246585846, 0.44609957933425903, 0.07931811362504959, 0.8829805850982666, 0.11467280983924866, 0.24453076720237732, 0.7531547546386719, 0.9967787265777588, 0.9315746426582336, 0.04903024435043335, 0.12595774233341217, 0.0007727468619123101, 0.4350564777851105, 0.3701457381248474, 0.06645622849464417, 0.9973558783531189, 0.053256455808877945, 0.1760791540145874, 0.06224348396062851, 0.1737491935491562, 0.21801862120628357, 0.03628095984458923, 0.019638318568468094, 0.03128816932439804, 0.14013105630874634, 0.06823483854532242, 0.020969729870557785, 0.5529367923736572, 0.2542238235473633, 0.19225674867630005, 0.07019874453544617, 0.9259549379348755, 0.92989182472229, 0.03321042284369469, 0.9926254153251648, 0.003696116618812084, 0.9905592799186707, 0.15940341353416443, 0.0036023370921611786, 0.0504327192902565, 0.01666080951690674, 0.04187716916203499, 0.08420462906360626, 0.1598537117242813, 0.18642094731330872, 0.2913390100002289, 0.0063040899112820625, 0.9988992214202881, 0.9989542961120605, 0.5817028880119324, 0.028177212923765182, 0.16718479990959167, 0.03819577768445015, 0.15654006600379944, 0.026298731565475464, 0.0012523205950856209, 0.32143884897232056, 0.0011122451396659017, 0.6762450933456421, 0.7043136358261108, 0.060477737337350845, 0.0012599528999999166, 0.1423746794462204, 0.09071660786867142, 0.07588987052440643, 0.5824880599975586, 0.023965222761034966, 0.09053528308868408, 0.2263382077217102, 0.8815144300460815, 0.032080430537462234, 0.08508288115262985, 0.9882697463035583, 0.5379679203033447, 0.4046483635902405, 0.05627559870481491, 0.0006699475925415754, 0.9786949157714844, 0.08738512545824051, 0.625455379486084, 0.018346048891544342, 0.010621395893394947, 0.19359908998012543, 0.01810465194284916, 0.01882883906364441, 0.02800186350941658, 0.9921888709068298, 0.9897695779800415, 0.0012456465046852827, 0.6004016399383545, 0.39736124873161316, 0.9579379558563232, 0.2710460126399994, 0.0671161562204361, 0.48730918765068054, 0.06826344132423401, 0.04789913818240166, 0.02524026297032833, 0.03298443555831909, 0.27965301275253296, 0.3922525644302368, 0.08491114526987076, 0.11998313665390015, 0.10060124844312668, 0.022150732576847076, 0.3347078561782837, 0.0005987618351355195, 0.5472683310508728, 0.06346875429153442, 0.05328980088233948, 0.2212093025445938, 0.7775048613548279, 0.03141104057431221, 0.031672798097133636, 0.046593040227890015, 0.23767685890197754, 0.08009815216064453, 0.05208997428417206, 0.05706338584423065, 0.0575869046151638, 0.03507566079497337, 0.08559507876634598, 0.03402862697839737, 0.0010470346314832568, 0.03769324719905853, 0.01413496769964695, 0.07486297935247421, 0.12328832596540451, 0.9552181363105774, 0.03879058361053467, 0.9148883819580078, 0.06099255755543709, 0.04286894574761391, 0.3119906485080719, 0.06549422442913055, 0.4691767990589142, 0.032151710242033005, 0.07859306782484055, 0.06634625792503357, 0.013494153507053852, 0.25526440143585205, 0.07759138196706772, 0.060723692178726196, 0.5262719988822937, 0.5223972797393799, 0.0565587617456913, 0.21697998046875, 0.12442927807569504, 0.07918226718902588, 0.8476545214653015, 0.15073354542255402, 0.9966158270835876, 0.7060815095901489, 0.1991512030363083, 0.09310965240001678, 0.6568817496299744, 0.14647707343101501, 0.06493313610553741, 0.042282041162252426, 0.0468122623860836, 0.024161167442798615, 0.019630948081612587, 0.9909156560897827, 0.09838508814573288, 0.07534553855657578, 0.0336252823472023, 0.02677568793296814, 0.04234294965863228, 0.15006838738918304, 0.19926093518733978, 0.23849043250083923, 0.0336252823472023, 0.021794164553284645, 0.005604213569313288, 0.04794716089963913, 0.027398377656936646, 0.03867194056510925, 0.0051382300443947315, 0.20850396156311035, 0.11304105818271637, 0.0754508525133133, 0.09843766689300537, 0.03840150684118271, 0.03191111236810684, 0.017578154802322388, 0.041376274079084396, 0.12466968595981598, 0.05327533185482025, 0.14441131055355072, 0.008653860539197922, 0.24964433908462524, 0.7497541904449463, 0.0022069658152759075, 0.9512022733688354, 0.0441393181681633, 0.9957230091094971, 0.1412011682987213, 0.04282330721616745, 0.09259092807769775, 0.0011573866941034794, 0.23147733509540558, 0.4363347589969635, 0.05323978513479233, 0.6407601833343506, 0.000946470012422651, 0.000946470012422651, 0.356819212436676, 0.9967198371887207, 0.9925746321678162, 0.0061136409640312195, 0.9939033389091492, 0.9930618405342102, 0.9948132634162903, 0.9906107187271118, 0.9917800426483154, 0.19652245938777924, 0.1705121397972107, 0.6300278902053833, 0.9694628715515137, 0.18018856644630432, 0.7133996486663818, 0.007354635279625654, 0.0974489152431488, 0.029639525339007378, 0.9484648108482361, 0.0034712853375822306, 0.9927875995635986, 0.11085017770528793, 0.8775638937950134, 0.31043028831481934, 0.05715859308838844, 0.05945807695388794, 0.3866417706012726, 0.06471404433250427, 0.12154413759708405, 0.6003615856170654, 0.07204338908195496, 0.23645009100437164, 0.08866878598928452, 0.05722712352871895, 0.07232872396707535, 0.8703292012214661, 0.112479068338871, 0.8873348832130432, 0.8762948513031006, 0.003198156366124749, 0.0863502249121666, 0.031981565058231354, 0.11900536715984344, 0.13928036391735077, 0.0568581223487854, 0.09035593271255493, 0.5941453576087952, 0.23690196871757507, 0.0014386759139597416, 0.16113169491291046, 0.08344320207834244, 0.018702786415815353, 0.13475598394870758, 0.08823879063129425, 0.12036921828985214, 0.04795586317777634, 0.1069415733218193, 0.9535360336303711, 0.10714602470397949, 0.89253169298172, 0.17291484773159027, 0.8243063688278198, 0.9924343824386597, 0.07150518894195557, 0.059658173471689224, 0.04146454855799675, 0.14216415584087372, 0.07446694374084473, 0.21324624121189117, 0.10493069142103195, 0.08927571028470993, 0.1273553967475891, 0.07573626190423965, 0.0390341579914093, 0.9602402448654175, 0.06214247643947601, 0.4131634831428528, 0.04198816046118736, 0.32498836517333984, 0.07054010778665543, 0.08649560809135437, 0.27873358130455017, 0.7205559611320496, 0.8779875040054321, 0.11843819916248322, 0.9986405968666077, 0.9981293082237244, 0.18040981888771057, 0.06986969709396362, 0.08446933329105377, 0.5641717314720154, 0.0010428313398733735, 0.06152704730629921, 0.009385481476783752, 0.028156446292996407, 0.9959665536880493, 0.09215670824050903, 0.23746199905872345, 0.12677642703056335, 0.018528861925005913, 0.09703272581100464, 0.06826423108577728, 0.03364451229572296, 0.09752032905817032, 0.04827256128191948, 0.09752032905817032, 0.035594917833805084, 0.027793293818831444, 0.015115650370717049, 0.004388414788991213, 0.481568843126297, 0.5170528888702393, 0.9880167245864868, 0.9904748201370239, 0.9554541707038879, 0.01802743785083294, 0.9888702630996704, 0.009782974608242512, 0.9782974720001221, 0.06672477722167969, 0.9285865426063538, 0.9914020299911499, 0.985619068145752, 0.11567114293575287, 0.6285526156425476, 0.019642269238829613, 0.10257629305124283, 0.004364948719739914, 0.093846395611763, 0.004364948719739914, 0.030554641038179398, 0.07583995908498764, 0.0012432780349627137, 0.46747255325317383, 0.013676058501005173, 0.04351473227143288, 0.3978489637374878, 0.9907596111297607, 0.014231283217668533, 0.382663369178772, 0.6024576425552368, 0.9862980246543884, 0.0005053281784057617, 0.21173250675201416, 0.7483910322189331, 0.03891026973724365, 0.9792308807373047, 0.9822080135345459, 0.004601779393851757, 0.2162836194038391, 0.773098886013031, 0.9782637357711792, 0.03365497291088104, 0.9423392415046692, 0.9854734539985657, 0.9765706062316895, 0.9880658388137817, 0.01677236147224903, 0.0033544725738465786, 0.7715286612510681, 0.20462282001972198, 0.07928923517465591, 0.057301465421915054, 0.19322587549686432, 0.6329813003540039, 0.0366462841629982, 0.997388482093811, 0.9771093130111694, 0.15453094244003296, 0.8402619361877441, 0.9973971843719482, 0.6328620314598083, 0.04921405762434006, 0.0822797566652298, 0.08112630248069763, 0.04998302832245827, 0.06651587784290314, 0.037679512053728104, 0.9964023232460022, 0.8434236645698547, 0.06689222157001495, 0.084342360496521, 0.9935742020606995, 0.9928898215293884, 0.14993888139724731, 0.25261440873146057, 0.1168002188205719, 0.2053510844707489, 0.2640228271484375, 0.010865136049687862, 0.9989525675773621, 0.9945204854011536, 0.9948030710220337, 0.9970318078994751, 0.9957757592201233, 0.12021703273057938, 0.8735771179199219, 0.9889362454414368, 0.14001597464084625, 0.1950511634349823, 0.1100703701376915, 0.06717529892921448, 0.2525143623352051, 0.03803902491927147, 0.13758796453475952, 0.059891227632761, 0.04044442996382713, 0.012999995611608028, 0.0382777638733387, 0.07944441586732864, 0.06138886883854866, 0.5062776207923889, 0.09533330053091049, 0.08016663789749146, 0.08449997007846832, 0.9483405947685242, 0.054592981934547424, 0.8497516512870789, 0.0640874132514, 0.031450305134058, 0.2319403737783432, 0.7662109732627869, 0.001153932185843587, 0.1621769815683365, 0.6840541958808899, 0.14970183372497559, 0.993340015411377, 0.007959993556141853, 0.9916825294494629, 0.9968277215957642, 0.9948579668998718, 0.975043773651123, 0.08162352442741394, 0.13343670964241028, 0.10007753223180771, 0.30449122190475464, 0.02484194189310074, 0.15260049700737, 0.07736490666866302, 0.12633901834487915, 0.019849445670843124, 0.8932250142097473, 0.05624009296298027, 0.026465926319360733, 0.9719367623329163, 0.9541040062904358, 0.04780206456780434, 0.07170309871435165, 0.08830104023218155, 0.027220619842410088, 0.051121652126312256, 0.006639175582677126, 0.007303093560039997, 0.05045773461461067, 0.05709691345691681, 0.5915505886077881, 0.9841845631599426, 0.9933071136474609, 0.003998308442533016, 0.993579626083374, 0.9939497113227844, 0.9960065484046936, 0.021257221698760986, 0.001932474784553051, 0.9758997559547424, 0.9925103783607483, 0.33331024646759033, 0.5601115822792053, 0.03320571780204773, 0.0006265229894779623, 0.026940489187836647, 0.04510965570807457, 0.05953723192214966, 0.8857182860374451, 0.05382817983627319, 0.26573774218559265, 0.7333537340164185, 0.9996355772018433, 0.0030798977240920067, 0.10779642313718796, 0.8839306831359863, 0.13965383172035217, 0.8561386466026306, 0.9849410057067871, 0.9254967570304871, 0.07211662828922272, 0.8412073850631714, 0.14020122587680817, 0.1244174987077713, 0.8501862287521362, 0.9958177804946899, 0.3498061001300812, 0.6480382680892944, 0.995576024055481, 0.9940879940986633, 0.9989725947380066, 0.1463257223367691, 0.8531534075737, 0.023269500583410263, 0.5954254269599915, 0.04927658662199974, 0.16836167871952057, 0.08075885474681854, 0.04517020657658577, 0.03695743903517723, 0.10055316239595413, 0.8924093246459961, 0.13930511474609375, 0.013930510729551315, 0.10308577865362167, 0.05154288932681084, 0.6895602941513062, 0.9902222752571106, 0.9889128804206848, 0.5808179974555969, 0.0006389636546373367, 0.1584629863500595, 0.2594192624092102, 0.01311391033232212, 0.12622138857841492, 0.8606003522872925, 0.9935771226882935, 0.5917907357215881, 0.021949443966150284, 0.031235747039318085, 0.20429866015911102, 0.07429042458534241, 0.049808353185653687, 0.02617049030959606, 0.3442693054676056, 0.6529245972633362, 0.9728779196739197, 0.9981327056884766, 0.9218133687973022, 0.07041629403829575, 0.9782780408859253, 0.9999216198921204, 0.9849005341529846, 0.018717141821980476, 0.049413252621889114, 0.008235542103648186, 0.000748685619328171, 0.053905367851257324, 0.0501619391143322, 0.021711884066462517, 0.04566982388496399, 0.06588433682918549, 0.6154196262359619, 0.06887907534837723, 0.1657789796590805, 0.046758171170949936, 0.05738503113389015, 0.05632234364748001, 0.4505787491798401, 0.11795812100172043, 0.06694920361042023, 0.02444177307188511, 0.01275222934782505, 0.054895807057619095, 0.03152432292699814, 0.03913364186882973, 0.05054762214422226, 0.04022068902850151, 0.03913364186882973, 0.24458527565002441, 0.030980801209807396, 0.057613421231508255, 0.05870046466588974, 0.02119738981127739, 0.07337558269500732, 0.05543932691216469, 0.06739683449268341, 0.05054762214422226, 0.049460578709840775, 0.033698417246341705, 0.012008028104901314, 0.05667789280414581, 0.18108107149600983, 0.06868592649698257, 0.2718617618083954, 0.014409634284675121, 0.1316079944372177, 0.06388270854949951, 0.096544548869133, 0.05523693189024925, 0.028338946402072906, 0.019212845712900162, 0.9957578778266907, 0.8216971755027771, 0.1781555861234665, 0.994481086730957, 0.8135519623756409, 0.11598499864339828, 0.06959100067615509, 0.9899755120277405, 0.08576959371566772, 0.2713693678379059, 0.09279988706111908, 0.09631503373384476, 0.00210908823646605, 0.0794423297047615, 0.01406058855354786, 0.06678779423236847, 0.0674908235669136, 0.047102972865104675, 0.053430236876010895, 0.04921206086874008, 0.012654529884457588, 0.04991509020328522, 0.011248471215367317, 0.6377049684524536, 0.35963594913482666, 0.7827273607254028, 0.21598078310489655, 0.12642012536525726, 0.12407901883125305, 0.012642012909054756, 0.00046822268632240593, 0.07397918403148651, 0.0009364453726448119, 0.019665353000164032, 0.03886248543858528, 0.1100323349237442, 0.009364454075694084, 0.0945809856057167, 0.11986500769853592, 0.1507677137851715, 0.022474689409136772, 0.0945809856057167, 0.0807659700512886, 0.8999636769294739, 0.017306992784142494, 0.27343031764030457, 0.07574247568845749, 0.26434123516082764, 0.0007574247429147363, 0.07952959835529327, 0.2794897258281708, 0.024995015934109688, 0.9149870872497559, 0.032678112387657166, 0.7671759724617004, 0.06729613989591599, 0.002691845642402768, 0.15881888568401337, 0.12035465985536575, 0.5064274072647095, 0.20788532495498657, 0.16411998867988586, 0.9039111733436584, 0.037565138190984726, 0.05634770914912224, 0.993238091468811, 0.0674663856625557, 0.6926549077033997, 0.23838123679161072, 0.9198383092880249, 0.04380182549357414, 0.9967669248580933, 0.973718523979187, 0.1500498652458191, 0.0005930825136601925, 0.02609563060104847, 0.4958169758319855, 0.20105496048927307, 0.05100509524345398, 0.01364089734852314, 0.0237233005464077, 0.03677111491560936, 0.9641060829162598, 0.012203874066472054, 0.012203874066472054, 0.9356974959373474, 0.05848109349608421, 0.03742894530296326, 0.11763383448123932, 0.8180898427963257, 0.016040977090597153, 0.992323637008667, 0.9497786164283752, 0.5310843586921692, 0.4684674143791199, 0.9948915243148804, 0.9928734302520752, 0.948426365852356, 0.9831815361976624, 0.008953562937676907, 0.9132634401321411, 0.07162850350141525, 0.9928528666496277, 0.996124267578125, 0.9829168319702148, 0.00982916820794344, 0.46345648169517517, 0.0006630278658121824, 0.03447744995355606, 0.5005860328674316, 0.013592635281383991, 0.9650771021842957, 0.026593845337629318, 0.08076649159193039, 0.621508002281189, 0.06599213182926178, 0.11720991134643555, 0.0068947006948292255, 0.08076649159193039, 0.9978883862495422, 0.9743738174438477, 0.9937431216239929, 0.9875708818435669, 0.3043946325778961, 0.5069513916969299, 0.18800845742225647, 0.9921976923942566, 0.9957207441329956, 0.9889665842056274, 0.005851873196661472, 0.7022997140884399, 0.0031352664809674025, 0.22554323077201843, 0.04095441848039627, 0.00019595415506046265, 0.027629535645246506, 0.7663899064064026, 0.22991697490215302, 0.9913529753684998, 0.6904444694519043, 0.10327161103487015, 0.20359261333942413, 0.997667670249939, 0.027598638087511063, 0.38313403725624084, 0.014069894328713417, 0.2229537069797516, 0.22998864948749542, 0.016775643453001976, 0.10606535524129868, 0.06540331244468689, 0.3275953531265259, 0.20662817358970642, 0.09492162615060806, 0.14759154617786407, 0.1574309915304184, 0.9107375144958496, 0.08548438549041748, 0.9926797151565552, 0.9962233901023865, 0.02395656704902649, 0.04539139196276665, 0.08006536960601807, 0.10150019079446793, 0.051065314561128616, 0.08636973053216934, 0.1405872255563736, 0.02395656704902649, 0.3158484399318695, 0.05232618749141693, 0.07943493127822876, 0.9275920391082764, 0.07010869681835175, 0.0032145625445991755, 0.1596565991640091, 0.8357862234115601, 0.0351426862180233, 0.11394750326871872, 0.13631102442741394, 0.02209729515016079, 0.00026623246958479285, 0.07268146425485611, 0.3008427023887634, 0.036207616329193115, 0.13710972666740417, 0.024759620428085327, 0.12086954712867737, 0.9562845826148987, 0.01804310455918312, 0.9967477917671204, 0.22885321080684662, 0.07134835422039032, 0.14135050773620605, 0.5573248863220215, 0.9834288954734802, 0.9406171441078186, 0.05814383178949356, 0.029762424528598785, 0.12355915457010269, 0.6520676612854004, 0.10101186484098434, 0.033369991928339005, 0.037879448384046555, 0.021645398810505867, 0.1013437956571579, 0.8903776407241821, 0.00723884254693985, 0.18399198353290558, 0.022205930203199387, 0.047936610877513885, 0.1617860645055771, 0.07507719099521637, 0.12195637077093124, 0.04370690882205963, 0.04053463414311409, 0.12442369759082794, 0.041592057794332504, 0.07084748893976212, 0.06556036323308945, 0.9905393719673157, 0.17493882775306702, 0.11454593390226364, 0.0006685560219921172, 0.0022285201121121645, 0.07822106033563614, 0.26608529686927795, 0.016491049900650978, 0.2625196874141693, 0.03097642958164215, 0.0534844845533371, 0.9567705392837524, 0.018760206177830696, 0.9958685040473938, 0.826241672039032, 0.1652483344078064, 0.9836080074310303, 0.017452621832489967, 0.08144557476043701, 0.6879242062568665, 0.1847069263458252, 0.027633318677544594, 0.008977041579782963, 0.9874746203422546, 0.9933536052703857, 0.004824969917535782, 0.03136230632662773, 0.0530746690928936, 0.9095068573951721, 0.062178969383239746, 0.019357603043317795, 0.14723511040210724, 0.24050356447696686, 0.03343586251139641, 0.16014017164707184, 0.06716501712799072, 0.22613200545310974, 0.025516841560602188, 0.018184415996074677, 0.19922718405723572, 0.7969087362289429, 0.0018619362963363528, 0.9826552867889404, 0.16217386722564697, 0.03004986234009266, 0.21559584140777588, 0.014309458434581757, 0.5776251554489136, 0.6300897002220154, 0.28809410333633423, 0.0006195572204887867, 0.00743468664586544, 0.05637970566749573, 0.017347602173686028, 0.9954032301902771, 0.00213605840690434, 0.9950251579284668, 0.0018942815950140357, 0.0018942815950140357, 0.18563960492610931, 0.16101394593715668, 0.5152446031570435, 0.13259971141815186, 0.004732993897050619, 0.8074487447738647, 0.1022326648235321, 0.08330068737268448, 0.0009465987677685916, 0.1020095944404602, 0.8809919357299805, 0.09578303247690201, 0.899296224117279, 0.9868984222412109, 0.1593220829963684, 0.8370354175567627, 0.002377941505983472, 0.9931340217590332, 0.10367459803819656, 0.4277666211128235, 0.017859909683465958, 0.21214087307453156, 0.0583714134991169, 0.06490552425384521, 0.11456478387117386, 0.9008413553237915, 0.09613721817731857, 0.8476672768592834, 0.13745956122875214, 0.9852982759475708, 0.9858289957046509, 0.9973708987236023, 0.007355376612395048, 0.9856204390525818, 0.03582042083144188, 0.0024582641199231148, 0.0003511805844027549, 0.5232590436935425, 0.025636183097958565, 0.07620618492364883, 0.26935550570487976, 0.06672430783510208, 0.004403508268296719, 0.11834428459405899, 0.05834648385643959, 0.042383767664432526, 0.39191222190856934, 0.1199956014752388, 0.11669296771287918, 0.04678727686405182, 0.09962937235832214, 0.041001372039318085, 0.9498651623725891, 0.9988474249839783, 0.9915799498558044, 0.9950451850891113, 0.21958522498607635, 0.11711211502552032, 0.6587556600570679, 0.0010695640230551362, 0.15615634620189667, 0.028878228738904, 0.2984083592891693, 0.43531256914138794, 0.07914774119853973, 0.01601247303187847, 0.5684428215026855, 0.3490719199180603, 0.012809978798031807, 0.03202494606375694, 0.019214969128370285, 0.9884979724884033, 0.4086538255214691, 0.5908722877502441, 0.9939811825752258, 0.9247741103172302, 0.07360447198152542, 0.9498188495635986, 0.9495200514793396, 0.9969672560691833, 0.9923674464225769, 0.996849536895752, 0.7609381675720215, 0.23750020563602448, 0.8755285739898682, 0.11958438903093338, 0.042044200003147125, 0.06115520000457764, 0.7567955851554871, 0.01528880000114441, 0.042044200003147125, 0.08026620000600815, 0.0022566672414541245, 0.11824936419725418, 0.17963071167469025, 0.11599269509315491, 0.11915203183889389, 0.14848870038986206, 0.07672668993473053, 0.06995668262243271, 0.07943468540906906, 0.07085935026407242, 0.018956005573272705, 0.06609012186527252, 0.1144246906042099, 0.000986419734545052, 0.05326666682958603, 0.032551851123571396, 0.09765555709600449, 0.14796295762062073, 0.28014320135116577, 0.20517531037330627, 0.37141767144203186, 0.1388273537158966, 0.3016405701637268, 0.13010521233081818, 0.05669389292597771, 0.9747189283370972, 0.007495716214179993, 0.6871073246002197, 0.13992002606391907, 0.009994287975132465, 0.0024985719937831163, 0.14991432428359985, 0.9967994093894958, 0.9955899119377136, 0.03225736320018768, 0.08492244780063629, 0.8827984929084778, 0.8631007671356201, 0.02213078923523426, 0.08852315694093704, 0.04060745611786842, 0.944123387336731, 0.8760177493095398, 0.0051530455239117146, 0.11336699873209, 0.9963468909263611, 0.04794269800186157, 0.9348825812339783, 0.6357629299163818, 0.002031191485002637, 0.24374298751354218, 0.11780910938978195, 0.055102378129959106, 0.5047377943992615, 0.08375561237335205, 0.16457243263721466, 0.0007346983766183257, 0.041877806186676025, 0.046285998076200485, 0.0007346983766183257, 0.10285776853561401, 0.9907072186470032, 0.9832150936126709, 0.8355752825737, 0.0827874094247818, 0.023103462532162666, 0.05775865912437439, 0.8799778819084167, 0.11604104191064835, 0.9904059767723083, 0.9186429381370544, 0.05862351506948471, 0.022451559081673622, 0.9947100281715393, 0.0984693095088005, 0.895175576210022, 0.9949548840522766, 0.1319800168275833, 0.8676463961601257, 0.17761166393756866, 0.10282780975103378, 0.05608789622783661, 0.09425882250070572, 0.3435383439064026, 0.2251305729150772, 0.16550101339817047, 0.038471728563308716, 0.6184511780738831, 0.13356222212314606, 0.04427878186106682, 0.8410071730613708, 0.15017984807491302, 0.8211158514022827, 0.0034646238200366497, 0.17323119938373566, 0.9909939169883728, 0.9963276386260986, 0.9993213415145874, 0.09945748746395111, 0.038565147668123245, 0.8616265654563904, 0.9972807765007019, 0.9950737357139587, 0.993213951587677, 0.9683476090431213, 0.018802866339683533, 0.0206100195646286, 0.3945346772670746, 0.03631289303302765, 0.5476376414299011, 0.9991405606269836, 0.07749006152153015, 0.03254582732915878, 0.07594025880098343, 0.8136456608772278, 0.9894042015075684, 0.9728162288665771, 0.8585954308509827, 0.1376374363899231, 0.009541135281324387, 0.9827369451522827, 0.9904296398162842, 0.9905885457992554, 0.06303797662258148, 0.0023347397800534964, 0.9315611720085144, 0.9868941307067871, 0.8353999853134155, 0.0026353311259299517, 0.0026353311259299517, 0.02635331079363823, 0.13176655769348145, 0.990725576877594, 0.004380636848509312, 0.1796061098575592, 0.8104178309440613, 0.9945204854011536, 0.06214248389005661, 0.8476872444152832, 0.004780190996825695, 0.07807645201683044, 0.004780190996825695, 0.15889844298362732, 0.002482788171619177, 0.09186316281557083, 0.7423536777496338, 0.002482788171619177, 0.9926318526268005, 0.96034836769104, 0.004205905366688967, 0.035049211233854294, 0.11269831657409668, 0.6090505123138428, 0.0023978366516530514, 0.01918269321322441, 0.016784856095910072, 0.05994591489434242, 0.17983774840831757, 0.9849099516868591, 0.005367205943912268, 0.012429319322109222, 0.060451690107584, 0.2039538323879242, 0.1110164225101471, 0.018361493945121765, 0.026553545147180557, 0.050564732402563095, 0.16355854272842407, 0.1813550740480423, 0.052824608981609344, 0.08813517540693283, 0.0257060918956995, 0.20089471340179443, 0.2790638506412506, 0.06331700831651688, 0.10474665462970734, 0.2126200795173645, 0.13914108276367188, 0.9823753833770752, 0.9944902062416077, 0.016071826219558716, 0.8349313735961914, 0.14889399707317352, 0.8382964134216309, 0.1601564586162567, 0.9483336210250854, 0.034695133566856384, 0.970899224281311, 0.9997418522834778, 0.028971482068300247, 0.5591955780982971, 0.10048045217990875, 0.01954425312578678, 0.024372832849621773, 0.2671814262866974, 0.006446040701121092, 0.006446040701121092, 0.9797981977462769, 0.8450848460197449, 0.1502373069524765, 0.21735334396362305, 0.028661979362368584, 0.46623489260673523, 0.13280050456523895, 0.024840382859110832, 0.12945660948753357, 0.9899857640266418, 0.9795809388160706, 0.1231435164809227, 0.16927692294120789, 0.026154374703764915, 0.14566533267498016, 0.03160320222377777, 0.04504364728927612, 0.16927692294120789, 0.11515190452337265, 0.07337754964828491, 0.047586433589458466, 0.03450924530625343, 0.008354869671165943, 0.01126091182231903, 0.9436630606651306, 0.0028001870959997177, 0.05040336772799492, 0.02698216401040554, 0.9726740717887878, 0.9912285804748535, 0.004254200030118227, 0.1148839071393013, 0.0671352818608284, 0.10662662237882614, 0.02620789036154747, 0.10985773056745529, 0.024053817614912987, 0.16622264683246613, 0.0890350267291069, 0.02297678031027317, 0.05564689263701439, 0.06282713264226913, 0.012924439273774624, 0.033388134092092514, 0.040209367871284485, 0.06857132911682129, 0.0062548695132136345, 0.9882693290710449, 0.997818112373352, 0.9683505892753601, 0.9959467649459839, 0.9929853081703186, 0.12806302309036255, 0.0753311887383461, 0.7947440147399902, 0.0008019428350962698, 0.05373017117381096, 0.09543119370937347, 0.18685267865657806, 0.4458802044391632, 0.03528548404574394, 0.15236914157867432, 0.02886994183063507, 0.10058210790157318, 0.6725671291351318, 0.02060115523636341, 0.13330158591270447, 0.07270995527505875, 0.9943665266036987, 0.9890361428260803, 0.0573422908782959, 0.06881074607372284, 0.7626524567604065, 0.10895035415887833, 0.062161076813936234, 0.20975536108016968, 0.0006124243955127895, 0.20608080923557281, 0.15218746662139893, 0.27130401134490967, 0.07808411121368408, 0.01929136924445629, 0.9889082908630371, 0.9943167567253113, 0.0017769155092537403, 0.7534121870994568, 0.005330746527761221, 0.10572647303342819, 0.0017769155092537403, 0.1314917504787445, 0.7945408225059509, 0.19863520562648773, 0.038457516580820084, 0.9422091245651245, 0.8147796392440796, 0.1847371608018875, 0.9914977550506592, 0.9816098809242249, 0.9889128804206848, 0.9805563688278198, 0.037012044340372086, 0.18241649866104126, 0.77989661693573, 0.9844815731048584, 0.8242860436439514, 0.17509883642196655, 0.9990765452384949, 0.9477460384368896, 0.050494663417339325, 0.0657920092344284, 0.23027203977108002, 0.5936945676803589, 0.014098288491368294, 0.05169372260570526, 0.04229486361145973, 0.9949950575828552, 0.9929053783416748, 0.10307572782039642, 0.6986243724822998, 0.050392575562000275, 0.14659658074378967, 0.002709123073145747, 0.04673237353563309, 0.10023755580186844, 0.10836492478847504, 0.08669193834066391, 0.16254737973213196, 0.39079099893569946, 0.006095526739954948, 0.09549658745527267, 0.10085399448871613, 0.8954612016677856, 0.9845189452171326, 0.14725863933563232, 0.8467372059822083, 0.04722416773438454, 0.07463105022907257, 0.014757552184164524, 0.024877017363905907, 0.13619112968444824, 0.04300772398710251, 0.05650034546852112, 0.026141950860619545, 0.2428671419620514, 0.11004917323589325, 0.04047785699367523, 0.10414615273475647, 0.026563594117760658, 0.03836963698267937, 0.013914263807237148, 0.9923674464225769, 0.1103692576289177, 0.06131625175476074, 0.04138847067952156, 0.03219103068113327, 0.08584275096654892, 0.668347179889679, 0.14450213313102722, 0.5764874219894409, 0.02205558866262436, 0.07225106656551361, 0.1840500831604004, 0.805035412311554, 0.05771952122449875, 0.1336662620306015, 0.28110700845718384, 0.10757116228342056, 0.19960573315620422, 0.038709819316864014, 0.15892092883586884, 0.21356233954429626, 0.0001316660491283983, 0.0001316660491283983, 0.9984778165817261, 0.9769893884658813, 0.09385540336370468, 0.0012189013650640845, 0.24012356996536255, 0.11823342740535736, 0.04388044774532318, 0.03534813970327377, 0.4668392241001129, 0.02302246168255806, 0.9669434428215027, 0.9876154661178589, 0.9291457533836365, 0.06831954419612885, 0.9864168763160706, 0.23739714920520782, 0.18181607127189636, 0.004710260778665543, 0.07112494111061096, 0.05605210363864899, 0.09373418986797333, 0.12152472883462906, 0.23362894356250763, 0.9250873327255249, 0.006379912607371807, 0.06379912793636322, 0.9952822327613831, 0.9984985589981079, 0.9813852310180664, 0.9951749444007874, 0.8668527603149414, 0.12554419040679932, 0.01992190256714821, 0.941309928894043, 0.03486333042383194, 0.9945827126502991, 0.9952796697616577, 0.9990894794464111, 0.9547653198242188, 0.02386913262307644, 0.9426624178886414, 0.052678193897008896, 0.9962708353996277, 0.9876676797866821, 0.9993032217025757, 0.8557849526405334, 0.14319908618927002, 0.9778639674186707, 0.20042085647583008, 0.20979727804660797, 0.5063263773918152, 0.08204362541437149, 0.05084894970059395, 0.0033899301197379827, 0.04067916050553322, 0.8881616592407227, 0.013559720478951931, 0.09847404807806015, 0.05621839314699173, 0.023883629590272903, 0.1300739198923111, 0.0014697618316859007, 0.0264557134360075, 0.6628625988960266, 0.9806392788887024, 0.3600279986858368, 0.06785143166780472, 0.2111702561378479, 0.05469655990600586, 0.07754448801279068, 0.07685212790966034, 0.04361877590417862, 0.04015696793794632, 0.06369725614786148, 0.004154169000685215, 0.9933998584747314, 0.990427553653717, 0.9538201689720154, 0.04491673782467842, 0.9946218132972717, 0.8760145902633667, 0.12133985757827759, 0.9894480109214783, 0.005126673262566328, 0.9833046793937683, 0.9821094274520874, 0.9927808046340942, 0.10062038898468018, 0.030063409358263016, 0.03803941607475281, 0.7997480630874634, 0.031290486454963684, 0.9984297156333923, 0.17883288860321045, 0.8086356520652771, 0.9944671988487244, 0.07791836559772491, 0.9186164736747742, 0.9940058588981628, 0.9983935356140137, 0.9687607884407043, 0.03081437759101391, 0.8835325241088867, 0.09004984050989151, 0.02479633316397667, 0.4679863750934601, 0.2157009094953537, 0.026222463697195053, 0.03045189380645752, 0.0289715938270092, 0.06555616110563278, 0.06322997063398361, 0.02157009020447731, 0.08014769107103348, 0.6544508337974548, 0.2159687727689743, 0.08507861196994781, 0.04253930598497391, 0.8447305560112, 0.152982696890831, 0.2200934886932373, 0.2060636281967163, 0.09689375013113022, 0.02893659472465515, 0.22140879929065704, 0.14249080419540405, 0.08374074846506119, 0.05595361441373825, 0.0004440763150341809, 0.13411104679107666, 0.1718575358390808, 0.09725271165370941, 0.04529578238725662, 0.17807459831237793, 0.208271786570549, 0.10835462063550949, 0.0012360131368041039, 0.9727423191070557, 0.024720262736082077, 0.12181171029806137, 0.08709537237882614, 0.06456020474433899, 0.07856855541467667, 0.005481526721268892, 0.051160916686058044, 0.0377616286277771, 0.07856855541467667, 0.06943267583847046, 0.2521502375602722, 0.06943267583847046, 0.05542432889342308, 0.02862575091421604, 0.3143710494041443, 0.04022255912423134, 0.07621116191148758, 0.5260687470436096, 0.04233953729271889, 0.9876248836517334, 0.9985717535018921, 0.9927737712860107, 0.013753567822277546, 0.976503312587738, 0.17966552078723907, 0.16316929459571838, 0.13663186132907867, 0.040523361414670944, 0.17858968675136566, 0.05056454986333847, 0.000717227638233453, 0.10901860147714615, 0.06813662499189377, 0.042675044387578964, 0.030482174828648567, 0.9955999851226807, 0.9302035570144653, 0.04044363275170326, 0.04283048212528229, 0.15133437514305115, 0.8052130341529846, 0.9380494952201843, 0.05926674231886864, 0.7402454018592834, 0.14755721390247345, 0.0836157575249672, 0.027052156627178192, 0.03348110616207123, 0.9441672563552856, 0.013392442837357521, 0.07172434031963348, 0.6400017738342285, 0.06068982556462288, 0.22620752453804016, 0.9080691337585449, 0.06985147297382355, 0.9916114807128906, 0.8498629927635193, 0.13707467913627625, 0.9103270769119263, 0.08517680317163467, 0.30175772309303284, 0.6145170331001282, 0.08172605186700821, 0.9939920902252197, 0.9921445846557617, 0.045596878975629807, 0.702191948890686, 0.25078284740448, 0.9758594036102295, 0.054998837411403656, 0.6368286609649658, 0.3068356215953827, 0.9937330484390259, 0.0368935689330101, 0.9223392605781555, 0.9323469996452332, 0.06500022858381271, 0.06530943512916565, 0.09581323713064194, 0.14743506908416748, 0.034805625677108765, 0.08603637665510178, 0.015251903794705868, 0.20492301881313324, 0.07469522207975388, 0.030503807589411736, 0.11536696553230286, 0.10715439915657043, 0.022682318463921547, 0.009657645598053932, 0.09454327076673508, 0.09810134768486023, 0.1458812803030014, 0.019315291196107864, 0.07573627680540085, 0.03710569068789482, 0.11843323707580566, 0.15909700095653534, 0.0594707652926445, 0.10013453662395477, 0.08183584362268448, 0.8040840029716492, 0.18741807341575623, 0.9973174929618835, 0.26297327876091003, 0.10469777137041092, 0.07938352227210999, 0.012534240260720253, 0.210870161652565, 0.07078159600496292, 0.16491128504276276, 0.08061236888170242, 0.013025779277086258, 0.9940030574798584, 0.8568944334983826, 0.14127178490161896, 0.9921092391014099, 0.02536962181329727, 0.21775591373443604, 0.042987413704395294, 0.06342405080795288, 0.04087327793240547, 0.0465109720826149, 0.02255077473819256, 0.11768685281276703, 0.2565150558948517, 0.059195782989263535, 0.10641146451234818, 0.08986148238182068, 0.1311977654695511, 0.7782004475593567, 0.017831740900874138, 0.962913990020752, 0.08119948953390121, 0.9134942293167114, 0.9993175268173218, 0.9984926581382751, 0.24077090620994568, 0.7589211463928223, 0.9950832724571228, 0.9907031655311584, 0.3752271831035614, 0.6203756332397461, 0.9690914750099182, 0.023257529363036156, 0.9762753248214722, 0.8155953288078308, 0.17634493112564087, 0.99140465259552, 0.019083283841609955, 0.10904733836650848, 0.8369383215904236, 0.029988018795847893, 0.04354530945420265, 0.21250110864639282, 0.11495961993932724, 0.019508298486471176, 0.330595999956131, 0.047028932720422745, 0.17557469010353088, 0.05643472075462341, 0.998791515827179, 0.007608531974256039, 0.9872070550918579, 0.0019021329935640097, 0.9860875010490417, 0.9899776577949524, 0.9145016670227051, 0.08404610306024551, 0.9989973902702332, 0.05090184509754181, 0.05372972413897514, 0.04241820424795151, 0.45387476682662964, 0.13998007774353027, 0.03534850478172302, 0.03252062201499939, 0.05090184509754181, 0.025450922548770905, 0.11452914774417877, 0.9849005341529846, 0.9938689470291138, 0.9885082840919495, 0.6091108918190002, 0.03392025828361511, 0.12017462402582169, 0.1901957243680954, 0.001453725271858275, 0.030285945162177086, 0.01453725341707468, 0.8016757369041443, 0.18862958252429962, 0.05297057703137398, 0.2991964519023895, 0.062284085899591446, 0.05820942670106888, 0.08614995330572128, 0.012806073762476444, 0.01979120448231697, 0.14144890010356903, 0.08382157236337662, 0.08207529038190842, 0.05238848552107811, 0.026776336133480072, 0.02153748832643032, 0.9461259841918945, 0.04799189791083336, 0.996621310710907, 0.1816110461950302, 0.5168008208274841, 0.11260843276977539, 0.18879881501197815, 0.001827217754907906, 0.6614528298377991, 0.009136089123785496, 0.3252447545528412, 0.7974940538406372, 0.20072980225086212, 0.984284520149231, 0.5661567449569702, 0.004659726284444332, 0.426364928483963, 0.9982993602752686, 0.0074217007495462894, 0.9870861768722534, 0.9905802607536316, 0.9984613060951233, 0.1748872697353363, 0.0518733449280262, 0.7588329315185547, 0.0029641911387443542, 0.01037466898560524, 0.584260880947113, 0.09387016296386719, 0.05017198622226715, 0.18126653134822845, 0.03560592606663704, 0.014566060155630112, 0.0388428270816803, 0.004890034906566143, 0.12958592176437378, 0.0024450174532830715, 0.03178522735834122, 0.7970756888389587, 0.02200515754520893, 0.012225087732076645, 0.39573678374290466, 0.6032835245132446, 0.9771078824996948, 0.25207626819610596, 0.6021209955215454, 0.11448004841804504, 0.025317702442407608, 0.005503848195075989, 0.9792231917381287, 0.4968353807926178, 0.5014075040817261, 0.982036828994751, 0.9885503053665161, 0.9969959259033203, 0.10873536020517349, 0.05061401054263115, 0.6705750823020935, 0.08185423910617828, 0.06901910156011581, 0.0191316120326519, 0.07169480621814728, 0.9141087532043457, 0.9331395626068115, 0.06048126891255379, 0.004320090636610985, 0.9261112213134766, 0.057881951332092285, 0.9946004152297974, 0.03158589452505112, 0.27865779399871826, 0.1488046646118164, 0.005615270230919123, 0.17828483879566193, 0.2962055206298828, 0.050537433475255966, 0.007019088137894869, 0.002105726394802332, 0.985706090927124, 0.9945223927497864, 0.9883992075920105, 0.9771078824996948, 0.2864278256893158, 0.16072477400302887, 0.25984880328178406, 0.22232551872730255, 0.057535719126462936, 0.012820458970963955, 0.8466355800628662, 0.003974814899265766, 0.14706814289093018, 0.9928913116455078, 0.05275401845574379, 0.05913087725639343, 0.04173944517970085, 0.14666776359081268, 0.020869722589850426, 0.4144958555698395, 0.00811600312590599, 0.11710232496261597, 0.13913147151470184, 0.9974156022071838, 0.22601720690727234, 0.7724324464797974, 0.9849981069564819, 0.007242633029818535, 0.9910145401954651, 0.14205262064933777, 0.3002142906188965, 0.05418501794338226, 0.5023097395896912, 0.9690914750099182, 0.04250969737768173, 0.13469940423965454, 0.055313821882009506, 0.7487852573394775, 0.017925774678587914, 0.5666980743408203, 0.051947321742773056, 0.0015741613460704684, 0.16056445240974426, 0.21723425388336182, 0.09679127484560013, 0.3871650993824005, 0.008888994343578815, 0.11456926167011261, 0.3921034336090088, 0.3478708565235138, 0.6506253480911255, 0.9925375580787659, 0.9713537693023682, 0.2888004779815674, 0.0391593873500824, 0.5417048335075378, 0.0342644639313221, 0.026922078803181648, 0.0685289278626442, 0.08451133966445923, 0.003380453446879983, 0.8214501738548279, 0.08789178729057312, 0.20206378400325775, 0.7614355087280273, 0.03449869528412819, 0.9947779774665833, 0.9824590086936951, 0.05392202362418175, 0.05100731924176216, 0.0641234889626503, 0.13699108362197876, 0.6922422051429749, 0.09058745950460434, 0.05817786976695061, 0.015939142554998398, 0.6718348860740662, 0.16337621212005615, 0.0019675653893500566, 0.07673504948616028, 0.18495115637779236, 0.03738374263048172, 0.051156699657440186, 0.6453614830970764, 0.9794667363166809, 0.020051946863532066, 0.0025064933579415083, 0.0985887423157692, 0.08020778745412827, 0.006683982443064451, 0.0492943711578846, 0.6408268213272095, 0.10109523683786392, 0.9878988862037659, 0.9903441071510315, 0.9771460294723511, 0.19736410677433014, 0.5723559260368347, 0.08791673928499222, 0.09509361535310745, 0.028707506135106087, 0.0017942191334441304, 0.016147973015904427, 0.026165243238210678, 0.056244973093271255, 0.0492401048541069, 0.8271925449371338, 0.04120510816574097, 0.9962069392204285, 0.9923264384269714, 0.0034100564662367105, 0.9926254153251648, 0.8198441863059998, 0.1787126362323761, 0.18354712426662445, 0.014146213419735432, 0.1824861466884613, 0.1121087372303009, 0.16232779622077942, 0.04349960759282112, 0.06790182739496231, 0.1520717889070511, 0.0813407301902771, 0.03284692019224167, 0.9525607228279114, 0.9973537921905518, 0.9948776364326477, 0.23608216643333435, 0.06069156527519226, 0.17191487550735474, 0.06577147543430328, 0.19784915447235107, 0.027538463473320007, 0.14517849683761597, 0.05748319998383522, 0.03716355934739113, 0.899509608745575, 0.09439298510551453, 0.9765134453773499, 0.9879708290100098, 0.9953219294548035, 0.9899662137031555, 0.08198387175798416, 0.742539644241333, 0.08666866272687912, 0.05855990946292877, 0.01873917132616043, 0.014054377563297749, 0.9954058527946472, 0.9880849719047546, 0.05476910248398781, 0.4577715992927551, 0.03923756629228592, 0.17275428771972656, 0.045232195407152176, 0.0555865503847599, 0.17411670088768005, 0.00027248309925198555, 0.07538821548223495, 0.6012669801712036, 0.06067832186818123, 0.044129688292741776, 0.1379052698612213, 0.033097267150878906, 0.045968424528837204, 0.9936367869377136, 0.9941402077674866, 0.9892924427986145, 0.9658337235450745, 0.0319523960351944, 0.9876154661178589, 0.3134916126728058, 0.14967046678066254, 0.5366366505622864, 0.9887023568153381, 0.004713019821792841, 0.6955070495605469, 0.15687623620033264, 0.07136858254671097, 0.07002200931310654, 0.9964080452919006, 0.9989165663719177, 0.9852982759475708, 0.9924960136413574, 0.04718640819191933, 0.06890713423490524, 0.11609353870153427, 0.32356393337249756, 0.11834051460027695, 0.04419044405221939, 0.011234859004616737, 0.03520255908370018, 0.04269246384501457, 0.021720727905631065, 0.01273284014314413, 0.08388694375753403, 0.07489906251430511, 0.8545071482658386, 0.14344243705272675, 0.03405901417136192, 0.9195933938026428, 0.04379016160964966, 0.0006273209000937641, 0.1104084774851799, 0.8882864117622375, 0.9983483552932739, 0.06006190553307533, 0.13805273175239563, 0.06633702665567398, 0.04571876302361488, 0.05199388787150383, 0.2823805809020996, 0.025100497528910637, 0.3307887017726898, 0.9302675127983093, 0.06916255503892899, 0.9976050853729248, 0.9936599135398865, 0.9945758581161499, 0.9967159032821655, 0.009593910537660122, 0.9785788059234619, 0.05120225250720978, 0.7680338025093079, 0.1772385686635971, 0.003938634879887104, 0.9979308843612671, 0.021699098870158195, 0.24519981443881989, 0.14538396894931793, 0.588045597076416, 0.9944208264350891, 0.9934235215187073, 0.9700741767883301, 0.9804571866989136, 0.9510816931724548, 0.043035369366407394, 0.9826552867889404, 0.03652755916118622, 0.20654675364494324, 0.26167014241218567, 0.07836821675300598, 0.25170809030532837, 0.09696406871080399, 0.06840615719556808, 0.4373355805873871, 0.5278322100639343, 0.03476157784461975, 0.9928019642829895, 0.8803118467330933, 0.11411449313163757, 0.15365923941135406, 0.13213834166526794, 0.0895269513130188, 0.004304180387407541, 0.026470709592103958, 0.018507976084947586, 0.05638476088643074, 0.28493672609329224, 0.06262582540512085, 0.01097565982490778, 0.11233910918235779, 0.024749036878347397, 0.023457782343029976, 0.9958625435829163, 0.24954789876937866, 0.7500073313713074, 0.9114532470703125, 0.035055894404649734, 0.2691529393196106, 0.05034962669014931, 0.16161608695983887, 0.02113441191613674, 0.20947931706905365, 0.23807293176651, 0.05034962669014931, 0.9927522540092468, 0.002663453109562397, 0.07990359514951706, 0.3515758216381073, 0.5646520853042603, 0.013979324139654636, 0.09785526990890503, 0.8806974291801453, 0.11090415716171265, 0.8798395991325378, 0.9504726529121399, 0.020222822204232216, 0.0008901827968657017, 0.029376033693552017, 0.2608235776424408, 0.37654733657836914, 0.10237102210521698, 0.15845254063606262, 0.07032444328069687, 0.8460583090782166, 0.1515328288078308, 0.9833046793937683, 0.0011864911066368222, 0.15661682188510895, 0.6976567506790161, 0.14356541633605957, 0.9890784621238708, 0.01184140145778656, 0.9828363060951233, 0.0348416268825531, 0.012783617712557316, 0.18473580479621887, 0.0639180913567543, 0.17445878684520721, 0.3855138123035431, 0.13159607350826263, 0.012282299809157848, 0.998680591583252, 0.02970072068274021, 0.04708651080727577, 0.3382984697818756, 0.22818847000598907, 0.3556842505931854, 0.9837185144424438, 0.9979537725448608, 0.9985435605049133, 0.9940058588981628, 0.18894895911216736, 0.0024018934927880764, 0.03762966766953468, 0.029623353853821754, 0.009607573971152306, 0.6661251783370972, 0.06565175950527191, 0.9951330423355103, 0.9298590421676636, 0.04226632043719292, 0.9984268546104431, 0.9827713370323181, 0.9984701871871948, 0.02856033481657505, 0.8425298929214478, 0.1142413392663002, 0.0736670196056366, 0.22100107371807098, 0.6910668015480042, 0.010523860342800617, 0.08472215384244919, 0.9145350456237793, 0.995154857635498, 0.13001446425914764, 0.23432840406894684, 0.5109871029853821, 0.12396728247404099, 0.43027907609939575, 0.5689731240272522, 0.7502194046974182, 0.2491735816001892, 0.8591994643211365, 0.009970232844352722, 0.13019950687885284, 0.9925668835639954, 0.16442574560642242, 0.41414183378219604, 0.1679428666830063, 0.06946328282356262, 0.042645178735256195, 0.02374061569571495, 0.11738415062427521, 0.9839028120040894, 0.04829882085323334, 0.9478643536567688, 0.04949929192662239, 0.11095814406871796, 0.3019789159297943, 0.01661050133407116, 0.09434764087200165, 0.06810305267572403, 0.35779017210006714, 0.20805932581424713, 0.06362979114055634, 0.4052106738090515, 0.0016159947263076901, 0.17836542427539825, 0.020199934020638466, 0.000605998036917299, 0.00040399868157692254, 0.053731825202703476, 0.06807377934455872, 0.9981438517570496, 0.9981110095977783, 0.04544641077518463, 0.9214651584625244, 0.03134235367178917, 0.016875986009836197, 0.047463711351156235, 0.08191884309053421, 0.18352633714675903, 0.017930734902620316, 0.16665035486221313, 0.025665560737252235, 0.06926185637712479, 0.1423911303281784, 0.09211475402116776, 0.054846953600645065, 0.039728883653879166, 0.061878614127635956, 0.778915286064148, 0.020421000197529793, 0.10064635425806046, 0.08605992794036865, 0.011669143103063107, 0.180416077375412, 0.15526717901229858, 0.0377233624458313, 0.09567519277334213, 0.47181540727615356, 0.031709492206573486, 0.027335770428180695, 0.9956419467926025, 0.04754961282014847, 0.03444299101829529, 0.4767153561115265, 0.06705714762210846, 0.007924935780465603, 0.017983507364988327, 0.11948364973068237, 0.08077338337898254, 0.045415978878736496, 0.01737389713525772, 0.07772532850503922, 0.007924935780465603, 0.5186021327972412, 0.20216692984104156, 0.2786387801170349, 0.08336306363344193, 0.5323536396026611, 0.04972533881664276, 0.03802525997161865, 0.030712708830833435, 0.04972533881664276, 0.21498896181583405, 0.9482921361923218, 0.01051928661763668, 0.9867090582847595, 0.9995348453521729, 0.9923650026321411, 0.2806927561759949, 0.07157106697559357, 0.07939914613962173, 0.020129362121224403, 0.47304001450538635, 0.053678300231695175, 0.006709787528961897, 0.010064681060612202, 0.005591489374637604, 0.08622395247220993, 0.2685944437980652, 0.09067200869321823, 0.16526256501674652, 0.06158853694796562, 0.26585718989372253, 0.06158853694796562, 0.991549015045166, 0.3576388955116272, 0.04887473210692406, 0.26958680152893066, 0.07680314779281616, 0.029479997232556343, 0.13925419747829437, 0.0779668316245079, 0.9840496182441711, 0.0066043599508702755, 0.0029423623345792294, 0.8106207847595215, 0.08091495931148529, 0.10445386171340942, 0.991809606552124, 0.8207995295524597, 0.17047375440597534, 0.10074113309383392, 0.8982750773429871, 0.061625633388757706, 0.9346554279327393, 0.9889128804206848, 0.9932898283004761, 0.9949184656143188, 0.9240902662277222, 0.02369462139904499, 0.03554193302989006, 0.18270856142044067, 0.08120380342006683, 0.7353455424308777, 0.9932461380958557, 0.8042324185371399, 0.19412507116794586, 0.380621075630188, 0.032206397503614426, 0.23813216388225555, 0.12394583970308304, 0.0009759514941833913, 0.057581137865781784, 0.16493579745292664, 0.47978338599205017, 0.20143336057662964, 0.08101467043161392, 0.0031523217912763357, 0.20206382870674133, 0.03246891498565674, 0.36352887749671936, 0.2815556824207306, 0.00010389507951913401, 0.0603630430996418, 0.1456609070301056, 0.00031168523128144443, 0.1483621746301651, 0.20481397211551666, 0.48713597655296326, 0.1654575616121292, 0.107627734541893, 0.03453725948929787, 0.09791800379753113, 0.13268180191516876, 0.13036420941352844, 0.6384949088096619, 0.0021190999541431665, 0.05085839703679085, 0.20060811936855316, 0.673873782157898, 0.0600411631166935, 0.011301865801215172, 0.9976605176925659, 0.31915387511253357, 0.6390842795372009, 0.040379565209150314, 0.11231181770563126, 0.6872413754463196, 0.20055681467056274, 0.9900100231170654, 0.9992238879203796, 0.0806260034441948, 0.07774650305509567, 0.13677625358104706, 0.4564007818698883, 0.0007198750390671194, 0.005759000312536955, 0.1864476352930069, 0.054710503667593, 0.061780039221048355, 0.8626323938369751, 0.006864449009299278, 0.009152598679065704, 0.0572037398815155, 0.9920938014984131, 0.027654852718114853, 0.045121077448129654, 0.08696723729372025, 0.15173780918121338, 0.13063278794288635, 0.1029779389500618, 0.05021539330482483, 0.030565891414880753, 0.0884227529168129, 0.032385289669036865, 0.04766823351383209, 0.04002676159143448, 0.13427159190177917, 0.03165752813220024, 0.029337115585803986, 0.9387876987457275, 0.03484107553958893, 0.07665036618709564, 0.7386307716369629, 0.1393643021583557, 0.003484107553958893, 0.9864128232002258, 0.9871211051940918, 0.9853518605232239, 0.8755139112472534, 0.12392210960388184, 0.00041865577804856, 0.9549802541732788, 0.9643396139144897, 0.9703639149665833, 0.021563641726970673, 0.050432030111551285, 0.16963501274585724, 0.09398696571588516, 0.685417115688324, 0.006838897708803415, 0.10258346050977707, 0.8890566825866699, 0.10289783030748367, 0.07078322023153305, 0.47123274207115173, 0.23659947514533997, 0.06029681861400604, 0.05701981857419014, 0.9968377351760864, 0.76202392578125, 0.04130597040057182, 0.19513510167598724, 0.9983028173446655, 0.9948899745941162, 0.9841228723526001, 0.9906086921691895, 0.3947451412677765, 0.0925978422164917, 0.050313279032707214, 0.024889014661312103, 0.04067882150411606, 0.12819181382656097, 0.027297629043459892, 0.005887723993510008, 0.21757817268371582, 0.017127925530076027, 0.0002676238364074379, 0.23279784619808197, 0.19500242173671722, 0.41848835349082947, 0.15282493829727173, 0.9947701692581177, 0.912808358669281, 0.004326106049120426, 0.07786990702152252, 0.9963321685791016, 0.039204519242048264, 0.16040629148483276, 0.0309649258852005, 0.03734396770596504, 0.6846836805343628, 0.00013289667549543083, 0.047045424580574036, 0.9275161027908325, 0.06416778266429901, 0.9987598061561584, 0.9884518384933472, 0.04656863212585449, 0.07880845665931702, 0.874057412147522, 0.9905187487602234, 0.03560526669025421, 0.08901315927505493, 0.05077046900987625, 0.010549708269536495, 0.03758333623409271, 0.7299079298973083, 0.002637427067384124, 0.043517544865608215, 0.10829364508390427, 0.7681629657745361, 0.12128888815641403, 0.5430983901023865, 0.25170662999153137, 0.2050398588180542, 0.09979646652936935, 0.8981682062149048, 0.9835551381111145, 0.9918816089630127, 0.9970283508300781, 0.996243417263031, 0.9887046217918396, 0.9957806468009949, 0.1741134375333786, 0.8249660730361938, 0.01130832452327013, 0.9725159406661987, 0.9984009265899658, 0.9861440062522888, 0.10051850974559784, 0.8858193755149841, 0.006282406859099865, 0.9924343824386597, 0.9949544072151184, 0.06997109204530716, 0.8863005042076111, 0.023323697969317436, 0.9926766157150269, 0.9027339816093445, 0.037613917142152786, 0.995546281337738, 0.047968100756406784, 0.9492635130882263, 0.15237833559513092, 0.8447059988975525, 0.007032491732388735, 0.984548807144165, 0.10187503695487976, 0.6067224740982056, 0.28751400113105774, 0.98427814245224, 0.9798918962478638, 0.0010758860735222697, 0.043035443872213364, 0.04949076101183891, 0.0010758860735222697, 0.1291063278913498, 0.2840339243412018, 0.4475686252117157, 0.043035443872213364, 0.10800810158252716, 0.89106684923172, 0.08799327909946442, 0.9119003415107727, 0.9955148100852966, 0.09874226152896881, 0.03987668454647064, 0.020887786522507668, 0.6038469076156616, 0.17279896140098572, 0.0018988896626979113, 0.06076446920633316, 0.9894302487373352, 0.8781247138977051, 0.07341235876083374, 0.04517683386802673, 0.9979389905929565, 0.13303950428962708, 0.8618646264076233, 0.20099301636219025, 0.3803103268146515, 0.27652961015701294, 0.0006568399257957935, 0.1405637413263321, 0.8877050876617432, 0.10894563049077988], \"Term\": [\"2d\", \"2d\", \"2d\", \"2d\", \"2d\", \"3d\", \"3d\", \"3d\", \"acceleration\", \"acceleration\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"acoustic\", \"acoustic\", \"acoustic_speech\", \"acoustic_speech\", \"acquisition\", \"acquisition\", \"acquisition\", \"action\", \"action\", \"action_pair\", \"action_potential\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"adapt\", \"adapt\", \"adapt\", \"adapt\", \"adapt\", \"adaptation\", \"adaptation\", \"adaptation\", \"adaptive\", \"adaptive\", \"adaptive\", \"adaptive\", \"adaptive\", \"adaptive\", \"adaptive\", \"adapts\", \"adapts\", \"adapts\", \"adelson\", \"afosr_grant\", \"afosr_grant\", \"agent\", \"aggregate\", \"ai\", \"ai\", \"ai\", \"ai\", \"ai\", \"ai\", \"ai\", \"ai\", \"ai\", \"aij\", \"aij\", \"albus\", \"altered\", \"altered\", \"altered\", \"altered\", \"amplifier\", \"amplifier\", \"amplitude\", \"amplitude\", \"amplitude\", \"analog\", \"analog\", \"analog_vlsi\", \"analysis_pca\", \"angle\", \"angle\", \"angle\", \"angle\", \"angle\", \"angle\", \"animal\", \"animal\", \"animal\", \"annealed\", \"annual_conference\", \"annual_conference\", \"ansatz\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"arbitrary\", \"arbitrary\", \"arbitrary\", \"arbitrary\", \"arbitrary\", \"arbitrary\", \"arbitrary\", \"arbitrary\", \"arc\", \"arc\", \"arc\", \"arc\", \"archi_tecture\", \"archi_tecture\", \"archi_tecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"arm\", \"arm_movement\", \"array\", \"array\", \"array\", \"array\", \"array\", \"artifact\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"artificial\", \"assignment\", \"assignment\", \"assignment\", \"assignment\", \"associative\", \"associative\", \"associative\", \"associative\", \"associative_memory\", \"associative_memory\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"asymptotic\", \"asymptotic\", \"asymptotic\", \"attentional\", \"attractor\", \"audio\", \"audio\", \"auditory\", \"auditory_nerve\", \"automatic_speech\", \"automaton\", \"autonomous\", \"autonomous\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"average\", \"axon\", \"back_prop\", \"back_propagation\", \"back_propagation\", \"back_propagation\", \"backgammon\", \"backprop\", \"backprop\", \"backpropagation\", \"backpropagation\", \"backpropagation\", \"bagging\", \"band\", \"band\", \"band\", \"band\", \"band\", \"band_pas\", \"bandpass\", \"bandpass\", \"bandpass\", \"bar\", \"bar\", \"bar\", \"bar\", \"barto\", \"barto\", \"barto\", \"barto_sutton\", \"basin\", \"basin_attraction\", \"basin_attraction\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis_function\", \"basis_function\", \"bayesian\", \"becker\", \"becker\", \"becker\", \"behav\", \"behav\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"belief\", \"bell_sejnowski\", \"bellman\", \"bellman_equation\", \"bertsekas\", \"bias\", \"bias\", \"bias\", \"bias\", \"bias\", \"bias\", \"bias_variance\", \"bifurcation\", \"binding\", \"binding\", \"binding\", \"binocular\", \"biological\", \"biological\", \"biological\", \"biological\", \"biological\", \"biological\", \"biological\", \"bipolar\", \"bird\", \"bird\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"blind_deconvolution\", \"blind_separation\", \"blind_source\", \"block\", \"block\", \"block\", \"block\", \"block\", \"block\", \"boolean\", \"boolean\", \"boolean\", \"boolean_function\", \"boost\", \"boosting\", \"bootstrap\", \"bound\", \"bound\", \"bounded\", \"bounded\", \"bounded\", \"bp\", \"bp\", \"branch\", \"branch\", \"branching\", \"branching\", \"break_symmetry\", \"breaking\", \"breaking\", \"breaking\", \"breaking\", \"breast_cancer\", \"buhmann\", \"buhmann\", \"burges\", \"burges\", \"burst\", \"burst\", \"bus\", \"c_\", \"calcium\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"capacitor\", \"capacitor\", \"capacitor\", \"capacity\", \"capacity\", \"capacity\", \"capacity\", \"capacity\", \"cardinality\", \"cascade_correlation\", \"categorization\", \"category\", \"category\", \"category\", \"category\", \"category\", \"cell\", \"cell\", \"cell\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center_surround\", \"cerebellar\", \"cerebellum\", \"cerebellum\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"channel\", \"channel\", \"channel\", \"chaos\", \"character\", \"charge\", \"chip\", \"circuit\", \"circuit\", \"clamping\", \"class\", \"class\", \"class\", \"class\", \"class_label\", \"class_label\", \"classi_fication\", \"classifi_cation\", \"classifica_tion\", \"classification\", \"classification\", \"classification\", \"classified\", \"classified\", \"classifier\", \"classifier\", \"classifier\", \"classify\", \"classify\", \"classify\", \"classifying\", \"classifying\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster_center\", \"clustering\", \"clutter\", \"cmac\", \"cmos\", \"cn\", \"cn\", \"co_co\", \"co_co\", \"cochlea\", \"cochlear\", \"code\", \"code\", \"code\", \"code\", \"code\", \"coding\", \"coding\", \"coding\", \"coding\", \"coding\", \"coding\", \"coding\", \"coefficient\", \"coefficient\", \"coefficient\", \"coefficient\", \"coefficient\", \"coefficient\", \"coefficient\", \"cognition\", \"cognition\", \"cognition_vol\", \"collective_computational\", \"collective_computational\", \"color\", \"color\", \"color\", \"column\", \"column\", \"column\", \"column\", \"column\", \"column\", \"combinatorial_optimization\", \"command\", \"command\", \"committee\", \"committee\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"comparison\", \"competition\", \"competition\", \"competition\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component_analysis\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer_vision\", \"con_vergence\", \"concept\", \"concept\", \"concept\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"conditional\", \"conditional\", \"conditional_density\", \"conditional_distribution\", \"conditional_probability\", \"conditional_probability\", \"conductance\", \"conductance\", \"cone\", \"cone\", \"confidence_interval\", \"confidence_interval\", \"confidence_interval\", \"connection\", \"connection\", \"connection\", \"connection\", \"connection\", \"connection\", \"connectionist\", \"connectionist\", \"connectionist\", \"connectionist\", \"connectionist\", \"consonant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"content\", \"content\", \"content\", \"content_addressable\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context_dependent\", \"context_dependent\", \"continually_running\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous_speech\", \"contour\", \"contour\", \"contralateral\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"control\", \"control\", \"control\", \"control\", \"controlled\", \"controlled\", \"controlled\", \"controlled\", \"controlled\", \"controller\", \"controller\", \"conver_gence\", \"converge\", \"converge\", \"converge\", \"converge\", \"converge\", \"convergence\", \"convergence\", \"convergent\", \"convergent\", \"converges\", \"converges\", \"converges\", \"converges\", \"converter\", \"coordinate\", \"coordinate\", \"coordinate\", \"corollary\", \"correction\", \"correction\", \"correction\", \"correction\", \"correction\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correspondence\", \"correspondence\", \"correspondence\", \"cortes\", \"cortex\", \"cortex\", \"cortical\", \"cortical\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"cost_function\", \"covariance\", \"covariance\", \"critic\", \"critical\", \"critical\", \"critical\", \"critical\", \"critical\", \"cross_validation\", \"cross_validation\", \"cross_validation\", \"crossover\", \"crossover\", \"cue\", \"cue\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"curve\", \"curve\", \"curve\", \"curve\", \"curve\", \"curve\", \"curve\", \"curve\", \"database\", \"database\", \"database\", \"database\", \"datasets\", \"datasets\", \"david_rumelhart\", \"db\", \"db\", \"db\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision_boundary\", \"decoder\", \"decoder\", \"decoder\", \"decoder\", \"decoding\", \"decoding\", \"decoding\", \"decoding\", \"decomposition\", \"decomposition\", \"decomposition\", \"decomposition\", \"decrease\", \"decrease\", \"decrease\", \"decrease\", \"decrease\", \"decrease\", \"decrease\", \"decrease\", \"decrease\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"defined\", \"definition\", \"definition\", \"definition\", \"definition\", \"deformation\", \"delay\", \"delay\", \"delay\", \"delay\", \"demon\", \"dempster_laird\", \"dendrite\", \"dendritic\", \"dendritic_tree\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density_estimation\", \"department_physic\", \"department_physic\", \"derivative\", \"derivative\", \"derivative\", \"derivative\", \"derivative\", \"derivative\", \"descending\", \"descent\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"desired\", \"desired\", \"desired\", \"desired\", \"desired\", \"desired\", \"desired\", \"desired\", \"desired\", \"desired\", \"desired_trajectory\", \"detection\", \"detection\", \"detection\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"developmental\", \"device\", \"device\", \"diagnosis\", \"dichotomy\", \"dictionary\", \"differential_equation\", \"differential_equation\", \"digital\", \"dilemma\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension_reduction\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensionality\", \"dimensionality\", \"dimensionality\", \"direc\", \"direction\", \"direction\", \"direction\", \"direction\", \"direction\", \"direction\", \"direction\", \"disagree\", \"disagree\", \"discontinuity\", \"discontinuity\", \"discontinuity\", \"discontinuity\", \"discount_factor\", \"discounted\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance_measure\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"document\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain_specific\", \"domain_specific\", \"dot_product\", \"dot_product\", \"dot_product\", \"drain\", \"dt\", \"dt\", \"dt\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic_programming\", \"dynamic_programming\", \"dynamical_system\", \"dynamical_system\", \"ear\", \"early_vision\", \"early_vision\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"eeg\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvectors\", \"eigenvectors\", \"eighth\", \"eighth\", \"electron\", \"electronic\", \"electronic\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"element\", \"em\", \"em_algorithm\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"ensemble\", \"ensemble\", \"ensemble\", \"entropy\", \"entropy\", \"entropy\", \"entropy\", \"entropy\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"epoch\", \"epoch\", \"epoch\", \"epsp\", \"eq\", \"eq\", \"eq\", \"eq\", \"eq_eq\", \"equation\", \"equation\", \"equation\", \"equation\", \"equation\", \"equation\", \"equation\", \"equation\", \"equilibrium_point\", \"error_correcting\", \"error_rate\", \"error_rate\", \"error_rate\", \"esti\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimated\", \"estimated\", \"estimated\", \"estimated\", \"estimated\", \"estimated\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimator\", \"estimator\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"et_al\", \"euclidean\", \"euclidean\", \"euler\", \"euler\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"excitatory\", \"excitatory\", \"excitatory_inhibitory\", \"exemplar\", \"exemplar\", \"exemplar\", \"exists\", \"exists\", \"exists\", \"exists\", \"exists\", \"exists\", \"exists\", \"expectation_maximization\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"expert\", \"expert\", \"exploration\", \"exploration\", \"exploration\", \"exponent\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"expression\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye_movement\", \"fabricated\", \"face\", \"face\", \"face_recognition\", \"facial\", \"factor_analysis\", \"fahlman\", \"failure\", \"failure\", \"failure\", \"false_negative\", \"family\", \"family\", \"family\", \"family\", \"faster_convergence\", \"faster_convergence\", \"fault\", \"fault\", \"fault_tolerance\", \"fault_tolerance\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feed_forward\", \"feed_forward\", \"feed_forward\", \"feed_forward\", \"feedback\", \"feedback\", \"feedback\", \"feedforward_net\", \"feedforward_net\", \"fiber\", \"fiber\", \"fiber\", \"fiber\", \"field\", \"field\", \"field\", \"field\", \"field\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"figure_5a\", \"filter\", \"filter\", \"filtering\", \"filtering\", \"financial\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"finger\", \"finger\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite\", \"finite_state\", \"finite_state\", \"fire\", \"fire\", \"firing\", \"firing_rate\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"fitness\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed\", \"fixed_point\", \"fixed_point\", \"flight\", \"floating_gate\", \"fold_cross\", \"fold_cross\", \"following_theorem\", \"font\", \"font\", \"food\", \"food\", \"forecasting\", \"formant\", \"formulation\", \"formulation\", \"formulation\", \"formulation\", \"formulation\", \"formulation\", \"formulation\", \"formulation\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"forward\", \"fractal\", \"frame\", \"frame\", \"frame\", \"frank\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency_band\", \"frey\", \"front\", \"front\", \"front\", \"fulfill\", \"fully_recurrent\", \"fully_recurrent\", \"function_approximator\", \"functionals\", \"fusion\", \"ga\", \"ga\", \"ga\", \"ga\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"game\", \"gardner\", \"gas\", \"gas\", \"gating_network\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian_process\", \"gaussians\", \"gaussians\", \"gaussians\", \"gaze\", \"gene\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization_error\", \"generative_model\", \"genetic\", \"genetic_algorithm\", \"gibbs_sampling\", \"giles\", \"giles\", \"girosi\", \"global\", \"global\", \"global\", \"global\", \"global\", \"global\", \"global\", \"global\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"gra_dient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient_descent\", \"gradient_descent\", \"gradient_descent\", \"grammar\", \"grammar\", \"grammar\", \"grammatical\", \"graph\", \"graph\", \"graphical_model\", \"grating\", \"gray_scale\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"grouping\", \"grouping\", \"grouping\", \"grouping\", \"grown\", \"gun_kyoto\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand_printed\", \"handwritten\", \"hardware\", \"hardware\", \"hardware_implementation\", \"haussler\", \"head\", \"head\", \"head\", \"hearing\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden_layer\", \"hidden_layer\", \"hidden_layer\", \"hidden_markov\", \"hidden_markov\", \"hidden_unit\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"high_frequency\", \"high_frequency\", \"high_speed\", \"hill_climbing\", \"hill_climbing\", \"hillsdale\", \"hillsdale\", \"hillsdale_nj\", \"hillsdale_nj\", \"hint\", \"hinton\", \"hinton\", \"hippocampal\", \"hippocampus\", \"hmm\", \"hmms\", \"hmms\", \"hold\", \"hold\", \"hold\", \"hold\", \"hold\", \"hold\", \"hold\", \"holland\", \"holland\", \"hopfield\", \"hopfield\", \"hopfield\", \"hopfield\", \"hopfield\", \"host\", \"hubel_wiesel\", \"human\", \"human\", \"human\", \"human\", \"hybrid\", \"hybrid\", \"hybrid\", \"hyperparameters\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hz\", \"hz\", \"ic_uci\", \"ica\", \"illumination\", \"illumination\", \"im_age\", \"image\", \"imagery\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"independent_component\", \"inequality\", \"inequality\", \"inf\", \"inference\", \"inference\", \"inference\", \"infomax\", \"information_processing\", \"information_processing\", \"information_processing\", \"information_processing\", \"information_processing\", \"information_processing\", \"information_processing\", \"information_processing\", \"information_processing\", \"information_processing\", \"information_processing\", \"information_processing\", \"information_processing\", \"information_processing\", \"information_processing\", \"inhibition\", \"inhibition\", \"inhibitory\", \"inhibitory\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"initial\", \"insect\", \"insect\", \"insect\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"instance\", \"institute_advanced\", \"institute_advanced\", \"integer\", \"integer\", \"integer\", \"integer\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"internal_representation\", \"internal_representation\", \"internal_representation\", \"interneurons\", \"invariance\", \"invariance\", \"invariance\", \"inverse_temperature\", \"inverse_temperature\", \"item\", \"iterates\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"jacob_jordan\", \"jacob_jordan\", \"jacob_jordan\", \"jacobian\", \"jacobian\", \"ji\", \"ji\", \"ji\", \"ji\", \"jij\", \"john_denker\", \"joint\", \"joint\", \"joint_angle\", \"joint_distribution\", \"jp_abstract\", \"judgment\", \"junction\", \"junction\", \"junction\", \"kalman\", \"kawato\", \"kearns\", \"kearns\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kinematics\", \"kinematics\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"kohonen\", \"lagrange\", \"lagrange_multiplier\", \"lagrangian\", \"language\", \"language\", \"language\", \"large_vocabulary\", \"latent_variable\", \"lateral_connection\", \"lateral_connection\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layered\", \"layered\", \"lazzaro\", \"le_cun\", \"le_cun\", \"le_cun\", \"leaf\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"least_square\", \"least_square\", \"leg\", \"lemma\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"lesion\", \"lesion\", \"letter\", \"letter\", \"letter\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level_hierarchy\", \"level_hierarchy\", \"lgn\", \"light\", \"light\", \"light\", \"light\", \"lighting\", \"likelihood\", \"likelihood\", \"limit\", \"limit\", \"limit\", \"limit\", \"limit\", \"limit\", \"limit\", \"limit_cycle\", \"limit_cycle\", \"limit_cycle\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line_segment\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear_discriminant\", \"linear_discriminant\", \"linear_regression\", \"linguistic\", \"linguistic\", \"linguistics\", \"link\", \"link\", \"link\", \"link\", \"link\", \"linsker\", \"linsker\", \"littman\", \"load\", \"load\", \"load\", \"load\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local_minimum\", \"local_minimum\", \"local_minimum\", \"locally_tuned\", \"location\", \"location\", \"location\", \"location\", \"location\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log_likelihood\", \"log_likelihood\", \"logp\", \"loop\", \"loop\", \"loop\", \"loop\", \"loop\", \"loop\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"low_frequency\", \"low_frequency\", \"low_pas\", \"low_pas\", \"low_power\", \"lower_bound\", \"lower_bound\", \"lower_bound\", \"maass\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"mackay\", \"mackay\", \"macroscopic\", \"macroscopic\", \"male\", \"male_female\", \"manifold\", \"manipulator\", \"manipulator\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mar\", \"mar\", \"margin\", \"marginal\", \"markov_decision\", \"mass\", \"mass\", \"mass\", \"match\", \"match\", \"match\", \"match\", \"match\", \"match\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matching\", \"matlab\", \"matrix\", \"matrix\", \"maximum_entropy\", \"maximum_likelihood\", \"maximum_likelihood\", \"mcclelland_ed\", \"mcclelland_editor\", \"mdp\", \"mdps\", \"mead\", \"mean_field\", \"mean_field\", \"mean_squared\", \"mean_squared\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measured\", \"measured\", \"measured\", \"measured\", \"measured\", \"measured\", \"measured\", \"measured\", \"measured\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mem_ory\", \"member\", \"member\", \"member\", \"member\", \"member\", \"member\", \"membrane\", \"membrane_potential\", \"memory\", \"memory\", \"memory\", \"men\", \"men\", \"men\", \"merge\", \"merge\", \"merging\", \"merging\", \"merging\", \"message\", \"microscopic\", \"microscopic\", \"minimization\", \"minimization\", \"minimization\", \"minimization\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"minimum\", \"misclassification\", \"misclassified\", \"missing\", \"missing\", \"missing\", \"missing\", \"mistake\", \"mistake\", \"mixing_matrix\", \"mixture\", \"mixture\", \"mixture\", \"mixture_density\", \"mixture_expert\", \"mixture_expert\", \"mixture_gaussians\", \"mlp\", \"mlp\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"module\", \"module\", \"module\", \"module\", \"module\", \"modulo\", \"modulo\", \"monkey\", \"monkey\", \"monkey\", \"monocular\", \"moody\", \"motion\", \"motor\", \"motor\", \"motor\", \"motor_command\", \"motor_control\", \"mouse\", \"mouth\", \"mouth\", \"move\", \"move\", \"move\", \"move\", \"movement\", \"moving\", \"moving\", \"moving\", \"moving\", \"moving_object\", \"mozer_smolensky\", \"mse\", \"mse\", \"mt\", \"mt\", \"multinomial\", \"murray\", \"muscle\", \"muscle\", \"muscle\", \"mutation\", \"mutual_information\", \"mutual_information\", \"mutual_information\", \"mutual_information\", \"mutual_information\", \"nat\", \"navigation\", \"navigation\", \"navigation\", \"neal\", \"nearest_neighbor\", \"nearest_neighbor\", \"nearest_neighbor\", \"nearest_neighbor\", \"nearest_neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"nerve\", \"net\", \"net\", \"net\", \"net_work\", \"net_work\", \"net_work\", \"net_work\", \"net_work\", \"net_work\", \"net_work\", \"nettalk\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural_net\", \"neural_net\", \"neural_net\", \"neural_net\", \"neural_net\", \"neural_net\", \"neurodynamics\", \"neuromorphic\", \"neuron\", \"neuron\", \"neuron\", \"neuronal\", \"neuronal\", \"newton\", \"newton\", \"newton_method\", \"node\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise_ratio\", \"noise_ratio\", \"noise_ratio\", \"noiseless\", \"noiseless\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonparametric\", \"nonparametric_regression\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"ob\", \"ob\", \"ob\", \"object\", \"object\", \"object_recognition\", \"object_recognition\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"obtained\", \"occlusion\", \"occlusion\", \"ocular_dominance\", \"ogy\", \"oja\", \"olfactory\", \"onset\", \"onset\", \"onset\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"opper\", \"opposite_direction\", \"optical\", \"optical\", \"optical\", \"optical\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal_brain\", \"optimal_policy\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization_problem\", \"optimization_problem\", \"organ\", \"organ\", \"orientation\", \"orientation\", \"orientation_preference\", \"orientation_selective\", \"orientation_selectivity\", \"orientation_tuning\", \"orthogonal\", \"orthogonal\", \"orthogonal\", \"orthonormal\", \"oscillation\", \"oscillation\", \"oscillator\", \"outlier\", \"outlier\", \"overlap\", \"overlap\", \"overlap\", \"overlap\", \"overlap\", \"overlap\", \"pac\", \"packet\", \"paradigm\", \"paradigm\", \"paradigm\", \"paradigm\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parent\", \"parent\", \"parietal\", \"parity\", \"parity\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"partially_observable\", \"partition\", \"partition\", \"partition\", \"partition\", \"partition\", \"partition\", \"path\", \"path\", \"path\", \"path\", \"path\", \"patient\", \"patient\", \"patient\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pca\", \"pdp\", \"peak\", \"peak\", \"peak\", \"peak\", \"peak\", \"peak\", \"peak\", \"pentland\", \"pentland\", \"periodicity\", \"perturbation\", \"perturbation\", \"perturbed\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase_transition\", \"phase_transition\", \"phase_transition\", \"phone\", \"phoneme\", \"phonemic\", \"phonetic\", \"phrase\", \"phrase\", \"phys\", \"phys\", \"phys\", \"phys_rev\", \"physiol\", \"pixel\", \"pixel_intensity\", \"pixel_intensity\", \"planning\", \"planning\", \"plant\", \"pointer\", \"policy\", \"polynomial\", \"polynomial\", \"polynomial_degree\", \"population\", \"population\", \"population\", \"population\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"positioning\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive_definite\", \"post_synaptic\", \"posterior\", \"posterior\", \"posterior_distribution\", \"posterior_probability\", \"posterior_probability\", \"postsynaptic\", \"postsynaptic\", \"pouget\", \"power_law\", \"power_spectrum\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"predictor\", \"pressure\", \"pressure\", \"presynaptic\", \"price\", \"price\", \"primary_visual\", \"principal_component\", \"prior\", \"prior\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability_distribution\", \"probability_distribution\", \"probability_distribution\", \"probability_distribution\", \"probe\", \"probe\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processor\", \"processor\", \"processor\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"projection\", \"projection\", \"projection\", \"projection\", \"projection\", \"projection_onto\", \"proof\", \"proof_theorem\", \"prop\", \"prop\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"proposition\", \"prospect\", \"prospect\", \"protein\", \"protein\", \"protein\", \"prototype\", \"prototype\", \"prove\", \"prove\", \"prove\", \"prove\", \"proximity\", \"proximity\", \"proximity\", \"pruning\", \"pruning\", \"pruning\", \"pruning\", \"psych\", \"psych\", \"psychological\", \"psychological_review\", \"psychological_review\", \"psychology\", \"psychology\", \"pulse\", \"pulse\", \"pulse\", \"pyramid\", \"pyramidal_cell\", \"quadratic\", \"quadratic\", \"quadratic\", \"quarter\", \"query\", \"query\", \"query\", \"quinlan\", \"quired\", \"quired\", \"radial_basis\", \"radial_basis\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"range\", \"ranking\", \"ranking\", \"rat\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rational\", \"rbf\", \"rbf\", \"rbfs\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"recall\", \"recall\", \"recall\", \"recalled\", \"recalled\", \"receiver\", \"receiver\", \"receptive_field\", \"receptor\", \"recognition\", \"recognition\", \"recognizer\", \"reconstruction_error\", \"recording\", \"recording\", \"recur_rent\", \"recurrent\", \"recurrent\", \"reflex\", \"reflex\", \"refractory_period\", \"regime\", \"regime\", \"regime\", \"regime\", \"region\", \"region\", \"region\", \"region\", \"region\", \"region\", \"region\", \"region\", \"regression\", \"regularization\", \"regularization\", \"regularization\", \"regularization_term\", \"regularizer\", \"reinforcement\", \"reinforcement\", \"reinforcement_learning\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"relevant\", \"rendered\", \"replica\", \"replication\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representational\", \"representational\", \"required\", \"required\", \"required\", \"required\", \"required\", \"required\", \"required\", \"required\", \"required\", \"required\", \"required\", \"required\", \"required\", \"resistive\", \"resistive\", \"resistor\", \"response\", \"response\", \"response\", \"response\", \"retina\", \"retina\", \"retina\", \"retina\", \"retinal\", \"retinal\", \"retinotopic\", \"retrieval\", \"retrieval\", \"retrieval\", \"reward\", \"rf\", \"rf\", \"ridge\", \"risk\", \"rl\", \"rl\", \"rl\", \"rl\", \"rl\", \"rn\", \"rn\", \"rn\", \"rn\", \"rn\", \"rn\", \"rn\", \"road\", \"road\", \"road\", \"road\", \"road\", \"road\", \"road\", \"robot\", \"robot\", \"robotic\", \"role\", \"role\", \"role\", \"role\", \"role\", \"root_node\", \"rotation\", \"rotation\", \"rotation_translation\", \"rotational\", \"routing\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule_extraction\", \"rule_extraction\", \"rumelhart\", \"rumelhart\", \"rumelhart\", \"rumelhart_et\", \"rumelhart_et\", \"rumelhart_hinton\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"saad\", \"saccade\", \"saccadic\", \"saccadic_eye\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample_size\", \"sample_size\", \"sample_size\", \"sanger\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scene\", \"schedule\", \"schedule\", \"scheduling\", \"scheduling\", \"schmidhuber\", \"score\", \"score\", \"score\", \"score\", \"se_quences\", \"search\", \"search\", \"search\", \"search\", \"search\", \"second_order\", \"second_order\", \"second_order\", \"second_order\", \"second_order\", \"segment\", \"segment\", \"segment\", \"segment\", \"segment\", \"segmentation\", \"segmentation\", \"segregation\", \"sejnowski_rosenberg\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selective\", \"selective\", \"selective\", \"selective\", \"self_organizing\", \"self_organizing\", \"self_organizing\", \"semantic\", \"sensed\", \"sensor\", \"sensor\", \"sensor\", \"sensor\", \"sensor\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"sequential\", \"shadow\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shavlik\", \"shepard\", \"sification\", \"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signature\", \"silicon\", \"silicon\", \"simd\", \"similarity\", \"similarity\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"sine_wave\", \"sine_wave\", \"singh\", \"singular_value\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"skill\", \"skill\", \"smc\", \"smola\", \"smolensky\", \"smoother\", \"smoothing\", \"smoothing\", \"smoothing\", \"smoothing\", \"smoothing\", \"smoothing\", \"snr\", \"sollich\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"soma\", \"song\", \"sontag\", \"sound\", \"sound\", \"sound_source\", \"source\", \"source\", \"source\", \"source_separation\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial_frequency\", \"speaker\", \"speaker_dependent\", \"speaker_independent\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"spectral\", \"spectral\", \"spectrum\", \"spectrum\", \"spectrum\", \"speech\", \"speech\", \"speech\", \"speech_recognition\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"spike\", \"spike\", \"spike_train\", \"spiking\", \"spiking_neuron\", \"spin\", \"spin_glass\", \"spin_glass\", \"spiral\", \"spiral\", \"spiral\", \"spiral\", \"spline\", \"split\", \"split\", \"split\", \"split\", \"spoken\", \"spring\", \"squashing_function\", \"stabilized\", \"stack\", \"stack\", \"stacked\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"state\", \"state\", \"state\", \"statistical_mechanic\", \"std\", \"std\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"stereo\", \"stimulus\", \"stimulus\", \"stochas_tic\", \"stochas_tic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic_gradient\", \"stored\", \"stored\", \"stored\", \"stored\", \"stored_memory\", \"stored_memory\", \"stored_memory\", \"storing\", \"storing\", \"story\", \"story\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"stress\", \"stress\", \"striate_cortex\", \"string\", \"string\", \"string\", \"string\", \"stripe\", \"stroke\", \"stroke\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"student\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subjective\", \"subspace\", \"support_vector\", \"suppression\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"surround\", \"suspect\", \"suspect\", \"sutton\", \"sutton_barto\", \"svm\", \"switched\", \"switched\", \"switched\", \"switching\", \"switching\", \"switching\", \"switching\", \"symbol\", \"symbol\", \"symbolic\", \"symmetric\", \"symmetric\", \"symmetric\", \"symmetric\", \"synapse\", \"synapse\", \"synapsis\", \"synapsis\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic_strength\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"tanaka\", \"tangent\", \"tangent\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"td\", \"tdnn\", \"teacher\", \"teacher\", \"teacher\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"temperature\", \"temperature\", \"temperature\", \"temperature\", \"temperature\", \"temporal\", \"temporal\", \"temporal\", \"temporal\", \"temporal\", \"temporal\", \"temporal\", \"temporal_difference\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test_set\", \"test_set\", \"test_set\", \"testing\", \"testing\", \"testing\", \"testing\", \"testing\", \"testing\", \"testing\", \"teukolsky\", \"texture\", \"texture\", \"theorem\", \"theorem_let\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theoretical\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"thermodynamic_limit\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold_circuit\", \"threshold_circuit\", \"time_series\", \"time_series\", \"time_series\", \"time_series\", \"time_warping\", \"tishby\", \"tishby\", \"token\", \"token\", \"tone\", \"tone\", \"topography\", \"torque\", \"tour\", \"touretzky_editor\", \"touretzky_editor\", \"touretzky_editor\", \"tracking\", \"tracking\", \"tracking\", \"trading\", \"traffic\", \"traffic\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"trained\", \"trained\", \"trained\", \"trained\", \"trained\", \"trained\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"training_set\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"transformation\", \"transformation\", \"transformation\", \"transformation\", \"transformation\", \"transformation\", \"transistor\", \"transition\", \"transition\", \"transition\", \"translation\", \"translation\", \"translation\", \"translation_rotation\", \"tree\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"tuned\", \"tuned\", \"tuned\", \"tuned\", \"tuned\", \"tuning_curve\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"uci_repository\", \"uci_repository\", \"ui\", \"ui\", \"ui\", \"ui\", \"ui\", \"undirected\", \"uniform_convergence\", \"union\", \"unit\", \"unit\", \"unit\", \"university_maryland\", \"university_wisconsin\", \"unlabeled\", \"unlabeled\", \"unsupervised\", \"unsupervised\", \"unsupervised\", \"unsupervised\", \"unsupervised_learning\", \"unsupervised_learning\", \"unsupervised_learning\", \"update\", \"update\", \"update\", \"update\", \"update\", \"update\", \"upper_bound\", \"user\", \"user\", \"user\", \"utterance\", \"v1\", \"valiant\", \"vapnik_chervonenkis\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variance\", \"variance\", \"variance\", \"variance\", \"variational\", \"vc\", \"vc\", \"vc\", \"vc_dimension\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector_quantization\", \"vector_quantization\", \"velocity\", \"venkatesh\", \"vertex\", \"vertex\", \"vertex\", \"via_em\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"vision\", \"vision\", \"vision\", \"visual\", \"visual\", \"visual\", \"visual_cortex\", \"visual_cortex\", \"visual_scene\", \"visualization\", \"viterbi\", \"vlsi\", \"vlsi_implementation\", \"vocabulary\", \"voltage\", \"voltage\", \"voting\", \"voting\", \"vowel\", \"vv\", \"w2\", \"w2\", \"w2\", \"wahba\", \"waibel\", \"walter\", \"walter\", \"walter\", \"warmuth\", \"wasted\", \"wasted\", \"watkins\", \"waveform\", \"waveform\", \"wavelet\", \"wavelet\", \"weigend\", \"weigend\", \"weight_decay\", \"weight_decay\", \"weight_decay\", \"white_noise\", \"williams_zipser\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"winner_take\", \"winner_take\", \"word\", \"word\", \"workspace\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"writer\", \"wt\", \"wt\", \"wt\", \"wta\", \"x0\", \"x0\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xor\", \"xor\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [10, 1, 14, 13, 9, 7, 3, 6, 17, 8, 5, 12, 18, 20, 19, 11, 15, 4, 16, 2]};\n","\n","function LDAvis_load_lib(url, callback){\n","  var s = document.createElement('script');\n","  s.src = url;\n","  s.async = true;\n","  s.onreadystatechange = s.onload = callback;\n","  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n","  document.getElementsByTagName(\"head\")[0].appendChild(s);\n","}\n","\n","if(typeof(LDAvis) !== \"undefined\"){\n","   // already loaded: just create the visualization\n","   !function(LDAvis){\n","       new LDAvis(\"#\" + \"ldavis_el1211396432034480483565259572\", ldavis_el1211396432034480483565259572_data);\n","   }(LDAvis);\n","}else if(typeof define === \"function\" && define.amd){\n","   // require.js is available: use it to load d3/LDAvis\n","   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n","   require([\"d3\"], function(d3){\n","      window.d3 = d3;\n","      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n","        new LDAvis(\"#\" + \"ldavis_el1211396432034480483565259572\", ldavis_el1211396432034480483565259572_data);\n","      });\n","    });\n","}else{\n","    // require.js not available: dynamically load d3 & LDAvis\n","    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n","         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n","                 new LDAvis(\"#\" + \"ldavis_el1211396432034480483565259572\", ldavis_el1211396432034480483565259572_data);\n","            })\n","         });\n","}\n","</script>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"markdown","metadata":{"id":"vmFbSkUQxbpd","colab_type":"text"},"source":["## Topic Models with Scikit-Learn\n","\n","### Text Representation with Feature Engineering"]},{"cell_type":"code","metadata":{"id":"SXTT7wrkwPN7","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","cv = CountVectorizer(min_df=20, max_df=0.6, ngram_range=(1,2), token_pattern=None, tokenizer=lambda doc: doc,\n","                     preprocessor=lambda doc: doc)\n","\n","cv_features = cv.fit_transform(norm_papers)\n","\n","cv_features.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZX35XYJxaFq","colab_type":"code","colab":{}},"source":["# validating vocabulary size\n","vocabulary = np.array(cv.get_feature_names())\n","print('Total Vocabulary Size:', len(vocabulary))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zD5ZqiLQydqk","colab_type":"text"},"source":["### Latent Dirichlet Allocation"]},{"cell_type":"code","metadata":{"id":"opxukOXyyjdB","colab_type":"code","colab":{}},"source":["TOTAL_TOPICS = 20"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZNBcojlsx-s5","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import LatentDirichletAllocation\n","\n","lda_model = LatentDirichletAllocation(n_components =TOTAL_TOPICS, max_iter=500, max_doc_update_iter=50, learning_method='online',\n","                                      batch_size=1740, learning_offset=50., random_state=42, n_jobs=16)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wW01uOC5Oat","colab_type":"code","colab":{}},"source":["# document_topics = lda_model.fit_transform(cv_features)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1v9S33X044C","colab_type":"code","colab":{}},"source":["# pickle.dump(document_topics, open(\"drive/My Drive/nipstxt/document_topics.pkl\", \"wb\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LkfIE1UV080e","colab_type":"code","colab":{}},"source":["document_topics = pickle.load(open(\"drive/My Drive/nipstxt/document_topics.pkl\", \"rb\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"biWjLjYvytQh","colab_type":"text"},"source":["We can then obtain the topic-term matrix and build a dataframe from it to showcase the topics and terms in an easy-to-interpret format"]},{"cell_type":"code","metadata":{"id":"Pqj9oxcYEikN","colab_type":"code","colab":{}},"source":["top_terms = 20"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yXzKuaHRx-hX","colab_type":"code","colab":{}},"source":["topic_terms = lda_model.components_\n","\n","topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n","topic_keyterms = vocabulary[topic_key_term_idxs]\n","topics = [', '.join(topic) for topic in topic_keyterms]\n","\n","pd.set_option('display.max_colwidth', -1)\n","topics_df = pd.DataFrame(topics, columns = ['Terms per Topic'], index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"29-W1NVCx-eE","colab_type":"code","colab":{}},"source":["topics_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9bWrwWqmVXhv","colab_type":"code","colab":{}},"source":["document_topics"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xLUO-JzGx-al","colab_type":"code","colab":{}},"source":["dt_df = pd.DataFrame(document_topics,\n","columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n","\n","pd.options.display.float_format = '{:,.5f}'.format\n","pd.set_option('display.max_colwidth', 200)\n","\n","max_contrib_topics = dt_df.max(axis=0)\n","dominant_topics = max_contrib_topics.index\n","contrib_perc = max_contrib_topics.values\n","document_numbers = [dt_df[dt_df[t] == max_contrib_topics.loc[t]].index[0] for t in dominant_topics]\n","documents = [papers[i] for i in document_numbers]\n","\n","results_df = pd.DataFrame( {'Dominant Topic': dominant_topics, 'Contribution %': contrib_perc, 'Paper Num': document_numbers,\n","                            'Topic': topics_df['Terms per Topic'], 'Paper Name': documents})\n","\n","results_df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OZDE0jJl1O6u","colab_type":"text"},"source":["* Viewing each topic and corresponding paper with its maximum\n","contribution"]},{"cell_type":"markdown","metadata":{"id":"7PYLDgdkOOVB","colab_type":"text"},"source":["### WordClouds"]},{"cell_type":"code","metadata":{"id":"QmvuQwBSx-Ue","colab_type":"code","colab":{}},"source":["from wordcloud import WordCloud"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7nSBmaqwO89d","colab_type":"code","colab":{}},"source":["# wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n","#                max_font_size=150, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QQ6QDGMMQqHS","colab_type":"code","colab":{}},"source":["wc = WordCloud(background_color=\"white\", colormap=\"Dark2\",\n","               max_font_size=150, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpFpQID8RL3p","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [20, 15]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROu7WLnnx-R-","colab_type":"code","colab":{}},"source":["# Create subplots for each comedian\n","i=0\n","for i in range(20):\n","\n","    wc.generate(text=results_df[\"Topic\"][i])\n","    \n","    plt.subplot(5, 4, i+1)\n","    plt.imshow(wc, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.title(results_df[\"Dominant Topic\"][i])\n","    # print(topic)\n","    # print(type(topic))\n","    # print()\n","    # print()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6kwVHkO6x-PT","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cwkzS39OoJmq","colab_type":"text"},"source":["Source: Dipanjan Sarkar - Text Analytics with Python_ A Practitioner's Guide to Natural Language Processing"]}]}